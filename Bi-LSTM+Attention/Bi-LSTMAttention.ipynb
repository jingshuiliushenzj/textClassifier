{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import json\n",
    "\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from math import sqrt\n",
    "\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置参数\n",
    "\n",
    "class TrainingConfig(object):\n",
    "    epoches = 4\n",
    "    evaluateEvery = 100\n",
    "    checkpointEvery = 100\n",
    "    learningRate = 0.001\n",
    "    \n",
    "class ModelConfig(object):\n",
    "    embeddingSize = 200\n",
    "    \n",
    "    hiddenSizes = [256, 128]  # LSTM结构的神经元个数\n",
    "    \n",
    "    dropoutKeepProb = 0.5\n",
    "    l2RegLambda = 0.0\n",
    "    \n",
    "class Config(object):\n",
    "    sequenceLength = 200  # 取了所有序列长度的均值\n",
    "    batchSize = 128\n",
    "    \n",
    "    dataSource = \"../data/preProcess/labeledTrain.csv\"\n",
    "    \n",
    "    stopWordSource = \"../data/english\"\n",
    "    \n",
    "    numClasses = 2\n",
    "    \n",
    "    rate = 0.8  # 训练集的比例\n",
    "    \n",
    "    training = TrainingConfig()\n",
    "    \n",
    "    model = ModelConfig()\n",
    "\n",
    "    \n",
    "# 实例化配置参数对象\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据预处理的类，生成训练集和测试集\n",
    "\n",
    "class Dataset(object):\n",
    "    def __init__(self, config):\n",
    "        self._dataSource = config.dataSource\n",
    "        self._stopWordSource = config.stopWordSource  \n",
    "        \n",
    "        self._sequenceLength = config.sequenceLength  # 每条输入的序列处理为定长\n",
    "        self._embeddingSize = config.model.embeddingSize\n",
    "        self._batchSize = config.batchSize\n",
    "        self._rate = config.rate\n",
    "        \n",
    "        self._stopWordDict = {}\n",
    "        \n",
    "        self.trainReviews = []\n",
    "        self.trainLabels = []\n",
    "        \n",
    "        self.evalReviews = []\n",
    "        self.evalLabels = []\n",
    "        \n",
    "        self.wordEmbedding =None\n",
    "        \n",
    "        self._wordToIndex = {}\n",
    "        self._indexToWord = {}\n",
    "        \n",
    "    def _readData(self, filePath):\n",
    "        \"\"\"\n",
    "        从csv文件中读取数据集\n",
    "        \"\"\"\n",
    "        \n",
    "        df = pd.read_csv(filePath)\n",
    "        labels = df[\"sentiment\"].tolist()\n",
    "        review = df[\"review\"].tolist()\n",
    "        reviews = [line.strip().split() for line in review]\n",
    "\n",
    "        return reviews, labels\n",
    "\n",
    "    def _reviewProcess(self, review, sequenceLength, wordToIndex):\n",
    "        \"\"\"\n",
    "        将数据集中的每条评论用index表示\n",
    "        wordToIndex中“pad”对应的index为0\n",
    "        \"\"\"\n",
    "        \n",
    "        reviewVec = np.zeros((sequenceLength))\n",
    "        sequenceLen = sequenceLength\n",
    "        \n",
    "        # 判断当前的序列是否小于定义的固定序列长度\n",
    "        if len(review) < sequenceLength:\n",
    "            sequenceLen = len(review)\n",
    "            \n",
    "        for i in range(sequenceLen):\n",
    "            if review[i] in wordToIndex:\n",
    "                reviewVec[i] = wordToIndex[review[i]]\n",
    "            else:\n",
    "                reviewVec[i] = wordToIndex[\"UNK\"]\n",
    "\n",
    "        return reviewVec\n",
    "\n",
    "    def _genTrainEvalData(self, x, y, rate):\n",
    "        \"\"\"\n",
    "        生成训练集和验证集\n",
    "        \"\"\"\n",
    "        \n",
    "        reviews = []\n",
    "        labels = []\n",
    "        \n",
    "        # 遍历所有的文本，将文本中的词转换成index表示\n",
    "        for i in range(len(x)):\n",
    "            reviewVec = self._reviewProcess(x[i], self._sequenceLength, self._wordToIndex)\n",
    "            reviews.append(reviewVec)\n",
    "            \n",
    "            labels.append([y[i]])\n",
    "            \n",
    "        trainIndex = int(len(x) * rate)\n",
    "        \n",
    "        trainReviews = np.asarray(reviews[:trainIndex], dtype=\"int64\")\n",
    "        trainLabels = np.array(labels[:trainIndex], dtype=\"float32\")\n",
    "        \n",
    "        evalReviews = np.asarray(reviews[trainIndex:], dtype=\"int64\")\n",
    "        evalLabels = np.array(labels[trainIndex:], dtype=\"float32\")\n",
    "\n",
    "        return trainReviews, trainLabels, evalReviews, evalLabels\n",
    "        \n",
    "    def _genVocabulary(self, reviews):\n",
    "        \"\"\"\n",
    "        生成词向量和词汇-索引映射字典，可以用全数据集\n",
    "        \"\"\"\n",
    "        \n",
    "        allWords = [word for review in reviews for word in review]\n",
    "        \n",
    "        # 去掉停用词\n",
    "        subWords = [word for word in allWords if word not in self.stopWordDict]\n",
    "        \n",
    "        wordCount = Counter(subWords)  # 统计词频\n",
    "        sortWordCount = sorted(wordCount.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # 去除低频词\n",
    "        words = [item[0] for item in sortWordCount if item[1] >= 5]\n",
    "        \n",
    "        vocab, wordEmbedding = self._getWordEmbedding(words)\n",
    "        self.wordEmbedding = wordEmbedding\n",
    "        \n",
    "        self._wordToIndex = dict(zip(vocab, list(range(len(vocab)))))\n",
    "        self._indexToWord = dict(zip(list(range(len(vocab))), vocab))\n",
    "        \n",
    "        # 将词汇-索引映射表保存为json数据，之后做inference时直接加载来处理数据\n",
    "        with open(\"../data/wordJson/wordToIndex.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self._wordToIndex, f)\n",
    "        \n",
    "        with open(\"../data/wordJson/indexToWord.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self._indexToWord, f)\n",
    "            \n",
    "    def _getWordEmbedding(self, words):\n",
    "        \"\"\"\n",
    "        按照我们的数据集中的单词取出预训练好的word2vec中的词向量\n",
    "        \"\"\"\n",
    "        \n",
    "        wordVec = gensim.models.KeyedVectors.load_word2vec_format(\"../word2vec/word2Vec.bin\", binary=True)\n",
    "        vocab = []\n",
    "        wordEmbedding = []\n",
    "        \n",
    "        # 添加 \"pad\" 和 \"UNK\", \n",
    "        vocab.append(\"pad\")\n",
    "        vocab.append(\"UNK\")\n",
    "        wordEmbedding.append(np.zeros(self._embeddingSize))\n",
    "        wordEmbedding.append(np.zeros(self._embeddingSize))\n",
    "        \n",
    "        for word in words:\n",
    "            try:\n",
    "                vector = wordVec.wv[word]\n",
    "                vocab.append(word)\n",
    "                wordEmbedding.append(vector)\n",
    "            except:\n",
    "                print(word + \"不存在于词向量中\")\n",
    "                \n",
    "        return vocab, np.array(wordEmbedding)\n",
    "    \n",
    "    def _readStopWord(self, stopWordPath):\n",
    "        \"\"\"\n",
    "        读取停用词\n",
    "        \"\"\"\n",
    "        \n",
    "        with open(stopWordPath, \"r\") as f:\n",
    "            stopWords = f.read()\n",
    "            stopWordList = stopWords.splitlines()\n",
    "            # 将停用词用列表的形式生成，之后查找停用词时会比较快\n",
    "            self.stopWordDict = dict(zip(stopWordList, list(range(len(stopWordList)))))\n",
    "            \n",
    "    def dataGen(self):\n",
    "        \"\"\"\n",
    "        初始化训练集和验证集\n",
    "        \"\"\"\n",
    "        \n",
    "        # 初始化停用词\n",
    "        self._readStopWord(self._stopWordSource)\n",
    "        \n",
    "        # 初始化数据集\n",
    "        reviews, labels = self._readData(self._dataSource)\n",
    "        \n",
    "        # 初始化词汇-索引映射表和词向量矩阵\n",
    "        self._genVocabulary(reviews)\n",
    "        \n",
    "        # 初始化训练集和测试集\n",
    "        trainReviews, trainLabels, evalReviews, evalLabels = self._genTrainEvalData(reviews, labels, self._rate)\n",
    "        self.trainReviews = trainReviews\n",
    "        self.trainLabels = trainLabels\n",
    "        \n",
    "        self.evalReviews = evalReviews\n",
    "        self.evalLabels = evalLabels\n",
    "        \n",
    "        \n",
    "data = Dataset(config)\n",
    "data.dataGen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape: (20000, 200)\n",
      "train label shape: (20000, 1)\n",
      "eval data shape: (5000, 200)\n"
     ]
    }
   ],
   "source": [
    "print(\"train data shape: {}\".format(data.trainReviews.shape))\n",
    "print(\"train label shape: {}\".format(data.trainLabels.shape))\n",
    "print(\"eval data shape: {}\".format(data.evalReviews.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出batch数据集\n",
    "\n",
    "def nextBatch(x, y, batchSize):\n",
    "        \"\"\"\n",
    "        生成batch数据集，用生成器的方式输出\n",
    "        \"\"\"\n",
    "    \n",
    "        perm = np.arange(len(x))\n",
    "        np.random.shuffle(perm)\n",
    "        x = x[perm]\n",
    "        y = y[perm]\n",
    "        \n",
    "        numBatches = len(x) // batchSize\n",
    "\n",
    "        for i in range(numBatches):\n",
    "            start = i * batchSize\n",
    "            end = start + batchSize\n",
    "            batchX = np.array(x[start: end], dtype=\"int64\")\n",
    "            batchY = np.array(y[start: end], dtype=\"float32\")\n",
    "            \n",
    "            yield batchX, batchY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建模型\n",
    "class BiLSTMAttention(object):\n",
    "    \"\"\"\n",
    "    Text CNN 用于文本分类\n",
    "    \"\"\"\n",
    "    def __init__(self, config, wordEmbedding):\n",
    "\n",
    "        # 定义模型的输入\n",
    "        self.inputX = tf.placeholder(tf.int32, [None, config.sequenceLength], name=\"inputX\")\n",
    "        self.inputY = tf.placeholder(tf.float32, [None, 1], name=\"inputY\")\n",
    "        \n",
    "        self.dropoutKeepProb = tf.placeholder(tf.float32, name=\"dropoutKeepProb\")\n",
    "        \n",
    "        # 定义l2损失\n",
    "        l2Loss = tf.constant(0.0)\n",
    "        \n",
    "        # 词嵌入层\n",
    "        with tf.name_scope(\"embedding\"):\n",
    "\n",
    "            # 利用预训练的词向量初始化词嵌入矩阵\n",
    "            self.W = tf.Variable(tf.cast(wordEmbedding, dtype=tf.float32, name=\"word2vec\") ,name=\"W\")\n",
    "            # 利用词嵌入矩阵将输入的数据中的词转换成词向量，维度[batch_size, sequence_length, embedding_size]\n",
    "            self.embeddedWords = tf.nn.embedding_lookup(self.W, self.inputX)\n",
    "            \n",
    "        # 定义两层双向LSTM的模型结构\n",
    "        with tf.name_scope(\"Bi-LSTM\"):\n",
    "            for idx, hiddenSize in enumerate(config.model.hiddenSizes):\n",
    "                with tf.name_scope(\"Bi-LSTM\" + str(idx)):\n",
    "                    # 定义前向LSTM结构\n",
    "                    lstmFwCell = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(num_units=hiddenSize, state_is_tuple=True),\n",
    "                                                                 output_keep_prob=self.dropoutKeepProb)\n",
    "                    # 定义反向LSTM结构\n",
    "                    lstmBwCell = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(num_units=hiddenSize, state_is_tuple=True),\n",
    "                                                                 output_keep_prob=self.dropoutKeepProb)\n",
    "\n",
    "\n",
    "                    # 采用动态rnn，可以动态的输入序列的长度，若没有输入，则取序列的全长\n",
    "                    # outputs是一个元祖(output_fw, output_bw)，其中两个元素的维度都是[batch_size, max_time, hidden_size],fw和bw的hidden_size一样\n",
    "                    # self.current_state 是最终的状态，二元组(state_fw, state_bw)，state_fw=[batch_size, s]，s是一个元祖(h, c)\n",
    "                    outputs_, self.current_state = tf.nn.bidirectional_dynamic_rnn(lstmFwCell, lstmBwCell, \n",
    "                                                                                  self.embeddedWords, dtype=tf.float32,\n",
    "                                                                                  scope=\"bi-lstm\" + str(idx))\n",
    "        \n",
    "                    # 对outputs中的fw和bw的结果拼接 [batch_size, time_step, hidden_size * 2], 传入到下一层Bi-LSTM中\n",
    "                    self.embeddedWords = tf.concat(outputs_, 2)\n",
    "                \n",
    "        # 将最后一层Bi-LSTM输出的结果分割成前向和后向的输出\n",
    "        outputs = tf.split(self.embeddedWords, 2, -1)\n",
    "        \n",
    "        # 在Bi-LSTM+Attention的论文中，将前向和后向的输出相加\n",
    "        with tf.name_scope(\"Attention\"):\n",
    "            H = outputs[0] + outputs[1]\n",
    "\n",
    "            # 得到Attention的输出\n",
    "            output = self.attention(H)\n",
    "            outputSize = config.model.hiddenSizes[-1]\n",
    "        \n",
    "        # 全连接层的输出\n",
    "        with tf.name_scope(\"output\"):\n",
    "            outputW = tf.get_variable(\n",
    "                \"outputW\",\n",
    "                shape=[outputSize, 1],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "            outputB= tf.Variable(tf.constant(0.1, shape=[1]), name=\"outputB\")\n",
    "            l2Loss += tf.nn.l2_loss(outputW)\n",
    "            l2Loss += tf.nn.l2_loss(outputB)\n",
    "            self.predictions = tf.nn.xw_plus_b(output, outputW, outputB, name=\"predictions\")\n",
    "            self.binaryPreds = tf.cast(tf.greater_equal(self.predictions, 0.5), tf.float32, name=\"binaryPreds\")\n",
    "        \n",
    "        # 计算二元交叉熵损失\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            \n",
    "            losses = tf.nn.sigmoid_cross_entropy_with_logits(logits=self.predictions, labels=self.inputY)\n",
    "            self.loss = tf.reduce_mean(losses) + config.model.l2RegLambda * l2Loss\n",
    "    \n",
    "    def attention(self, H):\n",
    "        \"\"\"\n",
    "        利用Attention机制得到句子的向量表示\n",
    "        \"\"\"\n",
    "        # 获得最后一层LSTM的神经元数量\n",
    "        hiddenSize = config.model.hiddenSizes[-1]\n",
    "        \n",
    "        # 初始化一个权重向量，是可训练的参数\n",
    "        W = tf.Variable(tf.random_normal([hiddenSize], stddev=0.1))\n",
    "        \n",
    "        # 对Bi-LSTM的输出用激活函数做非线性转换\n",
    "        M = tf.tanh(H)\n",
    "        \n",
    "        # 对W和M做矩阵运算，W=[batch_size, time_step, hidden_size]，计算前做维度转换成[batch_size * time_step, hidden_size]\n",
    "        # newM = [batch_size, time_step, 1]，每一个时间步的输出由向量转换成一个数字\n",
    "        newM = tf.matmul(tf.reshape(M, [-1, hiddenSize]), tf.reshape(W, [-1, 1]))\n",
    "        \n",
    "        # 对newM做维度转换成[batch_size, time_step]\n",
    "        restoreM = tf.reshape(newM, [-1, config.sequenceLength])\n",
    "        \n",
    "        # 用softmax做归一化处理[batch_size, time_step]\n",
    "        self.alpha = tf.nn.softmax(restoreM)\n",
    "        \n",
    "        # 利用求得的alpha的值对H进行加权求和，用矩阵运算直接操作\n",
    "        r = tf.matmul(tf.transpose(H, [0, 2, 1]), tf.reshape(self.alpha, [-1, config.sequenceLength, 1]))\n",
    "        \n",
    "        # 将三维压缩成二维sequeezeR=[batch_size, hidden_size]\n",
    "        sequeezeR = tf.squeeze(r)\n",
    "        \n",
    "        sentenceRepren = tf.tanh(sequeezeR)\n",
    "        \n",
    "        # 对Attention的输出可以做dropout处理\n",
    "        output = tf.nn.dropout(sentenceRepren, self.dropoutKeepProb)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义性能指标函数\n",
    "\n",
    "def mean(item):\n",
    "    return sum(item) / len(item)\n",
    "\n",
    "\n",
    "def genMetrics(trueY, predY, binaryPredY):\n",
    "    \"\"\"\n",
    "    生成acc和auc值\n",
    "    \"\"\"\n",
    "    auc = roc_auc_score(trueY, predY)\n",
    "    accuracy = accuracy_score(trueY, binaryPredY)\n",
    "    precision = precision_score(trueY, binaryPredY)\n",
    "    recall = recall_score(trueY, binaryPredY)\n",
    "    \n",
    "    return round(accuracy, 4), round(auc, 4), round(precision, 4), round(recall, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name bi-lstm0/fw/lstm_cell/kernel:0/grad/hist is illegal; using bi-lstm0/fw/lstm_cell/kernel_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name bi-lstm0/fw/lstm_cell/kernel:0/grad/sparsity is illegal; using bi-lstm0/fw/lstm_cell/kernel_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name bi-lstm0/fw/lstm_cell/bias:0/grad/hist is illegal; using bi-lstm0/fw/lstm_cell/bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name bi-lstm0/fw/lstm_cell/bias:0/grad/sparsity is illegal; using bi-lstm0/fw/lstm_cell/bias_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name bi-lstm0/bw/lstm_cell/kernel:0/grad/hist is illegal; using bi-lstm0/bw/lstm_cell/kernel_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name bi-lstm0/bw/lstm_cell/kernel:0/grad/sparsity is illegal; using bi-lstm0/bw/lstm_cell/kernel_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name bi-lstm0/bw/lstm_cell/bias:0/grad/hist is illegal; using bi-lstm0/bw/lstm_cell/bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name bi-lstm0/bw/lstm_cell/bias:0/grad/sparsity is illegal; using bi-lstm0/bw/lstm_cell/bias_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name bi-lstm1/fw/lstm_cell/kernel:0/grad/hist is illegal; using bi-lstm1/fw/lstm_cell/kernel_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name bi-lstm1/fw/lstm_cell/kernel:0/grad/sparsity is illegal; using bi-lstm1/fw/lstm_cell/kernel_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name bi-lstm1/fw/lstm_cell/bias:0/grad/hist is illegal; using bi-lstm1/fw/lstm_cell/bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name bi-lstm1/fw/lstm_cell/bias:0/grad/sparsity is illegal; using bi-lstm1/fw/lstm_cell/bias_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name bi-lstm1/bw/lstm_cell/kernel:0/grad/hist is illegal; using bi-lstm1/bw/lstm_cell/kernel_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name bi-lstm1/bw/lstm_cell/kernel:0/grad/sparsity is illegal; using bi-lstm1/bw/lstm_cell/kernel_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name bi-lstm1/bw/lstm_cell/bias:0/grad/hist is illegal; using bi-lstm1/bw/lstm_cell/bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name bi-lstm1/bw/lstm_cell/bias:0/grad/sparsity is illegal; using bi-lstm1/bw/lstm_cell/bias_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name Attention/Variable:0/grad/hist is illegal; using Attention/Variable_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name Attention/Variable:0/grad/sparsity is illegal; using Attention/Variable_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name outputW:0/grad/hist is illegal; using outputW_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name outputW:0/grad/sparsity is illegal; using outputW_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/outputB:0/grad/hist is illegal; using output/outputB_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/outputB:0/grad/sparsity is illegal; using output/outputB_0/grad/sparsity instead.\n",
      "Writing to /data4T/share/jiangxinyang848/textClassifier/Bi-LSTM+Attention/summarys\n",
      "\n",
      "start training model\n",
      "2019-01-17T20:59:04.961261, step: 1, loss: 0.6945064067840576, acc: 0.4922, auc: 0.495, precision: 0.0, recall: 0.0\n",
      "2019-01-17T20:59:05.645589, step: 2, loss: 0.6974798440933228, acc: 0.4219, auc: 0.6111, precision: 0.0, recall: 0.0\n",
      "2019-01-17T20:59:06.393761, step: 3, loss: 0.706291139125824, acc: 0.4922, auc: 0.5543, precision: 0.4286, recall: 0.0952\n",
      "2019-01-17T20:59:07.156393, step: 4, loss: 0.6599031686782837, acc: 0.4766, auc: 0.7703, precision: 1.0, recall: 0.029\n",
      "2019-01-17T20:59:07.867091, step: 5, loss: 0.6531129479408264, acc: 0.5547, auc: 0.8002, precision: 0.6667, recall: 0.0345\n",
      "2019-01-17T20:59:08.545482, step: 6, loss: 0.6424497365951538, acc: 0.5469, auc: 0.7552, precision: 0.0, recall: 0.0\n",
      "2019-01-17T20:59:09.230033, step: 7, loss: 0.6743257641792297, acc: 0.4297, auc: 0.8356, precision: 0.0, recall: 0.0\n",
      "2019-01-17T20:59:10.000773, step: 8, loss: 0.6125870943069458, acc: 0.6875, auc: 0.786, precision: 0.8571, recall: 0.4\n",
      "2019-01-17T20:59:10.677236, step: 9, loss: 0.6443164944648743, acc: 0.7266, auc: 0.8418, precision: 0.64, recall: 0.8571\n",
      "2019-01-17T20:59:11.403999, step: 10, loss: 0.5339311957359314, acc: 0.6875, auc: 0.8453, precision: 0.9167, recall: 0.4714\n",
      "2019-01-17T20:59:12.144614, step: 11, loss: 0.6299803256988525, acc: 0.6016, auc: 0.7783, precision: 0.8636, recall: 0.2836\n",
      "2019-01-17T20:59:12.866854, step: 12, loss: 0.47851261496543884, acc: 0.7031, auc: 0.8705, precision: 0.9, recall: 0.4355\n",
      "2019-01-17T20:59:13.664927, step: 13, loss: 0.5158340334892273, acc: 0.8047, auc: 0.8539, precision: 0.7903, recall: 0.8033\n",
      "2019-01-17T20:59:14.362768, step: 14, loss: 0.6323463320732117, acc: 0.7031, auc: 0.7781, precision: 0.6951, recall: 0.8143\n",
      "2019-01-17T20:59:15.081060, step: 15, loss: 0.44934219121932983, acc: 0.7734, auc: 0.8827, precision: 0.8293, recall: 0.6071\n",
      "2019-01-17T20:59:15.841243, step: 16, loss: 0.6513361930847168, acc: 0.5469, auc: 0.8503, precision: 0.9167, recall: 0.2821\n",
      "2019-01-17T20:59:16.534945, step: 17, loss: 0.4906667470932007, acc: 0.6797, auc: 0.8651, precision: 1.0, recall: 0.2115\n",
      "2019-01-17T20:59:17.259880, step: 18, loss: 0.48431912064552307, acc: 0.7344, auc: 0.9019, precision: 0.9744, recall: 0.5352\n",
      "2019-01-17T20:59:17.984806, step: 19, loss: 0.49270159006118774, acc: 0.8281, auc: 0.8844, precision: 0.8438, recall: 0.8182\n",
      "2019-01-17T20:59:18.670550, step: 20, loss: 0.5248526334762573, acc: 0.8594, auc: 0.8883, precision: 0.8529, recall: 0.8788\n",
      "2019-01-17T20:59:19.340448, step: 21, loss: 0.4782412052154541, acc: 0.8281, auc: 0.8855, precision: 0.8485, recall: 0.8235\n",
      "2019-01-17T20:59:19.965764, step: 22, loss: 0.40565288066864014, acc: 0.8516, auc: 0.9336, precision: 0.8667, recall: 0.8254\n",
      "2019-01-17T20:59:20.605994, step: 23, loss: 0.37312573194503784, acc: 0.8359, auc: 0.9301, precision: 0.9535, recall: 0.6833\n",
      "2019-01-17T20:59:21.259733, step: 24, loss: 0.5656507015228271, acc: 0.7188, auc: 0.8827, precision: 0.8571, recall: 0.5455\n",
      "2019-01-17T20:59:21.952242, step: 25, loss: 0.43990716338157654, acc: 0.7891, auc: 0.8937, precision: 0.8824, recall: 0.6818\n",
      "2019-01-17T20:59:22.612889, step: 26, loss: 0.5444655418395996, acc: 0.8047, auc: 0.9054, precision: 0.716, recall: 0.9667\n",
      "2019-01-17T20:59:23.267557, step: 27, loss: 0.4949503540992737, acc: 0.7734, auc: 0.9103, precision: 0.6875, recall: 0.9322\n",
      "2019-01-17T20:59:23.958971, step: 28, loss: 0.4879785180091858, acc: 0.6797, auc: 0.8661, precision: 0.8298, recall: 0.5417\n",
      "2019-01-17T20:59:24.659984, step: 29, loss: 0.37451598048210144, acc: 0.7578, auc: 0.9424, precision: 0.9512, recall: 0.5735\n",
      "2019-01-17T20:59:25.383287, step: 30, loss: 0.52759850025177, acc: 0.6953, auc: 0.8444, precision: 0.9143, recall: 0.4706\n",
      "2019-01-17T20:59:26.109428, step: 31, loss: 0.46621543169021606, acc: 0.7188, auc: 0.8853, precision: 0.9268, recall: 0.5352\n",
      "2019-01-17T20:59:26.844918, step: 32, loss: 0.3958083689212799, acc: 0.8516, auc: 0.9377, precision: 0.9767, recall: 0.7\n",
      "2019-01-17T20:59:27.543614, step: 33, loss: 0.4452996551990509, acc: 0.8594, auc: 0.9182, precision: 0.875, recall: 0.7778\n",
      "2019-01-17T20:59:28.293275, step: 34, loss: 0.4684816002845764, acc: 0.8594, auc: 0.9091, precision: 0.8305, recall: 0.8596\n",
      "2019-01-17T20:59:29.022222, step: 35, loss: 0.3919455111026764, acc: 0.7891, auc: 0.9119, precision: 0.8696, recall: 0.6557\n",
      "2019-01-17T20:59:29.763925, step: 36, loss: 0.3538039028644562, acc: 0.8594, auc: 0.9316, precision: 0.9792, recall: 0.7344\n",
      "2019-01-17T20:59:30.506788, step: 37, loss: 0.45099571347236633, acc: 0.7344, auc: 0.8818, precision: 0.8571, recall: 0.6087\n",
      "2019-01-17T20:59:31.254470, step: 38, loss: 0.3064923882484436, acc: 0.8672, auc: 0.9527, precision: 0.9474, recall: 0.7941\n",
      "2019-01-17T20:59:31.944214, step: 39, loss: 0.30858537554740906, acc: 0.8906, auc: 0.9471, precision: 0.8852, recall: 0.8852\n",
      "2019-01-17T20:59:32.690297, step: 40, loss: 0.22436180710792542, acc: 0.9062, auc: 0.9743, precision: 0.9155, recall: 0.9155\n",
      "2019-01-17T20:59:33.437978, step: 41, loss: 0.2929781377315521, acc: 0.875, auc: 0.953, precision: 0.875, recall: 0.8448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-17T20:59:34.169483, step: 42, loss: 0.30529019236564636, acc: 0.875, auc: 0.9502, precision: 0.9153, recall: 0.8308\n",
      "2019-01-17T20:59:34.935471, step: 43, loss: 0.36477425694465637, acc: 0.8438, auc: 0.9325, precision: 0.8592, recall: 0.8592\n",
      "2019-01-17T20:59:35.616078, step: 44, loss: 0.3923361301422119, acc: 0.8672, auc: 0.9247, precision: 0.8462, recall: 0.8871\n",
      "2019-01-17T20:59:36.354007, step: 45, loss: 0.3005450367927551, acc: 0.8828, auc: 0.9435, precision: 0.8923, recall: 0.8788\n",
      "2019-01-17T20:59:37.065378, step: 46, loss: 0.39613187313079834, acc: 0.7891, auc: 0.9091, precision: 0.8833, recall: 0.726\n",
      "2019-01-17T20:59:37.811058, step: 47, loss: 0.28269505500793457, acc: 0.8672, auc: 0.9633, precision: 0.9623, recall: 0.7727\n",
      "2019-01-17T20:59:38.540161, step: 48, loss: 0.35594069957733154, acc: 0.875, auc: 0.9243, precision: 0.9, recall: 0.8438\n",
      "2019-01-17T20:59:39.220601, step: 49, loss: 0.2602846622467041, acc: 0.9219, auc: 0.9653, precision: 0.9429, recall: 0.9167\n",
      "2019-01-17T20:59:39.862383, step: 50, loss: 0.46295008063316345, acc: 0.8125, auc: 0.9014, precision: 0.7391, recall: 0.8947\n",
      "2019-01-17T20:59:40.557757, step: 51, loss: 0.273520827293396, acc: 0.8828, auc: 0.9631, precision: 0.9216, recall: 0.8103\n",
      "2019-01-17T20:59:41.298911, step: 52, loss: 0.3795483410358429, acc: 0.7656, auc: 0.9367, precision: 0.9286, recall: 0.5909\n",
      "2019-01-17T20:59:42.011995, step: 53, loss: 0.43457189202308655, acc: 0.7734, auc: 0.9125, precision: 0.9231, recall: 0.5806\n",
      "2019-01-17T20:59:42.801440, step: 54, loss: 0.32471245527267456, acc: 0.8594, auc: 0.9376, precision: 0.918, recall: 0.8116\n",
      "2019-01-17T20:59:43.497627, step: 55, loss: 0.32154473662376404, acc: 0.8906, auc: 0.9631, precision: 0.831, recall: 0.9672\n",
      "2019-01-17T20:59:44.202779, step: 56, loss: 0.4493868947029114, acc: 0.7969, auc: 0.9173, precision: 0.7333, recall: 0.9016\n",
      "2019-01-17T20:59:44.987521, step: 57, loss: 0.342436820268631, acc: 0.8672, auc: 0.9394, precision: 0.8594, recall: 0.873\n",
      "2019-01-17T20:59:45.724749, step: 58, loss: 0.42241907119750977, acc: 0.7578, auc: 0.8934, precision: 0.8627, recall: 0.6471\n",
      "2019-01-17T20:59:46.395521, step: 59, loss: 0.4492540955543518, acc: 0.7578, auc: 0.9235, precision: 0.9773, recall: 0.589\n",
      "2019-01-17T20:59:47.071675, step: 60, loss: 0.34709101915359497, acc: 0.8125, auc: 0.9398, precision: 0.9268, recall: 0.6441\n",
      "2019-01-17T20:59:47.722697, step: 61, loss: 0.39861059188842773, acc: 0.8125, auc: 0.9101, precision: 0.8596, recall: 0.7538\n",
      "2019-01-17T20:59:48.403319, step: 62, loss: 0.3520659804344177, acc: 0.8516, auc: 0.9377, precision: 0.8438, recall: 0.8571\n",
      "2019-01-17T20:59:49.077765, step: 63, loss: 0.2965238690376282, acc: 0.8672, auc: 0.957, precision: 0.9014, recall: 0.8649\n",
      "2019-01-17T20:59:49.773885, step: 64, loss: 0.5133882761001587, acc: 0.7969, auc: 0.8744, precision: 0.7353, recall: 0.8621\n",
      "2019-01-17T20:59:50.486743, step: 65, loss: 0.281802237033844, acc: 0.8828, auc: 0.9545, precision: 0.9483, recall: 0.8209\n",
      "2019-01-17T20:59:51.199429, step: 66, loss: 0.3131125569343567, acc: 0.8438, auc: 0.9382, precision: 0.925, recall: 0.6852\n",
      "2019-01-17T20:59:51.869936, step: 67, loss: 0.3615744709968567, acc: 0.8125, auc: 0.9183, precision: 0.8889, recall: 0.6154\n",
      "2019-01-17T20:59:52.571354, step: 68, loss: 0.3347630500793457, acc: 0.8125, auc: 0.9367, precision: 0.8958, recall: 0.6935\n",
      "2019-01-17T20:59:53.319887, step: 69, loss: 0.33686041831970215, acc: 0.8125, auc: 0.9274, precision: 0.8519, recall: 0.7419\n",
      "2019-01-17T20:59:54.003189, step: 70, loss: 0.2933482825756073, acc: 0.8672, auc: 0.9487, precision: 0.9153, recall: 0.8182\n",
      "2019-01-17T20:59:54.743085, step: 71, loss: 0.3004842698574066, acc: 0.8828, auc: 0.9558, precision: 0.8806, recall: 0.8939\n",
      "2019-01-17T20:59:55.406888, step: 72, loss: 0.3208749294281006, acc: 0.8672, auc: 0.9535, precision: 0.8333, recall: 0.9016\n",
      "2019-01-17T20:59:56.118542, step: 73, loss: 0.22456111013889313, acc: 0.9453, auc: 0.9696, precision: 0.9464, recall: 0.9298\n",
      "2019-01-17T20:59:56.854836, step: 74, loss: 0.4498991072177887, acc: 0.7969, auc: 0.9332, precision: 1.0, recall: 0.6486\n",
      "2019-01-17T20:59:57.547544, step: 75, loss: 0.385992169380188, acc: 0.8359, auc: 0.9099, precision: 0.8889, recall: 0.7619\n",
      "2019-01-17T20:59:58.297231, step: 76, loss: 0.20265290141105652, acc: 0.9531, auc: 0.9787, precision: 0.9825, recall: 0.918\n",
      "2019-01-17T20:59:59.023034, step: 77, loss: 0.3400110602378845, acc: 0.875, auc: 0.9358, precision: 0.859, recall: 0.9306\n",
      "2019-01-17T20:59:59.651088, step: 78, loss: 0.29405492544174194, acc: 0.8906, auc: 0.954, precision: 0.8644, recall: 0.8947\n",
      "2019-01-17T21:00:00.340306, step: 79, loss: 0.26114845275878906, acc: 0.9062, auc: 0.9658, precision: 0.9062, recall: 0.9062\n",
      "2019-01-17T21:00:00.996885, step: 80, loss: 0.24432966113090515, acc: 0.8672, auc: 0.9679, precision: 0.95, recall: 0.8028\n",
      "2019-01-17T21:00:01.733721, step: 81, loss: 0.3208407759666443, acc: 0.8281, auc: 0.9407, precision: 0.8478, recall: 0.7222\n",
      "2019-01-17T21:00:02.393121, step: 82, loss: 0.3280104100704193, acc: 0.8359, auc: 0.9391, precision: 0.8868, recall: 0.7581\n",
      "2019-01-17T21:00:03.134453, step: 83, loss: 0.33021581172943115, acc: 0.8281, auc: 0.9407, precision: 0.913, recall: 0.7\n",
      "2019-01-17T21:00:03.845657, step: 84, loss: 0.35983484983444214, acc: 0.8594, auc: 0.9345, precision: 0.8235, recall: 0.9032\n",
      "2019-01-17T21:00:04.552052, step: 85, loss: 0.29684221744537354, acc: 0.8828, auc: 0.949, precision: 0.8571, recall: 0.9\n",
      "2019-01-17T21:00:05.253546, step: 86, loss: 0.29698172211647034, acc: 0.8594, auc: 0.9578, precision: 0.8286, recall: 0.9062\n",
      "2019-01-17T21:00:05.977736, step: 87, loss: 0.2591724991798401, acc: 0.8984, auc: 0.9591, precision: 0.9825, recall: 0.8235\n",
      "2019-01-17T21:00:06.674572, step: 88, loss: 0.3851262927055359, acc: 0.8203, auc: 0.9189, precision: 0.8679, recall: 0.7419\n",
      "2019-01-17T21:00:07.332669, step: 89, loss: 0.3067989945411682, acc: 0.8594, auc: 0.9461, precision: 0.8864, recall: 0.75\n",
      "2019-01-17T21:00:07.990999, step: 90, loss: 0.4408881664276123, acc: 0.8047, auc: 0.8981, precision: 0.8974, recall: 0.625\n",
      "2019-01-17T21:00:08.731516, step: 91, loss: 0.31931382417678833, acc: 0.8125, auc: 0.9512, precision: 0.9464, recall: 0.7162\n",
      "2019-01-17T21:00:09.456371, step: 92, loss: 0.3786563277244568, acc: 0.8672, auc: 0.9196, precision: 0.8101, recall: 0.9697\n",
      "2019-01-17T21:00:10.161912, step: 93, loss: 0.24274739623069763, acc: 0.9297, auc: 0.976, precision: 0.9189, recall: 0.9577\n",
      "2019-01-17T21:00:10.949824, step: 94, loss: 0.31715744733810425, acc: 0.8828, auc: 0.9788, precision: 0.8028, recall: 0.9828\n",
      "2019-01-17T21:00:11.666509, step: 95, loss: 0.3210061192512512, acc: 0.8516, auc: 0.9513, precision: 0.8065, recall: 0.8772\n",
      "2019-01-17T21:00:12.350574, step: 96, loss: 0.3155462145805359, acc: 0.8359, auc: 0.94, precision: 0.8983, recall: 0.7794\n",
      "2019-01-17T21:00:13.083428, step: 97, loss: 0.3393298387527466, acc: 0.8125, auc: 0.9399, precision: 0.9348, recall: 0.6719\n",
      "2019-01-17T21:00:13.811614, step: 98, loss: 0.40554070472717285, acc: 0.7188, auc: 0.9582, precision: 1.0, recall: 0.4194\n",
      "2019-01-17T21:00:14.525776, step: 99, loss: 0.315421462059021, acc: 0.8594, auc: 0.9499, precision: 0.9636, recall: 0.7681\n",
      "2019-01-17T21:00:15.252848, step: 100, loss: 0.3153071701526642, acc: 0.8672, auc: 0.9394, precision: 0.8947, recall: 0.8226\n",
      "\n",
      "Evaluation:\n",
      "2019-01-17T21:00:43.228921, step: 100, loss: 0.32069360407499165, acc: 0.872797435897436, auc: 0.9451846153846156, precision: 0.8600974358974358, recall: 0.8930538461538463\n",
      "2019-01-17T21:00:43.961472, step: 101, loss: 0.3187805414199829, acc: 0.8828, auc: 0.9413, precision: 0.8714, recall: 0.9104\n",
      "2019-01-17T21:00:44.651923, step: 102, loss: 0.2791450023651123, acc: 0.8828, auc: 0.9609, precision: 0.8824, recall: 0.8955\n",
      "2019-01-17T21:00:45.305605, step: 103, loss: 0.4757082760334015, acc: 0.8125, auc: 0.9123, precision: 0.75, recall: 0.9\n",
      "2019-01-17T21:00:45.992558, step: 104, loss: 0.2873377501964569, acc: 0.8828, auc: 0.9583, precision: 0.8814, recall: 0.8667\n",
      "2019-01-17T21:00:46.642699, step: 105, loss: 0.2840331196784973, acc: 0.8516, auc: 0.9548, precision: 0.9107, recall: 0.7846\n",
      "2019-01-17T21:00:47.340629, step: 106, loss: 0.4223639965057373, acc: 0.7734, auc: 0.9206, precision: 0.9091, recall: 0.6154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-17T21:00:48.047387, step: 107, loss: 0.38088536262512207, acc: 0.8047, auc: 0.9386, precision: 0.9583, recall: 0.6667\n",
      "2019-01-17T21:00:48.820397, step: 108, loss: 0.30188071727752686, acc: 0.8359, auc: 0.9485, precision: 0.9111, recall: 0.7069\n",
      "2019-01-17T21:00:49.539057, step: 109, loss: 0.2493475079536438, acc: 0.8828, auc: 0.968, precision: 0.9796, recall: 0.7742\n",
      "2019-01-17T21:00:50.295764, step: 110, loss: 0.24506689608097076, acc: 0.9219, auc: 0.9761, precision: 0.9167, recall: 0.9429\n",
      "2019-01-17T21:00:51.018270, step: 111, loss: 0.39011698961257935, acc: 0.8359, auc: 0.9266, precision: 0.7903, recall: 0.8596\n",
      "2019-01-17T21:00:51.707186, step: 112, loss: 0.28663158416748047, acc: 0.875, auc: 0.9539, precision: 0.9524, recall: 0.8219\n",
      "2019-01-17T21:00:52.386520, step: 113, loss: 0.3912290930747986, acc: 0.8516, auc: 0.9132, precision: 0.8615, recall: 0.8485\n",
      "2019-01-17T21:00:53.101394, step: 114, loss: 0.3116287589073181, acc: 0.8516, auc: 0.9458, precision: 0.9583, recall: 0.7302\n",
      "2019-01-17T21:00:53.814871, step: 115, loss: 0.3231941759586334, acc: 0.8359, auc: 0.9488, precision: 0.9667, recall: 0.7532\n",
      "2019-01-17T21:00:54.554161, step: 116, loss: 0.2420688420534134, acc: 0.9062, auc: 0.9724, precision: 0.9365, recall: 0.8806\n",
      "2019-01-17T21:00:55.267988, step: 117, loss: 0.24217206239700317, acc: 0.9219, auc: 0.9709, precision: 0.918, recall: 0.918\n",
      "2019-01-17T21:00:55.918931, step: 118, loss: 0.24263250827789307, acc: 0.8906, auc: 0.9667, precision: 0.9, recall: 0.8333\n",
      "2019-01-17T21:00:56.626286, step: 119, loss: 0.305113822221756, acc: 0.8828, auc: 0.9385, precision: 0.8889, recall: 0.9014\n",
      "2019-01-17T21:00:57.300573, step: 120, loss: 0.27782773971557617, acc: 0.8828, auc: 0.9552, precision: 0.9508, recall: 0.8286\n",
      "2019-01-17T21:00:57.958274, step: 121, loss: 0.3124404549598694, acc: 0.8281, auc: 0.9448, precision: 0.9057, recall: 0.7385\n",
      "2019-01-17T21:00:58.719911, step: 122, loss: 0.2714567184448242, acc: 0.8828, auc: 0.9577, precision: 0.8571, recall: 0.8727\n",
      "2019-01-17T21:00:59.481837, step: 123, loss: 0.34824293851852417, acc: 0.8516, auc: 0.9245, precision: 0.902, recall: 0.7667\n",
      "2019-01-17T21:01:00.223716, step: 124, loss: 0.2942454218864441, acc: 0.8672, auc: 0.9469, precision: 0.8971, recall: 0.8592\n",
      "2019-01-17T21:01:00.913524, step: 125, loss: 0.28643232583999634, acc: 0.8672, auc: 0.9489, precision: 0.8824, recall: 0.8036\n",
      "2019-01-17T21:01:01.630802, step: 126, loss: 0.3574191927909851, acc: 0.8516, auc: 0.9291, precision: 0.8413, recall: 0.8548\n",
      "2019-01-17T21:01:02.329421, step: 127, loss: 0.27570685744285583, acc: 0.8984, auc: 0.9498, precision: 0.9219, recall: 0.8806\n",
      "2019-01-17T21:01:03.019883, step: 128, loss: 0.33950167894363403, acc: 0.8125, auc: 0.9292, precision: 0.8571, recall: 0.75\n",
      "2019-01-17T21:01:03.731797, step: 129, loss: 0.32910051941871643, acc: 0.8828, auc: 0.9346, precision: 0.9, recall: 0.8571\n",
      "2019-01-17T21:01:04.461640, step: 130, loss: 0.35042092204093933, acc: 0.8438, auc: 0.9328, precision: 0.9057, recall: 0.7619\n",
      "2019-01-17T21:01:05.251508, step: 131, loss: 0.1724405437707901, acc: 0.9297, auc: 0.9897, precision: 0.9483, recall: 0.9016\n",
      "2019-01-17T21:01:05.947940, step: 132, loss: 0.2824620306491852, acc: 0.8828, auc: 0.9516, precision: 0.913, recall: 0.875\n",
      "2019-01-17T21:01:06.622753, step: 133, loss: 0.30686286091804504, acc: 0.875, auc: 0.9435, precision: 0.8841, recall: 0.8841\n",
      "2019-01-17T21:01:07.322652, step: 134, loss: 0.2625989317893982, acc: 0.8828, auc: 0.9619, precision: 0.8871, recall: 0.873\n",
      "2019-01-17T21:01:07.976525, step: 135, loss: 0.37086522579193115, acc: 0.8281, auc: 0.9167, precision: 0.8276, recall: 0.8\n",
      "2019-01-17T21:01:08.635249, step: 136, loss: 0.32381197810173035, acc: 0.8828, auc: 0.9343, precision: 0.9254, recall: 0.8611\n",
      "2019-01-17T21:01:09.347281, step: 137, loss: 0.3249393403530121, acc: 0.8828, auc: 0.9379, precision: 0.8667, recall: 0.8814\n",
      "2019-01-17T21:01:10.062987, step: 138, loss: 0.36929503083229065, acc: 0.8203, auc: 0.9241, precision: 0.8889, recall: 0.7385\n",
      "2019-01-17T21:01:10.773494, step: 139, loss: 0.3684280514717102, acc: 0.8594, auc: 0.936, precision: 0.9792, recall: 0.7344\n",
      "2019-01-17T21:01:11.428647, step: 140, loss: 0.2958637475967407, acc: 0.8672, auc: 0.9473, precision: 0.902, recall: 0.7931\n",
      "2019-01-17T21:01:12.175988, step: 141, loss: 0.41074061393737793, acc: 0.7969, auc: 0.8998, precision: 0.7895, recall: 0.7627\n",
      "2019-01-17T21:01:12.946784, step: 142, loss: 0.2536037266254425, acc: 0.875, auc: 0.9683, precision: 0.9216, recall: 0.7966\n",
      "2019-01-17T21:01:13.633213, step: 143, loss: 0.2707643508911133, acc: 0.875, auc: 0.9612, precision: 0.9231, recall: 0.8451\n",
      "2019-01-17T21:01:14.339122, step: 144, loss: 0.18071608245372772, acc: 0.9297, auc: 0.9849, precision: 0.9697, recall: 0.9014\n",
      "2019-01-17T21:01:15.018093, step: 145, loss: 0.33478742837905884, acc: 0.8594, auc: 0.9365, precision: 0.8333, recall: 0.8871\n",
      "2019-01-17T21:01:15.686115, step: 146, loss: 0.3748452663421631, acc: 0.875, auc: 0.9285, precision: 0.8529, recall: 0.9062\n",
      "2019-01-17T21:01:16.373911, step: 147, loss: 0.24263811111450195, acc: 0.9062, auc: 0.9643, precision: 0.9153, recall: 0.8852\n",
      "2019-01-17T21:01:17.086908, step: 148, loss: 0.26694491505622864, acc: 0.8906, auc: 0.9587, precision: 0.9153, recall: 0.8571\n",
      "2019-01-17T21:01:17.741216, step: 149, loss: 0.29373547434806824, acc: 0.8906, auc: 0.9526, precision: 0.9455, recall: 0.8254\n",
      "2019-01-17T21:01:18.508133, step: 150, loss: 0.33290523290634155, acc: 0.8516, auc: 0.9364, precision: 0.9038, recall: 0.7705\n",
      "2019-01-17T21:01:19.174559, step: 151, loss: 0.3880336582660675, acc: 0.8203, auc: 0.9182, precision: 0.9, recall: 0.7143\n",
      "2019-01-17T21:01:19.921060, step: 152, loss: 0.31209999322891235, acc: 0.8281, auc: 0.9441, precision: 0.9038, recall: 0.7344\n",
      "2019-01-17T21:01:20.617655, step: 153, loss: 0.36381298303604126, acc: 0.8359, auc: 0.9207, precision: 0.8393, recall: 0.7966\n",
      "2019-01-17T21:01:21.360205, step: 154, loss: 0.2477426677942276, acc: 0.9453, auc: 0.9678, precision: 0.9265, recall: 0.9692\n",
      "2019-01-17T21:01:22.102477, step: 155, loss: 0.343412309885025, acc: 0.8672, auc: 0.9544, precision: 0.8095, recall: 0.9107\n",
      "2019-01-17T21:01:22.822304, step: 156, loss: 0.39063337445259094, acc: 0.8594, auc: 0.9267, precision: 0.7895, recall: 0.8824\n",
      "start training model\n",
      "2019-01-17T21:01:23.521403, step: 157, loss: 0.15027931332588196, acc: 0.9609, auc: 0.9961, precision: 0.9833, recall: 0.9365\n",
      "2019-01-17T21:01:24.206429, step: 158, loss: 0.22393806278705597, acc: 0.8984, auc: 0.978, precision: 0.9623, recall: 0.8226\n",
      "2019-01-17T21:01:24.895210, step: 159, loss: 0.27025237679481506, acc: 0.8516, auc: 0.9593, precision: 0.9744, recall: 0.6786\n",
      "2019-01-17T21:01:25.596497, step: 160, loss: 0.17151498794555664, acc: 0.9219, auc: 0.9968, precision: 1.0, recall: 0.8611\n",
      "2019-01-17T21:01:26.306508, step: 161, loss: 0.24098511040210724, acc: 0.8906, auc: 0.9786, precision: 1.0, recall: 0.8228\n",
      "2019-01-17T21:01:27.036234, step: 162, loss: 0.23613575100898743, acc: 0.8828, auc: 0.9708, precision: 0.9344, recall: 0.8382\n",
      "2019-01-17T21:01:27.753045, step: 163, loss: 0.23828381299972534, acc: 0.9297, auc: 0.9889, precision: 0.8846, recall: 1.0\n",
      "2019-01-17T21:01:28.527730, step: 164, loss: 0.2657126784324646, acc: 0.9219, auc: 0.9827, precision: 0.875, recall: 0.9844\n",
      "2019-01-17T21:01:29.247246, step: 165, loss: 0.1402740180492401, acc: 0.9297, auc: 0.9917, precision: 0.9355, recall: 0.9206\n",
      "2019-01-17T21:01:29.872954, step: 166, loss: 0.21770018339157104, acc: 0.9219, auc: 0.9681, precision: 0.9375, recall: 0.8654\n",
      "2019-01-17T21:01:30.497786, step: 167, loss: 0.37483522295951843, acc: 0.8281, auc: 0.9587, precision: 0.9492, recall: 0.7467\n",
      "2019-01-17T21:01:31.191377, step: 168, loss: 0.2297665923833847, acc: 0.8672, auc: 0.9861, precision: 0.9796, recall: 0.75\n",
      "2019-01-17T21:01:31.892069, step: 169, loss: 0.13081538677215576, acc: 0.9531, auc: 0.9902, precision: 0.9841, recall: 0.9254\n",
      "2019-01-17T21:01:32.588003, step: 170, loss: 0.2776808738708496, acc: 0.8906, auc: 0.9863, precision: 0.8194, recall: 0.9833\n",
      "2019-01-17T21:01:33.272243, step: 171, loss: 0.22105827927589417, acc: 0.9219, auc: 0.9814, precision: 0.8696, recall: 0.9836\n",
      "2019-01-17T21:01:33.979567, step: 172, loss: 0.19605687260627747, acc: 0.9141, auc: 0.9778, precision: 0.9077, recall: 0.9219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-17T21:01:34.662296, step: 173, loss: 0.20858250558376312, acc: 0.9141, auc: 0.9749, precision: 0.9649, recall: 0.8594\n",
      "2019-01-17T21:01:35.382455, step: 174, loss: 0.19043567776679993, acc: 0.9062, auc: 0.9873, precision: 1.0, recall: 0.8095\n",
      "2019-01-17T21:01:36.100366, step: 175, loss: 0.2959415316581726, acc: 0.8516, auc: 0.9608, precision: 1.0, recall: 0.6833\n",
      "2019-01-17T21:01:36.741669, step: 176, loss: 0.19541166722774506, acc: 0.8828, auc: 0.9764, precision: 0.9388, recall: 0.7931\n",
      "2019-01-17T21:01:37.445211, step: 177, loss: 0.23847684264183044, acc: 0.8984, auc: 0.9655, precision: 0.9, recall: 0.8852\n",
      "2019-01-17T21:01:38.148204, step: 178, loss: 0.1872313767671585, acc: 0.9375, auc: 0.9789, precision: 0.9552, recall: 0.9275\n",
      "2019-01-17T21:01:38.840816, step: 179, loss: 0.23561348021030426, acc: 0.9297, auc: 0.9609, precision: 0.9516, recall: 0.9077\n",
      "2019-01-17T21:01:39.584133, step: 180, loss: 0.19130921363830566, acc: 0.9062, auc: 0.9862, precision: 0.8919, recall: 0.9429\n",
      "2019-01-17T21:01:40.302789, step: 181, loss: 0.13707900047302246, acc: 0.9453, auc: 0.9932, precision: 0.9677, recall: 0.9231\n",
      "2019-01-17T21:01:40.985947, step: 182, loss: 0.2525073289871216, acc: 0.8906, auc: 0.9622, precision: 0.9245, recall: 0.8305\n",
      "2019-01-17T21:01:41.670767, step: 183, loss: 0.15442566573619843, acc: 0.9062, auc: 0.9944, precision: 1.0, recall: 0.8154\n",
      "2019-01-17T21:01:42.340907, step: 184, loss: 0.1965244859457016, acc: 0.9141, auc: 0.9783, precision: 0.9194, recall: 0.9048\n",
      "2019-01-17T21:01:43.077434, step: 185, loss: 0.1816834807395935, acc: 0.9375, auc: 0.9802, precision: 0.9516, recall: 0.9219\n",
      "2019-01-17T21:01:43.808835, step: 186, loss: 0.30269837379455566, acc: 0.8672, auc: 0.9509, precision: 0.9444, recall: 0.7846\n",
      "2019-01-17T21:01:44.533487, step: 187, loss: 0.2930144667625427, acc: 0.8984, auc: 0.9571, precision: 0.88, recall: 0.9429\n",
      "2019-01-17T21:01:45.272563, step: 188, loss: 0.16444432735443115, acc: 0.9609, auc: 0.9861, precision: 0.9516, recall: 0.9672\n",
      "2019-01-17T21:01:45.971814, step: 189, loss: 0.1448618322610855, acc: 0.9531, auc: 0.9854, precision: 1.0, recall: 0.9062\n",
      "2019-01-17T21:01:46.647747, step: 190, loss: 0.11885061860084534, acc: 0.9375, auc: 0.9951, precision: 0.9688, recall: 0.9118\n",
      "2019-01-17T21:01:47.329576, step: 191, loss: 0.2300095409154892, acc: 0.9141, auc: 0.9719, precision: 0.9516, recall: 0.8806\n",
      "2019-01-17T21:01:48.006973, step: 192, loss: 0.25151264667510986, acc: 0.9219, auc: 0.9569, precision: 0.9565, recall: 0.9041\n",
      "2019-01-17T21:01:48.768624, step: 193, loss: 0.16887448728084564, acc: 0.9297, auc: 0.9866, precision: 0.9118, recall: 0.9538\n",
      "2019-01-17T21:01:49.459614, step: 194, loss: 0.1992311328649521, acc: 0.9375, auc: 0.9748, precision: 0.9508, recall: 0.9206\n",
      "2019-01-17T21:01:50.160938, step: 195, loss: 0.2661724090576172, acc: 0.8828, auc: 0.9585, precision: 0.9138, recall: 0.8413\n",
      "2019-01-17T21:01:50.860719, step: 196, loss: 0.23854342103004456, acc: 0.9141, auc: 0.9644, precision: 0.9649, recall: 0.8594\n",
      "2019-01-17T21:01:51.583883, step: 197, loss: 0.16680264472961426, acc: 0.9453, auc: 0.9829, precision: 0.9683, recall: 0.9242\n",
      "2019-01-17T21:01:52.302372, step: 198, loss: 0.16843008995056152, acc: 0.9297, auc: 0.9906, precision: 0.8621, recall: 0.9804\n",
      "2019-01-17T21:01:53.010322, step: 199, loss: 0.1512492299079895, acc: 0.9453, auc: 0.9856, precision: 0.9365, recall: 0.9516\n",
      "2019-01-17T21:01:53.675379, step: 200, loss: 0.17399688065052032, acc: 0.9062, auc: 0.9813, precision: 0.9388, recall: 0.8364\n",
      "\n",
      "Evaluation:\n",
      "2019-01-17T21:02:20.476354, step: 200, loss: 0.30541808635760576, acc: 0.8673923076923077, auc: 0.9522538461538463, precision: 0.928779487179487, recall: 0.8013384615384614\n",
      "2019-01-17T21:02:21.172491, step: 201, loss: 0.2147943079471588, acc: 0.9141, auc: 0.972, precision: 0.9574, recall: 0.8333\n",
      "2019-01-17T21:02:21.870070, step: 202, loss: 0.2436566948890686, acc: 0.8984, auc: 0.961, precision: 0.902, recall: 0.8519\n",
      "2019-01-17T21:02:22.601797, step: 203, loss: 0.20494166016578674, acc: 0.9141, auc: 0.9745, precision: 0.9677, recall: 0.8696\n",
      "2019-01-17T21:02:23.289999, step: 204, loss: 0.2464291751384735, acc: 0.8828, auc: 0.9634, precision: 0.931, recall: 0.8308\n",
      "2019-01-17T21:02:23.934365, step: 205, loss: 0.23248136043548584, acc: 0.9453, auc: 0.9641, precision: 0.9275, recall: 0.9697\n",
      "2019-01-17T21:02:24.589380, step: 206, loss: 0.2504074275493622, acc: 0.9062, auc: 0.9678, precision: 0.8955, recall: 0.9231\n",
      "2019-01-17T21:02:25.287719, step: 207, loss: 0.17556436359882355, acc: 0.9219, auc: 0.9803, precision: 0.9623, recall: 0.8644\n",
      "2019-01-17T21:02:25.980420, step: 208, loss: 0.1678267866373062, acc: 0.9297, auc: 0.9843, precision: 0.963, recall: 0.8814\n",
      "2019-01-17T21:02:26.699284, step: 209, loss: 0.21567100286483765, acc: 0.8984, auc: 0.9759, precision: 0.9423, recall: 0.8305\n",
      "2019-01-17T21:02:27.384750, step: 210, loss: 0.17645175755023956, acc: 0.9141, auc: 0.987, precision: 1.0, recall: 0.8358\n",
      "2019-01-17T21:02:28.110759, step: 211, loss: 0.21100804209709167, acc: 0.9062, auc: 0.9735, precision: 0.9254, recall: 0.8986\n",
      "2019-01-17T21:02:28.848234, step: 212, loss: 0.19604352116584778, acc: 0.9219, auc: 0.9823, precision: 0.8947, recall: 0.9714\n",
      "2019-01-17T21:02:29.593946, step: 213, loss: 0.221734881401062, acc: 0.9219, auc: 0.9743, precision: 0.8923, recall: 0.9508\n",
      "2019-01-17T21:02:30.335041, step: 214, loss: 0.12553760409355164, acc: 0.9688, auc: 0.9946, precision: 0.9531, recall: 0.9839\n",
      "2019-01-17T21:02:31.049514, step: 215, loss: 0.27538856863975525, acc: 0.8828, auc: 0.9567, precision: 0.9474, recall: 0.8182\n",
      "2019-01-17T21:02:31.790934, step: 216, loss: 0.16833263635635376, acc: 0.9297, auc: 0.9873, precision: 1.0, recall: 0.8525\n",
      "2019-01-17T21:02:32.516279, step: 217, loss: 0.20968501269817352, acc: 0.875, auc: 0.9807, precision: 0.9524, recall: 0.7407\n",
      "2019-01-17T21:02:33.197187, step: 218, loss: 0.2824191153049469, acc: 0.8672, auc: 0.9654, precision: 0.9216, recall: 0.7833\n",
      "2019-01-17T21:02:33.937039, step: 219, loss: 0.12484321743249893, acc: 0.9375, auc: 0.9946, precision: 0.9655, recall: 0.9032\n",
      "2019-01-17T21:02:34.657837, step: 220, loss: 0.22070136666297913, acc: 0.9219, auc: 0.9808, precision: 0.875, recall: 0.9655\n",
      "2019-01-17T21:02:35.318049, step: 221, loss: 0.30652540922164917, acc: 0.8672, auc: 0.9607, precision: 0.8514, recall: 0.913\n",
      "2019-01-17T21:02:35.983738, step: 222, loss: 0.21354234218597412, acc: 0.9375, auc: 0.9724, precision: 0.9265, recall: 0.9545\n",
      "2019-01-17T21:02:36.617196, step: 223, loss: 0.19212713837623596, acc: 0.9297, auc: 0.9754, precision: 0.9552, recall: 0.9143\n",
      "2019-01-17T21:02:37.302635, step: 224, loss: 0.1825702041387558, acc: 0.9375, auc: 0.9775, precision: 0.9833, recall: 0.8939\n",
      "2019-01-17T21:02:38.017315, step: 225, loss: 0.23553335666656494, acc: 0.8984, auc: 0.9754, precision: 1.0, recall: 0.8194\n",
      "2019-01-17T21:02:38.837891, step: 226, loss: 0.23497146368026733, acc: 0.9297, auc: 0.9646, precision: 0.9667, recall: 0.8923\n",
      "2019-01-17T21:02:39.586133, step: 227, loss: 0.21996891498565674, acc: 0.9141, auc: 0.9716, precision: 0.9571, recall: 0.8933\n",
      "2019-01-17T21:02:40.325734, step: 228, loss: 0.21711388230323792, acc: 0.9375, auc: 0.9678, precision: 0.9254, recall: 0.9538\n",
      "2019-01-17T21:02:41.033217, step: 229, loss: 0.2724076211452484, acc: 0.8828, auc: 0.9689, precision: 0.8485, recall: 0.918\n",
      "2019-01-17T21:02:41.727733, step: 230, loss: 0.12870606780052185, acc: 0.9531, auc: 0.996, precision: 0.9726, recall: 0.9467\n",
      "2019-01-17T21:02:42.470998, step: 231, loss: 0.2396906018257141, acc: 0.9219, auc: 0.9624, precision: 0.9344, recall: 0.9048\n",
      "2019-01-17T21:02:43.140247, step: 232, loss: 0.24973800778388977, acc: 0.8438, auc: 0.9657, precision: 0.902, recall: 0.7541\n",
      "2019-01-17T21:02:43.858011, step: 233, loss: 0.22405335307121277, acc: 0.9297, auc: 0.9691, precision: 0.96, recall: 0.8727\n",
      "2019-01-17T21:02:44.623448, step: 234, loss: 0.23538415133953094, acc: 0.8906, auc: 0.9726, precision: 0.9815, recall: 0.803\n",
      "2019-01-17T21:02:45.377647, step: 235, loss: 0.3098708391189575, acc: 0.8828, auc: 0.9461, precision: 0.9565, recall: 0.7719\n",
      "2019-01-17T21:02:46.124420, step: 236, loss: 0.1895713210105896, acc: 0.8906, auc: 0.979, precision: 0.9804, recall: 0.7937\n",
      "2019-01-17T21:02:46.912110, step: 237, loss: 0.2640199661254883, acc: 0.875, auc: 0.9602, precision: 0.8769, recall: 0.8769\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-17T21:02:47.643310, step: 238, loss: 0.21910765767097473, acc: 0.9297, auc: 0.9904, precision: 0.8806, recall: 0.9833\n",
      "2019-01-17T21:02:48.310941, step: 239, loss: 0.22293990850448608, acc: 0.9062, auc: 0.9751, precision: 0.8939, recall: 0.9219\n",
      "2019-01-17T21:02:49.010640, step: 240, loss: 0.2462482452392578, acc: 0.9062, auc: 0.9648, precision: 0.9032, recall: 0.9032\n",
      "2019-01-17T21:02:49.704750, step: 241, loss: 0.20884238183498383, acc: 0.9375, auc: 0.9711, precision: 0.9796, recall: 0.8727\n",
      "2019-01-17T21:02:50.413171, step: 242, loss: 0.20496313273906708, acc: 0.8906, auc: 0.9822, precision: 0.963, recall: 0.8125\n",
      "2019-01-17T21:02:51.104267, step: 243, loss: 0.21850597858428955, acc: 0.8672, auc: 0.9765, precision: 0.9412, recall: 0.7742\n",
      "2019-01-17T21:02:51.824423, step: 244, loss: 0.19041579961776733, acc: 0.9141, auc: 0.9809, precision: 0.9552, recall: 0.8889\n",
      "2019-01-17T21:02:52.539412, step: 245, loss: 0.2131832242012024, acc: 0.9219, auc: 0.9754, precision: 0.9138, recall: 0.9138\n",
      "2019-01-17T21:02:53.250833, step: 246, loss: 0.21055063605308533, acc: 0.9297, auc: 0.9734, precision: 0.9178, recall: 0.9571\n",
      "2019-01-17T21:02:53.985605, step: 247, loss: 0.17297209799289703, acc: 0.9219, auc: 0.9828, precision: 0.9286, recall: 0.9286\n",
      "2019-01-17T21:02:54.719432, step: 248, loss: 0.2586139142513275, acc: 0.9141, auc: 0.9635, precision: 0.9016, recall: 0.9167\n",
      "2019-01-17T21:02:55.364479, step: 249, loss: 0.1870109885931015, acc: 0.9297, auc: 0.9832, precision: 0.95, recall: 0.9048\n",
      "2019-01-17T21:02:56.008570, step: 250, loss: 0.19708046317100525, acc: 0.9453, auc: 0.9753, precision: 0.9792, recall: 0.8868\n",
      "2019-01-17T21:02:56.683402, step: 251, loss: 0.23938986659049988, acc: 0.8906, auc: 0.9838, precision: 1.0, recall: 0.7627\n",
      "2019-01-17T21:02:57.452868, step: 252, loss: 0.19747312366962433, acc: 0.9219, auc: 0.9761, precision: 0.9091, recall: 0.9091\n",
      "2019-01-17T21:02:58.188123, step: 253, loss: 0.13921770453453064, acc: 0.9531, auc: 0.9898, precision: 0.9863, recall: 0.9351\n",
      "2019-01-17T21:02:58.940462, step: 254, loss: 0.18680880963802338, acc: 0.9297, auc: 0.9883, precision: 0.9104, recall: 0.9531\n",
      "2019-01-17T21:02:59.684213, step: 255, loss: 0.15282182395458221, acc: 0.9531, auc: 0.9909, precision: 0.9355, recall: 0.9667\n",
      "2019-01-17T21:03:00.352358, step: 256, loss: 0.2335560917854309, acc: 0.9062, auc: 0.9741, precision: 0.8906, recall: 0.9194\n",
      "2019-01-17T21:03:01.090541, step: 257, loss: 0.218502014875412, acc: 0.8984, auc: 0.9709, precision: 0.9464, recall: 0.8413\n",
      "2019-01-17T21:03:01.796519, step: 258, loss: 0.10721677541732788, acc: 0.9609, auc: 0.9973, precision: 1.0, recall: 0.9153\n",
      "2019-01-17T21:03:02.444620, step: 259, loss: 0.3627855181694031, acc: 0.8203, auc: 0.9492, precision: 0.92, recall: 0.7077\n",
      "2019-01-17T21:03:03.141272, step: 260, loss: 0.1864132434129715, acc: 0.8906, auc: 0.9874, precision: 0.9778, recall: 0.7719\n",
      "2019-01-17T21:03:03.869720, step: 261, loss: 0.1278720647096634, acc: 0.9531, auc: 0.9934, precision: 0.9394, recall: 0.9688\n",
      "2019-01-17T21:03:04.525597, step: 262, loss: 0.24457459151744843, acc: 0.9375, auc: 0.9616, precision: 0.9333, recall: 0.9589\n",
      "2019-01-17T21:03:05.259573, step: 263, loss: 0.19925272464752197, acc: 0.9375, auc: 0.9834, precision: 0.9219, recall: 0.9516\n",
      "2019-01-17T21:03:05.996756, step: 264, loss: 0.3913264572620392, acc: 0.8438, auc: 0.9399, precision: 0.8056, recall: 0.9062\n",
      "2019-01-17T21:03:06.691262, step: 265, loss: 0.16116215288639069, acc: 0.9609, auc: 0.9875, precision: 0.9683, recall: 0.9531\n",
      "2019-01-17T21:03:07.355906, step: 266, loss: 0.2746647000312805, acc: 0.8672, auc: 0.9651, precision: 0.9796, recall: 0.75\n",
      "2019-01-17T21:03:08.100767, step: 267, loss: 0.22295427322387695, acc: 0.8828, auc: 0.9807, precision: 0.9811, recall: 0.7879\n",
      "2019-01-17T21:03:08.832694, step: 268, loss: 0.30026236176490784, acc: 0.8906, auc: 0.9462, precision: 0.9773, recall: 0.7679\n",
      "2019-01-17T21:03:09.571922, step: 269, loss: 0.25172632932662964, acc: 0.9141, auc: 0.9663, precision: 0.9833, recall: 0.8551\n",
      "2019-01-17T21:03:10.367183, step: 270, loss: 0.2583967447280884, acc: 0.875, auc: 0.9585, precision: 0.9054, recall: 0.8816\n",
      "2019-01-17T21:03:11.118961, step: 271, loss: 0.235733300447464, acc: 0.9453, auc: 0.9931, precision: 0.8889, recall: 1.0\n",
      "2019-01-17T21:03:11.848490, step: 272, loss: 0.1828700751066208, acc: 0.9297, auc: 0.9845, precision: 0.9375, recall: 0.9494\n",
      "2019-01-17T21:03:12.514659, step: 273, loss: 0.2695395350456238, acc: 0.8984, auc: 0.9741, precision: 0.8571, recall: 0.9524\n",
      "2019-01-17T21:03:13.318511, step: 274, loss: 0.24364645779132843, acc: 0.8672, auc: 0.9637, precision: 0.8909, recall: 0.8167\n",
      "2019-01-17T21:03:14.073221, step: 275, loss: 0.2529281973838806, acc: 0.9062, auc: 0.9621, precision: 0.9531, recall: 0.8714\n",
      "2019-01-17T21:03:14.811295, step: 276, loss: 0.22146722674369812, acc: 0.8828, auc: 0.9728, precision: 0.9592, recall: 0.7833\n",
      "2019-01-17T21:03:15.593619, step: 277, loss: 0.18385007977485657, acc: 0.8984, auc: 0.985, precision: 0.9828, recall: 0.8261\n",
      "2019-01-17T21:03:16.271883, step: 278, loss: 0.29695412516593933, acc: 0.8672, auc: 0.9519, precision: 0.9623, recall: 0.7727\n",
      "2019-01-17T21:03:16.979166, step: 279, loss: 0.16885626316070557, acc: 0.9375, auc: 0.9852, precision: 0.963, recall: 0.8966\n",
      "2019-01-17T21:03:17.707979, step: 280, loss: 0.224680095911026, acc: 0.9297, auc: 0.9696, precision: 0.9322, recall: 0.9167\n",
      "2019-01-17T21:03:18.438910, step: 281, loss: 0.24810314178466797, acc: 0.9453, auc: 0.9581, precision: 0.9298, recall: 0.9464\n",
      "2019-01-17T21:03:19.098099, step: 282, loss: 0.25015848875045776, acc: 0.8906, auc: 0.9704, precision: 0.8548, recall: 0.9138\n",
      "2019-01-17T21:03:19.800918, step: 283, loss: 0.27711015939712524, acc: 0.8906, auc: 0.9543, precision: 0.8871, recall: 0.8871\n",
      "2019-01-17T21:03:20.560934, step: 284, loss: 0.17610320448875427, acc: 0.9219, auc: 0.9831, precision: 0.9811, recall: 0.8525\n",
      "2019-01-17T21:03:21.300433, step: 285, loss: 0.2803939878940582, acc: 0.8672, auc: 0.9728, precision: 0.9808, recall: 0.7612\n",
      "2019-01-17T21:03:21.951919, step: 286, loss: 0.2710258364677429, acc: 0.8828, auc: 0.9677, precision: 0.9574, recall: 0.7759\n",
      "2019-01-17T21:03:22.573982, step: 287, loss: 0.16858863830566406, acc: 0.9375, auc: 0.9819, precision: 0.9655, recall: 0.9032\n",
      "2019-01-17T21:03:23.194516, step: 288, loss: 0.2454695999622345, acc: 0.9141, auc: 0.9604, precision: 0.9189, recall: 0.9315\n",
      "2019-01-17T21:03:23.918856, step: 289, loss: 0.3550502359867096, acc: 0.875, auc: 0.9607, precision: 0.7879, recall: 0.963\n",
      "2019-01-17T21:03:24.680402, step: 290, loss: 0.18769903481006622, acc: 0.9375, auc: 0.9843, precision: 0.9167, recall: 0.9706\n",
      "2019-01-17T21:03:25.375069, step: 291, loss: 0.18858253955841064, acc: 0.9375, auc: 0.9813, precision: 0.9322, recall: 0.9322\n",
      "2019-01-17T21:03:26.123443, step: 292, loss: 0.1451171338558197, acc: 0.9141, auc: 0.9839, precision: 0.9302, recall: 0.8333\n",
      "2019-01-17T21:03:26.893996, step: 293, loss: 0.2256723791360855, acc: 0.8594, auc: 0.9861, precision: 0.98, recall: 0.7424\n",
      "2019-01-17T21:03:27.616882, step: 294, loss: 0.21423780918121338, acc: 0.8828, auc: 0.9827, precision: 0.9615, recall: 0.7937\n",
      "2019-01-17T21:03:28.263880, step: 295, loss: 0.1996789425611496, acc: 0.8984, auc: 0.9802, precision: 0.9649, recall: 0.8333\n",
      "2019-01-17T21:03:29.007748, step: 296, loss: 0.2876511812210083, acc: 0.8672, auc: 0.9518, precision: 0.8676, recall: 0.8806\n",
      "2019-01-17T21:03:29.750821, step: 297, loss: 0.2538096308708191, acc: 0.9141, auc: 0.959, precision: 0.9167, recall: 0.9296\n",
      "2019-01-17T21:03:30.468925, step: 298, loss: 0.26520487666130066, acc: 0.9141, auc: 0.9729, precision: 0.8649, recall: 0.9846\n",
      "2019-01-17T21:03:31.142889, step: 299, loss: 0.266604483127594, acc: 0.9062, auc: 0.9712, precision: 0.8788, recall: 0.9355\n",
      "2019-01-17T21:03:31.852179, step: 300, loss: 0.2404724657535553, acc: 0.9219, auc: 0.9646, precision: 0.9298, recall: 0.8983\n",
      "\n",
      "Evaluation:\n",
      "2019-01-17T21:03:59.439362, step: 300, loss: 0.30030295023551357, acc: 0.8643871794871796, auc: 0.9517692307692306, precision: 0.9295948717948715, recall: 0.7914333333333332\n",
      "2019-01-17T21:04:00.127592, step: 301, loss: 0.23581737279891968, acc: 0.875, auc: 0.9755, precision: 0.9811, recall: 0.7761\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-17T21:04:00.828065, step: 302, loss: 0.20460377633571625, acc: 0.8984, auc: 0.979, precision: 0.9821, recall: 0.8209\n",
      "2019-01-17T21:04:01.512212, step: 303, loss: 0.2109317183494568, acc: 0.9062, auc: 0.9761, precision: 0.9194, recall: 0.8906\n",
      "2019-01-17T21:04:02.237354, step: 304, loss: 0.30218616127967834, acc: 0.8672, auc: 0.9477, precision: 0.9138, recall: 0.8154\n",
      "2019-01-17T21:04:02.955572, step: 305, loss: 0.19454339146614075, acc: 0.9219, auc: 0.9835, precision: 0.9836, recall: 0.8696\n",
      "2019-01-17T21:04:03.714798, step: 306, loss: 0.15619586408138275, acc: 0.9688, auc: 0.9828, precision: 0.9821, recall: 0.9483\n",
      "2019-01-17T21:04:04.413721, step: 307, loss: 0.23532328009605408, acc: 0.9141, auc: 0.9665, precision: 0.9811, recall: 0.8387\n",
      "2019-01-17T21:04:05.149400, step: 308, loss: 0.23637895286083221, acc: 0.8906, auc: 0.9673, precision: 0.8961, recall: 0.92\n",
      "2019-01-17T21:04:05.894358, step: 309, loss: 0.2540436387062073, acc: 0.9062, auc: 0.9662, precision: 0.8873, recall: 0.9403\n",
      "2019-01-17T21:04:06.599202, step: 310, loss: 0.18337729573249817, acc: 0.9375, auc: 0.984, precision: 0.9296, recall: 0.9565\n",
      "2019-01-17T21:04:07.366421, step: 311, loss: 0.23125477135181427, acc: 0.9141, auc: 0.9679, precision: 0.9254, recall: 0.9118\n",
      "2019-01-17T21:04:08.089250, step: 312, loss: 0.1802808940410614, acc: 0.9062, auc: 0.9803, precision: 0.9265, recall: 0.9\n",
      "start training model\n",
      "2019-01-17T21:04:08.856465, step: 313, loss: 0.1417616754770279, acc: 0.9219, auc: 0.9929, precision: 0.9825, recall: 0.8615\n",
      "2019-01-17T21:04:09.545574, step: 314, loss: 0.17543211579322815, acc: 0.9375, auc: 0.977, precision: 1.0, recall: 0.8788\n",
      "2019-01-17T21:04:10.240904, step: 315, loss: 0.0981387346982956, acc: 0.9766, auc: 0.9975, precision: 0.9861, recall: 0.9726\n",
      "2019-01-17T21:04:10.895315, step: 316, loss: 0.1115194782614708, acc: 0.9609, auc: 0.9941, precision: 0.9545, recall: 0.9692\n",
      "2019-01-17T21:04:11.575836, step: 317, loss: 0.09231309592723846, acc: 0.9531, auc: 0.9988, precision: 0.9821, recall: 0.9167\n",
      "2019-01-17T21:04:12.262605, step: 318, loss: 0.12955714762210846, acc: 0.9688, auc: 0.9829, precision: 0.9692, recall: 0.9692\n",
      "2019-01-17T21:04:13.003616, step: 319, loss: 0.06720877438783646, acc: 0.9922, auc: 0.9944, precision: 0.9853, recall: 1.0\n",
      "2019-01-17T21:04:13.699507, step: 320, loss: 0.0874282568693161, acc: 0.9688, auc: 0.9942, precision: 0.9808, recall: 0.9444\n",
      "2019-01-17T21:04:14.374689, step: 321, loss: 0.04559895396232605, acc: 0.9844, auc: 0.9988, precision: 0.9846, recall: 0.9846\n",
      "2019-01-17T21:04:15.036751, step: 322, loss: 0.2236252725124359, acc: 0.9219, auc: 0.9743, precision: 0.9434, recall: 0.8772\n",
      "2019-01-17T21:04:15.689228, step: 323, loss: 0.16600483655929565, acc: 0.9375, auc: 0.9838, precision: 0.9636, recall: 0.8983\n",
      "2019-01-17T21:04:16.314929, step: 324, loss: 0.11328430473804474, acc: 0.9688, auc: 0.9943, precision: 0.9492, recall: 0.9825\n",
      "2019-01-17T21:04:16.951684, step: 325, loss: 0.1504552662372589, acc: 0.9453, auc: 0.9853, precision: 0.9565, recall: 0.898\n",
      "2019-01-17T21:04:17.632110, step: 326, loss: 0.16742685437202454, acc: 0.9375, auc: 0.9824, precision: 0.9344, recall: 0.9344\n",
      "2019-01-17T21:04:18.334787, step: 327, loss: 0.13611845672130585, acc: 0.9531, auc: 0.9862, precision: 0.9583, recall: 0.92\n",
      "2019-01-17T21:04:19.008391, step: 328, loss: 0.09598761051893234, acc: 0.9688, auc: 0.9961, precision: 1.0, recall: 0.9403\n",
      "2019-01-17T21:04:19.733959, step: 329, loss: 0.05613529682159424, acc: 0.9766, auc: 0.9993, precision: 1.0, recall: 0.9545\n",
      "2019-01-17T21:04:20.468071, step: 330, loss: 0.24439328908920288, acc: 0.9141, auc: 0.9714, precision: 0.9032, recall: 0.918\n",
      "2019-01-17T21:04:21.211353, step: 331, loss: 0.0819290429353714, acc: 0.9844, auc: 0.9881, precision: 0.9825, recall: 0.9825\n",
      "2019-01-17T21:04:22.046688, step: 332, loss: 0.11214714497327805, acc: 0.9688, auc: 0.9924, precision: 0.9516, recall: 0.9833\n",
      "2019-01-17T21:04:22.799135, step: 333, loss: 0.09327381104230881, acc: 0.9688, auc: 0.9963, precision: 0.9851, recall: 0.9565\n",
      "2019-01-17T21:04:23.510204, step: 334, loss: 0.1656264066696167, acc: 0.9453, auc: 0.9811, precision: 0.9643, recall: 0.9153\n",
      "2019-01-17T21:04:24.198084, step: 335, loss: 0.15170687437057495, acc: 0.9219, auc: 0.988, precision: 0.9783, recall: 0.8333\n",
      "2019-01-17T21:04:24.858995, step: 336, loss: 0.12662407755851746, acc: 0.9531, auc: 0.988, precision: 0.9516, recall: 0.9516\n",
      "2019-01-17T21:04:25.636485, step: 337, loss: 0.11535514891147614, acc: 0.9688, auc: 0.9834, precision: 0.9839, recall: 0.9531\n",
      "2019-01-17T21:04:26.379922, step: 338, loss: 0.13065661489963531, acc: 0.9531, auc: 0.9891, precision: 0.9474, recall: 0.9474\n",
      "2019-01-17T21:04:27.075269, step: 339, loss: 0.06906862556934357, acc: 0.9922, auc: 0.9998, precision: 1.0, recall: 0.9833\n",
      "2019-01-17T21:04:27.793232, step: 340, loss: 0.11184501647949219, acc: 0.9453, auc: 0.9956, precision: 0.9483, recall: 0.9322\n",
      "2019-01-17T21:04:28.549126, step: 341, loss: 0.06866741180419922, acc: 0.9688, auc: 0.9983, precision: 0.9825, recall: 0.9492\n",
      "2019-01-17T21:04:29.220109, step: 342, loss: 0.1021587923169136, acc: 0.9688, auc: 0.9909, precision: 0.9825, recall: 0.9492\n",
      "2019-01-17T21:04:29.872529, step: 343, loss: 0.08614033460617065, acc: 0.9609, auc: 0.9978, precision: 1.0, recall: 0.9219\n",
      "2019-01-17T21:04:30.535508, step: 344, loss: 0.07103859633207321, acc: 0.9766, auc: 0.998, precision: 1.0, recall: 0.9524\n",
      "2019-01-17T21:04:31.170239, step: 345, loss: 0.08081645518541336, acc: 0.9766, auc: 0.9956, precision: 0.9552, recall: 1.0\n",
      "2019-01-17T21:04:31.850926, step: 346, loss: 0.06944835931062698, acc: 0.9766, auc: 0.9985, precision: 0.9655, recall: 0.9825\n",
      "2019-01-17T21:04:32.593702, step: 347, loss: 0.10756926983594894, acc: 0.9609, auc: 0.992, precision: 0.9722, recall: 0.9589\n",
      "2019-01-17T21:04:33.277107, step: 348, loss: 0.059544917196035385, acc: 0.9844, auc: 0.999, precision: 0.9853, recall: 0.9853\n",
      "2019-01-17T21:04:34.010134, step: 349, loss: 0.1375720053911209, acc: 0.9375, auc: 0.9909, precision: 0.931, recall: 0.931\n",
      "2019-01-17T21:04:34.704731, step: 350, loss: 0.08474203944206238, acc: 0.9688, auc: 0.9958, precision: 0.9818, recall: 0.9474\n",
      "2019-01-17T21:04:35.380680, step: 351, loss: 0.10610605776309967, acc: 0.9531, auc: 0.9934, precision: 0.9844, recall: 0.9265\n",
      "2019-01-17T21:04:36.120160, step: 352, loss: 0.07464271038770676, acc: 0.9844, auc: 0.9963, precision: 1.0, recall: 0.9688\n",
      "2019-01-17T21:04:36.838366, step: 353, loss: 0.1638248711824417, acc: 0.9375, auc: 0.9888, precision: 0.9118, recall: 0.9688\n",
      "2019-01-17T21:04:37.466013, step: 354, loss: 0.09715907275676727, acc: 0.9609, auc: 0.9958, precision: 0.942, recall: 0.9848\n",
      "2019-01-17T21:04:38.201273, step: 355, loss: 0.19867253303527832, acc: 0.9375, auc: 0.9778, precision: 0.9231, recall: 0.9524\n",
      "2019-01-17T21:04:38.888058, step: 356, loss: 0.1823474019765854, acc: 0.9375, auc: 0.9841, precision: 0.9828, recall: 0.8906\n",
      "2019-01-17T21:04:39.588900, step: 357, loss: 0.08286558836698532, acc: 0.9609, auc: 0.9998, precision: 1.0, recall: 0.9231\n",
      "2019-01-17T21:04:40.315544, step: 358, loss: 0.04268613085150719, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9667\n",
      "2019-01-17T21:04:41.042690, step: 359, loss: 0.11083145439624786, acc: 0.9531, auc: 0.9951, precision: 0.9375, recall: 0.9677\n",
      "2019-01-17T21:04:41.744879, step: 360, loss: 0.11181795597076416, acc: 0.9609, auc: 0.9913, precision: 0.9718, recall: 0.9583\n",
      "2019-01-17T21:04:42.406752, step: 361, loss: 0.12152589857578278, acc: 0.9453, auc: 0.9966, precision: 0.92, recall: 0.9857\n",
      "2019-01-17T21:04:43.116502, step: 362, loss: 0.09169317036867142, acc: 0.9609, auc: 0.9975, precision: 0.9459, recall: 0.9859\n",
      "2019-01-17T21:04:43.775117, step: 363, loss: 0.07651595771312714, acc: 0.9688, auc: 0.998, precision: 1.0, recall: 0.9355\n",
      "2019-01-17T21:04:44.432414, step: 364, loss: 0.08209101855754852, acc: 0.9609, auc: 0.9973, precision: 0.9833, recall: 0.9365\n",
      "2019-01-17T21:04:45.138181, step: 365, loss: 0.08964275568723679, acc: 0.9609, auc: 0.9949, precision: 0.9828, recall: 0.9344\n",
      "2019-01-17T21:04:45.815988, step: 366, loss: 0.1183820515871048, acc: 0.9453, auc: 0.9985, precision: 1.0, recall: 0.9054\n",
      "2019-01-17T21:04:46.561405, step: 367, loss: 0.06279705464839935, acc: 0.9922, auc: 0.9995, precision: 0.9844, recall: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-17T21:04:47.286610, step: 368, loss: 0.09831704199314117, acc: 0.9766, auc: 0.9973, precision: 0.9706, recall: 0.9851\n",
      "2019-01-17T21:04:47.966036, step: 369, loss: 0.16715316474437714, acc: 0.9297, auc: 0.9836, precision: 0.9355, recall: 0.9206\n",
      "2019-01-17T21:04:48.684728, step: 370, loss: 0.0625847727060318, acc: 0.9766, auc: 0.9978, precision: 0.9859, recall: 0.9722\n",
      "2019-01-17T21:04:49.385805, step: 371, loss: 0.04316984489560127, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9839\n",
      "2019-01-17T21:04:50.062564, step: 372, loss: 0.06703036278486252, acc: 0.9531, auc: 0.9983, precision: 0.9851, recall: 0.9296\n",
      "2019-01-17T21:04:50.742647, step: 373, loss: 0.05461795628070831, acc: 0.9844, auc: 0.9995, precision: 1.0, recall: 0.9692\n",
      "2019-01-17T21:04:51.411710, step: 374, loss: 0.130939319729805, acc: 0.9688, auc: 0.9842, precision: 0.9821, recall: 0.9483\n",
      "2019-01-17T21:04:52.143966, step: 375, loss: 0.07114975154399872, acc: 0.9844, auc: 0.9968, precision: 0.9672, recall: 1.0\n",
      "2019-01-17T21:04:52.835030, step: 376, loss: 0.08602075278759003, acc: 0.9453, auc: 0.9971, precision: 0.9825, recall: 0.9032\n",
      "2019-01-17T21:04:53.519766, step: 377, loss: 0.0769120305776596, acc: 0.9609, auc: 0.9971, precision: 0.9825, recall: 0.9333\n",
      "2019-01-17T21:04:54.171315, step: 378, loss: 0.19436731934547424, acc: 0.9688, auc: 0.9726, precision: 0.9531, recall: 0.9839\n",
      "2019-01-17T21:04:54.814778, step: 379, loss: 0.21167324483394623, acc: 0.9531, auc: 0.966, precision: 0.9571, recall: 0.9571\n",
      "2019-01-17T21:04:55.566037, step: 380, loss: 0.05864829197525978, acc: 0.9844, auc: 0.9998, precision: 0.9692, recall: 1.0\n",
      "2019-01-17T21:04:56.219850, step: 381, loss: 0.09197701513767242, acc: 0.9609, auc: 0.9961, precision: 0.9677, recall: 0.9524\n",
      "2019-01-17T21:04:56.887181, step: 382, loss: 0.13732197880744934, acc: 0.9531, auc: 0.9895, precision: 0.9831, recall: 0.9206\n",
      "2019-01-17T21:04:57.609829, step: 383, loss: 0.1516548991203308, acc: 0.9688, auc: 0.9812, precision: 1.0, recall: 0.9298\n",
      "2019-01-17T21:04:58.297800, step: 384, loss: 0.1048627495765686, acc: 0.9609, auc: 0.9944, precision: 0.9692, recall: 0.9545\n",
      "2019-01-17T21:04:58.986252, step: 385, loss: 0.05153976380825043, acc: 0.9688, auc: 0.9995, precision: 1.0, recall: 0.9429\n",
      "2019-01-17T21:04:59.700186, step: 386, loss: 0.1343936026096344, acc: 0.9531, auc: 0.9985, precision: 0.9, recall: 1.0\n",
      "2019-01-17T21:05:00.405135, step: 387, loss: 0.21315868198871613, acc: 0.9453, auc: 0.9851, precision: 0.9275, recall: 0.9697\n",
      "2019-01-17T21:05:01.159064, step: 388, loss: 0.1965385377407074, acc: 0.9219, auc: 0.981, precision: 0.95, recall: 0.8906\n",
      "2019-01-17T21:05:01.912746, step: 389, loss: 0.10328018665313721, acc: 0.9609, auc: 0.99, precision: 0.9592, recall: 0.94\n",
      "2019-01-17T21:05:02.632573, step: 390, loss: 0.19212394952774048, acc: 0.9297, auc: 0.9866, precision: 1.0, recall: 0.8636\n",
      "2019-01-17T21:05:03.375462, step: 391, loss: 0.20435474812984467, acc: 0.9297, auc: 0.9773, precision: 0.9818, recall: 0.871\n",
      "2019-01-17T21:05:04.139579, step: 392, loss: 0.10301604866981506, acc: 0.9609, auc: 0.9946, precision: 0.9306, recall: 1.0\n",
      "2019-01-17T21:05:04.854021, step: 393, loss: 0.12493869662284851, acc: 0.9609, auc: 0.9946, precision: 0.9412, recall: 0.9846\n",
      "2019-01-17T21:05:05.581458, step: 394, loss: 0.09124819189310074, acc: 0.9844, auc: 0.997, precision: 0.9863, recall: 0.9863\n",
      "2019-01-17T21:05:06.284146, step: 395, loss: 0.14956733584403992, acc: 0.9531, auc: 0.9894, precision: 0.9492, recall: 0.9492\n",
      "2019-01-17T21:05:06.992816, step: 396, loss: 0.19562800228595734, acc: 0.9375, auc: 0.9712, precision: 0.9583, recall: 0.9324\n",
      "2019-01-17T21:05:07.720530, step: 397, loss: 0.13781970739364624, acc: 0.9297, auc: 0.9921, precision: 0.9839, recall: 0.8841\n",
      "2019-01-17T21:05:08.437178, step: 398, loss: 0.12221449613571167, acc: 0.9453, auc: 0.9985, precision: 1.0, recall: 0.8923\n",
      "2019-01-17T21:05:09.166673, step: 399, loss: 0.09927740693092346, acc: 0.9609, auc: 0.9971, precision: 0.9524, recall: 0.9677\n",
      "2019-01-17T21:05:09.861633, step: 400, loss: 0.08617295324802399, acc: 0.9766, auc: 0.9936, precision: 0.9718, recall: 0.9857\n",
      "\n",
      "Evaluation:\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "\n",
    "# 生成训练集和验证集\n",
    "trainReviews = data.trainReviews\n",
    "trainLabels = data.trainLabels\n",
    "evalReviews = data.evalReviews\n",
    "evalLabels = data.evalLabels\n",
    "\n",
    "wordEmbedding = data.wordEmbedding\n",
    "\n",
    "# 定义计算图\n",
    "with tf.Graph().as_default():\n",
    "\n",
    "    session_conf = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
    "    session_conf.gpu_options.allow_growth=True\n",
    "    session_conf.gpu_options.per_process_gpu_memory_fraction = 0.9  # 配置gpu占用率  \n",
    "\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    \n",
    "    # 定义会话\n",
    "    with sess.as_default():\n",
    "        lstm = BiLSTMAttention(config, wordEmbedding)\n",
    "        \n",
    "        globalStep = tf.Variable(0, name=\"globalStep\", trainable=False)\n",
    "        # 定义优化函数，传入学习速率参数\n",
    "        optimizer = tf.train.AdamOptimizer(config.training.learningRate)\n",
    "        # 计算梯度,得到梯度和变量\n",
    "        gradsAndVars = optimizer.compute_gradients(lstm.loss)\n",
    "        # 将梯度应用到变量下，生成训练器\n",
    "        trainOp = optimizer.apply_gradients(gradsAndVars, global_step=globalStep)\n",
    "        \n",
    "        # 用summary绘制tensorBoard\n",
    "        gradSummaries = []\n",
    "        for g, v in gradsAndVars:\n",
    "            if g is not None:\n",
    "                tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "        \n",
    "        outDir = os.path.abspath(os.path.join(os.path.curdir, \"summarys\"))\n",
    "        print(\"Writing to {}\\n\".format(outDir))\n",
    "        \n",
    "        lossSummary = tf.summary.scalar(\"loss\", lstm.loss)\n",
    "        summaryOp = tf.summary.merge_all()\n",
    "        \n",
    "        trainSummaryDir = os.path.join(outDir, \"train\")\n",
    "        trainSummaryWriter = tf.summary.FileWriter(trainSummaryDir, sess.graph)\n",
    "        \n",
    "        evalSummaryDir = os.path.join(outDir, \"eval\")\n",
    "        evalSummaryWriter = tf.summary.FileWriter(evalSummaryDir, sess.graph)\n",
    "        \n",
    "        \n",
    "        # 初始化所有变量\n",
    "#         saver = tf.train.Saver(tf.global_variables(), max_to_keep=5)\n",
    "        \n",
    "        # 保存模型的一种方式，保存为pb文件\n",
    "#         builder = tf.saved_model.builder.SavedModelBuilder(\"../model/Bi-LSTM/savedModel\")\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        def trainStep(batchX, batchY):\n",
    "            \"\"\"\n",
    "            训练函数\n",
    "            \"\"\"   \n",
    "            feed_dict = {\n",
    "              lstm.inputX: batchX,\n",
    "              lstm.inputY: batchY,\n",
    "              lstm.dropoutKeepProb: config.model.dropoutKeepProb\n",
    "            }\n",
    "            _, summary, step, loss, predictions, binaryPreds = sess.run(\n",
    "                [trainOp, summaryOp, globalStep, lstm.loss, lstm.predictions, lstm.binaryPreds],\n",
    "                feed_dict)\n",
    "            timeStr = datetime.datetime.now().isoformat()\n",
    "            acc, auc, precision, recall = genMetrics(batchY, predictions, binaryPreds)\n",
    "            print(\"{}, step: {}, loss: {}, acc: {}, auc: {}, precision: {}, recall: {}\".format(timeStr, step, loss, acc, auc, precision, recall))\n",
    "            trainSummaryWriter.add_summary(summary, step)\n",
    "\n",
    "        def devStep(batchX, batchY):\n",
    "            \"\"\"\n",
    "            验证函数\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              lstm.inputX: batchX,\n",
    "              lstm.inputY: batchY,\n",
    "              lstm.dropoutKeepProb: 1.0\n",
    "            }\n",
    "            summary, step, loss, predictions, binaryPreds = sess.run(\n",
    "                [summaryOp, globalStep, lstm.loss, lstm.predictions, lstm.binaryPreds],\n",
    "                feed_dict)\n",
    "            \n",
    "            acc, auc, precision, recall = genMetrics(batchY, predictions, binaryPreds)\n",
    "            \n",
    "            evalSummaryWriter.add_summary(summary, step)\n",
    "            \n",
    "            return loss, acc, auc, precision, recall\n",
    "        \n",
    "        for i in range(config.training.epoches):\n",
    "            # 训练模型\n",
    "            print(\"start training model\")\n",
    "            for batchTrain in nextBatch(trainReviews, trainLabels, config.batchSize):\n",
    "                trainStep(batchTrain[0], batchTrain[1])\n",
    "\n",
    "                currentStep = tf.train.global_step(sess, globalStep) \n",
    "                if currentStep % config.training.evaluateEvery == 0:\n",
    "                    print(\"\\nEvaluation:\")\n",
    "                    \n",
    "                    losses = []\n",
    "                    accs = []\n",
    "                    aucs = []\n",
    "                    precisions = []\n",
    "                    recalls = []\n",
    "                    \n",
    "                    for batchEval in nextBatch(evalReviews, evalLabels, config.batchSize):\n",
    "                        loss, acc, auc, precision, recall = devStep(batchEval[0], batchEval[1])\n",
    "                        losses.append(loss)\n",
    "                        accs.append(acc)\n",
    "                        aucs.append(auc)\n",
    "                        precisions.append(precision)\n",
    "                        recalls.append(recall)\n",
    "                        \n",
    "                    time_str = datetime.datetime.now().isoformat()\n",
    "                    print(\"{}, step: {}, loss: {}, acc: {}, auc: {}, precision: {}, recall: {}\".format(time_str, currentStep, mean(losses), \n",
    "                                                                                                       mean(accs), mean(aucs), mean(precisions),\n",
    "                                                                                                       mean(recalls)))\n",
    "                    \n",
    "#                 if currentStep % config.training.checkpointEvery == 0:\n",
    "#                     # 保存模型的另一种方法，保存checkpoint文件\n",
    "#                     path = saver.save(sess, \"../model/Bi-LSTM/model/my-model\", global_step=currentStep)\n",
    "#                     print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "                    \n",
    "#         inputs = {\"inputX\": tf.saved_model.utils.build_tensor_info(lstm.inputX),\n",
    "#                   \"keepProb\": tf.saved_model.utils.build_tensor_info(lstm.dropoutKeepProb)}\n",
    "\n",
    "#         outputs = {\"binaryPreds\": tf.saved_model.utils.build_tensor_info(lstm.binaryPreds)}\n",
    "\n",
    "#         prediction_signature = tf.saved_model.signature_def_utils.build_signature_def(inputs=inputs, outputs=outputs,\n",
    "#                                                                                       method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME)\n",
    "#         legacy_init_op = tf.group(tf.tables_initializer(), name=\"legacy_init_op\")\n",
    "#         builder.add_meta_graph_and_variables(sess, [tf.saved_model.tag_constants.SERVING],\n",
    "#                                             signature_def_map={\"predict\": prediction_signature}, legacy_init_op=legacy_init_op)\n",
    "\n",
    "#         builder.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
