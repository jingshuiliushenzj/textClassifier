{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import json\n",
    "\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from math import sqrt\n",
    "\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置参数\n",
    "\n",
    "class TrainingConfig(object):\n",
    "    epoches = 5\n",
    "    evaluateEvery = 100\n",
    "    checkpointEvery = 100\n",
    "    learningRate = 0.001\n",
    "    \n",
    "class ModelConfig(object):\n",
    "    embeddingSize = 200\n",
    "    \n",
    "    hiddenSizes = [128]  # LSTM结构的神经元个数\n",
    "    \n",
    "    dropoutKeepProb = 0.5\n",
    "    l2RegLambda = 0.0\n",
    "    \n",
    "    outputSize = 128  # 从高维映射到低维的神经元个数\n",
    "    \n",
    "class Config(object):\n",
    "    sequenceLength = 200  # 取了所有序列长度的均值\n",
    "    batchSize = 128\n",
    "    \n",
    "    dataSource = \"../data/preProcess/labeledTrain.csv\"\n",
    "    \n",
    "    stopWordSource = \"../data/english\"\n",
    "    \n",
    "    numClasses = 2\n",
    "    \n",
    "    rate = 0.8  # 训练集的比例\n",
    "    \n",
    "    training = TrainingConfig()\n",
    "    \n",
    "    model = ModelConfig()\n",
    "\n",
    "    \n",
    "# 实例化配置参数对象\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据预处理的类，生成训练集和测试集\n",
    "\n",
    "class Dataset(object):\n",
    "    def __init__(self, config):\n",
    "        self._dataSource = config.dataSource\n",
    "        self._stopWordSource = config.stopWordSource  \n",
    "        \n",
    "        self._sequenceLength = config.sequenceLength  # 每条输入的序列处理为定长\n",
    "        self._embeddingSize = config.model.embeddingSize\n",
    "        self._batchSize = config.batchSize\n",
    "        self._rate = config.rate\n",
    "        \n",
    "        self._stopWordDict = {}\n",
    "        \n",
    "        self.trainReviews = []\n",
    "        self.trainLabels = []\n",
    "        \n",
    "        self.evalReviews = []\n",
    "        self.evalLabels = []\n",
    "        \n",
    "        self.wordEmbedding =None\n",
    "        \n",
    "        self._wordToIndex = {}\n",
    "        self._indexToWord = {}\n",
    "        \n",
    "    def _readData(self, filePath):\n",
    "        \"\"\"\n",
    "        从csv文件中读取数据集\n",
    "        \"\"\"\n",
    "        \n",
    "        df = pd.read_csv(filePath)\n",
    "        labels = df[\"sentiment\"].tolist()\n",
    "        review = df[\"review\"].tolist()\n",
    "        reviews = [line.strip().split() for line in review]\n",
    "\n",
    "        return reviews, labels\n",
    "\n",
    "    def _reviewProcess(self, review, sequenceLength, wordToIndex):\n",
    "        \"\"\"\n",
    "        将数据集中的每条评论用index表示\n",
    "        wordToIndex中“pad”对应的index为0\n",
    "        \"\"\"\n",
    "        \n",
    "        reviewVec = np.zeros((sequenceLength))\n",
    "        sequenceLen = sequenceLength\n",
    "        \n",
    "        # 判断当前的序列是否小于定义的固定序列长度\n",
    "        if len(review) < sequenceLength:\n",
    "            sequenceLen = len(review)\n",
    "            \n",
    "        for i in range(sequenceLen):\n",
    "            if review[i] in wordToIndex:\n",
    "                reviewVec[i] = wordToIndex[review[i]]\n",
    "            else:\n",
    "                reviewVec[i] = wordToIndex[\"UNK\"]\n",
    "\n",
    "        return reviewVec\n",
    "\n",
    "    def _genTrainEvalData(self, x, y, rate):\n",
    "        \"\"\"\n",
    "        生成训练集和验证集\n",
    "        \"\"\"\n",
    "        \n",
    "        reviews = []\n",
    "        labels = []\n",
    "        \n",
    "        # 遍历所有的文本，将文本中的词转换成index表示\n",
    "        for i in range(len(x)):\n",
    "            reviewVec = self._reviewProcess(x[i], self._sequenceLength, self._wordToIndex)\n",
    "            reviews.append(reviewVec)\n",
    "            \n",
    "            labels.append([y[i]])\n",
    "            \n",
    "        trainIndex = int(len(x) * rate)\n",
    "        \n",
    "        trainReviews = np.asarray(reviews[:trainIndex], dtype=\"int64\")\n",
    "        trainLabels = np.array(labels[:trainIndex], dtype=\"float32\")\n",
    "        \n",
    "        evalReviews = np.asarray(reviews[trainIndex:], dtype=\"int64\")\n",
    "        evalLabels = np.array(labels[trainIndex:], dtype=\"float32\")\n",
    "\n",
    "        return trainReviews, trainLabels, evalReviews, evalLabels\n",
    "        \n",
    "    def _genVocabulary(self, reviews):\n",
    "        \"\"\"\n",
    "        生成词向量和词汇-索引映射字典，可以用全数据集\n",
    "        \"\"\"\n",
    "        \n",
    "        allWords = [word for review in reviews for word in review]\n",
    "        \n",
    "        # 去掉停用词\n",
    "        subWords = [word for word in allWords if word not in self.stopWordDict]\n",
    "        \n",
    "        wordCount = Counter(subWords)  # 统计词频\n",
    "        sortWordCount = sorted(wordCount.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # 去除低频词\n",
    "        words = [item[0] for item in sortWordCount if item[1] >= 5]\n",
    "        \n",
    "        vocab, wordEmbedding = self._getWordEmbedding(words)\n",
    "        self.wordEmbedding = wordEmbedding\n",
    "        \n",
    "        self._wordToIndex = dict(zip(vocab, list(range(len(vocab)))))\n",
    "        self._indexToWord = dict(zip(list(range(len(vocab))), vocab))\n",
    "        \n",
    "        # 将词汇-索引映射表保存为json数据，之后做inference时直接加载来处理数据\n",
    "        with open(\"../data/wordJson/wordToIndex.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self._wordToIndex, f)\n",
    "        \n",
    "        with open(\"../data/wordJson/indexToWord.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self._indexToWord, f)\n",
    "            \n",
    "    def _getWordEmbedding(self, words):\n",
    "        \"\"\"\n",
    "        按照我们的数据集中的单词取出预训练好的word2vec中的词向量\n",
    "        \"\"\"\n",
    "        \n",
    "        wordVec = gensim.models.KeyedVectors.load_word2vec_format(\"../word2vec/word2Vec.bin\", binary=True)\n",
    "        vocab = []\n",
    "        wordEmbedding = []\n",
    "        \n",
    "        # 添加 \"pad\" 和 \"UNK\", \n",
    "        vocab.append(\"pad\")\n",
    "        vocab.append(\"UNK\")\n",
    "        wordEmbedding.append(np.zeros(self._embeddingSize))\n",
    "        wordEmbedding.append(np.random.randn(self._embeddingSize))\n",
    "        \n",
    "        for word in words:\n",
    "            try:\n",
    "                vector = wordVec.wv[word]\n",
    "                vocab.append(word)\n",
    "                wordEmbedding.append(vector)\n",
    "            except:\n",
    "                print(word + \"不存在于词向量中\")\n",
    "                \n",
    "        return vocab, np.array(wordEmbedding)\n",
    "    \n",
    "    def _readStopWord(self, stopWordPath):\n",
    "        \"\"\"\n",
    "        读取停用词\n",
    "        \"\"\"\n",
    "        \n",
    "        with open(stopWordPath, \"r\") as f:\n",
    "            stopWords = f.read()\n",
    "            stopWordList = stopWords.splitlines()\n",
    "            # 将停用词用列表的形式生成，之后查找停用词时会比较快\n",
    "            self.stopWordDict = dict(zip(stopWordList, list(range(len(stopWordList)))))\n",
    "            \n",
    "    def dataGen(self):\n",
    "        \"\"\"\n",
    "        初始化训练集和验证集\n",
    "        \"\"\"\n",
    "        \n",
    "        # 初始化停用词\n",
    "        self._readStopWord(self._stopWordSource)\n",
    "        \n",
    "        # 初始化数据集\n",
    "        reviews, labels = self._readData(self._dataSource)\n",
    "        \n",
    "        # 初始化词汇-索引映射表和词向量矩阵\n",
    "        self._genVocabulary(reviews)\n",
    "        \n",
    "        # 初始化训练集和测试集\n",
    "        trainReviews, trainLabels, evalReviews, evalLabels = self._genTrainEvalData(reviews, labels, self._rate)\n",
    "        self.trainReviews = trainReviews\n",
    "        self.trainLabels = trainLabels\n",
    "        \n",
    "        self.evalReviews = evalReviews\n",
    "        self.evalLabels = evalLabels\n",
    "        \n",
    "        \n",
    "data = Dataset(config)\n",
    "data.dataGen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape: (20000, 200)\n",
      "train label shape: (20000, 1)\n",
      "eval data shape: (5000, 200)\n"
     ]
    }
   ],
   "source": [
    "print(\"train data shape: {}\".format(data.trainReviews.shape))\n",
    "print(\"train label shape: {}\".format(data.trainLabels.shape))\n",
    "print(\"eval data shape: {}\".format(data.evalReviews.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出batch数据集\n",
    "\n",
    "def nextBatch(x, y, batchSize):\n",
    "        \"\"\"\n",
    "        生成batch数据集，用生成器的方式输出\n",
    "        \"\"\"\n",
    "    \n",
    "        perm = np.arange(len(x))\n",
    "        np.random.shuffle(perm)\n",
    "        x = x[perm]\n",
    "        y = y[perm]\n",
    "        \n",
    "        numBatches = len(x) // batchSize\n",
    "\n",
    "        for i in range(numBatches):\n",
    "            start = i * batchSize\n",
    "            end = start + batchSize\n",
    "            batchX = np.array(x[start: end], dtype=\"int64\")\n",
    "            batchY = np.array(y[start: end], dtype=\"float32\")\n",
    "            \n",
    "            yield batchX, batchY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "构建模型，模型的架构如下：\n",
    "1，利用Bi-LSTM获得上下文的信息\n",
    "2，将Bi-LSTM获得的隐层输出和词向量拼接[fwOutput;wordEmbedding;bwOutput]\n",
    "3，将2所得的词表示映射到低维\n",
    "4，hidden_size上每个位置的值都取时间步上最大的值，类似于max-pool\n",
    "5，softmax分类\n",
    "\"\"\"\n",
    "\n",
    "class RCNN(object):\n",
    "    \"\"\"\n",
    "    RCNN 用于文本分类\n",
    "    \"\"\"\n",
    "    def __init__(self, config, wordEmbedding):\n",
    "\n",
    "        # 定义模型的输入\n",
    "        self.inputX = tf.placeholder(tf.int32, [None, config.sequenceLength], name=\"inputX\")\n",
    "        self.inputY = tf.placeholder(tf.float32, [None, 1], name=\"inputY\")\n",
    "        \n",
    "        self.dropoutKeepProb = tf.placeholder(tf.float32, name=\"dropoutKeepProb\")\n",
    "        \n",
    "        # 定义l2损失\n",
    "        l2Loss = tf.constant(0.0)\n",
    "        \n",
    "        # 词嵌入层\n",
    "        with tf.name_scope(\"embedding\"):\n",
    "\n",
    "            # 利用预训练的词向量初始化词嵌入矩阵\n",
    "            self.W = tf.Variable(tf.cast(wordEmbedding, dtype=tf.float32, name=\"word2vec\") ,name=\"W\")\n",
    "            # 利用词嵌入矩阵将输入的数据中的词转换成词向量，维度[batch_size, sequence_length, embedding_size]\n",
    "            self.embeddedWords = tf.nn.embedding_lookup(self.W, self.inputX)\n",
    "            \n",
    "        # 定义两层双向LSTM的模型结构\n",
    "\n",
    "        with tf.name_scope(\"Bi-LSTM\"):\n",
    "            fwHiddenLayers = []\n",
    "            bwHiddenLayers = []\n",
    "            for idx, hiddenSize in enumerate(config.model.hiddenSizes):\n",
    "\n",
    "                with tf.name_scope(\"Bi-LSTM-\" + str(idx)):\n",
    "                    # 定义前向LSTM结构\n",
    "                    lstmFwCell = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(num_units=hiddenSize, state_is_tuple=True),\n",
    "                                                                 output_keep_prob=self.dropoutKeepProb)\n",
    "                    # 定义反向LSTM结构\n",
    "                    lstmBwCell = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(num_units=hiddenSize, state_is_tuple=True),\n",
    "                                                                 output_keep_prob=self.dropoutKeepProb)\n",
    "\n",
    "                fwHiddenLayers.append(lstmFwCell)\n",
    "                bwHiddenLayers.append(lstmBwCell)\n",
    "\n",
    "            # 实现多层的LSTM结构， state_is_tuple=True，则状态会以元祖的形式组合(h, c)，否则列向拼接\n",
    "            fwMultiLstm = tf.nn.rnn_cell.MultiRNNCell(cells=fwHiddenLayers, state_is_tuple=True)\n",
    "            bwMultiLstm = tf.nn.rnn_cell.MultiRNNCell(cells=bwHiddenLayers, state_is_tuple=True)\n",
    "\n",
    "            # 采用动态rnn，可以动态的输入序列的长度，若没有输入，则取序列的全长\n",
    "            # outputs是一个元祖(output_fw, output_bw)，其中两个元素的维度都是[batch_size, max_time, hidden_size],fw和bw的hidden_size一样\n",
    "            # self.current_state 是最终的状态，二元组(state_fw, state_bw)，state_fw=[batch_size, s]，s是一个元祖(h, c)\n",
    "            outputs, self.current_state = tf.nn.bidirectional_dynamic_rnn(fwMultiLstm, bwMultiLstm, self.embeddedWords, dtype=tf.float32)\n",
    "            fwOutput, bwOutput = outputs\n",
    "            \n",
    "        with tf.name_scope(\"context\"):\n",
    "            shape = [tf.shape(fwOutput)[0], 1, tf.shape(fwOutput)[2]]\n",
    "            self.contextLeft = tf.concat([tf.zeros(shape), fwOutput[:, :-1]], axis=1, name=\"contextLeft\")\n",
    "            self.contextRight = tf.concat([bwOutput[:, 1:], tf.zeros(shape)], axis=1, name=\"contextRight\")\n",
    "            \n",
    "        # 将前向，后向的输出和最早的词向量拼接在一起得到最终的词表征\n",
    "        with tf.name_scope(\"wordRepresentation\"):\n",
    "            self.wordRepre = tf.concat([self.contextLeft, self.embeddedWords, self.contextRight], axis=2)\n",
    "            wordSize = config.model.hiddenSizes[-1] * 2 + config.model.embeddingSize \n",
    "        \n",
    "        with tf.name_scope(\"textRepresentation\"):\n",
    "            outputSize = config.model.outputSize\n",
    "            textW = tf.Variable(tf.random_uniform([wordSize, outputSize], -1.0, 1.0), name=\"W2\")\n",
    "            textB = tf.Variable(tf.constant(0.1, shape=[outputSize]), name=\"b2\")\n",
    "            \n",
    "            # tf.einsum可以指定维度的消除运算\n",
    "            self.textRepre = tf.tanh(tf.einsum('aij,jk->aik', self.wordRepre, textW) + textB)\n",
    "            \n",
    "        # 做max-pool的操作，将时间步的维度消失\n",
    "        output = tf.reduce_max(self.textRepre, axis=1)\n",
    "        \n",
    "        # 全连接层的输出\n",
    "        with tf.name_scope(\"output\"):\n",
    "            outputW = tf.get_variable(\n",
    "                \"outputW\",\n",
    "                shape=[outputSize, 1],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "            outputB= tf.Variable(tf.constant(0.1, shape=[1]), name=\"outputB\")\n",
    "            l2Loss += tf.nn.l2_loss(outputW)\n",
    "            l2Loss += tf.nn.l2_loss(outputB)\n",
    "            self.predictions = tf.nn.xw_plus_b(output, outputW, outputB, name=\"predictions\")\n",
    "            self.binaryPreds = tf.cast(tf.greater_equal(self.predictions, 0.5), tf.float32, name=\"binaryPreds\")\n",
    "        \n",
    "        # 计算二元交叉熵损失\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            \n",
    "            losses = tf.nn.sigmoid_cross_entropy_with_logits(logits=self.predictions, labels=self.inputY)\n",
    "            self.loss = tf.reduce_mean(losses) + config.model.l2RegLambda * l2Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义性能指标函数\n",
    "\n",
    "def mean(item):\n",
    "    return sum(item) / len(item)\n",
    "\n",
    "\n",
    "def genMetrics(trueY, predY, binaryPredY):\n",
    "    \"\"\"\n",
    "    生成acc和auc值\n",
    "    \"\"\"\n",
    "    auc = roc_auc_score(trueY, predY)\n",
    "    accuracy = accuracy_score(trueY, binaryPredY)\n",
    "    precision = precision_score(trueY, binaryPredY)\n",
    "    recall = recall_score(trueY, binaryPredY)\n",
    "    \n",
    "    return round(accuracy, 4), round(auc, 4), round(precision, 4), round(recall, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name bidirectional_rnn/fw/multi_rnn_cell/cell_0/lstm_cell/kernel:0/grad/hist is illegal; using bidirectional_rnn/fw/multi_rnn_cell/cell_0/lstm_cell/kernel_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name bidirectional_rnn/fw/multi_rnn_cell/cell_0/lstm_cell/kernel:0/grad/sparsity is illegal; using bidirectional_rnn/fw/multi_rnn_cell/cell_0/lstm_cell/kernel_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name bidirectional_rnn/fw/multi_rnn_cell/cell_0/lstm_cell/bias:0/grad/hist is illegal; using bidirectional_rnn/fw/multi_rnn_cell/cell_0/lstm_cell/bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name bidirectional_rnn/fw/multi_rnn_cell/cell_0/lstm_cell/bias:0/grad/sparsity is illegal; using bidirectional_rnn/fw/multi_rnn_cell/cell_0/lstm_cell/bias_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name bidirectional_rnn/bw/multi_rnn_cell/cell_0/lstm_cell/kernel:0/grad/hist is illegal; using bidirectional_rnn/bw/multi_rnn_cell/cell_0/lstm_cell/kernel_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name bidirectional_rnn/bw/multi_rnn_cell/cell_0/lstm_cell/kernel:0/grad/sparsity is illegal; using bidirectional_rnn/bw/multi_rnn_cell/cell_0/lstm_cell/kernel_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name bidirectional_rnn/bw/multi_rnn_cell/cell_0/lstm_cell/bias:0/grad/hist is illegal; using bidirectional_rnn/bw/multi_rnn_cell/cell_0/lstm_cell/bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name bidirectional_rnn/bw/multi_rnn_cell/cell_0/lstm_cell/bias:0/grad/sparsity is illegal; using bidirectional_rnn/bw/multi_rnn_cell/cell_0/lstm_cell/bias_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name textRepresentation/W2:0/grad/hist is illegal; using textRepresentation/W2_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name textRepresentation/W2:0/grad/sparsity is illegal; using textRepresentation/W2_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name textRepresentation/b2:0/grad/hist is illegal; using textRepresentation/b2_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name textRepresentation/b2:0/grad/sparsity is illegal; using textRepresentation/b2_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name outputW:0/grad/hist is illegal; using outputW_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name outputW:0/grad/sparsity is illegal; using outputW_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/outputB:0/grad/hist is illegal; using output/outputB_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/outputB:0/grad/sparsity is illegal; using output/outputB_0/grad/sparsity instead.\n",
      "Writing to /data4T/share/jiangxinyang848/textClassifier/RCNN/summarys\n",
      "\n",
      "start training model\n",
      "2018-12-28T16:14:46.648474, step: 1, loss: 1.500516414642334, acc: 0.4375, auc: 0.5154, precision: 0.4375, recall: 1.0\n",
      "2018-12-28T16:14:47.056449, step: 2, loss: 1.5817852020263672, acc: 0.375, auc: 0.5475, precision: 0.375, recall: 1.0\n",
      "2018-12-28T16:14:47.449372, step: 3, loss: 1.156528353691101, acc: 0.5156, auc: 0.5501, precision: 0.5156, recall: 1.0\n",
      "2018-12-28T16:14:47.873562, step: 4, loss: 1.0561031103134155, acc: 0.5312, auc: 0.4972, precision: 0.5312, recall: 1.0\n",
      "2018-12-28T16:14:48.289202, step: 5, loss: 1.074438214302063, acc: 0.4922, auc: 0.5437, precision: 0.4922, recall: 1.0\n",
      "2018-12-28T16:14:48.679241, step: 6, loss: 0.9744560718536377, acc: 0.5312, auc: 0.4809, precision: 0.5312, recall: 1.0\n",
      "2018-12-28T16:14:49.101060, step: 7, loss: 0.9960308074951172, acc: 0.4922, auc: 0.4357, precision: 0.4922, recall: 1.0\n",
      "2018-12-28T16:14:49.465338, step: 8, loss: 1.0010440349578857, acc: 0.4609, auc: 0.4453, precision: 0.4609, recall: 1.0\n",
      "2018-12-28T16:14:49.847270, step: 9, loss: 0.8470257520675659, acc: 0.5312, auc: 0.5196, precision: 0.5312, recall: 1.0\n",
      "2018-12-28T16:14:50.236763, step: 10, loss: 0.7336037158966064, acc: 0.5859, auc: 0.4738, precision: 0.5859, recall: 1.0\n",
      "2018-12-28T16:14:50.614599, step: 11, loss: 0.8141654133796692, acc: 0.4766, auc: 0.425, precision: 0.4766, recall: 1.0\n",
      "2018-12-28T16:14:51.009772, step: 12, loss: 0.7952684760093689, acc: 0.4688, auc: 0.5463, precision: 0.4688, recall: 1.0\n",
      "2018-12-28T16:14:51.439482, step: 13, loss: 0.7934751510620117, acc: 0.4375, auc: 0.4934, precision: 0.4375, recall: 1.0\n",
      "2018-12-28T16:14:51.890870, step: 14, loss: 0.7281407713890076, acc: 0.5234, auc: 0.5259, precision: 0.5159, recall: 1.0\n",
      "2018-12-28T16:14:52.289211, step: 15, loss: 0.7366560697555542, acc: 0.5312, auc: 0.4564, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:14:52.678383, step: 16, loss: 0.6936843991279602, acc: 0.4609, auc: 0.4882, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:14:53.074548, step: 17, loss: 0.7009863257408142, acc: 0.4844, auc: 0.4335, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:14:53.479986, step: 18, loss: 0.6976292133331299, acc: 0.4766, auc: 0.439, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:14:53.875968, step: 19, loss: 0.6907132863998413, acc: 0.5, auc: 0.5114, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:14:54.285778, step: 20, loss: 0.6928348541259766, acc: 0.625, auc: 0.5548, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:14:54.678143, step: 21, loss: 0.7003390789031982, acc: 0.4766, auc: 0.4, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:14:55.067831, step: 22, loss: 0.7078036069869995, acc: 0.4609, auc: 0.4657, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:14:55.455279, step: 23, loss: 0.7131733894348145, acc: 0.4453, auc: 0.4198, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:14:55.871034, step: 24, loss: 0.6984578371047974, acc: 0.4844, auc: 0.529, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:14:56.291526, step: 25, loss: 0.6870852708816528, acc: 0.5469, auc: 0.523, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:14:56.717717, step: 26, loss: 0.6932896375656128, acc: 0.5312, auc: 0.4352, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:14:57.115514, step: 27, loss: 0.711235761642456, acc: 0.4766, auc: 0.4364, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:14:57.466621, step: 28, loss: 0.7104988098144531, acc: 0.4844, auc: 0.5191, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:14:57.877928, step: 29, loss: 0.7166624069213867, acc: 0.4688, auc: 0.5238, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:14:58.272453, step: 30, loss: 0.7015894651412964, acc: 0.5156, auc: 0.4715, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:14:58.633197, step: 31, loss: 0.6913557648658752, acc: 0.5469, auc: 0.5074, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:14:58.993439, step: 32, loss: 0.7015600204467773, acc: 0.5156, auc: 0.4632, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:14:59.382826, step: 33, loss: 0.7213686108589172, acc: 0.4531, auc: 0.5635, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:14:59.795317, step: 34, loss: 0.7465195655822754, acc: 0.3672, auc: 0.4246, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:00.189089, step: 35, loss: 0.7123045921325684, acc: 0.4688, auc: 0.6447, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:00.598676, step: 36, loss: 0.6840094327926636, acc: 0.5625, auc: 0.5327, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:01.016302, step: 37, loss: 0.7220156192779541, acc: 0.4062, auc: 0.4644, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:01.433853, step: 38, loss: 0.6887611150741577, acc: 0.5469, auc: 0.4764, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:01.815957, step: 39, loss: 0.6873985528945923, acc: 0.5547, auc: 0.45, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:02.240946, step: 40, loss: 0.7002428770065308, acc: 0.4688, auc: 0.5656, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:02.658990, step: 41, loss: 0.6966565847396851, acc: 0.4844, auc: 0.4456, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:03.057942, step: 42, loss: 0.6940878629684448, acc: 0.5, auc: 0.4672, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:03.442998, step: 43, loss: 0.6958698034286499, acc: 0.4609, auc: 0.5176, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:03.797004, step: 44, loss: 0.6927729845046997, acc: 0.5234, auc: 0.4601, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:04.174324, step: 45, loss: 0.6931639313697815, acc: 0.5, auc: 0.4781, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:04.582682, step: 46, loss: 0.6938485503196716, acc: 0.5391, auc: 0.4108, precision: 0.0, recall: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-28T16:15:04.971202, step: 47, loss: 0.6929523944854736, acc: 0.4922, auc: 0.5355, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:05.383545, step: 48, loss: 0.6917513608932495, acc: 0.4922, auc: 0.5342, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:05.767563, step: 49, loss: 0.6976913213729858, acc: 0.5625, auc: 0.4441, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:06.175974, step: 50, loss: 0.6926724314689636, acc: 0.4844, auc: 0.513, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:06.564988, step: 51, loss: 0.699394941329956, acc: 0.5703, auc: 0.5416, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:06.956706, step: 52, loss: 0.6993716955184937, acc: 0.5703, auc: 0.5239, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:07.362967, step: 53, loss: 0.6948812007904053, acc: 0.5156, auc: 0.4768, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:07.744026, step: 54, loss: 0.6936355829238892, acc: 0.5, auc: 0.571, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:08.124826, step: 55, loss: 0.6956313848495483, acc: 0.5391, auc: 0.4702, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:08.564724, step: 56, loss: 0.6933587789535522, acc: 0.5, auc: 0.6302, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:08.982595, step: 57, loss: 0.6926333904266357, acc: 0.4844, auc: 0.5473, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:09.383682, step: 58, loss: 0.6943265199661255, acc: 0.5547, auc: 0.4451, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:09.795098, step: 59, loss: 0.6931540966033936, acc: 0.4922, auc: 0.4541, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:10.227349, step: 60, loss: 0.693172812461853, acc: 0.5078, auc: 0.453, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:10.668480, step: 61, loss: 0.6937459707260132, acc: 0.4688, auc: 0.5246, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:11.096743, step: 62, loss: 0.6936248540878296, acc: 0.4922, auc: 0.5392, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:11.495721, step: 63, loss: 0.6950681209564209, acc: 0.4375, auc: 0.5098, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:11.896313, step: 64, loss: 0.6914820075035095, acc: 0.5625, auc: 0.489, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:12.280791, step: 65, loss: 0.6958376169204712, acc: 0.4219, auc: 0.3834, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:12.661278, step: 66, loss: 0.6925795078277588, acc: 0.5234, auc: 0.4695, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:13.043532, step: 67, loss: 0.69489586353302, acc: 0.4375, auc: 0.4768, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:13.447966, step: 68, loss: 0.6930413246154785, acc: 0.5078, auc: 0.5281, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:13.833317, step: 69, loss: 0.6919649243354797, acc: 0.5781, auc: 0.5109, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:14.205122, step: 70, loss: 0.6927844285964966, acc: 0.5312, auc: 0.4243, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:14.586930, step: 71, loss: 0.6934797167778015, acc: 0.4844, auc: 0.4919, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:14.953335, step: 72, loss: 0.6927650570869446, acc: 0.5234, auc: 0.5623, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:15.360464, step: 73, loss: 0.6936490535736084, acc: 0.4766, auc: 0.4895, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:15.768878, step: 74, loss: 0.6944711804389954, acc: 0.4375, auc: 0.5766, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:16.149192, step: 75, loss: 0.692389726638794, acc: 0.5547, auc: 0.4637, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:16.544287, step: 76, loss: 0.6936700344085693, acc: 0.4688, auc: 0.5085, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:16.955817, step: 77, loss: 0.692929744720459, acc: 0.5234, auc: 0.4923, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:17.416436, step: 78, loss: 0.6932807564735413, acc: 0.4844, auc: 0.5265, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:17.842271, step: 79, loss: 0.6934255361557007, acc: 0.4375, auc: 0.4919, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:18.254006, step: 80, loss: 0.6931702494621277, acc: 0.5078, auc: 0.481, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:18.666244, step: 81, loss: 0.6933304071426392, acc: 0.5156, auc: 0.6315, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:19.040033, step: 82, loss: 0.6930878758430481, acc: 0.4922, auc: 0.5172, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:19.391840, step: 83, loss: 0.6946704387664795, acc: 0.5703, auc: 0.5895, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:19.787380, step: 84, loss: 0.6937255859375, acc: 0.5312, auc: 0.4141, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:20.186947, step: 85, loss: 0.6928480863571167, acc: 0.4766, auc: 0.4497, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:20.593872, step: 86, loss: 0.6928995847702026, acc: 0.4688, auc: 0.4243, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:20.977938, step: 87, loss: 0.6934272050857544, acc: 0.5469, auc: 0.5879, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:21.376266, step: 88, loss: 0.6932414174079895, acc: 0.5234, auc: 0.4722, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:21.767085, step: 89, loss: 0.6932247877120972, acc: 0.4766, auc: 0.3999, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:22.200106, step: 90, loss: 0.6933130025863647, acc: 0.4844, auc: 0.4432, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:22.664237, step: 91, loss: 0.6934463977813721, acc: 0.4688, auc: 0.4594, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:23.080327, step: 92, loss: 0.693255603313446, acc: 0.4844, auc: 0.469, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:23.498532, step: 93, loss: 0.6929356455802917, acc: 0.5547, auc: 0.5766, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:23.899839, step: 94, loss: 0.6933714747428894, acc: 0.4609, auc: 0.5879, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:24.288598, step: 95, loss: 0.693310022354126, acc: 0.4688, auc: 0.5141, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:24.656060, step: 96, loss: 0.6931389570236206, acc: 0.4922, auc: 0.4499, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:25.043336, step: 97, loss: 0.6932052969932556, acc: 0.5078, auc: 0.4984, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:25.456893, step: 98, loss: 0.6935811042785645, acc: 0.5234, auc: 0.5098, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:25.854436, step: 99, loss: 0.693539023399353, acc: 0.5391, auc: 0.417, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:26.264862, step: 100, loss: 0.6937447786331177, acc: 0.5859, auc: 0.5408, precision: 0.0, recall: 0.0\n",
      "\n",
      "Evaluation:\n",
      "2018-12-28T16:15:41.575785, step: 100, loss: 0.7006655915787345, acc: 0.4938289473684209, auc: 0.4981736842105261, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:41.951329, step: 101, loss: 0.692837655544281, acc: 0.5625, auc: 0.5069, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:42.355599, step: 102, loss: 0.693365216255188, acc: 0.4922, auc: 0.5376, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:42.806768, step: 103, loss: 0.693821370601654, acc: 0.4844, auc: 0.5506, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:43.214502, step: 104, loss: 0.6928092837333679, acc: 0.5156, auc: 0.5111, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:43.595990, step: 105, loss: 0.6972987055778503, acc: 0.4297, auc: 0.5133, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:43.980686, step: 106, loss: 0.6969040632247925, acc: 0.4375, auc: 0.4804, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:44.361422, step: 107, loss: 0.6942814588546753, acc: 0.4844, auc: 0.5005, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:44.716597, step: 108, loss: 0.6912751197814941, acc: 0.5547, auc: 0.475, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:45.095693, step: 109, loss: 0.6968932151794434, acc: 0.3906, auc: 0.6044, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:45.480306, step: 110, loss: 0.6937463283538818, acc: 0.4688, auc: 0.4936, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:45.850667, step: 111, loss: 0.6931384801864624, acc: 0.4531, auc: 0.4898, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:46.287124, step: 112, loss: 0.6909676790237427, acc: 0.3984, auc: 0.4665, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:46.770134, step: 113, loss: 0.6915163397789001, acc: 0.4609, auc: 0.4679, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:47.192950, step: 114, loss: 0.6944998502731323, acc: 0.5078, auc: 0.5432, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:47.611244, step: 115, loss: 0.6960503458976746, acc: 0.5156, auc: 0.525, precision: 0.0, recall: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-28T16:15:48.020024, step: 116, loss: 0.6930278539657593, acc: 0.4844, auc: 0.4635, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:48.424290, step: 117, loss: 0.6860237121582031, acc: 0.4375, auc: 0.4901, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:48.810438, step: 118, loss: 0.6977503895759583, acc: 0.5078, auc: 0.491, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:49.199376, step: 119, loss: 0.6894234418869019, acc: 0.4531, auc: 0.4953, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:49.572671, step: 120, loss: 0.6965242624282837, acc: 0.5, auc: 0.5466, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:49.958755, step: 121, loss: 0.7061136960983276, acc: 0.5547, auc: 0.539, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:50.357681, step: 122, loss: 0.7016212940216064, acc: 0.5312, auc: 0.5115, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:50.748059, step: 123, loss: 0.702599048614502, acc: 0.5469, auc: 0.4772, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:51.138703, step: 124, loss: 0.69191575050354, acc: 0.4766, auc: 0.4993, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:51.599173, step: 125, loss: 0.6942501664161682, acc: 0.4922, auc: 0.5457, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:51.994375, step: 126, loss: 0.6945167779922485, acc: 0.5, auc: 0.472, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:52.405733, step: 127, loss: 0.693801760673523, acc: 0.5078, auc: 0.4726, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:52.812431, step: 128, loss: 0.6980012655258179, acc: 0.5781, auc: 0.4489, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:53.196557, step: 129, loss: 0.6915249824523926, acc: 0.4453, auc: 0.6704, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:53.603043, step: 130, loss: 0.6932761669158936, acc: 0.5, auc: 0.5098, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:53.964260, step: 131, loss: 0.6931257247924805, acc: 0.5078, auc: 0.5604, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:54.352174, step: 132, loss: 0.6932723522186279, acc: 0.4922, auc: 0.5431, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:54.772016, step: 133, loss: 0.6916686296463013, acc: 0.5469, auc: 0.518, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:55.169356, step: 134, loss: 0.6917117834091187, acc: 0.5547, auc: 0.5646, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:55.584228, step: 135, loss: 0.6945385932922363, acc: 0.4766, auc: 0.5116, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:56.004808, step: 136, loss: 0.6885207891464233, acc: 0.5312, auc: 0.5696, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:56.432076, step: 137, loss: 0.6943440437316895, acc: 0.4531, auc: 0.5507, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:56.832054, step: 138, loss: 0.691404402256012, acc: 0.5, auc: 0.5474, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:57.231125, step: 139, loss: 0.6953624486923218, acc: 0.5391, auc: 0.4272, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:57.644413, step: 140, loss: 0.6902300119400024, acc: 0.4922, auc: 0.6098, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:58.054149, step: 141, loss: 0.6956802606582642, acc: 0.4531, auc: 0.5419, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:58.461211, step: 142, loss: 0.690619707107544, acc: 0.6016, auc: 0.5477, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:58.874469, step: 143, loss: 0.6933785676956177, acc: 0.4922, auc: 0.5661, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:59.229383, step: 144, loss: 0.6893362402915955, acc: 0.5938, auc: 0.5124, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:59.613090, step: 145, loss: 0.6931974291801453, acc: 0.5, auc: 0.5447, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:15:59.984198, step: 146, loss: 0.6811380982398987, acc: 0.5781, auc: 0.5938, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:00.429297, step: 147, loss: 0.6932554841041565, acc: 0.5, auc: 0.5771, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:00.838353, step: 148, loss: 0.7035362720489502, acc: 0.4375, auc: 0.6079, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:01.290372, step: 149, loss: 0.6928793787956238, acc: 0.5156, auc: 0.4905, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:01.695705, step: 150, loss: 0.6869804859161377, acc: 0.5312, auc: 0.615, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:02.095486, step: 151, loss: 0.6890943050384521, acc: 0.5625, auc: 0.4563, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:02.495822, step: 152, loss: 0.6974394917488098, acc: 0.4766, auc: 0.5546, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:02.893374, step: 153, loss: 0.6833703517913818, acc: 0.5391, auc: 0.6512, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:03.307268, step: 154, loss: 0.7010563611984253, acc: 0.4609, auc: 0.4657, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:03.713970, step: 155, loss: 0.6926496624946594, acc: 0.4375, auc: 0.6334, precision: 0.0, recall: 0.0\n",
      "start training model\n",
      "2018-12-28T16:16:04.135249, step: 156, loss: 0.6976920366287231, acc: 0.5078, auc: 0.4212, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:04.532793, step: 157, loss: 0.6912753582000732, acc: 0.4844, auc: 0.5555, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:04.945045, step: 158, loss: 0.6947644948959351, acc: 0.5625, auc: 0.6166, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:05.370128, step: 159, loss: 0.6948280930519104, acc: 0.5078, auc: 0.4515, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:05.782418, step: 160, loss: 0.6933096051216125, acc: 0.4922, auc: 0.5493, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:06.140380, step: 161, loss: 0.6957899332046509, acc: 0.5391, auc: 0.4687, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:06.517618, step: 162, loss: 0.6956135034561157, acc: 0.5312, auc: 0.4632, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:06.903697, step: 163, loss: 0.6909089684486389, acc: 0.4609, auc: 0.4848, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:07.322167, step: 164, loss: 0.6952206492424011, acc: 0.5391, auc: 0.4352, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:07.731275, step: 165, loss: 0.6941499710083008, acc: 0.5156, auc: 0.4844, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:08.151941, step: 166, loss: 0.6897692680358887, acc: 0.4062, auc: 0.4682, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:08.569702, step: 167, loss: 0.6926732063293457, acc: 0.4844, auc: 0.5919, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:08.986108, step: 168, loss: 0.6964446902275085, acc: 0.5938, auc: 0.5536, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:09.392141, step: 169, loss: 0.6920458674430847, acc: 0.4688, auc: 0.4523, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:09.820003, step: 170, loss: 0.6923055648803711, acc: 0.4688, auc: 0.4728, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:10.221731, step: 171, loss: 0.693596363067627, acc: 0.5312, auc: 0.4037, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:10.607885, step: 172, loss: 0.6931918263435364, acc: 0.4922, auc: 0.4236, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:11.004739, step: 173, loss: 0.6930965185165405, acc: 0.5469, auc: 0.5543, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:11.430971, step: 174, loss: 0.6943268775939941, acc: 0.4062, auc: 0.5851, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:11.853645, step: 175, loss: 0.6923893690109253, acc: 0.5938, auc: 0.5261, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:12.240960, step: 176, loss: 0.69338458776474, acc: 0.4688, auc: 0.515, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:12.630789, step: 177, loss: 0.6914739608764648, acc: 0.5625, auc: 0.4467, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:13.018877, step: 178, loss: 0.692999005317688, acc: 0.5078, auc: 0.5261, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:13.409826, step: 179, loss: 0.6912163496017456, acc: 0.5234, auc: 0.5602, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:13.845531, step: 180, loss: 0.6929993033409119, acc: 0.5, auc: 0.5105, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:14.263189, step: 181, loss: 0.691729724407196, acc: 0.5234, auc: 0.4434, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:14.675768, step: 182, loss: 0.6906692981719971, acc: 0.5391, auc: 0.5246, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:15.071336, step: 183, loss: 0.6860268115997314, acc: 0.5938, auc: 0.5259, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:15.476237, step: 184, loss: 0.6906725764274597, acc: 0.5234, auc: 0.4697, precision: 0.0, recall: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-28T16:16:15.897013, step: 185, loss: 0.6982711553573608, acc: 0.4688, auc: 0.4955, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:16.289376, step: 186, loss: 0.6976273059844971, acc: 0.4766, auc: 0.5049, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:16.691428, step: 187, loss: 0.6880168914794922, acc: 0.5547, auc: 0.5516, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:17.095153, step: 188, loss: 0.7034684419631958, acc: 0.4297, auc: 0.4666, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:17.492774, step: 189, loss: 0.6853783130645752, acc: 0.5781, auc: 0.5683, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:17.922889, step: 190, loss: 0.693905770778656, acc: 0.5, auc: 0.516, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:18.324669, step: 191, loss: 0.6956160664558411, acc: 0.4844, auc: 0.4949, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:18.710665, step: 192, loss: 0.6902813911437988, acc: 0.5391, auc: 0.4778, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:19.106655, step: 193, loss: 0.6919229030609131, acc: 0.5156, auc: 0.6447, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:19.496899, step: 194, loss: 0.6916053891181946, acc: 0.5312, auc: 0.5058, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:19.955993, step: 195, loss: 0.6912407279014587, acc: 0.5391, auc: 0.5352, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:20.383037, step: 196, loss: 0.6985176205635071, acc: 0.4297, auc: 0.3984, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:20.791913, step: 197, loss: 0.693920373916626, acc: 0.4922, auc: 0.5149, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:21.219904, step: 198, loss: 0.6930238604545593, acc: 0.5391, auc: 0.5122, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:21.623390, step: 199, loss: 0.6955883502960205, acc: 0.4609, auc: 0.5279, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:22.028490, step: 200, loss: 0.6933586597442627, acc: 0.4844, auc: 0.5448, precision: 0.0, recall: 0.0\n",
      "\n",
      "Evaluation:\n",
      "2018-12-28T16:16:37.161098, step: 200, loss: 0.6983354640634436, acc: 0.4952736842105263, auc: 0.4979684210526316, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:37.571873, step: 201, loss: 0.692322850227356, acc: 0.5469, auc: 0.5567, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:37.973249, step: 202, loss: 0.691163957118988, acc: 0.4453, auc: 0.582, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:38.338299, step: 203, loss: 0.6927592158317566, acc: 0.5234, auc: 0.4742, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:38.718286, step: 204, loss: 0.6946387887001038, acc: 0.5156, auc: 0.5989, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:39.124393, step: 205, loss: 0.6889832019805908, acc: 0.4375, auc: 0.5735, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:39.510725, step: 206, loss: 0.6940168142318726, acc: 0.5547, auc: 0.6219, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:39.893778, step: 207, loss: 0.6912183165550232, acc: 0.5078, auc: 0.5967, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:40.312751, step: 208, loss: 0.6947509050369263, acc: 0.5391, auc: 0.5986, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:40.704447, step: 209, loss: 0.6914222240447998, acc: 0.4766, auc: 0.6266, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:41.095219, step: 210, loss: 0.6961137056350708, acc: 0.5391, auc: 0.4428, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:41.471133, step: 211, loss: 0.6915523409843445, acc: 0.5156, auc: 0.4565, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:41.856771, step: 212, loss: 0.691739022731781, acc: 0.5078, auc: 0.4794, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:42.243706, step: 213, loss: 0.6912073493003845, acc: 0.5391, auc: 0.5528, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:42.639653, step: 214, loss: 0.692352294921875, acc: 0.5547, auc: 0.5256, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:43.035395, step: 215, loss: 0.6910239458084106, acc: 0.5234, auc: 0.5341, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:43.433452, step: 216, loss: 0.6908279061317444, acc: 0.4688, auc: 0.6206, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:43.843115, step: 217, loss: 0.6978800296783447, acc: 0.4609, auc: 0.5222, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:44.298665, step: 218, loss: 0.7040214538574219, acc: 0.3828, auc: 0.6006, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:44.715682, step: 219, loss: 0.6917029023170471, acc: 0.4844, auc: 0.5657, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:45.159893, step: 220, loss: 0.6864557266235352, acc: 0.5625, auc: 0.5818, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:45.566751, step: 221, loss: 0.6914222836494446, acc: 0.4531, auc: 0.5749, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:45.995923, step: 222, loss: 0.6945876479148865, acc: 0.4844, auc: 0.4902, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:46.394541, step: 223, loss: 0.6916188597679138, acc: 0.4141, auc: 0.527, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:46.809486, step: 224, loss: 0.6845943927764893, acc: 0.4766, auc: 0.6871, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:47.214823, step: 225, loss: 0.6929017305374146, acc: 0.5078, auc: 0.5995, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:47.640617, step: 226, loss: 0.6943820714950562, acc: 0.5234, auc: 0.5836, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:48.055486, step: 227, loss: 0.6914726495742798, acc: 0.5, auc: 0.5974, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:48.436926, step: 228, loss: 0.6904729604721069, acc: 0.4922, auc: 0.5597, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:48.817520, step: 229, loss: 0.6861859560012817, acc: 0.4609, auc: 0.5856, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:49.213523, step: 230, loss: 0.692116916179657, acc: 0.5391, auc: 0.6362, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:49.670951, step: 231, loss: 0.6899293661117554, acc: 0.5156, auc: 0.6188, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:50.121463, step: 232, loss: 0.6929194927215576, acc: 0.5469, auc: 0.58, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:50.522717, step: 233, loss: 0.6874064207077026, acc: 0.5469, auc: 0.6722, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:50.912410, step: 234, loss: 0.6856563687324524, acc: 0.5703, auc: 0.6852, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:51.309581, step: 235, loss: 0.6904017925262451, acc: 0.4922, auc: 0.5685, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:51.705278, step: 236, loss: 0.6877666711807251, acc: 0.5391, auc: 0.6458, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:52.113532, step: 237, loss: 0.6855267882347107, acc: 0.5469, auc: 0.5739, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:52.525796, step: 238, loss: 0.6816648244857788, acc: 0.5859, auc: 0.6314, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:52.949784, step: 239, loss: 0.6960312128067017, acc: 0.4688, auc: 0.6429, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:53.377079, step: 240, loss: 0.7017294764518738, acc: 0.4688, auc: 0.4922, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:53.799291, step: 241, loss: 0.6825336217880249, acc: 0.5312, auc: 0.6417, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:54.225775, step: 242, loss: 0.6853353381156921, acc: 0.5391, auc: 0.6057, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:54.637388, step: 243, loss: 0.6817028522491455, acc: 0.5234, auc: 0.6929, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:55.041555, step: 244, loss: 0.6801397800445557, acc: 0.5703, auc: 0.6381, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:55.411393, step: 245, loss: 0.6837465167045593, acc: 0.5156, auc: 0.6535, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:55.781504, step: 246, loss: 0.6830357313156128, acc: 0.5078, auc: 0.6884, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:56.186594, step: 247, loss: 0.6997782588005066, acc: 0.4062, auc: 0.6546, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:56.598000, step: 248, loss: 0.6933075785636902, acc: 0.4688, auc: 0.601, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:57.001568, step: 249, loss: 0.6802628040313721, acc: 0.5625, auc: 0.6625, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:57.461914, step: 250, loss: 0.6873486638069153, acc: 0.4609, auc: 0.6573, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:57.841333, step: 251, loss: 0.6850345134735107, acc: 0.4375, auc: 0.6895, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:58.238784, step: 252, loss: 0.6819925308227539, acc: 0.4609, auc: 0.7296, precision: 0.0, recall: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-28T16:16:58.675464, step: 253, loss: 0.6751733422279358, acc: 0.4766, auc: 0.7487, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:59.087036, step: 254, loss: 0.6791950464248657, acc: 0.5391, auc: 0.7696, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:59.546517, step: 255, loss: 0.6841363310813904, acc: 0.4844, auc: 0.6068, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:16:59.978120, step: 256, loss: 0.6834651827812195, acc: 0.4766, auc: 0.6565, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:17:00.360912, step: 257, loss: 0.6902270913124084, acc: 0.5781, auc: 0.5766, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:17:00.776083, step: 258, loss: 0.6883634924888611, acc: 0.5703, auc: 0.6682, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:17:01.183456, step: 259, loss: 0.6876323223114014, acc: 0.5391, auc: 0.6008, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:17:01.576849, step: 260, loss: 0.6919277310371399, acc: 0.4375, auc: 0.5851, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:17:01.965977, step: 261, loss: 0.6902967095375061, acc: 0.5078, auc: 0.6371, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:17:02.379063, step: 262, loss: 0.6882584095001221, acc: 0.5625, auc: 0.5975, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:17:02.765319, step: 263, loss: 0.6913777589797974, acc: 0.5, auc: 0.6245, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:17:03.153240, step: 264, loss: 0.6888152360916138, acc: 0.4844, auc: 0.5973, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:17:03.553232, step: 265, loss: 0.6908148527145386, acc: 0.4688, auc: 0.6686, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:17:03.948219, step: 266, loss: 0.6792172789573669, acc: 0.5781, auc: 0.7625, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:17:04.361865, step: 267, loss: 0.6810550093650818, acc: 0.5156, auc: 0.674, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:17:04.798102, step: 268, loss: 0.6738471984863281, acc: 0.4453, auc: 0.7361, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:17:05.227238, step: 269, loss: 0.6812913417816162, acc: 0.4844, auc: 0.6733, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:17:05.646295, step: 270, loss: 0.6838517785072327, acc: 0.4688, auc: 0.6257, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:17:06.083021, step: 271, loss: 0.6756126284599304, acc: 0.4688, auc: 0.7248, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:17:06.484345, step: 272, loss: 0.6872994899749756, acc: 0.5234, auc: 0.6844, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:17:06.895682, step: 273, loss: 0.673324704170227, acc: 0.4375, auc: 0.7245, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:17:07.339385, step: 274, loss: 0.6775223016738892, acc: 0.4453, auc: 0.6625, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:17:07.738101, step: 275, loss: 0.6734236478805542, acc: 0.4375, auc: 0.7584, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:17:08.171832, step: 276, loss: 0.681732714176178, acc: 0.5156, auc: 0.6532, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:17:08.622645, step: 277, loss: 0.6735988855361938, acc: 0.5234, auc: 0.749, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:17:09.020828, step: 278, loss: 0.6804375648498535, acc: 0.5469, auc: 0.6916, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:17:09.417911, step: 279, loss: 0.6786800622940063, acc: 0.4609, auc: 0.7485, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:17:09.819033, step: 280, loss: 0.6859703063964844, acc: 0.4453, auc: 0.6738, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:17:10.261608, step: 281, loss: 0.6705072522163391, acc: 0.4688, auc: 0.8105, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:17:10.685564, step: 282, loss: 0.6813075542449951, acc: 0.4922, auc: 0.6916, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:17:11.090595, step: 283, loss: 0.6734879016876221, acc: 0.5156, auc: 0.7473, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:17:11.486103, step: 284, loss: 0.679853081703186, acc: 0.5391, auc: 0.7347, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:17:11.910383, step: 285, loss: 0.6771471500396729, acc: 0.5, auc: 0.7241, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:17:12.338074, step: 286, loss: 0.6728716492652893, acc: 0.4844, auc: 0.7314, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:17:12.727668, step: 287, loss: 0.6749210953712463, acc: 0.4766, auc: 0.6858, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:17:13.119927, step: 288, loss: 0.6661630272865295, acc: 0.4766, auc: 0.7901, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:17:13.536917, step: 289, loss: 0.6649592518806458, acc: 0.5, auc: 0.7993, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:17:13.914354, step: 290, loss: 0.6702648401260376, acc: 0.4922, auc: 0.7541, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:17:14.337752, step: 291, loss: 0.6670627593994141, acc: 0.5, auc: 0.7839, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:17:14.757315, step: 292, loss: 0.6708396077156067, acc: 0.4766, auc: 0.7587, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:17:15.169472, step: 293, loss: 0.673653781414032, acc: 0.5234, auc: 0.7047, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:17:15.561382, step: 294, loss: 0.669223427772522, acc: 0.4531, auc: 0.733, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:17:15.967759, step: 295, loss: 0.6731514930725098, acc: 0.5703, auc: 0.7778, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:17:16.385056, step: 296, loss: 0.6641478538513184, acc: 0.4922, auc: 0.7668, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:17:16.765922, step: 297, loss: 0.6621736884117126, acc: 0.5156, auc: 0.7898, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:17:17.166496, step: 298, loss: 0.667167067527771, acc: 0.5078, auc: 0.7668, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:17:17.609799, step: 299, loss: 0.6664347648620605, acc: 0.5391, auc: 0.7585, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:17:18.010015, step: 300, loss: 0.6765498518943787, acc: 0.4609, auc: 0.7453, precision: 0.0, recall: 0.0\n",
      "\n",
      "Evaluation:\n",
      "2018-12-28T16:17:33.584103, step: 300, loss: 0.6647515375363199, acc: 0.4950631578947369, auc: 0.7370394736842104, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:17:33.956327, step: 301, loss: 0.6772840023040771, acc: 0.4062, auc: 0.8345, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:17:34.336497, step: 302, loss: 0.6560078859329224, acc: 0.4297, auc: 0.8366, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:17:34.710728, step: 303, loss: 0.664088249206543, acc: 0.5781, auc: 0.8178, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:17:35.124986, step: 304, loss: 0.6527352333068848, acc: 0.375, auc: 0.7878, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:17:35.556166, step: 305, loss: 0.6724274158477783, acc: 0.4922, auc: 0.6906, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:17:35.966545, step: 306, loss: 0.6516186594963074, acc: 0.5234, auc: 0.9065, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:17:36.367214, step: 307, loss: 0.6566656827926636, acc: 0.5547, auc: 0.8127, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:17:36.768515, step: 308, loss: 0.6414410471916199, acc: 0.4297, auc: 0.8827, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:17:37.160557, step: 309, loss: 0.6647338271141052, acc: 0.4766, auc: 0.7375, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:17:37.549876, step: 310, loss: 0.6600198149681091, acc: 0.4844, auc: 0.8416, precision: 0.0, recall: 0.0\n",
      "start training model\n",
      "2018-12-28T16:17:37.979081, step: 311, loss: 0.6613234281539917, acc: 0.5391, auc: 0.7757, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:17:38.369325, step: 312, loss: 0.655364990234375, acc: 0.6016, auc: 0.8322, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:17:38.796689, step: 313, loss: 0.6504692435264587, acc: 0.5156, auc: 0.839, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:17:39.184190, step: 314, loss: 0.6493281126022339, acc: 0.4922, auc: 0.8303, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:17:39.599795, step: 315, loss: 0.6482934951782227, acc: 0.5547, auc: 0.8342, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:17:39.996942, step: 316, loss: 0.6463280320167542, acc: 0.5078, auc: 0.8129, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:17:40.389778, step: 317, loss: 0.6354877948760986, acc: 0.625, auc: 0.8318, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:17:40.797204, step: 318, loss: 0.6532659530639648, acc: 0.4375, auc: 0.8051, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:17:41.164813, step: 319, loss: 0.6325032711029053, acc: 0.5625, auc: 0.8358, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:17:41.554827, step: 320, loss: 0.6357443928718567, acc: 0.4922, auc: 0.842, precision: 0.0, recall: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-28T16:17:41.972832, step: 321, loss: 0.6333403587341309, acc: 0.5469, auc: 0.8328, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:17:42.379525, step: 322, loss: 0.6351828575134277, acc: 0.5469, auc: 0.7978, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:17:42.778954, step: 323, loss: 0.6750363111495972, acc: 0.4609, auc: 0.7033, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:17:43.195568, step: 324, loss: 0.6370224952697754, acc: 0.5078, auc: 0.8552, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:17:43.630258, step: 325, loss: 0.6349721550941467, acc: 0.5234, auc: 0.8263, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:17:44.040438, step: 326, loss: 0.6451938152313232, acc: 0.5469, auc: 0.8158, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:17:44.423336, step: 327, loss: 0.649198055267334, acc: 0.4453, auc: 0.806, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:17:44.859943, step: 328, loss: 0.6412972807884216, acc: 0.5234, auc: 0.8077, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:17:45.272943, step: 329, loss: 0.6225714087486267, acc: 0.5625, auc: 0.8795, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:17:45.691126, step: 330, loss: 0.6339151859283447, acc: 0.4922, auc: 0.8733, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:17:46.096729, step: 331, loss: 0.6344367861747742, acc: 0.4922, auc: 0.8703, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:17:46.453374, step: 332, loss: 0.6279363632202148, acc: 0.4844, auc: 0.8568, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:17:46.877010, step: 333, loss: 0.631304144859314, acc: 0.4922, auc: 0.8545, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:17:47.291087, step: 334, loss: 0.6239141225814819, acc: 0.5, auc: 0.8862, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:17:47.728844, step: 335, loss: 0.6314016580581665, acc: 0.4844, auc: 0.8148, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:17:48.165844, step: 336, loss: 0.6213648915290833, acc: 0.5859, auc: 0.8589, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:17:48.593009, step: 337, loss: 0.6090156435966492, acc: 0.4453, auc: 0.8923, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:17:48.996681, step: 338, loss: 0.617360532283783, acc: 0.4688, auc: 0.8544, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:17:49.370868, step: 339, loss: 0.5986065864562988, acc: 0.5312, auc: 0.8917, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:17:49.755090, step: 340, loss: 0.5808604955673218, acc: 0.4688, auc: 0.8868, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:17:50.161649, step: 341, loss: 0.6174605488777161, acc: 0.4375, auc: 0.7919, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:17:50.569228, step: 342, loss: 0.6297048330307007, acc: 0.5234, auc: 0.7751, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:17:50.983752, step: 343, loss: 0.5939391851425171, acc: 0.5078, auc: 0.8557, precision: 0.0, recall: 0.0\n",
      "2018-12-28T16:17:51.431264, step: 344, loss: 0.5748428106307983, acc: 0.6094, auc: 0.891, precision: 0.8889, recall: 0.338\n",
      "2018-12-28T16:17:51.859482, step: 345, loss: 0.5883292555809021, acc: 0.5938, auc: 0.8103, precision: 0.7727, recall: 0.2656\n",
      "2018-12-28T16:17:52.268444, step: 346, loss: 0.5972112417221069, acc: 0.625, auc: 0.7795, precision: 0.7778, recall: 0.3333\n",
      "2018-12-28T16:17:52.716073, step: 347, loss: 0.6005197763442993, acc: 0.6406, auc: 0.8115, precision: 0.9355, recall: 0.3973\n",
      "2018-12-28T16:17:53.122195, step: 348, loss: 0.6201184391975403, acc: 0.6328, auc: 0.7446, precision: 0.7568, recall: 0.4242\n",
      "2018-12-28T16:17:53.542239, step: 349, loss: 0.5917149186134338, acc: 0.7734, auc: 0.832, precision: 0.9, recall: 0.6522\n",
      "2018-12-28T16:17:53.931669, step: 350, loss: 0.5894870758056641, acc: 0.7422, auc: 0.8273, precision: 0.7258, recall: 0.7377\n",
      "2018-12-28T16:17:54.360556, step: 351, loss: 0.6166889667510986, acc: 0.7266, auc: 0.8116, precision: 0.7069, recall: 0.6949\n",
      "2018-12-28T16:17:54.780593, step: 352, loss: 0.5998860001564026, acc: 0.7734, auc: 0.809, precision: 0.8837, recall: 0.6129\n",
      "2018-12-28T16:17:55.188789, step: 353, loss: 0.6050190925598145, acc: 0.6562, auc: 0.7873, precision: 0.8387, recall: 0.4\n",
      "2018-12-28T16:17:55.584235, step: 354, loss: 0.6330707669258118, acc: 0.5312, auc: 0.7897, precision: 0.9444, recall: 0.2237\n",
      "2018-12-28T16:17:55.968732, step: 355, loss: 0.5926817059516907, acc: 0.6406, auc: 0.8416, precision: 0.8846, recall: 0.3485\n",
      "2018-12-28T16:17:56.356117, step: 356, loss: 0.5918390154838562, acc: 0.7031, auc: 0.8193, precision: 0.75, recall: 0.4444\n",
      "2018-12-28T16:17:56.776603, step: 357, loss: 0.5910530090332031, acc: 0.7734, auc: 0.858, precision: 0.7778, recall: 0.7119\n",
      "2018-12-28T16:17:57.200616, step: 358, loss: 0.5577983856201172, acc: 0.8281, auc: 0.9132, precision: 0.9091, recall: 0.7895\n",
      "2018-12-28T16:17:57.632215, step: 359, loss: 0.5894794464111328, acc: 0.7422, auc: 0.8585, precision: 0.9231, recall: 0.5455\n",
      "2018-12-28T16:17:58.046432, step: 360, loss: 0.609133243560791, acc: 0.6719, auc: 0.7764, precision: 0.7895, recall: 0.4688\n",
      "2018-12-28T16:17:58.423313, step: 361, loss: 0.599948525428772, acc: 0.6562, auc: 0.7712, precision: 0.913, recall: 0.3333\n",
      "2018-12-28T16:17:58.839585, step: 362, loss: 0.6009300351142883, acc: 0.625, auc: 0.8145, precision: 0.9545, recall: 0.3088\n",
      "2018-12-28T16:17:59.259928, step: 363, loss: 0.6110765933990479, acc: 0.6719, auc: 0.7673, precision: 0.9565, recall: 0.3492\n",
      "2018-12-28T16:17:59.680307, step: 364, loss: 0.6142786145210266, acc: 0.6328, auc: 0.7812, precision: 0.9, recall: 0.2857\n",
      "2018-12-28T16:18:00.104350, step: 365, loss: 0.6102052927017212, acc: 0.7109, auc: 0.8018, precision: 0.8077, recall: 0.3962\n",
      "2018-12-28T16:18:00.533134, step: 366, loss: 0.5915757417678833, acc: 0.6094, auc: 0.8358, precision: 0.9286, recall: 0.3514\n",
      "2018-12-28T16:18:00.943102, step: 367, loss: 0.6321108341217041, acc: 0.7734, auc: 0.8234, precision: 0.7833, recall: 0.746\n",
      "2018-12-28T16:18:01.328878, step: 368, loss: 0.5651891827583313, acc: 0.8203, auc: 0.9068, precision: 0.9286, recall: 0.7324\n",
      "2018-12-28T16:18:01.724241, step: 369, loss: 0.590843677520752, acc: 0.7656, auc: 0.8504, precision: 0.825, recall: 0.5893\n",
      "2018-12-28T16:18:02.132306, step: 370, loss: 0.5796401500701904, acc: 0.7188, auc: 0.847, precision: 0.8611, recall: 0.5\n",
      "2018-12-28T16:18:02.555934, step: 371, loss: 0.5657150745391846, acc: 0.7812, auc: 0.8926, precision: 0.8947, recall: 0.5862\n",
      "2018-12-28T16:18:02.970134, step: 372, loss: 0.5696890950202942, acc: 0.7031, auc: 0.8692, precision: 0.9, recall: 0.5143\n",
      "2018-12-28T16:18:03.419584, step: 373, loss: 0.5503939986228943, acc: 0.6953, auc: 0.8958, precision: 0.9565, recall: 0.3667\n",
      "2018-12-28T16:18:03.868958, step: 374, loss: 0.5689388513565063, acc: 0.6094, auc: 0.8468, precision: 0.8462, recall: 0.3235\n",
      "2018-12-28T16:18:04.279789, step: 375, loss: 0.5596787929534912, acc: 0.7656, auc: 0.8975, precision: 0.8438, recall: 0.5192\n",
      "2018-12-28T16:18:04.674039, step: 376, loss: 0.5411669015884399, acc: 0.5781, auc: 0.9261, precision: 1.0, recall: 0.1148\n",
      "2018-12-28T16:18:05.075681, step: 377, loss: 0.5568251609802246, acc: 0.5469, auc: 0.8621, precision: 0.5, recall: 0.0345\n",
      "2018-12-28T16:18:05.475764, step: 378, loss: 0.5589050650596619, acc: 0.4922, auc: 0.848, precision: 1.0, recall: 0.0441\n",
      "2018-12-28T16:18:05.906081, step: 379, loss: 0.5445897579193115, acc: 0.4531, auc: 0.9191, precision: 1.0, recall: 0.0411\n",
      "2018-12-28T16:18:06.327428, step: 380, loss: 0.5323703289031982, acc: 0.5391, auc: 0.8966, precision: 1.0, recall: 0.0484\n",
      "2018-12-28T16:18:06.738598, step: 381, loss: 0.5081552267074585, acc: 0.5469, auc: 0.9254, precision: 1.0, recall: 0.0492\n",
      "2018-12-28T16:18:07.143681, step: 382, loss: 0.5366343259811401, acc: 0.5859, auc: 0.8476, precision: 1.0, recall: 0.0364\n",
      "2018-12-28T16:18:07.537931, step: 383, loss: 0.5942452549934387, acc: 0.3672, auc: 0.8921, precision: 1.0, recall: 0.0241\n",
      "2018-12-28T16:18:07.956752, step: 384, loss: 0.5089583396911621, acc: 0.7031, auc: 0.8911, precision: 0.931, recall: 0.4286\n",
      "2018-12-28T16:18:08.350899, step: 385, loss: 0.5045380592346191, acc: 0.8281, auc: 0.8867, precision: 0.9038, recall: 0.7344\n",
      "2018-12-28T16:18:08.754310, step: 386, loss: 0.5050116777420044, acc: 0.8125, auc: 0.8801, precision: 0.8431, recall: 0.7288\n",
      "2018-12-28T16:18:09.222182, step: 387, loss: 0.5021986961364746, acc: 0.8125, auc: 0.8882, precision: 0.86, recall: 0.7167\n",
      "2018-12-28T16:18:09.629978, step: 388, loss: 0.49596530199050903, acc: 0.7578, auc: 0.8966, precision: 0.8462, recall: 0.569\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-28T16:18:10.029484, step: 389, loss: 0.47857344150543213, acc: 0.8438, auc: 0.9229, precision: 0.9767, recall: 0.6885\n",
      "2018-12-28T16:18:10.428891, step: 390, loss: 0.5006482601165771, acc: 0.8203, auc: 0.9001, precision: 0.9762, recall: 0.6508\n",
      "2018-12-28T16:18:10.826082, step: 391, loss: 0.5040305256843567, acc: 0.8047, auc: 0.9155, precision: 0.85, recall: 0.6415\n",
      "2018-12-28T16:18:11.213971, step: 392, loss: 0.5332952737808228, acc: 0.7734, auc: 0.8524, precision: 0.8409, recall: 0.6271\n",
      "2018-12-28T16:18:11.638734, step: 393, loss: 0.5097752213478088, acc: 0.7578, auc: 0.8831, precision: 0.9231, recall: 0.5625\n",
      "2018-12-28T16:18:12.077022, step: 394, loss: 0.504897952079773, acc: 0.7891, auc: 0.8721, precision: 0.9206, recall: 0.725\n",
      "2018-12-28T16:18:12.471665, step: 395, loss: 0.5161001682281494, acc: 0.7891, auc: 0.8869, precision: 0.8654, recall: 0.6923\n",
      "2018-12-28T16:18:12.865681, step: 396, loss: 0.47923311591148376, acc: 0.875, auc: 0.9152, precision: 0.8667, recall: 0.8667\n",
      "2018-12-28T16:18:13.255456, step: 397, loss: 0.4850805699825287, acc: 0.8203, auc: 0.8717, precision: 0.8676, recall: 0.8082\n",
      "2018-12-28T16:18:13.631366, step: 398, loss: 0.5454404354095459, acc: 0.7656, auc: 0.8164, precision: 0.7447, recall: 0.6604\n",
      "2018-12-28T16:18:14.036906, step: 399, loss: 0.45835888385772705, acc: 0.8672, auc: 0.9056, precision: 0.8929, recall: 0.8197\n",
      "2018-12-28T16:18:14.419530, step: 400, loss: 0.5299868583679199, acc: 0.7656, auc: 0.8354, precision: 0.8, recall: 0.6984\n",
      "\n",
      "Evaluation:\n",
      "2018-12-28T16:18:29.830146, step: 400, loss: 0.4766534786475332, acc: 0.8129157894736845, auc: 0.8700605263157897, precision: 0.8350973684210526, recall: 0.7845605263157892\n",
      "2018-12-28T16:18:30.239376, step: 401, loss: 0.49031853675842285, acc: 0.8359, auc: 0.8821, precision: 0.8095, recall: 0.85\n",
      "2018-12-28T16:18:30.650980, step: 402, loss: 0.4796811044216156, acc: 0.8203, auc: 0.8685, precision: 0.8028, recall: 0.8636\n",
      "2018-12-28T16:18:31.065148, step: 403, loss: 0.4930247962474823, acc: 0.8359, auc: 0.8702, precision: 0.8421, recall: 0.8767\n",
      "2018-12-28T16:18:31.497742, step: 404, loss: 0.5143347382545471, acc: 0.8125, auc: 0.8401, precision: 0.8382, recall: 0.8143\n",
      "2018-12-28T16:18:31.905085, step: 405, loss: 0.49060797691345215, acc: 0.7969, auc: 0.8575, precision: 0.8772, recall: 0.7246\n",
      "2018-12-28T16:18:32.335639, step: 406, loss: 0.4806498885154724, acc: 0.7812, auc: 0.8749, precision: 0.8864, recall: 0.629\n",
      "2018-12-28T16:18:32.750858, step: 407, loss: 0.4629344046115875, acc: 0.7578, auc: 0.9009, precision: 0.8824, recall: 0.5263\n",
      "2018-12-28T16:18:33.147005, step: 408, loss: 0.44936931133270264, acc: 0.8125, auc: 0.9133, precision: 0.9268, recall: 0.6441\n",
      "2018-12-28T16:18:33.573502, step: 409, loss: 0.4201722741127014, acc: 0.8516, auc: 0.9342, precision: 0.9508, recall: 0.7838\n",
      "2018-12-28T16:18:34.007372, step: 410, loss: 0.43062418699264526, acc: 0.875, auc: 0.9204, precision: 0.8889, recall: 0.8615\n",
      "2018-12-28T16:18:34.448568, step: 411, loss: 0.45074671506881714, acc: 0.8438, auc: 0.917, precision: 0.8, recall: 0.9231\n",
      "2018-12-28T16:18:34.877228, step: 412, loss: 0.47155576944351196, acc: 0.8594, auc: 0.9082, precision: 0.8256, recall: 0.9595\n",
      "2018-12-28T16:18:35.304037, step: 413, loss: 0.5046917796134949, acc: 0.7734, auc: 0.8622, precision: 0.7846, recall: 0.7727\n",
      "2018-12-28T16:18:35.722863, step: 414, loss: 0.3622986674308777, acc: 0.9141, auc: 0.9736, precision: 0.9322, recall: 0.8871\n",
      "2018-12-28T16:18:36.123303, step: 415, loss: 0.4661140441894531, acc: 0.8047, auc: 0.8914, precision: 0.9123, recall: 0.7222\n",
      "2018-12-28T16:18:36.514695, step: 416, loss: 0.45533275604248047, acc: 0.7969, auc: 0.9047, precision: 0.8846, recall: 0.697\n",
      "2018-12-28T16:18:36.901247, step: 417, loss: 0.42162182927131653, acc: 0.8203, auc: 0.935, precision: 0.9348, recall: 0.6825\n",
      "2018-12-28T16:18:37.295174, step: 418, loss: 0.38871243596076965, acc: 0.8516, auc: 0.9366, precision: 0.9167, recall: 0.7458\n",
      "2018-12-28T16:18:37.702098, step: 419, loss: 0.4336302578449249, acc: 0.8438, auc: 0.8923, precision: 0.9028, recall: 0.8333\n",
      "2018-12-28T16:18:38.089561, step: 420, loss: 0.4088939428329468, acc: 0.8828, auc: 0.9131, precision: 0.9194, recall: 0.8507\n",
      "2018-12-28T16:18:38.488095, step: 421, loss: 0.4005240201950073, acc: 0.8594, auc: 0.9482, precision: 0.8429, recall: 0.8939\n",
      "2018-12-28T16:18:38.845784, step: 422, loss: 0.4363669753074646, acc: 0.8594, auc: 0.8939, precision: 0.8889, recall: 0.8\n",
      "2018-12-28T16:18:39.205934, step: 423, loss: 0.4392813742160797, acc: 0.8281, auc: 0.8966, precision: 0.8163, recall: 0.7547\n",
      "2018-12-28T16:18:39.608008, step: 424, loss: 0.41801801323890686, acc: 0.8516, auc: 0.8929, precision: 0.8936, recall: 0.75\n",
      "2018-12-28T16:18:40.102867, step: 425, loss: 0.43988561630249023, acc: 0.8359, auc: 0.8824, precision: 0.8983, recall: 0.7794\n",
      "2018-12-28T16:18:40.508501, step: 426, loss: 0.40547293424606323, acc: 0.8438, auc: 0.9104, precision: 0.9783, recall: 0.7031\n",
      "2018-12-28T16:18:40.908108, step: 427, loss: 0.5182604789733887, acc: 0.7969, auc: 0.8363, precision: 0.8182, recall: 0.6667\n",
      "2018-12-28T16:18:41.320112, step: 428, loss: 0.363357812166214, acc: 0.875, auc: 0.9492, precision: 0.9216, recall: 0.7966\n",
      "2018-12-28T16:18:41.718863, step: 429, loss: 0.4595615267753601, acc: 0.8125, auc: 0.8796, precision: 0.7692, recall: 0.8475\n",
      "2018-12-28T16:18:42.124845, step: 430, loss: 0.4106537699699402, acc: 0.8672, auc: 0.9165, precision: 0.8442, recall: 0.9286\n",
      "2018-12-28T16:18:42.551555, step: 431, loss: 0.46518397331237793, acc: 0.8281, auc: 0.9029, precision: 0.7324, recall: 0.9455\n",
      "2018-12-28T16:18:42.953310, step: 432, loss: 0.43587470054626465, acc: 0.8516, auc: 0.9221, precision: 0.8108, recall: 0.9231\n",
      "2018-12-28T16:18:43.364090, step: 433, loss: 0.3683944642543793, acc: 0.875, auc: 0.9323, precision: 0.9024, recall: 0.7551\n",
      "2018-12-28T16:18:43.764712, step: 434, loss: 0.5135563015937805, acc: 0.7578, auc: 0.8339, precision: 0.9375, recall: 0.5085\n",
      "2018-12-28T16:18:44.178539, step: 435, loss: 0.45778214931488037, acc: 0.7422, auc: 0.916, precision: 0.9429, recall: 0.5156\n",
      "2018-12-28T16:18:44.587253, step: 436, loss: 0.4426655173301697, acc: 0.7891, auc: 0.9126, precision: 0.9524, recall: 0.6154\n",
      "2018-12-28T16:18:44.986584, step: 437, loss: 0.46542733907699585, acc: 0.8047, auc: 0.8678, precision: 0.8154, recall: 0.803\n",
      "2018-12-28T16:18:45.381596, step: 438, loss: 0.40345942974090576, acc: 0.8594, auc: 0.918, precision: 0.8657, recall: 0.8657\n",
      "2018-12-28T16:18:45.814391, step: 439, loss: 0.4525938630104065, acc: 0.8438, auc: 0.9068, precision: 0.8133, recall: 0.9104\n",
      "2018-12-28T16:18:46.218871, step: 440, loss: 0.3812505602836609, acc: 0.8828, auc: 0.9164, precision: 0.8734, recall: 0.9324\n",
      "2018-12-28T16:18:46.613419, step: 441, loss: 0.4080215096473694, acc: 0.8672, auc: 0.9307, precision: 0.8095, recall: 0.9855\n",
      "2018-12-28T16:18:47.017672, step: 442, loss: 0.4990212917327881, acc: 0.7578, auc: 0.8209, precision: 0.8, recall: 0.7671\n",
      "2018-12-28T16:18:47.447107, step: 443, loss: 0.4059101939201355, acc: 0.8438, auc: 0.9074, precision: 0.85, recall: 0.8226\n",
      "2018-12-28T16:18:47.851163, step: 444, loss: 0.39834094047546387, acc: 0.8359, auc: 0.9244, precision: 0.8551, recall: 0.8429\n",
      "2018-12-28T16:18:48.256852, step: 445, loss: 0.4316611886024475, acc: 0.8047, auc: 0.8913, precision: 0.8627, recall: 0.7097\n",
      "2018-12-28T16:18:48.689933, step: 446, loss: 0.3690798878669739, acc: 0.8828, auc: 0.9342, precision: 0.8772, recall: 0.8621\n",
      "2018-12-28T16:18:49.100906, step: 447, loss: 0.4000578820705414, acc: 0.8672, auc: 0.8967, precision: 0.8286, recall: 0.9206\n",
      "2018-12-28T16:18:49.485753, step: 448, loss: 0.3525274395942688, acc: 0.8359, auc: 0.959, precision: 0.9074, recall: 0.7538\n",
      "2018-12-28T16:18:49.855453, step: 449, loss: 0.38218164443969727, acc: 0.8594, auc: 0.934, precision: 0.9206, recall: 0.8169\n",
      "2018-12-28T16:18:50.233382, step: 450, loss: 0.4072128236293793, acc: 0.8047, auc: 0.927, precision: 0.9231, recall: 0.6957\n",
      "2018-12-28T16:18:50.636064, step: 451, loss: 0.4010487198829651, acc: 0.8438, auc: 0.9076, precision: 0.871, recall: 0.8182\n",
      "2018-12-28T16:18:51.022116, step: 452, loss: 0.3488100469112396, acc: 0.8984, auc: 0.9447, precision: 0.9107, recall: 0.8644\n",
      "2018-12-28T16:18:51.437198, step: 453, loss: 0.4025995433330536, acc: 0.8438, auc: 0.9133, precision: 0.8533, recall: 0.8767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-28T16:18:51.854613, step: 454, loss: 0.4001741409301758, acc: 0.8516, auc: 0.914, precision: 0.8714, recall: 0.8592\n",
      "2018-12-28T16:18:52.256085, step: 455, loss: 0.4346463084220886, acc: 0.8438, auc: 0.9155, precision: 0.7619, recall: 0.9057\n",
      "2018-12-28T16:18:52.668589, step: 456, loss: 0.37355220317840576, acc: 0.8438, auc: 0.9162, precision: 0.8909, recall: 0.7778\n",
      "2018-12-28T16:18:53.066840, step: 457, loss: 0.33626580238342285, acc: 0.8828, auc: 0.9392, precision: 0.9143, recall: 0.8767\n",
      "2018-12-28T16:18:53.465817, step: 458, loss: 0.45421284437179565, acc: 0.8125, auc: 0.8759, precision: 0.8542, recall: 0.7069\n",
      "2018-12-28T16:18:53.856984, step: 459, loss: 0.3714432716369629, acc: 0.875, auc: 0.9, precision: 0.9259, recall: 0.8065\n",
      "2018-12-28T16:18:54.247873, step: 460, loss: 0.4051814675331116, acc: 0.8203, auc: 0.9027, precision: 0.8679, recall: 0.7419\n",
      "2018-12-28T16:18:54.648785, step: 461, loss: 0.3981325030326843, acc: 0.8438, auc: 0.9048, precision: 0.86, recall: 0.7679\n",
      "2018-12-28T16:18:55.051805, step: 462, loss: 0.3451744019985199, acc: 0.8906, auc: 0.921, precision: 0.9608, recall: 0.8033\n",
      "2018-12-28T16:18:55.448849, step: 463, loss: 0.37079352140426636, acc: 0.8438, auc: 0.9344, precision: 0.8421, recall: 0.8136\n",
      "2018-12-28T16:18:55.884059, step: 464, loss: 0.3964667320251465, acc: 0.8203, auc: 0.9138, precision: 0.9608, recall: 0.7\n",
      "2018-12-28T16:18:56.297661, step: 465, loss: 0.35290467739105225, acc: 0.8359, auc: 0.9605, precision: 0.9821, recall: 0.7333\n",
      "start training model\n",
      "2018-12-28T16:18:56.716840, step: 466, loss: 0.32249361276626587, acc: 0.8672, auc: 0.9584, precision: 0.931, recall: 0.806\n",
      "2018-12-28T16:18:57.130515, step: 467, loss: 0.3558781147003174, acc: 0.8984, auc: 0.951, precision: 0.8852, recall: 0.9\n",
      "2018-12-28T16:18:57.572479, step: 468, loss: 0.3119717836380005, acc: 0.9297, auc: 0.9595, precision: 0.931, recall: 0.9153\n",
      "2018-12-28T16:18:57.988823, step: 469, loss: 0.3160686492919922, acc: 0.8984, auc: 0.9583, precision: 0.8986, recall: 0.9118\n",
      "2018-12-28T16:18:58.395060, step: 470, loss: 0.44550102949142456, acc: 0.8281, auc: 0.8978, precision: 0.8143, recall: 0.8636\n",
      "2018-12-28T16:18:58.815031, step: 471, loss: 0.31471025943756104, acc: 0.9062, auc: 0.9362, precision: 0.9394, recall: 0.8857\n",
      "2018-12-28T16:18:59.240399, step: 472, loss: 0.279660701751709, acc: 0.9297, auc: 0.9701, precision: 0.9143, recall: 0.9552\n",
      "2018-12-28T16:18:59.674877, step: 473, loss: 0.41335350275039673, acc: 0.8359, auc: 0.8951, precision: 0.8133, recall: 0.8971\n",
      "2018-12-28T16:19:00.094330, step: 474, loss: 0.31125205755233765, acc: 0.9219, auc: 0.9224, precision: 0.9, recall: 0.931\n",
      "2018-12-28T16:19:00.525332, step: 475, loss: 0.25236743688583374, acc: 0.9453, auc: 0.9591, precision: 0.9344, recall: 0.95\n",
      "2018-12-28T16:19:00.939306, step: 476, loss: 0.3547435402870178, acc: 0.8906, auc: 0.894, precision: 0.9216, recall: 0.8246\n",
      "2018-12-28T16:19:01.344101, step: 477, loss: 0.3066333830356598, acc: 0.9062, auc: 0.933, precision: 0.9074, recall: 0.875\n",
      "2018-12-28T16:19:01.789903, step: 478, loss: 0.3316168189048767, acc: 0.8906, auc: 0.9482, precision: 0.9804, recall: 0.7937\n",
      "2018-12-28T16:19:02.188853, step: 479, loss: 0.3722565770149231, acc: 0.875, auc: 0.9341, precision: 0.94, recall: 0.7833\n",
      "2018-12-28T16:19:02.648576, step: 480, loss: 0.40504810214042664, acc: 0.8516, auc: 0.928, precision: 0.9231, recall: 0.7619\n",
      "2018-12-28T16:19:03.050463, step: 481, loss: 0.3792808949947357, acc: 0.8359, auc: 0.9272, precision: 0.9783, recall: 0.6923\n",
      "2018-12-28T16:19:03.441410, step: 482, loss: 0.31470659375190735, acc: 0.8984, auc: 0.9514, precision: 0.9649, recall: 0.8333\n",
      "2018-12-28T16:19:03.824873, step: 483, loss: 0.31456106901168823, acc: 0.8906, auc: 0.9518, precision: 0.9818, recall: 0.806\n",
      "2018-12-28T16:19:04.230846, step: 484, loss: 0.29003092646598816, acc: 0.8828, auc: 0.9528, precision: 0.9062, recall: 0.8657\n",
      "2018-12-28T16:19:04.669750, step: 485, loss: 0.2904598116874695, acc: 0.9141, auc: 0.9548, precision: 0.9062, recall: 0.9206\n",
      "2018-12-28T16:19:05.066686, step: 486, loss: 0.3395809829235077, acc: 0.8984, auc: 0.9335, precision: 0.9014, recall: 0.9143\n",
      "2018-12-28T16:19:05.475231, step: 487, loss: 0.3779401183128357, acc: 0.8516, auc: 0.9456, precision: 0.7922, recall: 0.9531\n",
      "2018-12-28T16:19:05.892914, step: 488, loss: 0.3360888361930847, acc: 0.8828, auc: 0.9426, precision: 0.8356, recall: 0.9531\n",
      "2018-12-28T16:19:06.304382, step: 489, loss: 0.32162153720855713, acc: 0.8828, auc: 0.9529, precision: 0.8814, recall: 0.8667\n",
      "2018-12-28T16:19:06.686977, step: 490, loss: 0.27565306425094604, acc: 0.9219, auc: 0.9487, precision: 0.9643, recall: 0.871\n",
      "2018-12-28T16:19:07.042562, step: 491, loss: 0.32804208993911743, acc: 0.8594, auc: 0.932, precision: 0.9556, recall: 0.7288\n",
      "2018-12-28T16:19:07.425890, step: 492, loss: 0.5559225082397461, acc: 0.7422, auc: 0.835, precision: 0.9268, recall: 0.5588\n",
      "2018-12-28T16:19:07.799456, step: 493, loss: 0.30721044540405273, acc: 0.8438, auc: 0.9641, precision: 0.94, recall: 0.7344\n",
      "2018-12-28T16:19:08.188329, step: 494, loss: 0.3284672498703003, acc: 0.8438, auc: 0.9365, precision: 0.9091, recall: 0.7692\n",
      "2018-12-28T16:19:08.610215, step: 495, loss: 0.36416250467300415, acc: 0.8516, auc: 0.9205, precision: 0.8462, recall: 0.8\n",
      "2018-12-28T16:19:09.059694, step: 496, loss: 0.34712034463882446, acc: 0.8984, auc: 0.9499, precision: 0.8305, recall: 0.9423\n",
      "2018-12-28T16:19:09.496316, step: 497, loss: 0.3033974766731262, acc: 0.8984, auc: 0.9484, precision: 0.9118, recall: 0.8986\n",
      "2018-12-28T16:19:09.907429, step: 498, loss: 0.31192710995674133, acc: 0.875, auc: 0.9641, precision: 0.8472, recall: 0.9242\n",
      "2018-12-28T16:19:10.310044, step: 499, loss: 0.3124917149543762, acc: 0.9219, auc: 0.9376, precision: 0.88, recall: 0.9851\n",
      "2018-12-28T16:19:10.721036, step: 500, loss: 0.30481743812561035, acc: 0.8984, auc: 0.9612, precision: 0.8824, recall: 0.9231\n",
      "\n",
      "Evaluation:\n",
      "2018-12-28T16:19:26.179577, step: 500, loss: 0.36718427742782395, acc: 0.8651342105263159, auc: 0.9124184210526316, precision: 0.8532947368421052, recall: 0.8878789473684208\n",
      "2018-12-28T16:19:26.583443, step: 501, loss: 0.2935149073600769, acc: 0.9062, auc: 0.9629, precision: 0.8806, recall: 0.9365\n",
      "2018-12-28T16:19:26.979017, step: 502, loss: 0.24184535443782806, acc: 0.9219, auc: 0.9749, precision: 0.9153, recall: 0.9153\n",
      "2018-12-28T16:19:27.359992, step: 503, loss: 0.27162623405456543, acc: 0.9141, auc: 0.9576, precision: 0.9841, recall: 0.8611\n",
      "2018-12-28T16:19:27.726449, step: 504, loss: 0.4706629514694214, acc: 0.8203, auc: 0.8698, precision: 0.92, recall: 0.7077\n",
      "2018-12-28T16:19:28.135922, step: 505, loss: 0.37114810943603516, acc: 0.8438, auc: 0.9316, precision: 0.9792, recall: 0.7121\n",
      "2018-12-28T16:19:28.541930, step: 506, loss: 0.3434348702430725, acc: 0.8438, auc: 0.9436, precision: 0.9412, recall: 0.7385\n",
      "2018-12-28T16:19:28.945749, step: 507, loss: 0.26581043004989624, acc: 0.9062, auc: 0.9626, precision: 0.9464, recall: 0.8548\n",
      "2018-12-28T16:19:29.356230, step: 508, loss: 0.21004869043827057, acc: 0.9375, auc: 0.9887, precision: 0.96, recall: 0.8889\n",
      "2018-12-28T16:19:29.780406, step: 509, loss: 0.3824859857559204, acc: 0.8906, auc: 0.9302, precision: 0.8246, recall: 0.9216\n",
      "2018-12-28T16:19:30.182815, step: 510, loss: 0.2826249897480011, acc: 0.8984, auc: 0.9619, precision: 0.8788, recall: 0.9206\n",
      "2018-12-28T16:19:30.575401, step: 511, loss: 0.2634080648422241, acc: 0.9219, auc: 0.9596, precision: 0.9464, recall: 0.8833\n",
      "2018-12-28T16:19:31.004122, step: 512, loss: 0.28899043798446655, acc: 0.8906, auc: 0.9673, precision: 0.8657, recall: 0.9206\n",
      "2018-12-28T16:19:31.453853, step: 513, loss: 0.3075810670852661, acc: 0.8672, auc: 0.9366, precision: 0.8929, recall: 0.8197\n",
      "2018-12-28T16:19:31.864720, step: 514, loss: 0.28258001804351807, acc: 0.875, auc: 0.9534, precision: 0.9153, recall: 0.8308\n",
      "2018-12-28T16:19:32.288394, step: 515, loss: 0.24097706377506256, acc: 0.8906, auc: 0.9769, precision: 0.9412, recall: 0.8136\n",
      "2018-12-28T16:19:32.686823, step: 516, loss: 0.2856777608394623, acc: 0.8906, auc: 0.9748, precision: 1.0, recall: 0.7705\n",
      "2018-12-28T16:19:33.083123, step: 517, loss: 0.3299843668937683, acc: 0.8516, auc: 0.9326, precision: 0.9074, recall: 0.7778\n",
      "2018-12-28T16:19:33.464635, step: 518, loss: 0.24244382977485657, acc: 0.9219, auc: 0.9792, precision: 0.9667, recall: 0.8788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-28T16:19:33.857206, step: 519, loss: 0.27170294523239136, acc: 0.9219, auc: 0.9528, precision: 0.8852, recall: 0.9474\n",
      "2018-12-28T16:19:34.260189, step: 520, loss: 0.3733632266521454, acc: 0.8828, auc: 0.9045, precision: 0.8451, recall: 0.9375\n",
      "2018-12-28T16:19:34.642630, step: 521, loss: 0.3312065303325653, acc: 0.8828, auc: 0.9367, precision: 0.8545, recall: 0.8704\n",
      "2018-12-28T16:19:35.075667, step: 522, loss: 0.36691024899482727, acc: 0.8594, auc: 0.9276, precision: 0.8, recall: 0.9123\n",
      "2018-12-28T16:19:35.485913, step: 523, loss: 0.2242797613143921, acc: 0.9375, auc: 0.9737, precision: 0.9722, recall: 0.9211\n",
      "2018-12-28T16:19:35.885612, step: 524, loss: 0.2844694256782532, acc: 0.8828, auc: 0.9464, precision: 0.9194, recall: 0.8507\n",
      "2018-12-28T16:19:36.282268, step: 525, loss: 0.23778952658176422, acc: 0.9141, auc: 0.9688, precision: 0.8947, recall: 0.9107\n",
      "2018-12-28T16:19:36.692190, step: 526, loss: 0.2423611581325531, acc: 0.9141, auc: 0.9721, precision: 0.9545, recall: 0.8873\n",
      "2018-12-28T16:19:37.128598, step: 527, loss: 0.23713064193725586, acc: 0.9219, auc: 0.9663, precision: 0.9516, recall: 0.8939\n",
      "2018-12-28T16:19:37.535483, step: 528, loss: 0.2480616420507431, acc: 0.9219, auc: 0.9697, precision: 0.8833, recall: 0.9464\n",
      "2018-12-28T16:19:37.894713, step: 529, loss: 0.2442432940006256, acc: 0.9375, auc: 0.9591, precision: 0.9538, recall: 0.9254\n",
      "2018-12-28T16:19:38.295415, step: 530, loss: 0.22129619121551514, acc: 0.9219, auc: 0.9723, precision: 0.9273, recall: 0.8947\n",
      "2018-12-28T16:19:38.682424, step: 531, loss: 0.29264742136001587, acc: 0.8672, auc: 0.9539, precision: 0.918, recall: 0.8235\n",
      "2018-12-28T16:19:39.071148, step: 532, loss: 0.3005725145339966, acc: 0.8906, auc: 0.9343, precision: 0.8871, recall: 0.8871\n",
      "2018-12-28T16:19:39.472387, step: 533, loss: 0.2612032890319824, acc: 0.9062, auc: 0.9639, precision: 0.9385, recall: 0.8841\n",
      "2018-12-28T16:19:39.834613, step: 534, loss: 0.2967943251132965, acc: 0.9141, auc: 0.9359, precision: 0.9394, recall: 0.8986\n",
      "2018-12-28T16:19:40.196609, step: 535, loss: 0.34464791417121887, acc: 0.8906, auc: 0.9327, precision: 0.8689, recall: 0.8983\n",
      "2018-12-28T16:19:40.593542, step: 536, loss: 0.20073992013931274, acc: 0.9453, auc: 0.9712, precision: 0.9254, recall: 0.9688\n",
      "2018-12-28T16:19:41.002267, step: 537, loss: 0.31809496879577637, acc: 0.8672, auc: 0.935, precision: 0.8833, recall: 0.8413\n",
      "2018-12-28T16:19:41.421264, step: 538, loss: 0.21483220160007477, acc: 0.9219, auc: 0.9869, precision: 0.9844, recall: 0.875\n",
      "2018-12-28T16:19:41.799645, step: 539, loss: 0.20766408741474152, acc: 0.9375, auc: 0.9773, precision: 0.9167, recall: 0.9483\n",
      "2018-12-28T16:19:42.184044, step: 540, loss: 0.2801700234413147, acc: 0.8984, auc: 0.9456, precision: 0.9508, recall: 0.8529\n",
      "2018-12-28T16:19:42.590902, step: 541, loss: 0.2527672052383423, acc: 0.9062, auc: 0.9593, precision: 0.8871, recall: 0.9167\n",
      "2018-12-28T16:19:42.996064, step: 542, loss: 0.31488966941833496, acc: 0.875, auc: 0.9474, precision: 0.9636, recall: 0.791\n",
      "2018-12-28T16:19:43.429724, step: 543, loss: 0.18275052309036255, acc: 0.9453, auc: 0.9807, precision: 0.9831, recall: 0.9062\n",
      "2018-12-28T16:19:43.797209, step: 544, loss: 0.26095518469810486, acc: 0.8984, auc: 0.9523, precision: 0.9344, recall: 0.8636\n",
      "2018-12-28T16:19:44.232336, step: 545, loss: 0.369028776884079, acc: 0.8516, auc: 0.9145, precision: 0.8929, recall: 0.7937\n",
      "2018-12-28T16:19:44.634916, step: 546, loss: 0.29154103994369507, acc: 0.8906, auc: 0.9494, precision: 0.8704, recall: 0.8704\n",
      "2018-12-28T16:19:45.037921, step: 547, loss: 0.34306883811950684, acc: 0.8594, auc: 0.9288, precision: 0.8971, recall: 0.8472\n",
      "2018-12-28T16:19:45.461727, step: 548, loss: 0.2170501947402954, acc: 0.9609, auc: 0.9626, precision: 0.9403, recall: 0.9844\n",
      "2018-12-28T16:19:45.911680, step: 549, loss: 0.3820454478263855, acc: 0.8594, auc: 0.8846, precision: 0.8421, recall: 0.8421\n",
      "2018-12-28T16:19:46.333485, step: 550, loss: 0.26582422852516174, acc: 0.8828, auc: 0.9605, precision: 0.9375, recall: 0.8451\n",
      "2018-12-28T16:19:46.749817, step: 551, loss: 0.223287433385849, acc: 0.9219, auc: 0.9656, precision: 0.9605, recall: 0.9125\n",
      "2018-12-28T16:19:47.144747, step: 552, loss: 0.2004634588956833, acc: 0.9141, auc: 0.9761, precision: 0.9508, recall: 0.8788\n",
      "2018-12-28T16:19:47.551112, step: 553, loss: 0.2843558192253113, acc: 0.9062, auc: 0.9461, precision: 0.875, recall: 0.9333\n",
      "2018-12-28T16:19:48.008477, step: 554, loss: 0.20609326660633087, acc: 0.9062, auc: 0.9783, precision: 0.9833, recall: 0.8429\n",
      "2018-12-28T16:19:48.434926, step: 555, loss: 0.2962186634540558, acc: 0.8906, auc: 0.9425, precision: 0.9091, recall: 0.8475\n",
      "2018-12-28T16:19:48.860351, step: 556, loss: 0.29471009969711304, acc: 0.875, auc: 0.9423, precision: 0.9206, recall: 0.8406\n",
      "2018-12-28T16:19:49.279221, step: 557, loss: 0.2133638858795166, acc: 0.9297, auc: 0.9681, precision: 0.9048, recall: 0.95\n",
      "2018-12-28T16:19:49.693260, step: 558, loss: 0.23274138569831848, acc: 0.9219, auc: 0.9746, precision: 0.9655, recall: 0.875\n",
      "2018-12-28T16:19:50.068006, step: 559, loss: 0.30591291189193726, acc: 0.9062, auc: 0.9418, precision: 0.8571, recall: 0.9677\n",
      "2018-12-28T16:19:50.454801, step: 560, loss: 0.2443712204694748, acc: 0.9141, auc: 0.9728, precision: 0.9688, recall: 0.8732\n",
      "2018-12-28T16:19:50.848999, step: 561, loss: 0.25252029299736023, acc: 0.8984, auc: 0.9629, precision: 0.9667, recall: 0.8406\n",
      "2018-12-28T16:19:51.271981, step: 562, loss: 0.27425259351730347, acc: 0.8672, auc: 0.9722, precision: 0.9825, recall: 0.7778\n",
      "2018-12-28T16:19:51.673196, step: 563, loss: 0.2873896658420563, acc: 0.8828, auc: 0.951, precision: 0.9057, recall: 0.8276\n",
      "2018-12-28T16:19:52.088164, step: 564, loss: 0.34353408217430115, acc: 0.8047, auc: 0.9405, precision: 0.9286, recall: 0.7123\n",
      "2018-12-28T16:19:52.491481, step: 565, loss: 0.23756888508796692, acc: 0.9141, auc: 0.9633, precision: 0.9365, recall: 0.8939\n",
      "2018-12-28T16:19:52.886624, step: 566, loss: 0.2855793237686157, acc: 0.8672, auc: 0.9572, precision: 0.9, recall: 0.863\n",
      "2018-12-28T16:19:53.284656, step: 567, loss: 0.26995018124580383, acc: 0.8984, auc: 0.9653, precision: 0.9014, recall: 0.9143\n",
      "2018-12-28T16:19:53.670429, step: 568, loss: 0.3186214864253998, acc: 0.8828, auc: 0.9525, precision: 0.8514, recall: 0.9403\n",
      "2018-12-28T16:19:54.063131, step: 569, loss: 0.33340325951576233, acc: 0.9062, auc: 0.964, precision: 0.8485, recall: 0.9655\n",
      "2018-12-28T16:19:54.451111, step: 570, loss: 0.32165494561195374, acc: 0.8594, auc: 0.9506, precision: 0.8308, recall: 0.8852\n",
      "2018-12-28T16:19:54.886462, step: 571, loss: 0.3063204288482666, acc: 0.8984, auc: 0.9477, precision: 0.8906, recall: 0.9048\n",
      "2018-12-28T16:19:55.326513, step: 572, loss: 0.2944471836090088, acc: 0.8594, auc: 0.959, precision: 0.902, recall: 0.7797\n",
      "2018-12-28T16:19:55.729353, step: 573, loss: 0.3410455882549286, acc: 0.8359, auc: 0.9467, precision: 0.9318, recall: 0.6949\n",
      "2018-12-28T16:19:56.129738, step: 574, loss: 0.2563985288143158, acc: 0.8828, auc: 0.9751, precision: 0.9608, recall: 0.7903\n",
      "2018-12-28T16:19:56.532592, step: 575, loss: 0.33213892579078674, acc: 0.8672, auc: 0.9406, precision: 0.9298, recall: 0.803\n",
      "2018-12-28T16:19:56.923669, step: 576, loss: 0.3201724588871002, acc: 0.8516, auc: 0.9453, precision: 0.9273, recall: 0.7727\n",
      "2018-12-28T16:19:57.328838, step: 577, loss: 0.24371762573719025, acc: 0.8828, auc: 0.9662, precision: 0.9245, recall: 0.8167\n",
      "2018-12-28T16:19:57.729803, step: 578, loss: 0.19124341011047363, acc: 0.9297, auc: 0.9826, precision: 0.9333, recall: 0.918\n",
      "2018-12-28T16:19:58.131725, step: 579, loss: 0.26507827639579773, acc: 0.9062, auc: 0.9645, precision: 0.8727, recall: 0.9057\n",
      "2018-12-28T16:19:58.542331, step: 580, loss: 0.3586583733558655, acc: 0.8672, auc: 0.9021, precision: 0.8718, recall: 0.9067\n",
      "2018-12-28T16:19:58.908023, step: 581, loss: 0.33898627758026123, acc: 0.8828, auc: 0.9499, precision: 0.8333, recall: 0.9322\n",
      "2018-12-28T16:19:59.316537, step: 582, loss: 0.3031938374042511, acc: 0.8984, auc: 0.9392, precision: 0.8939, recall: 0.9077\n",
      "2018-12-28T16:19:59.715635, step: 583, loss: 0.1985771358013153, acc: 0.9375, auc: 0.9812, precision: 0.9385, recall: 0.9385\n",
      "2018-12-28T16:20:00.113437, step: 584, loss: 0.20101948082447052, acc: 0.9297, auc: 0.9673, precision: 0.9545, recall: 0.913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-28T16:20:00.522433, step: 585, loss: 0.2020949274301529, acc: 0.9297, auc: 0.9785, precision: 0.913, recall: 0.9545\n",
      "2018-12-28T16:20:00.955634, step: 586, loss: 0.22120273113250732, acc: 0.9062, auc: 0.9809, precision: 0.9444, recall: 0.85\n",
      "2018-12-28T16:20:01.352766, step: 587, loss: 0.33069878816604614, acc: 0.8438, auc: 0.9514, precision: 0.9216, recall: 0.746\n",
      "2018-12-28T16:20:01.758477, step: 588, loss: 0.2833271026611328, acc: 0.8984, auc: 0.936, precision: 0.9194, recall: 0.8769\n",
      "2018-12-28T16:20:02.137824, step: 589, loss: 0.26276513934135437, acc: 0.9141, auc: 0.9606, precision: 0.8947, recall: 0.9107\n",
      "2018-12-28T16:20:02.584971, step: 590, loss: 0.2750913202762604, acc: 0.8984, auc: 0.9631, precision: 0.9032, recall: 0.8889\n",
      "2018-12-28T16:20:03.032486, step: 591, loss: 0.3170105516910553, acc: 0.8438, auc: 0.9369, precision: 0.902, recall: 0.7541\n",
      "2018-12-28T16:20:03.422182, step: 592, loss: 0.2994408905506134, acc: 0.8906, auc: 0.9512, precision: 0.9516, recall: 0.8429\n",
      "2018-12-28T16:20:03.827314, step: 593, loss: 0.3449643850326538, acc: 0.8594, auc: 0.9181, precision: 0.9231, recall: 0.7742\n",
      "2018-12-28T16:20:04.210718, step: 594, loss: 0.21313121914863586, acc: 0.9375, auc: 0.9655, precision: 0.9787, recall: 0.8679\n",
      "2018-12-28T16:20:04.614980, step: 595, loss: 0.27151307463645935, acc: 0.9062, auc: 0.9592, precision: 1.0, recall: 0.8356\n",
      "2018-12-28T16:20:05.023483, step: 596, loss: 0.2457318753004074, acc: 0.9062, auc: 0.9631, precision: 0.9104, recall: 0.9104\n",
      "2018-12-28T16:20:05.413438, step: 597, loss: 0.2821604311466217, acc: 0.9141, auc: 0.942, precision: 0.8929, recall: 0.9091\n",
      "2018-12-28T16:20:05.784592, step: 598, loss: 0.22196948528289795, acc: 0.9297, auc: 0.9786, precision: 0.9455, recall: 0.8966\n",
      "2018-12-28T16:20:06.177675, step: 599, loss: 0.22808781266212463, acc: 0.9297, auc: 0.9663, precision: 0.9153, recall: 0.931\n",
      "2018-12-28T16:20:06.563687, step: 600, loss: 0.19061027467250824, acc: 0.9141, auc: 0.9877, precision: 0.9403, recall: 0.9\n",
      "\n",
      "Evaluation:\n",
      "2018-12-28T16:20:22.312953, step: 600, loss: 0.3550116823692071, acc: 0.8624605263157894, auc: 0.9190578947368419, precision: 0.9113473684210528, recall: 0.8070815789473684\n",
      "2018-12-28T16:20:22.754505, step: 601, loss: 0.33020102977752686, acc: 0.8906, auc: 0.9328, precision: 0.875, recall: 0.9032\n",
      "2018-12-28T16:20:23.149846, step: 602, loss: 0.30780088901519775, acc: 0.8672, auc: 0.9309, precision: 0.9123, recall: 0.8125\n",
      "2018-12-28T16:20:23.566462, step: 603, loss: 0.3628539443016052, acc: 0.8359, auc: 0.943, precision: 0.98, recall: 0.7101\n",
      "2018-12-28T16:20:23.968296, step: 604, loss: 0.29791557788848877, acc: 0.8672, auc: 0.9402, precision: 0.8983, recall: 0.8281\n",
      "2018-12-28T16:20:24.330979, step: 605, loss: 0.28457844257354736, acc: 0.9062, auc: 0.9475, precision: 0.9219, recall: 0.8939\n",
      "2018-12-28T16:20:24.735455, step: 606, loss: 0.2830827534198761, acc: 0.9141, auc: 0.9506, precision: 0.8923, recall: 0.9355\n",
      "2018-12-28T16:20:25.134209, step: 607, loss: 0.2795541286468506, acc: 0.8984, auc: 0.9765, precision: 0.8667, recall: 0.9559\n",
      "2018-12-28T16:20:25.558184, step: 608, loss: 0.24722188711166382, acc: 0.9297, auc: 0.975, precision: 0.9123, recall: 0.9286\n",
      "2018-12-28T16:20:26.002167, step: 609, loss: 0.36967432498931885, acc: 0.8594, auc: 0.9275, precision: 0.8281, recall: 0.8833\n",
      "2018-12-28T16:20:26.404596, step: 610, loss: 0.22636446356773376, acc: 0.9141, auc: 0.9722, precision: 0.9531, recall: 0.8841\n",
      "2018-12-28T16:20:26.781529, step: 611, loss: 0.3194617033004761, acc: 0.875, auc: 0.921, precision: 0.9048, recall: 0.76\n",
      "2018-12-28T16:20:27.174158, step: 612, loss: 0.3418474793434143, acc: 0.8438, auc: 0.9367, precision: 0.9464, recall: 0.7571\n",
      "2018-12-28T16:20:27.606524, step: 613, loss: 0.38793158531188965, acc: 0.7969, auc: 0.9467, precision: 0.9623, recall: 0.68\n",
      "2018-12-28T16:20:28.021752, step: 614, loss: 0.2742549180984497, acc: 0.8906, auc: 0.9639, precision: 0.9804, recall: 0.7937\n",
      "2018-12-28T16:20:28.445474, step: 615, loss: 0.342121958732605, acc: 0.8203, auc: 0.9383, precision: 0.9259, recall: 0.7246\n",
      "2018-12-28T16:20:28.830247, step: 616, loss: 0.30605316162109375, acc: 0.8672, auc: 0.9448, precision: 0.9286, recall: 0.8\n",
      "2018-12-28T16:20:29.210865, step: 617, loss: 0.3303597867488861, acc: 0.8594, auc: 0.9316, precision: 0.871, recall: 0.8438\n",
      "2018-12-28T16:20:29.590671, step: 618, loss: 0.2630993723869324, acc: 0.9141, auc: 0.9539, precision: 0.9178, recall: 0.9306\n",
      "2018-12-28T16:20:30.028549, step: 619, loss: 0.39460599422454834, acc: 0.8359, auc: 0.9297, precision: 0.8243, recall: 0.8841\n",
      "2018-12-28T16:20:30.441348, step: 620, loss: 0.2946416139602661, acc: 0.9219, auc: 0.9394, precision: 0.8955, recall: 0.9524\n",
      "start training model\n",
      "2018-12-28T16:20:30.898489, step: 621, loss: 0.3128579258918762, acc: 0.8906, auc: 0.9728, precision: 0.8108, recall: 1.0\n",
      "2018-12-28T16:20:31.312672, step: 622, loss: 0.1585356593132019, acc: 0.9688, auc: 0.9727, precision: 0.9872, recall: 0.9625\n",
      "2018-12-28T16:20:31.714595, step: 623, loss: 0.16966025531291962, acc: 0.9688, auc: 0.9895, precision: 0.9672, recall: 0.9672\n",
      "2018-12-28T16:20:32.099719, step: 624, loss: 0.20510418713092804, acc: 0.9453, auc: 0.9651, precision: 0.913, recall: 0.9844\n",
      "2018-12-28T16:20:32.501223, step: 625, loss: 0.2118385136127472, acc: 0.9297, auc: 0.9712, precision: 0.9661, recall: 0.8906\n",
      "2018-12-28T16:20:32.922829, step: 626, loss: 0.18127834796905518, acc: 0.9297, auc: 0.988, precision: 0.9538, recall: 0.9118\n",
      "2018-12-28T16:20:33.321206, step: 627, loss: 0.12950216233730316, acc: 0.9609, auc: 0.9932, precision: 0.9841, recall: 0.9394\n",
      "2018-12-28T16:20:33.773270, step: 628, loss: 0.19376444816589355, acc: 0.9375, auc: 0.9748, precision: 0.9688, recall: 0.9118\n",
      "2018-12-28T16:20:34.169280, step: 629, loss: 0.25588250160217285, acc: 0.8984, auc: 0.9755, precision: 0.98, recall: 0.8033\n",
      "2018-12-28T16:20:34.563154, step: 630, loss: 0.21935656666755676, acc: 0.9297, auc: 0.9604, precision: 0.9254, recall: 0.9394\n",
      "2018-12-28T16:20:34.965806, step: 631, loss: 0.21050246059894562, acc: 0.9453, auc: 0.9659, precision: 0.9492, recall: 0.9333\n",
      "2018-12-28T16:20:35.384979, step: 632, loss: 0.15715718269348145, acc: 0.9531, auc: 0.9811, precision: 0.9492, recall: 0.9492\n",
      "2018-12-28T16:20:35.834755, step: 633, loss: 0.3055124580860138, acc: 0.8906, auc: 0.9331, precision: 0.9219, recall: 0.8676\n",
      "2018-12-28T16:20:36.245137, step: 634, loss: 0.22101160883903503, acc: 0.9375, auc: 0.9647, precision: 0.9412, recall: 0.9412\n",
      "2018-12-28T16:20:36.632716, step: 635, loss: 0.18211683630943298, acc: 0.9531, auc: 0.9664, precision: 0.95, recall: 0.95\n",
      "2018-12-28T16:20:37.031764, step: 636, loss: 0.1702376902103424, acc: 0.9453, auc: 0.9796, precision: 0.9333, recall: 0.9492\n",
      "2018-12-28T16:20:37.437970, step: 637, loss: 0.16954246163368225, acc: 0.9531, auc: 0.9654, precision: 0.9455, recall: 0.9455\n",
      "2018-12-28T16:20:37.825440, step: 638, loss: 0.1639309525489807, acc: 0.9609, auc: 0.9897, precision: 0.9452, recall: 0.9857\n",
      "2018-12-28T16:20:38.209584, step: 639, loss: 0.2795466482639313, acc: 0.8906, auc: 0.9528, precision: 0.8852, recall: 0.8852\n",
      "2018-12-28T16:20:38.635367, step: 640, loss: 0.14985936880111694, acc: 0.9609, auc: 0.9934, precision: 0.9552, recall: 0.9697\n",
      "2018-12-28T16:20:39.042094, step: 641, loss: 0.13721132278442383, acc: 0.9688, auc: 0.9941, precision: 0.9851, recall: 0.9565\n",
      "2018-12-28T16:20:39.463758, step: 642, loss: 0.2622113823890686, acc: 0.9062, auc: 0.9547, precision: 0.9531, recall: 0.8714\n",
      "2018-12-28T16:20:39.871439, step: 643, loss: 0.1572468876838684, acc: 0.9609, auc: 0.9929, precision: 1.0, recall: 0.9254\n",
      "2018-12-28T16:20:40.239227, step: 644, loss: 0.19911834597587585, acc: 0.9375, auc: 0.9789, precision: 0.9643, recall: 0.9\n",
      "2018-12-28T16:20:40.632063, step: 645, loss: 0.2208300530910492, acc: 0.9297, auc: 0.9642, precision: 0.9286, recall: 0.9123\n",
      "2018-12-28T16:20:41.031765, step: 646, loss: 0.26713570952415466, acc: 0.8984, auc: 0.9501, precision: 0.9138, recall: 0.8689\n",
      "2018-12-28T16:20:41.428348, step: 647, loss: 0.26609599590301514, acc: 0.9062, auc: 0.9407, precision: 0.8772, recall: 0.9091\n",
      "2018-12-28T16:20:41.824908, step: 648, loss: 0.16752243041992188, acc: 0.9453, auc: 0.9801, precision: 0.942, recall: 0.9559\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-28T16:20:42.263103, step: 649, loss: 0.19746491312980652, acc: 0.9375, auc: 0.9719, precision: 0.931, recall: 0.931\n",
      "2018-12-28T16:20:42.681341, step: 650, loss: 0.20845240354537964, acc: 0.9375, auc: 0.9671, precision: 0.9623, recall: 0.8947\n",
      "2018-12-28T16:20:43.095950, step: 651, loss: 0.11273534595966339, acc: 0.9609, auc: 0.9985, precision: 0.9833, recall: 0.9365\n",
      "2018-12-28T16:20:43.468193, step: 652, loss: 0.19911807775497437, acc: 0.8984, auc: 0.9811, precision: 0.9825, recall: 0.8235\n",
      "2018-12-28T16:20:43.867848, step: 653, loss: 0.22460713982582092, acc: 0.9375, auc: 0.9422, precision: 0.9452, recall: 0.9452\n",
      "2018-12-28T16:20:44.263483, step: 654, loss: 0.1606791764497757, acc: 0.9453, auc: 0.9797, precision: 0.9545, recall: 0.9403\n",
      "2018-12-28T16:20:44.656203, step: 655, loss: 0.1971195489168167, acc: 0.9453, auc: 0.9613, precision: 0.9492, recall: 0.9333\n",
      "2018-12-28T16:20:45.063548, step: 656, loss: 0.21022692322731018, acc: 0.9375, auc: 0.966, precision: 0.9492, recall: 0.918\n",
      "2018-12-28T16:20:45.452576, step: 657, loss: 0.18376122415065765, acc: 0.9453, auc: 0.9732, precision: 0.9623, recall: 0.9107\n",
      "2018-12-28T16:20:45.866470, step: 658, loss: 0.21901240944862366, acc: 0.9062, auc: 0.9794, precision: 0.9608, recall: 0.8305\n",
      "2018-12-28T16:20:46.267170, step: 659, loss: 0.17940737307071686, acc: 0.9453, auc: 0.9866, precision: 0.9265, recall: 0.9692\n",
      "2018-12-28T16:20:46.674183, step: 660, loss: 0.19974595308303833, acc: 0.9453, auc: 0.972, precision: 0.8889, recall: 1.0\n",
      "2018-12-28T16:20:47.100059, step: 661, loss: 0.1910589337348938, acc: 0.9531, auc: 0.9711, precision: 0.9655, recall: 0.9333\n",
      "2018-12-28T16:20:47.526679, step: 662, loss: 0.17406141757965088, acc: 0.9297, auc: 0.9829, precision: 0.9565, recall: 0.9167\n",
      "2018-12-28T16:20:47.976762, step: 663, loss: 0.33404117822647095, acc: 0.8906, auc: 0.9544, precision: 0.8333, recall: 0.9483\n",
      "2018-12-28T16:20:48.377916, step: 664, loss: 0.21722355484962463, acc: 0.9219, auc: 0.9595, precision: 0.9577, recall: 0.9067\n",
      "2018-12-28T16:20:48.792347, step: 665, loss: 0.11665753275156021, acc: 0.9688, auc: 0.9917, precision: 0.9692, recall: 0.9692\n",
      "2018-12-28T16:20:49.203820, step: 666, loss: 0.16307613253593445, acc: 0.9219, auc: 0.9922, precision: 0.9831, recall: 0.8657\n",
      "2018-12-28T16:20:49.569767, step: 667, loss: 0.19264549016952515, acc: 0.9453, auc: 0.9676, precision: 0.9464, recall: 0.9298\n",
      "2018-12-28T16:20:49.960454, step: 668, loss: 0.23505836725234985, acc: 0.9141, auc: 0.9583, precision: 0.9552, recall: 0.8889\n",
      "2018-12-28T16:20:50.360115, step: 669, loss: 0.1439109444618225, acc: 0.9453, auc: 0.9921, precision: 0.9571, recall: 0.9437\n",
      "2018-12-28T16:20:50.761865, step: 670, loss: 0.12589456140995026, acc: 0.9766, auc: 0.9829, precision: 0.9846, recall: 0.9697\n",
      "2018-12-28T16:20:51.188049, step: 671, loss: 0.18536046147346497, acc: 0.9531, auc: 0.9869, precision: 0.9167, recall: 0.9821\n",
      "2018-12-28T16:20:51.558001, step: 672, loss: 0.14882484078407288, acc: 0.9688, auc: 0.9916, precision: 0.9825, recall: 0.9492\n",
      "2018-12-28T16:20:51.950783, step: 673, loss: 0.14642426371574402, acc: 0.9531, auc: 0.9924, precision: 0.9825, recall: 0.918\n",
      "2018-12-28T16:20:52.342882, step: 674, loss: 0.12737999856472015, acc: 0.9453, auc: 0.9954, precision: 0.9538, recall: 0.9394\n",
      "2018-12-28T16:20:52.718011, step: 675, loss: 0.3332529664039612, acc: 0.875, auc: 0.9328, precision: 0.8475, recall: 0.8772\n",
      "2018-12-28T16:20:53.101430, step: 676, loss: 0.15095959603786469, acc: 0.9531, auc: 0.9887, precision: 0.9857, recall: 0.9324\n",
      "2018-12-28T16:20:53.494056, step: 677, loss: 0.09974997490644455, acc: 0.9844, auc: 0.9949, precision: 1.0, recall: 0.9683\n",
      "2018-12-28T16:20:53.888651, step: 678, loss: 0.24316515028476715, acc: 0.9141, auc: 0.9533, precision: 0.9365, recall: 0.8939\n",
      "2018-12-28T16:20:54.279847, step: 679, loss: 0.20618008077144623, acc: 0.9219, auc: 0.973, precision: 0.9531, recall: 0.8971\n",
      "2018-12-28T16:20:54.649139, step: 680, loss: 0.19981302320957184, acc: 0.9297, auc: 0.9624, precision: 0.9508, recall: 0.9062\n",
      "2018-12-28T16:20:55.009286, step: 681, loss: 0.17777684330940247, acc: 0.9375, auc: 0.9739, precision: 0.9677, recall: 0.9091\n",
      "2018-12-28T16:20:55.393937, step: 682, loss: 0.1790308803319931, acc: 0.9453, auc: 0.9727, precision: 0.9672, recall: 0.9219\n",
      "2018-12-28T16:20:55.795726, step: 683, loss: 0.12575116753578186, acc: 0.9766, auc: 0.9863, precision: 0.9565, recall: 1.0\n",
      "2018-12-28T16:20:56.206287, step: 684, loss: 0.16753409802913666, acc: 0.9375, auc: 0.9836, precision: 0.9412, recall: 0.9412\n",
      "2018-12-28T16:20:56.605066, step: 685, loss: 0.2618604004383087, acc: 0.9297, auc: 0.9418, precision: 0.9286, recall: 0.942\n",
      "2018-12-28T16:20:56.985186, step: 686, loss: 0.2453221082687378, acc: 0.9297, auc: 0.9488, precision: 0.9455, recall: 0.8966\n",
      "2018-12-28T16:20:57.369739, step: 687, loss: 0.17928045988082886, acc: 0.9453, auc: 0.9831, precision: 0.9355, recall: 0.9508\n",
      "2018-12-28T16:20:57.768357, step: 688, loss: 0.1811494082212448, acc: 0.9297, auc: 0.9816, precision: 0.9677, recall: 0.8955\n",
      "2018-12-28T16:20:58.152307, step: 689, loss: 0.2962316870689392, acc: 0.8984, auc: 0.9395, precision: 0.8929, recall: 0.8772\n",
      "2018-12-28T16:20:58.561361, step: 690, loss: 0.1985826939344406, acc: 0.9219, auc: 0.9826, precision: 0.9524, recall: 0.8955\n",
      "2018-12-28T16:20:58.964561, step: 691, loss: 0.1627192348241806, acc: 0.9375, auc: 0.9756, precision: 0.9608, recall: 0.8909\n",
      "2018-12-28T16:20:59.380552, step: 692, loss: 0.28344255685806274, acc: 0.9062, auc: 0.9668, precision: 0.8788, recall: 0.9355\n",
      "2018-12-28T16:20:59.786124, step: 693, loss: 0.13584959506988525, acc: 0.9453, auc: 0.9956, precision: 0.9688, recall: 0.9254\n",
      "2018-12-28T16:21:00.216229, step: 694, loss: 0.2198771983385086, acc: 0.9141, auc: 0.9704, precision: 0.9508, recall: 0.8788\n",
      "2018-12-28T16:21:00.674131, step: 695, loss: 0.20687882602214813, acc: 0.9297, auc: 0.9792, precision: 0.9355, recall: 0.9206\n",
      "2018-12-28T16:21:01.085641, step: 696, loss: 0.2009979635477066, acc: 0.9375, auc: 0.9706, precision: 0.971, recall: 0.9178\n",
      "2018-12-28T16:21:01.484500, step: 697, loss: 0.2414626181125641, acc: 0.8984, auc: 0.9607, precision: 0.9516, recall: 0.8551\n",
      "2018-12-28T16:21:01.893826, step: 698, loss: 0.20334333181381226, acc: 0.9453, auc: 0.9759, precision: 0.918, recall: 0.9655\n",
      "2018-12-28T16:21:02.283231, step: 699, loss: 0.21127143502235413, acc: 0.9453, auc: 0.9687, precision: 0.9074, recall: 0.9608\n",
      "2018-12-28T16:21:02.702379, step: 700, loss: 0.21460211277008057, acc: 0.9219, auc: 0.9736, precision: 0.9688, recall: 0.8857\n",
      "\n",
      "Evaluation:\n",
      "2018-12-28T16:21:18.224294, step: 700, loss: 0.36715946621016454, acc: 0.829163157894737, auc: 0.9239447368421053, precision: 0.9141684210526315, recall: 0.7318894736842106\n",
      "2018-12-28T16:21:18.634607, step: 701, loss: 0.17481690645217896, acc: 0.9453, auc: 0.9863, precision: 0.9677, recall: 0.9231\n",
      "2018-12-28T16:21:19.043568, step: 702, loss: 0.1928153783082962, acc: 0.9375, auc: 0.9755, precision: 0.9545, recall: 0.9265\n",
      "2018-12-28T16:21:19.457783, step: 703, loss: 0.17073562741279602, acc: 0.9453, auc: 0.9832, precision: 0.9828, recall: 0.9048\n",
      "2018-12-28T16:21:19.880979, step: 704, loss: 0.1790851503610611, acc: 0.9453, auc: 0.9762, precision: 0.9483, recall: 0.9322\n",
      "2018-12-28T16:21:20.269480, step: 705, loss: 0.2504139542579651, acc: 0.9062, auc: 0.9599, precision: 0.9464, recall: 0.8548\n",
      "2018-12-28T16:21:20.658229, step: 706, loss: 0.10555635392665863, acc: 0.9688, auc: 0.9968, precision: 0.9722, recall: 0.9722\n",
      "2018-12-28T16:21:21.051051, step: 707, loss: 0.15071894228458405, acc: 0.9609, auc: 0.9829, precision: 0.9538, recall: 0.9688\n",
      "2018-12-28T16:21:21.468586, step: 708, loss: 0.1315016895532608, acc: 0.9609, auc: 0.9815, precision: 0.9577, recall: 0.9714\n",
      "2018-12-28T16:21:21.884964, step: 709, loss: 0.1945434957742691, acc: 0.9453, auc: 0.958, precision: 0.9351, recall: 0.973\n",
      "2018-12-28T16:21:22.272428, step: 710, loss: 0.2395431101322174, acc: 0.9141, auc: 0.9586, precision: 0.8889, recall: 0.9333\n",
      "2018-12-28T16:21:22.696264, step: 711, loss: 0.1655421406030655, acc: 0.9531, auc: 0.977, precision: 0.95, recall: 0.95\n",
      "2018-12-28T16:21:23.141350, step: 712, loss: 0.2563389837741852, acc: 0.8906, auc: 0.964, precision: 0.9355, recall: 0.8529\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-28T16:21:23.564156, step: 713, loss: 0.15329992771148682, acc: 0.9453, auc: 0.9885, precision: 0.9508, recall: 0.9355\n",
      "2018-12-28T16:21:23.973165, step: 714, loss: 0.19166824221611023, acc: 0.9297, auc: 0.9799, precision: 1.0, recall: 0.8696\n",
      "2018-12-28T16:21:24.416072, step: 715, loss: 0.2981071174144745, acc: 0.9062, auc: 0.9365, precision: 0.9074, recall: 0.875\n",
      "2018-12-28T16:21:24.828134, step: 716, loss: 0.30151283740997314, acc: 0.8984, auc: 0.9358, precision: 0.8615, recall: 0.9333\n",
      "2018-12-28T16:21:25.248619, step: 717, loss: 0.2530254125595093, acc: 0.9297, auc: 0.9572, precision: 0.8793, recall: 0.9623\n",
      "2018-12-28T16:21:25.668705, step: 718, loss: 0.22168810665607452, acc: 0.9219, auc: 0.9713, precision: 0.9143, recall: 0.9412\n",
      "2018-12-28T16:21:26.098812, step: 719, loss: 0.2819673717021942, acc: 0.8906, auc: 0.9394, precision: 0.9118, recall: 0.8857\n",
      "2018-12-28T16:21:26.499299, step: 720, loss: 0.22787432372570038, acc: 0.9141, auc: 0.9705, precision: 0.9483, recall: 0.873\n",
      "2018-12-28T16:21:26.945263, step: 721, loss: 0.1349671632051468, acc: 0.9766, auc: 0.9853, precision: 0.9831, recall: 0.9667\n",
      "2018-12-28T16:21:27.367892, step: 722, loss: 0.21053114533424377, acc: 0.9375, auc: 0.9606, precision: 0.9836, recall: 0.8955\n",
      "2018-12-28T16:21:27.793247, step: 723, loss: 0.13912537693977356, acc: 0.9609, auc: 0.9914, precision: 0.9516, recall: 0.9672\n",
      "2018-12-28T16:21:28.170772, step: 724, loss: 0.16464877128601074, acc: 0.9531, auc: 0.9869, precision: 0.9608, recall: 0.9245\n",
      "2018-12-28T16:21:28.541191, step: 725, loss: 0.2489963322877884, acc: 0.9141, auc: 0.9584, precision: 1.0, recall: 0.8103\n",
      "2018-12-28T16:21:28.940438, step: 726, loss: 0.1351976841688156, acc: 0.9453, auc: 0.989, precision: 0.9661, recall: 0.9194\n",
      "2018-12-28T16:21:29.387683, step: 727, loss: 0.31475839018821716, acc: 0.8672, auc: 0.9386, precision: 0.875, recall: 0.8305\n",
      "2018-12-28T16:21:29.839910, step: 728, loss: 0.193942129611969, acc: 0.9141, auc: 0.986, precision: 0.9524, recall: 0.8824\n",
      "2018-12-28T16:21:30.241923, step: 729, loss: 0.23623895645141602, acc: 0.9062, auc: 0.9636, precision: 0.95, recall: 0.8636\n",
      "2018-12-28T16:21:30.638646, step: 730, loss: 0.12249784171581268, acc: 0.9609, auc: 0.9941, precision: 0.9846, recall: 0.9412\n",
      "2018-12-28T16:21:31.030876, step: 731, loss: 0.1545095294713974, acc: 0.9375, auc: 0.9914, precision: 0.9322, recall: 0.9322\n",
      "2018-12-28T16:21:31.398729, step: 732, loss: 0.12983165681362152, acc: 0.9609, auc: 0.9961, precision: 0.9667, recall: 0.9508\n",
      "2018-12-28T16:21:31.786644, step: 733, loss: 0.19136063754558563, acc: 0.9531, auc: 0.9682, precision: 0.9375, recall: 0.9677\n",
      "2018-12-28T16:21:32.200745, step: 734, loss: 0.14148305356502533, acc: 0.9609, auc: 0.9787, precision: 0.9714, recall: 0.9577\n",
      "2018-12-28T16:21:32.607415, step: 735, loss: 0.20458078384399414, acc: 0.9453, auc: 0.9634, precision: 0.9254, recall: 0.9688\n",
      "2018-12-28T16:21:33.068113, step: 736, loss: 0.15878871083259583, acc: 0.9297, auc: 0.9902, precision: 0.9545, recall: 0.913\n",
      "2018-12-28T16:21:33.486060, step: 737, loss: 0.18494001030921936, acc: 0.9219, auc: 0.9868, precision: 0.9118, recall: 0.9394\n",
      "2018-12-28T16:21:33.888067, step: 738, loss: 0.2019468992948532, acc: 0.9141, auc: 0.9851, precision: 0.8986, recall: 0.9394\n",
      "2018-12-28T16:21:34.318101, step: 739, loss: 0.14996345341205597, acc: 0.9453, auc: 0.9889, precision: 0.9815, recall: 0.8983\n",
      "2018-12-28T16:21:34.736128, step: 740, loss: 0.17895324528217316, acc: 0.9375, auc: 0.9827, precision: 0.9661, recall: 0.9048\n",
      "2018-12-28T16:21:35.130688, step: 741, loss: 0.1734461486339569, acc: 0.9375, auc: 0.9788, precision: 0.9524, recall: 0.9231\n",
      "2018-12-28T16:21:35.539589, step: 742, loss: 0.14465567469596863, acc: 0.9688, auc: 0.9922, precision: 0.963, recall: 0.963\n",
      "2018-12-28T16:21:35.956518, step: 743, loss: 0.11044230312108994, acc: 0.9609, auc: 0.9971, precision: 1.0, recall: 0.9167\n",
      "2018-12-28T16:21:36.362245, step: 744, loss: 0.16107486188411713, acc: 0.9453, auc: 0.988, precision: 1.0, recall: 0.8833\n",
      "2018-12-28T16:21:36.810888, step: 745, loss: 0.17898617684841156, acc: 0.9453, auc: 0.9771, precision: 0.9385, recall: 0.9531\n",
      "2018-12-28T16:21:37.242648, step: 746, loss: 0.1225844994187355, acc: 0.9766, auc: 0.9863, precision: 0.9701, recall: 0.9848\n",
      "2018-12-28T16:21:37.620986, step: 747, loss: 0.11171533912420273, acc: 0.9688, auc: 0.982, precision: 0.9818, recall: 0.9474\n",
      "2018-12-28T16:21:38.022326, step: 748, loss: 0.30115577578544617, acc: 0.8984, auc: 0.9407, precision: 0.8571, recall: 0.9524\n",
      "2018-12-28T16:21:38.453843, step: 749, loss: 0.2552955746650696, acc: 0.9141, auc: 0.9619, precision: 0.9483, recall: 0.873\n",
      "2018-12-28T16:21:38.847683, step: 750, loss: 0.2487659752368927, acc: 0.9062, auc: 0.9394, precision: 0.9206, recall: 0.8923\n",
      "2018-12-28T16:21:39.249209, step: 751, loss: 0.16728252172470093, acc: 0.9453, auc: 0.9768, precision: 1.0, recall: 0.8923\n",
      "2018-12-28T16:21:39.664987, step: 752, loss: 0.16187351942062378, acc: 0.9453, auc: 0.9797, precision: 0.9524, recall: 0.9375\n",
      "2018-12-28T16:21:40.101264, step: 753, loss: 0.15996916592121124, acc: 0.9531, auc: 0.9897, precision: 0.9437, recall: 0.971\n",
      "2018-12-28T16:21:40.529816, step: 754, loss: 0.15118104219436646, acc: 0.9609, auc: 0.9878, precision: 0.9692, recall: 0.9545\n",
      "2018-12-28T16:21:40.942198, step: 755, loss: 0.17373964190483093, acc: 0.9375, auc: 0.9903, precision: 0.8852, recall: 0.9818\n",
      "2018-12-28T16:21:41.356539, step: 756, loss: 0.24115222692489624, acc: 0.9219, auc: 0.9493, precision: 0.9028, recall: 0.9559\n",
      "2018-12-28T16:21:41.774602, step: 757, loss: 0.11869218945503235, acc: 0.9844, auc: 0.9802, precision: 0.9825, recall: 0.9825\n",
      "2018-12-28T16:21:42.157850, step: 758, loss: 0.13370972871780396, acc: 0.9609, auc: 0.9785, precision: 0.9672, recall: 0.9516\n",
      "2018-12-28T16:21:42.547837, step: 759, loss: 0.2682342231273651, acc: 0.8984, auc: 0.9724, precision: 0.9483, recall: 0.8462\n",
      "2018-12-28T16:21:42.926751, step: 760, loss: 0.24908345937728882, acc: 0.8828, auc: 0.9719, precision: 0.9808, recall: 0.7846\n",
      "2018-12-28T16:21:43.321956, step: 761, loss: 0.12783092260360718, acc: 0.9609, auc: 0.9937, precision: 0.9836, recall: 0.9375\n",
      "2018-12-28T16:21:43.710919, step: 762, loss: 0.22909514605998993, acc: 0.9297, auc: 0.9615, precision: 0.96, recall: 0.9231\n",
      "2018-12-28T16:21:44.115445, step: 763, loss: 0.08827266097068787, acc: 0.9922, auc: 0.9985, precision: 0.9833, recall: 1.0\n",
      "2018-12-28T16:21:44.502630, step: 764, loss: 0.21140888333320618, acc: 0.9375, auc: 0.985, precision: 0.8772, recall: 0.9804\n",
      "2018-12-28T16:21:44.909446, step: 765, loss: 0.15586817264556885, acc: 0.9609, auc: 0.9978, precision: 0.9254, recall: 1.0\n",
      "2018-12-28T16:21:45.328602, step: 766, loss: 0.26644042134284973, acc: 0.9141, auc: 0.9487, precision: 0.9079, recall: 0.9452\n",
      "2018-12-28T16:21:45.741258, step: 767, loss: 0.24877959489822388, acc: 0.9297, auc: 0.9506, precision: 0.9333, recall: 0.918\n",
      "2018-12-28T16:21:46.174635, step: 768, loss: 0.14544834196567535, acc: 0.9531, auc: 0.982, precision: 0.9706, recall: 0.9429\n",
      "2018-12-28T16:21:46.570166, step: 769, loss: 0.20524613559246063, acc: 0.9062, auc: 0.9763, precision: 0.9538, recall: 0.8732\n",
      "2018-12-28T16:21:46.985831, step: 770, loss: 0.1633821725845337, acc: 0.9453, auc: 0.9773, precision: 0.9508, recall: 0.9355\n",
      "2018-12-28T16:21:47.388944, step: 771, loss: 0.22103366255760193, acc: 0.9062, auc: 0.9721, precision: 0.9825, recall: 0.8358\n",
      "2018-12-28T16:21:47.805822, step: 772, loss: 0.2558445334434509, acc: 0.8906, auc: 0.967, precision: 0.9833, recall: 0.8194\n",
      "2018-12-28T16:21:48.228657, step: 773, loss: 0.18354393541812897, acc: 0.9062, auc: 0.987, precision: 0.9615, recall: 0.8333\n",
      "2018-12-28T16:21:48.640471, step: 774, loss: 0.25585243105888367, acc: 0.8828, auc: 0.9652, precision: 0.9038, recall: 0.8246\n",
      "2018-12-28T16:21:49.054009, step: 775, loss: 0.30027273297309875, acc: 0.8984, auc: 0.9457, precision: 0.9688, recall: 0.8493\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "\n",
    "# 生成训练集和验证集\n",
    "trainReviews = data.trainReviews\n",
    "trainLabels = data.trainLabels\n",
    "evalReviews = data.evalReviews\n",
    "evalLabels = data.evalLabels\n",
    "\n",
    "wordEmbedding = data.wordEmbedding\n",
    "\n",
    "# 定义计算图\n",
    "with tf.Graph().as_default():\n",
    "\n",
    "    session_conf = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
    "    session_conf.gpu_options.allow_growth=True\n",
    "    session_conf.gpu_options.per_process_gpu_memory_fraction = 0.9  # 配置gpu占用率  \n",
    "\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    \n",
    "    # 定义会话\n",
    "    with sess.as_default():\n",
    "        lstm = RCNN(config, wordEmbedding)\n",
    "        \n",
    "        globalStep = tf.Variable(0, name=\"globalStep\", trainable=False)\n",
    "        # 定义优化函数，传入学习速率参数\n",
    "        optimizer = tf.train.AdamOptimizer(config.training.learningRate)\n",
    "        # 计算梯度,得到梯度和变量\n",
    "        gradsAndVars = optimizer.compute_gradients(lstm.loss)\n",
    "        # 将梯度应用到变量下，生成训练器\n",
    "        trainOp = optimizer.apply_gradients(gradsAndVars, global_step=globalStep)\n",
    "        \n",
    "        # 用summary绘制tensorBoard\n",
    "        gradSummaries = []\n",
    "        for g, v in gradsAndVars:\n",
    "            if g is not None:\n",
    "                tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "        \n",
    "        outDir = os.path.abspath(os.path.join(os.path.curdir, \"summarys\"))\n",
    "        print(\"Writing to {}\\n\".format(outDir))\n",
    "        \n",
    "        lossSummary = tf.summary.scalar(\"loss\", lstm.loss)\n",
    "        summaryOp = tf.summary.merge_all()\n",
    "        \n",
    "        trainSummaryDir = os.path.join(outDir, \"train\")\n",
    "        trainSummaryWriter = tf.summary.FileWriter(trainSummaryDir, sess.graph)\n",
    "        \n",
    "        evalSummaryDir = os.path.join(outDir, \"eval\")\n",
    "        evalSummaryWriter = tf.summary.FileWriter(evalSummaryDir, sess.graph)\n",
    "        \n",
    "        \n",
    "        # 初始化所有变量\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=5)\n",
    "        \n",
    "        # 保存模型的一种方式，保存为pb文件\n",
    "        builder = tf.saved_model.builder.SavedModelBuilder(\"../model/Bi-LSTM/savedModel\")\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        def trainStep(batchX, batchY):\n",
    "            \"\"\"\n",
    "            训练函数\n",
    "            \"\"\"   \n",
    "            feed_dict = {\n",
    "              lstm.inputX: batchX,\n",
    "              lstm.inputY: batchY,\n",
    "              lstm.dropoutKeepProb: config.model.dropoutKeepProb\n",
    "            }\n",
    "            _, summary, step, loss, predictions, binaryPreds = sess.run(\n",
    "                [trainOp, summaryOp, globalStep, lstm.loss, lstm.predictions, lstm.binaryPreds],\n",
    "                feed_dict)\n",
    "            timeStr = datetime.datetime.now().isoformat()\n",
    "            acc, auc, precision, recall = genMetrics(batchY, predictions, binaryPreds)\n",
    "            print(\"{}, step: {}, loss: {}, acc: {}, auc: {}, precision: {}, recall: {}\".format(timeStr, step, loss, acc, auc, precision, recall))\n",
    "            trainSummaryWriter.add_summary(summary, step)\n",
    "\n",
    "        def devStep(batchX, batchY):\n",
    "            \"\"\"\n",
    "            验证函数\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              lstm.inputX: batchX,\n",
    "              lstm.inputY: batchY,\n",
    "              lstm.dropoutKeepProb: 1.0\n",
    "            }\n",
    "            summary, step, loss, predictions, binaryPreds = sess.run(\n",
    "                [summaryOp, globalStep, lstm.loss, lstm.predictions, lstm.binaryPreds],\n",
    "                feed_dict)\n",
    "            \n",
    "            acc, auc, precision, recall = genMetrics(batchY, predictions, binaryPreds)\n",
    "            \n",
    "            evalSummaryWriter.add_summary(summary, step)\n",
    "            \n",
    "            return loss, acc, auc, precision, recall\n",
    "        \n",
    "        for i in range(config.training.epoches):\n",
    "            # 训练模型\n",
    "            print(\"start training model\")\n",
    "            for batchTrain in nextBatch(trainReviews, trainLabels, config.batchSize):\n",
    "                trainStep(batchTrain[0], batchTrain[1])\n",
    "\n",
    "                currentStep = tf.train.global_step(sess, globalStep) \n",
    "                if currentStep % config.training.evaluateEvery == 0:\n",
    "                    print(\"\\nEvaluation:\")\n",
    "                    \n",
    "                    losses = []\n",
    "                    accs = []\n",
    "                    aucs = []\n",
    "                    precisions = []\n",
    "                    recalls = []\n",
    "                    \n",
    "                    for batchEval in nextBatch(evalReviews, evalLabels, config.batchSize):\n",
    "                        loss, acc, auc, precision, recall = devStep(batchEval[0], batchEval[1])\n",
    "                        losses.append(loss)\n",
    "                        accs.append(acc)\n",
    "                        aucs.append(auc)\n",
    "                        precisions.append(precision)\n",
    "                        recalls.append(recall)\n",
    "                        \n",
    "                    time_str = datetime.datetime.now().isoformat()\n",
    "                    print(\"{}, step: {}, loss: {}, acc: {}, auc: {}, precision: {}, recall: {}\".format(time_str, currentStep, mean(losses), \n",
    "                                                                                                       mean(accs), mean(aucs), mean(precisions),\n",
    "                                                                                                       mean(recalls)))\n",
    "                    \n",
    "                if currentStep % config.training.checkpointEvery == 0:\n",
    "                    # 保存模型的另一种方法，保存checkpoint文件\n",
    "                    path = saver.save(sess, \"../model/Bi-LSTM/model/my-model\", global_step=currentStep)\n",
    "                    print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "                    \n",
    "        inputs = {\"inputX\": tf.saved_model.utils.build_tensor_info(lstm.inputX),\n",
    "                  \"keepProb\": tf.saved_model.utils.build_tensor_info(lstm.dropoutKeepProb)}\n",
    "\n",
    "        outputs = {\"binaryPreds\": tf.saved_model.utils.build_tensor_info(lstm.binaryPreds)}\n",
    "\n",
    "        prediction_signature = tf.saved_model.signature_def_utils.build_signature_def(inputs=inputs, outputs=outputs,\n",
    "                                                                                      method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME)\n",
    "        legacy_init_op = tf.group(tf.tables_initializer(), name=\"legacy_init_op\")\n",
    "        builder.add_meta_graph_and_variables(sess, [tf.saved_model.tag_constants.SERVING],\n",
    "                                            signature_def_map={\"predict\": prediction_signature}, legacy_init_op=legacy_init_op)\n",
    "\n",
    "        builder.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
