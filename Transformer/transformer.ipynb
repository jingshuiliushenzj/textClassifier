{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import json\n",
    "\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from math import sqrt\n",
    "\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置参数\n",
    "\n",
    "class TrainingConfig(object):\n",
    "    epoches = 10\n",
    "    evaluateEvery = 100\n",
    "    checkpointEvery = 100\n",
    "    learningRate = 0.001\n",
    "    \n",
    "    \n",
    "class ModelConfig(object):\n",
    "    embeddingSize = 200\n",
    "    \n",
    "    filters = 128  # 内层一维卷积核的数量，外层卷积核的数量应该等于embeddingSize，因为要确保每个layer后的输出维度和输入维度是一致的。\n",
    "    numHeads = 8  # Attention 的头数\n",
    "    numBlocks = 1  # 设置transformer block的数量\n",
    "    epsilon = 1e-8  # LayerNorm 层中的最小除数\n",
    "    keepProp = 0.9  # multi head attention 中的dropout\n",
    "    \n",
    "    dropoutKeepProb = 0.5 # 全连接层的dropout\n",
    "    l2RegLambda = 0.0\n",
    "    \n",
    "    \n",
    "class Config(object):\n",
    "    sequenceLength = 200  # 取了所有序列长度的均值\n",
    "    batchSize = 128\n",
    "    \n",
    "    dataSource = \"../data/preProcess/labeledTrain.csv\"\n",
    "    \n",
    "    stopWordSource = \"../data/english\"\n",
    "    \n",
    "    numClasses = 2\n",
    "    \n",
    "    rate = 0.8  # 训练集的比例\n",
    "    \n",
    "    training = TrainingConfig()\n",
    "    \n",
    "    model = ModelConfig()\n",
    "\n",
    "    \n",
    "# 实例化配置参数对象\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据预处理的类，生成训练集和测试集\n",
    "\n",
    "class Dataset(object):\n",
    "    def __init__(self, config):\n",
    "        self._dataSource = config.dataSource\n",
    "        self._stopWordSource = config.stopWordSource  \n",
    "        \n",
    "        self._sequenceLength = config.sequenceLength  # 每条输入的序列处理为定长\n",
    "        self._embeddingSize = config.model.embeddingSize\n",
    "        self._batchSize = config.batchSize\n",
    "        self._rate = config.rate\n",
    "        \n",
    "        self._stopWordDict = {}\n",
    "        \n",
    "        self.trainReviews = []\n",
    "        self.trainLabels = []\n",
    "        \n",
    "        self.evalReviews = []\n",
    "        self.evalLabels = []\n",
    "        \n",
    "        self.wordEmbedding =None\n",
    "        \n",
    "        self._wordToIndex = {}\n",
    "        self._indexToWord = {}\n",
    "        \n",
    "    def _readData(self, filePath):\n",
    "        \"\"\"\n",
    "        从csv文件中读取数据集\n",
    "        \"\"\"\n",
    "        \n",
    "        df = pd.read_csv(filePath)\n",
    "        labels = df[\"sentiment\"].tolist()\n",
    "        review = df[\"review\"].tolist()\n",
    "        reviews = [line.strip().split() for line in review]\n",
    "\n",
    "        return reviews, labels\n",
    "\n",
    "    def _reviewProcess(self, review, sequenceLength, wordToIndex):\n",
    "        \"\"\"\n",
    "        将数据集中的每条评论用index表示\n",
    "        wordToIndex中“pad”对应的index为0\n",
    "        \"\"\"\n",
    "        \n",
    "        reviewVec = np.zeros((sequenceLength))\n",
    "        sequenceLen = sequenceLength\n",
    "        \n",
    "        # 判断当前的序列是否小于定义的固定序列长度\n",
    "        if len(review) < sequenceLength:\n",
    "            sequenceLen = len(review)\n",
    "            \n",
    "        for i in range(sequenceLen):\n",
    "            if review[i] in wordToIndex:\n",
    "                reviewVec[i] = wordToIndex[review[i]]\n",
    "            else:\n",
    "                reviewVec[i] = wordToIndex[\"UNK\"]\n",
    "\n",
    "        return reviewVec\n",
    "\n",
    "    def _genTrainEvalData(self, x, y, rate):\n",
    "        \"\"\"\n",
    "        生成训练集和验证集\n",
    "        \"\"\"\n",
    "        \n",
    "        reviews = []\n",
    "        labels = []\n",
    "        \n",
    "        # 遍历所有的文本，将文本中的词转换成index表示\n",
    "        for i in range(len(x)):\n",
    "            reviewVec = self._reviewProcess(x[i], self._sequenceLength, self._wordToIndex)\n",
    "            reviews.append(reviewVec)\n",
    "            \n",
    "            labels.append([y[i]])\n",
    "            \n",
    "        trainIndex = int(len(x) * rate)\n",
    "        \n",
    "        trainReviews = np.asarray(reviews[:trainIndex], dtype=\"int64\")\n",
    "        trainLabels = np.array(labels[:trainIndex], dtype=\"float32\")\n",
    "        \n",
    "        evalReviews = np.asarray(reviews[trainIndex:], dtype=\"int64\")\n",
    "        evalLabels = np.array(labels[trainIndex:], dtype=\"float32\")\n",
    "\n",
    "        return trainReviews, trainLabels, evalReviews, evalLabels\n",
    "        \n",
    "    def _genVocabulary(self, reviews):\n",
    "        \"\"\"\n",
    "        生成词向量和词汇-索引映射字典，可以用全数据集\n",
    "        \"\"\"\n",
    "        \n",
    "        allWords = [word for review in reviews for word in review]\n",
    "        \n",
    "        # 去掉停用词\n",
    "        subWords = [word for word in allWords if word not in self.stopWordDict]\n",
    "        \n",
    "        wordCount = Counter(subWords)  # 统计词频\n",
    "        sortWordCount = sorted(wordCount.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # 去除低频词\n",
    "        words = [item[0] for item in sortWordCount if item[1] >= 5]\n",
    "        \n",
    "        vocab, wordEmbedding = self._getWordEmbedding(words)\n",
    "        self.wordEmbedding = wordEmbedding\n",
    "        \n",
    "        self._wordToIndex = dict(zip(vocab, list(range(len(vocab)))))\n",
    "        self._indexToWord = dict(zip(list(range(len(vocab))), vocab))\n",
    "        \n",
    "        # 将词汇-索引映射表保存为json数据，之后做inference时直接加载来处理数据\n",
    "        with open(\"../data/wordJson/wordToIndex.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self._wordToIndex, f)\n",
    "        \n",
    "        with open(\"../data/wordJson/indexToWord.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self._indexToWord, f)\n",
    "            \n",
    "    def _getWordEmbedding(self, words):\n",
    "        \"\"\"\n",
    "        按照我们的数据集中的单词取出预训练好的word2vec中的词向量\n",
    "        \"\"\"\n",
    "        \n",
    "        wordVec = gensim.models.KeyedVectors.load_word2vec_format(\"../word2vec/word2Vec.bin\", binary=True)\n",
    "        vocab = []\n",
    "        wordEmbedding = []\n",
    "        \n",
    "        # 添加 \"pad\" 和 \"UNK\", \n",
    "        vocab.append(\"pad\")\n",
    "        vocab.append(\"UNK\")\n",
    "        wordEmbedding.append(np.zeros(self._embeddingSize))\n",
    "        wordEmbedding.append(np.random.randn(self._embeddingSize))\n",
    "        \n",
    "        for word in words:\n",
    "            try:\n",
    "                vector = wordVec.wv[word]\n",
    "                vocab.append(word)\n",
    "                wordEmbedding.append(vector)\n",
    "            except:\n",
    "                print(word + \"不存在于词向量中\")\n",
    "                \n",
    "        return vocab, np.array(wordEmbedding)\n",
    "    \n",
    "    def _readStopWord(self, stopWordPath):\n",
    "        \"\"\"\n",
    "        读取停用词\n",
    "        \"\"\"\n",
    "        \n",
    "        with open(stopWordPath, \"r\") as f:\n",
    "            stopWords = f.read()\n",
    "            stopWordList = stopWords.splitlines()\n",
    "            # 将停用词用列表的形式生成，之后查找停用词时会比较快\n",
    "            self.stopWordDict = dict(zip(stopWordList, list(range(len(stopWordList)))))\n",
    "            \n",
    "    def dataGen(self):\n",
    "        \"\"\"\n",
    "        初始化训练集和验证集\n",
    "        \"\"\"\n",
    "        \n",
    "        # 初始化停用词\n",
    "        self._readStopWord(self._stopWordSource)\n",
    "        \n",
    "        # 初始化数据集\n",
    "        reviews, labels = self._readData(self._dataSource)\n",
    "        \n",
    "        # 初始化词汇-索引映射表和词向量矩阵\n",
    "        self._genVocabulary(reviews)\n",
    "        \n",
    "        # 初始化训练集和测试集\n",
    "        trainReviews, trainLabels, evalReviews, evalLabels = self._genTrainEvalData(reviews, labels, self._rate)\n",
    "        self.trainReviews = trainReviews\n",
    "        self.trainLabels = trainLabels\n",
    "        \n",
    "        self.evalReviews = evalReviews\n",
    "        self.evalLabels = evalLabels\n",
    "        \n",
    "        \n",
    "data = Dataset(config)\n",
    "data.dataGen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape: (20000, 200)\n",
      "train label shape: (20000, 1)\n",
      "eval data shape: (5000, 200)\n"
     ]
    }
   ],
   "source": [
    "print(\"train data shape: {}\".format(data.trainReviews.shape))\n",
    "print(\"train label shape: {}\".format(data.trainLabels.shape))\n",
    "print(\"eval data shape: {}\".format(data.evalReviews.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出batch数据集\n",
    "\n",
    "def nextBatch(x, y, batchSize):\n",
    "        \"\"\"\n",
    "        生成batch数据集，用生成器的方式输出\n",
    "        \"\"\"\n",
    "    \n",
    "        perm = np.arange(len(x))\n",
    "        np.random.shuffle(perm)\n",
    "        x = x[perm]\n",
    "        y = y[perm]\n",
    "        \n",
    "        numBatches = len(x) // batchSize\n",
    "\n",
    "        for i in range(numBatches):\n",
    "            start = i * batchSize\n",
    "            end = start + batchSize\n",
    "            batchX = np.array(x[start: end], dtype=\"int64\")\n",
    "            batchY = np.array(y[start: end], dtype=\"float32\")\n",
    "            \n",
    "            yield batchX, batchY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成位置嵌入\n",
    "def fixedPositionEmbedding(batchSize, sequenceLen):\n",
    "    embeddedPosition = []\n",
    "    for batch in range(batchSize):\n",
    "        x = []\n",
    "        for step in range(sequenceLen):\n",
    "            a = np.zeros(sequenceLen)\n",
    "            a[step] = 1\n",
    "            x.append(a)\n",
    "        embeddedPosition.append(x)\n",
    "    \n",
    "    return np.array(embeddedPosition, dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型构建\n",
    "\n",
    "class Transformer(object):\n",
    "    \"\"\"\n",
    "    Transformer Encoder 用于文本分类\n",
    "    \"\"\"\n",
    "    def __init__(self, config, wordEmbedding):\n",
    "\n",
    "        # 定义模型的输入\n",
    "        self.inputX = tf.placeholder(tf.int32, [None, config.sequenceLength], name=\"inputX\")\n",
    "        self.inputY = tf.placeholder(tf.float32, [None, 1], name=\"inputY\")\n",
    "        \n",
    "        self.dropoutKeepProb = tf.placeholder(tf.float32, name=\"dropoutKeepProb\")\n",
    "        self.embeddedPosition = tf.placeholder(tf.float32, [None, config.sequenceLength, config.sequenceLength], name=\"embeddedPosition\")\n",
    "        \n",
    "        self.config = config\n",
    "        \n",
    "        # 定义l2损失\n",
    "        l2Loss = tf.constant(0.0)\n",
    "        \n",
    "        # 词嵌入层, 位置向量的定义方式有两种：一是直接用固定的one-hot的形式传入，然后和词向量拼接，在当前的数据集上表现效果更好。另一种\n",
    "        # 就是按照论文中的方法实现，这样的效果反而更差，可能是增大了模型的复杂度，在小数据集上表现不佳。\n",
    "        \n",
    "        with tf.name_scope(\"embedding\"):\n",
    "\n",
    "            # 利用预训练的词向量初始化词嵌入矩阵\n",
    "            self.W = tf.Variable(tf.cast(wordEmbedding, dtype=tf.float32, name=\"word2vec\") ,name=\"W\")\n",
    "            # 利用词嵌入矩阵将输入的数据中的词转换成词向量，维度[batch_size, sequence_length, embedding_size]\n",
    "            self.embedded = tf.nn.embedding_lookup(self.W, self.inputX)\n",
    "            self.embeddedWords = tf.concat([self.embedded, self.embeddedPosition], -1)\n",
    "\n",
    "        with tf.name_scope(\"transformer\"):\n",
    "            for i in range(config.model.numBlocks):\n",
    "                with tf.name_scope(\"transformer-{}\".format(i + 1)):\n",
    "            \n",
    "                    # 维度[batch_size, sequence_length, embedding_size]\n",
    "                    multiHeadAtt = self._multiheadAttention(rawKeys=self.embedded, queries=self.embeddedWords,\n",
    "                                                            keys=self.embeddedWords)\n",
    "                    # 维度[batch_size, sequence_length, embedding_size]\n",
    "                    self.embeddedWords = self._feedForward(multiHeadAtt, \n",
    "                                                           [config.model.filters, config.model.embeddingSize + config.sequenceLength])\n",
    "                \n",
    "            outputs = tf.reshape(self.embeddedWords, [-1, config.sequenceLength * (config.model.embeddingSize + config.sequenceLength)])\n",
    "\n",
    "        outputSize = outputs.get_shape()[-1].value\n",
    "\n",
    "#         with tf.name_scope(\"wordEmbedding\"):\n",
    "#             self.W = tf.Variable(tf.cast(wordEmbedding, dtype=tf.float32, name=\"word2vec\"), name=\"W\")\n",
    "#             self.wordEmbedded = tf.nn.embedding_lookup(self.W, self.inputX)\n",
    "        \n",
    "#         with tf.name_scope(\"positionEmbedding\"):\n",
    "#             print(self.wordEmbedded)\n",
    "#             self.positionEmbedded = self._positionEmbedding()\n",
    "            \n",
    "#         self.embeddedWords = self.wordEmbedded + self.positionEmbedded\n",
    "            \n",
    "#         with tf.name_scope(\"transformer\"):\n",
    "#             for i in range(config.model.numBlocks):\n",
    "#                 with tf.name_scope(\"transformer-{}\".format(i + 1)):\n",
    "            \n",
    "#                     # 维度[batch_size, sequence_length, embedding_size]\n",
    "#                     multiHeadAtt = self._multiheadAttention(rawKeys=self.wordEmbedded, queries=self.embeddedWords,\n",
    "#                                                             keys=self.embeddedWords)\n",
    "#                     # 维度[batch_size, sequence_length, embedding_size]\n",
    "#                     self.embeddedWords = self._feedForward(multiHeadAtt, [config.model.filters, config.model.embeddingSize])\n",
    "                \n",
    "#             outputs = tf.reshape(self.embeddedWords, [-1, config.sequenceLength * (config.model.embeddingSize)])\n",
    "\n",
    "#         outputSize = outputs.get_shape()[-1].value\n",
    "        \n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            outputs = tf.nn.dropout(outputs, keep_prob=self.dropoutKeepProb)\n",
    "    \n",
    "        # 全连接层的输出\n",
    "        with tf.name_scope(\"output\"):\n",
    "            outputW = tf.get_variable(\n",
    "                \"outputW\",\n",
    "                shape=[outputSize, 1],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "            outputB= tf.Variable(tf.constant(0.1, shape=[1]), name=\"outputB\")\n",
    "            l2Loss += tf.nn.l2_loss(outputW)\n",
    "            l2Loss += tf.nn.l2_loss(outputB)\n",
    "            self.predictions = tf.nn.xw_plus_b(outputs, outputW, outputB, name=\"predictions\")\n",
    "            self.binaryPreds = tf.cast(tf.greater_equal(self.predictions, 0.5), tf.float32, name=\"binaryPreds\")\n",
    "        \n",
    "        # 计算二元交叉熵损失\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            \n",
    "            losses = tf.nn.sigmoid_cross_entropy_with_logits(logits=self.predictions, labels=self.inputY)\n",
    "            self.loss = tf.reduce_mean(losses) + config.model.l2RegLambda * l2Loss\n",
    "            \n",
    "    def _layerNormalization(self, inputs, scope=\"layerNorm\"):\n",
    "        # LayerNorm层和BN层有所不同\n",
    "        epsilon = self.config.model.epsilon\n",
    "\n",
    "        inputsShape = inputs.get_shape() # [batch_size, sequence_length, embedding_size]\n",
    "\n",
    "        paramsShape = inputsShape[-1:]\n",
    "\n",
    "        # LayerNorm是在最后的维度上计算输入的数据的均值和方差，BN层是考虑所有维度的\n",
    "        # mean, variance的维度都是[batch_size, sequence_len, 1]\n",
    "        mean, variance = tf.nn.moments(inputs, [-1], keep_dims=True)\n",
    "\n",
    "        beta = tf.Variable(tf.zeros(paramsShape))\n",
    "\n",
    "        gamma = tf.Variable(tf.ones(paramsShape))\n",
    "        normalized = (inputs - mean) / ((variance + epsilon) ** .5)\n",
    "        \n",
    "        outputs = gamma * normalized + beta\n",
    "\n",
    "        return outputs\n",
    "            \n",
    "    def _multiheadAttention(self, rawKeys, queries, keys, numUnits=None, causality=False, scope=\"multiheadAttention\"):\n",
    "        # rawKeys 的作用是为了计算mask时用的，因为keys是加上了position embedding的，其中不存在padding为0的值\n",
    "        \n",
    "        numHeads = self.config.model.numHeads\n",
    "        keepProp = self.config.model.keepProp\n",
    "        \n",
    "        if numUnits is None:  # 若是没传入值，直接去输入数据的最后一维，即embedding size.\n",
    "            numUnits = queries.get_shape().as_list()[-1]\n",
    "\n",
    "        # tf.layers.dense可以做多维tensor数据的非线性映射，在计算self-Attention时，一定要对这三个值进行非线性映射，\n",
    "        # 其实这一步就是论文中Multi-Head Attention中的对分割后的数据进行权重映射的步骤，我们在这里先映射后分割，原则上是一样的。\n",
    "        # Q, K, V的维度都是[batch_size, sequence_length, embedding_size]\n",
    "        Q = tf.layers.dense(queries, numUnits, activation=tf.nn.relu)\n",
    "        K = tf.layers.dense(keys, numUnits, activation=tf.nn.relu)\n",
    "        V = tf.layers.dense(keys, numUnits, activation=tf.nn.relu)\n",
    "\n",
    "        # 将数据按最后一维分割成num_heads个, 然后按照第一维拼接\n",
    "        # Q, K, V 的维度都是[batch_size * numHeads, sequence_length, embedding_size/numHeads]\n",
    "        Q_ = tf.concat(tf.split(Q, numHeads, axis=-1), axis=0) \n",
    "        K_ = tf.concat(tf.split(K, numHeads, axis=-1), axis=0) \n",
    "        V_ = tf.concat(tf.split(V, numHeads, axis=-1), axis=0)\n",
    "\n",
    "        # 计算keys和queries之间的点积，维度[batch_size * numHeads, queries_len, key_len], 后两维是queries和keys的序列长度\n",
    "        similary = tf.matmul(Q_, tf.transpose(K_, [0, 2, 1]))\n",
    "\n",
    "        # 对计算的点积进行缩放处理，除以向量长度的根号值\n",
    "        scaledSimilary = similary / (K_.get_shape().as_list()[-1] ** 0.5)\n",
    "\n",
    "        # 在我们输入的序列中会存在padding这个样的填充词，这种词应该对最终的结果是毫无帮助的，原则上说当padding都是输入0时，\n",
    "        # 计算出来的权重应该也是0，但是在transformer中引入了位置向量，当和位置向量相加之后，其值就不为0了，因此在添加位置向量\n",
    "        # 之前，我们需要将其mask为0。虽然在queries中也存在这样的填充词，但原则上模型的结果之和输入有关，而且在self-Attention中\n",
    "        # queryies = keys，因此只要一方为0，计算出的权重就为0。\n",
    "        # 具体关于key mask的介绍可以看看这里： https://github.com/Kyubyong/transformer/issues/3\n",
    "\n",
    "        # 将每一时序上的向量中的值相加取平均值\n",
    "        keyMasks = tf.sign(tf.abs(tf.reduce_sum(rawKeys, axis=-1)))  # 维度[batch_size, time_step]\n",
    "\n",
    "        # 利用tf，tile进行张量扩张， 维度[batch_size * numHeads, keys_len] keys_len = keys 的序列长度\n",
    "        keyMasks = tf.tile(keyMasks, [numHeads, 1]) \n",
    "\n",
    "        # 增加一个维度，并进行扩张，得到维度[batch_size * numHeads, queries_len, keys_len]\n",
    "        keyMasks = tf.tile(tf.expand_dims(keyMasks, 1), [1, tf.shape(queries)[1], 1])\n",
    "\n",
    "        # tf.ones_like生成元素全为1，维度和scaledSimilary相同的tensor, 然后得到负无穷大的值\n",
    "        paddings = tf.ones_like(scaledSimilary) * (-2 ** (32 + 1))\n",
    "\n",
    "        # tf.where(condition, x, y),condition中的元素为bool值，其中对应的True用x中的元素替换，对应的False用y中的元素替换\n",
    "        # 因此condition,x,y的维度是一样的。下面就是keyMasks中的值为0就用paddings中的值替换\n",
    "        maskedSimilary = tf.where(tf.equal(keyMasks, 0), paddings, scaledSimilary) # 维度[batch_size * numHeads, queries_len, key_len]\n",
    "\n",
    "        # 在计算当前的词时，只考虑上文，不考虑下文，出现在Transformer Decoder中。在文本分类时，可以只用Transformer Encoder。\n",
    "        # Decoder是生成模型，主要用在语言生成中\n",
    "        if causality:\n",
    "            diagVals = tf.ones_like(maskedSimilary[0, :, :])  # [queries_len, keys_len]\n",
    "            tril = tf.contrib.linalg.LinearOperatorTriL(diagVals).to_dense()  # [queries_len, keys_len]\n",
    "            masks = tf.tile(tf.expand_dims(tril, 0), [tf.shape(maskedSimilary)[0], 1, 1])  # [batch_size * numHeads, queries_len, keys_len]\n",
    "\n",
    "            paddings = tf.ones_like(masks) * (-2 ** (32 + 1))\n",
    "            maskedSimilary = tf.where(tf.equal(masks, 0), paddings, maskedSimilary)  # [batch_size * numHeads, queries_len, keys_len]\n",
    "\n",
    "        # 通过softmax计算权重系数，维度 [batch_size * numHeads, queries_len, keys_len]\n",
    "        weights = tf.nn.softmax(maskedSimilary)\n",
    "\n",
    "        # 加权和得到输出值, 维度[batch_size * numHeads, sequence_length, embedding_size/numHeads]\n",
    "        outputs = tf.matmul(weights, V_)\n",
    "\n",
    "        # 将多头Attention计算的得到的输出重组成最初的维度[batch_size, sequence_length, embedding_size]\n",
    "        outputs = tf.concat(tf.split(outputs, numHeads, axis=0), axis=2)\n",
    "        \n",
    "        outputs = tf.nn.dropout(outputs, keep_prob=keepProp)\n",
    "\n",
    "        # 对每个subLayers建立残差连接，即H(x) = F(x) + x\n",
    "        outputs += queries\n",
    "        # normalization 层\n",
    "        outputs = self._layerNormalization(outputs)\n",
    "        return outputs\n",
    "\n",
    "    def _feedForward(self, inputs, filters, scope=\"multiheadAttention\"):\n",
    "        # 在这里的前向传播采用卷积神经网络\n",
    "        \n",
    "        # 内层\n",
    "        params = {\"inputs\": inputs, \"filters\": filters[0], \"kernel_size\": 1,\n",
    "                  \"activation\": tf.nn.relu, \"use_bias\": True}\n",
    "        outputs = tf.layers.conv1d(**params)\n",
    "\n",
    "        # 外层\n",
    "        params = {\"inputs\": outputs, \"filters\": filters[1], \"kernel_size\": 1,\n",
    "                  \"activation\": None, \"use_bias\": True}\n",
    "\n",
    "        # 这里用到了一维卷积，实际上卷积核尺寸还是二维的，只是只需要指定高度，宽度和embedding size的尺寸一致\n",
    "        # 维度[batch_size, sequence_length, embedding_size]\n",
    "        outputs = tf.layers.conv1d(**params)\n",
    "\n",
    "        # 残差连接\n",
    "        outputs += inputs\n",
    "\n",
    "        # 归一化处理\n",
    "        outputs = self._layerNormalization(outputs)\n",
    "\n",
    "        return outputs\n",
    "    \n",
    "    def _positionEmbedding(self, scope=\"positionEmbedding\"):\n",
    "        # 生成可训练的位置向量\n",
    "        batchSize = self.config.batchSize\n",
    "        sequenceLen = self.config.sequenceLength\n",
    "        embeddingSize = self.config.model.embeddingSize\n",
    "        \n",
    "        # 生成位置的索引，并扩张到batch中所有的样本上\n",
    "        positionIndex = tf.tile(tf.expand_dims(tf.range(sequenceLen), 0), [batchSize, 1])\n",
    "\n",
    "        # 根据正弦和余弦函数来获得每个位置上的embedding的第一部分\n",
    "        positionEmbedding = np.array([[pos / np.power(10000, (i-i%2) / embeddingSize) for i in range(embeddingSize)] \n",
    "                                      for pos in range(sequenceLen)])\n",
    "\n",
    "        # 然后根据奇偶性分别用sin和cos函数来包装\n",
    "        positionEmbedding[:, 0::2] = np.sin(positionEmbedding[:, 0::2])\n",
    "        positionEmbedding[:, 1::2] = np.cos(positionEmbedding[:, 1::2])\n",
    "\n",
    "        # 将positionEmbedding转换成tensor的格式\n",
    "        positionEmbedding_ = tf.cast(positionEmbedding, dtype=tf.float32)\n",
    "\n",
    "        # 得到三维的矩阵[batchSize, sequenceLen, embeddingSize]\n",
    "        positionEmbedded = tf.nn.embedding_lookup(positionEmbedding_, positionIndex)\n",
    "\n",
    "        return positionEmbedded\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义性能指标函数\n",
    "\n",
    "def mean(item):\n",
    "    return sum(item) / len(item)\n",
    "\n",
    "\n",
    "def genMetrics(trueY, predY, binaryPredY):\n",
    "    \"\"\"\n",
    "    生成acc和auc值\n",
    "    \"\"\"\n",
    "    auc = roc_auc_score(trueY, predY)\n",
    "    accuracy = accuracy_score(trueY, binaryPredY)\n",
    "    precision = precision_score(trueY, binaryPredY)\n",
    "    recall = recall_score(trueY, binaryPredY)\n",
    "    \n",
    "    return round(accuracy, 4), round(auc, 4), round(precision, 4), round(recall, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name dense/kernel:0/grad/hist is illegal; using dense/kernel_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name dense/kernel:0/grad/sparsity is illegal; using dense/kernel_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name dense/bias:0/grad/hist is illegal; using dense/bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name dense/bias:0/grad/sparsity is illegal; using dense/bias_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name dense_1/kernel:0/grad/hist is illegal; using dense_1/kernel_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name dense_1/kernel:0/grad/sparsity is illegal; using dense_1/kernel_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name dense_1/bias:0/grad/hist is illegal; using dense_1/bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name dense_1/bias:0/grad/sparsity is illegal; using dense_1/bias_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name dense_2/kernel:0/grad/hist is illegal; using dense_2/kernel_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name dense_2/kernel:0/grad/sparsity is illegal; using dense_2/kernel_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name dense_2/bias:0/grad/hist is illegal; using dense_2/bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name dense_2/bias:0/grad/sparsity is illegal; using dense_2/bias_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name transformer/transformer-1/Variable:0/grad/hist is illegal; using transformer/transformer-1/Variable_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name transformer/transformer-1/Variable:0/grad/sparsity is illegal; using transformer/transformer-1/Variable_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name transformer/transformer-1/Variable_1:0/grad/hist is illegal; using transformer/transformer-1/Variable_1_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name transformer/transformer-1/Variable_1:0/grad/sparsity is illegal; using transformer/transformer-1/Variable_1_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv1d/kernel:0/grad/hist is illegal; using conv1d/kernel_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv1d/kernel:0/grad/sparsity is illegal; using conv1d/kernel_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv1d/bias:0/grad/hist is illegal; using conv1d/bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv1d/bias:0/grad/sparsity is illegal; using conv1d/bias_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv1d_1/kernel:0/grad/hist is illegal; using conv1d_1/kernel_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv1d_1/kernel:0/grad/sparsity is illegal; using conv1d_1/kernel_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv1d_1/bias:0/grad/hist is illegal; using conv1d_1/bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv1d_1/bias:0/grad/sparsity is illegal; using conv1d_1/bias_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name transformer/transformer-1/Variable_2:0/grad/hist is illegal; using transformer/transformer-1/Variable_2_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name transformer/transformer-1/Variable_2:0/grad/sparsity is illegal; using transformer/transformer-1/Variable_2_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name transformer/transformer-1/Variable_3:0/grad/hist is illegal; using transformer/transformer-1/Variable_3_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name transformer/transformer-1/Variable_3:0/grad/sparsity is illegal; using transformer/transformer-1/Variable_3_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name outputW:0/grad/hist is illegal; using outputW_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name outputW:0/grad/sparsity is illegal; using outputW_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/outputB:0/grad/hist is illegal; using output/outputB_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/outputB:0/grad/sparsity is illegal; using output/outputB_0/grad/sparsity instead.\n",
      "Writing to /data4T/share/jiangxinyang848/textClassifier/Transformer/summarys\n",
      "\n",
      "start training model\n",
      "2019-01-14T17:47:06.529558, step: 1, loss: 1.161943793296814, acc: 0.4531, auc: 0.4388, precision: 0.4658, recall: 0.5231\n",
      "2019-01-14T17:47:06.749083, step: 2, loss: 22.561038970947266, acc: 0.4375, auc: 0.6124, precision: 0.0, recall: 0.0\n",
      "2019-01-14T17:47:06.961528, step: 3, loss: 6.254906177520752, acc: 0.4219, auc: 0.5128, precision: 0.0, recall: 0.0\n",
      "2019-01-14T17:47:07.186874, step: 4, loss: 11.581911087036133, acc: 0.5078, auc: 0.5165, precision: 0.5078, recall: 1.0\n",
      "2019-01-14T17:47:07.392005, step: 5, loss: 12.358612060546875, acc: 0.5312, auc: 0.4748, precision: 0.5312, recall: 1.0\n",
      "2019-01-14T17:47:07.634929, step: 6, loss: 7.0192718505859375, acc: 0.5078, auc: 0.5568, precision: 0.5078, recall: 1.0\n",
      "2019-01-14T17:47:07.871208, step: 7, loss: 3.2235946655273438, acc: 0.4844, auc: 0.5626, precision: 0.0, recall: 0.0\n",
      "2019-01-14T17:47:08.108671, step: 8, loss: 5.294422149658203, acc: 0.4375, auc: 0.5312, precision: 0.0, recall: 0.0\n",
      "2019-01-14T17:47:08.340422, step: 9, loss: 1.611175775527954, acc: 0.3984, auc: 0.4538, precision: 0.4, recall: 0.1096\n",
      "2019-01-14T17:47:08.566504, step: 10, loss: 3.848377227783203, acc: 0.5469, auc: 0.5411, precision: 0.5469, recall: 1.0\n",
      "2019-01-14T17:47:08.790762, step: 11, loss: 5.1824469566345215, acc: 0.5312, auc: 0.5103, precision: 0.5312, recall: 1.0\n",
      "2019-01-14T17:47:09.015491, step: 12, loss: 3.8226895332336426, acc: 0.4609, auc: 0.5113, precision: 0.4646, recall: 0.9833\n",
      "2019-01-14T17:47:09.242193, step: 13, loss: 1.4962658882141113, acc: 0.5078, auc: 0.5198, precision: 0.5833, recall: 0.3043\n",
      "2019-01-14T17:47:09.485040, step: 14, loss: 3.499901533126831, acc: 0.5, auc: 0.5645, precision: 1.0, recall: 0.0303\n",
      "2019-01-14T17:47:09.715041, step: 15, loss: 3.8533897399902344, acc: 0.4609, auc: 0.4628, precision: 1.0, recall: 0.0143\n",
      "2019-01-14T17:47:09.944345, step: 16, loss: 1.4965330362319946, acc: 0.5391, auc: 0.5158, precision: 0.5833, recall: 0.2222\n",
      "2019-01-14T17:47:10.189876, step: 17, loss: 2.518566131591797, acc: 0.4844, auc: 0.498, precision: 0.4828, recall: 0.9032\n",
      "2019-01-14T17:47:10.430356, step: 18, loss: 2.837937593460083, acc: 0.5312, auc: 0.5179, precision: 0.5203, recall: 0.9846\n",
      "2019-01-14T17:47:10.658641, step: 19, loss: 1.9301766157150269, acc: 0.5391, auc: 0.501, precision: 0.5575, recall: 0.875\n",
      "2019-01-14T17:47:10.903180, step: 20, loss: 1.3786370754241943, acc: 0.5156, auc: 0.5327, precision: 0.5781, recall: 0.5139\n",
      "2019-01-14T17:47:11.138495, step: 21, loss: 1.5548146963119507, acc: 0.5391, auc: 0.6197, precision: 0.5714, recall: 0.1935\n",
      "2019-01-14T17:47:11.369629, step: 22, loss: 2.467130184173584, acc: 0.4531, auc: 0.5009, precision: 0.0, recall: 0.0\n",
      "2019-01-14T17:47:11.609233, step: 23, loss: 1.4434618949890137, acc: 0.5156, auc: 0.6132, precision: 0.5862, recall: 0.2537\n",
      "2019-01-14T17:47:11.837145, step: 24, loss: 1.2097227573394775, acc: 0.6484, auc: 0.6393, precision: 0.6026, recall: 0.7705\n",
      "2019-01-14T17:47:12.070382, step: 25, loss: 1.6710671186447144, acc: 0.5703, auc: 0.533, precision: 0.5978, recall: 0.7534\n",
      "2019-01-14T17:47:12.290890, step: 26, loss: 1.6655457019805908, acc: 0.5391, auc: 0.6055, precision: 0.5, recall: 0.8136\n",
      "2019-01-14T17:47:12.517456, step: 27, loss: 1.7314577102661133, acc: 0.4766, auc: 0.4115, precision: 0.4091, recall: 0.4909\n",
      "2019-01-14T17:47:12.741957, step: 28, loss: 1.4314645528793335, acc: 0.5312, auc: 0.5576, precision: 0.4737, recall: 0.1525\n",
      "2019-01-14T17:47:12.977378, step: 29, loss: 1.5760104656219482, acc: 0.5781, auc: 0.5014, precision: 0.4615, recall: 0.1132\n",
      "2019-01-14T17:47:13.197658, step: 30, loss: 1.580709457397461, acc: 0.5391, auc: 0.5387, precision: 0.5, recall: 0.0847\n",
      "2019-01-14T17:47:13.426826, step: 31, loss: 1.2108442783355713, acc: 0.5234, auc: 0.5232, precision: 0.5769, recall: 0.4348\n",
      "2019-01-14T17:47:13.651572, step: 32, loss: 1.2516762018203735, acc: 0.5234, auc: 0.6232, precision: 0.4891, recall: 0.7627\n",
      "2019-01-14T17:47:13.875992, step: 33, loss: 1.5693861246109009, acc: 0.5234, auc: 0.5696, precision: 0.5051, recall: 0.8065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-14T17:47:14.107319, step: 34, loss: 1.0067329406738281, acc: 0.6094, auc: 0.6293, precision: 0.6234, recall: 0.6957\n",
      "2019-01-14T17:47:14.330665, step: 35, loss: 1.0151830911636353, acc: 0.5625, auc: 0.5846, precision: 0.5429, recall: 0.322\n",
      "2019-01-14T17:47:14.556159, step: 36, loss: 0.8511122465133667, acc: 0.6562, auc: 0.7115, precision: 0.6786, recall: 0.3519\n",
      "2019-01-14T17:47:14.775648, step: 37, loss: 1.3129496574401855, acc: 0.5312, auc: 0.6156, precision: 0.6667, recall: 0.1538\n",
      "2019-01-14T17:47:14.995183, step: 38, loss: 1.041608214378357, acc: 0.5781, auc: 0.5918, precision: 0.4565, recall: 0.42\n",
      "2019-01-14T17:47:15.215155, step: 39, loss: 1.0777957439422607, acc: 0.5938, auc: 0.6068, precision: 0.597, recall: 0.6154\n",
      "2019-01-14T17:47:15.440356, step: 40, loss: 1.0155107975006104, acc: 0.5938, auc: 0.623, precision: 0.5867, recall: 0.6769\n",
      "2019-01-14T17:47:15.661437, step: 41, loss: 0.8809636235237122, acc: 0.6172, auc: 0.6727, precision: 0.6721, recall: 0.5857\n",
      "2019-01-14T17:47:15.888936, step: 42, loss: 1.1937576532363892, acc: 0.5625, auc: 0.5321, precision: 0.4746, recall: 0.5283\n",
      "2019-01-14T17:47:16.120387, step: 43, loss: 1.1933856010437012, acc: 0.4922, auc: 0.524, precision: 0.439, recall: 0.3\n",
      "2019-01-14T17:47:16.344550, step: 44, loss: 1.2771806716918945, acc: 0.5469, auc: 0.5738, precision: 0.6333, recall: 0.2879\n",
      "2019-01-14T17:47:16.566608, step: 45, loss: 0.901960015296936, acc: 0.625, auc: 0.663, precision: 0.6061, recall: 0.3636\n",
      "2019-01-14T17:47:16.807438, step: 46, loss: 0.9298884868621826, acc: 0.5938, auc: 0.6416, precision: 0.6364, recall: 0.6\n",
      "2019-01-14T17:47:17.035235, step: 47, loss: 1.0822196006774902, acc: 0.6797, auc: 0.6917, precision: 0.6118, recall: 0.8667\n",
      "2019-01-14T17:47:17.262494, step: 48, loss: 0.8231915235519409, acc: 0.7109, auc: 0.7898, precision: 0.5846, recall: 0.7917\n",
      "2019-01-14T17:47:17.482155, step: 49, loss: 1.1101983785629272, acc: 0.5469, auc: 0.6459, precision: 0.7, recall: 0.2121\n",
      "2019-01-14T17:47:17.723737, step: 50, loss: 1.1783565282821655, acc: 0.6016, auc: 0.6141, precision: 0.6316, recall: 0.2143\n",
      "2019-01-14T17:47:17.959658, step: 51, loss: 0.8806886672973633, acc: 0.6328, auc: 0.6615, precision: 0.6042, recall: 0.5088\n",
      "2019-01-14T17:47:18.182393, step: 52, loss: 0.8770825266838074, acc: 0.6562, auc: 0.709, precision: 0.5694, recall: 0.7593\n",
      "2019-01-14T17:47:18.413414, step: 53, loss: 0.7585597038269043, acc: 0.6484, auc: 0.7578, precision: 0.6389, recall: 0.7077\n",
      "2019-01-14T17:47:18.646633, step: 54, loss: 0.868894636631012, acc: 0.6562, auc: 0.6747, precision: 0.6897, recall: 0.6061\n",
      "2019-01-14T17:47:18.867048, step: 55, loss: 0.8419317603111267, acc: 0.5859, auc: 0.7082, precision: 0.6429, recall: 0.4154\n",
      "2019-01-14T17:47:19.086389, step: 56, loss: 0.715196967124939, acc: 0.6641, auc: 0.7792, precision: 0.7381, recall: 0.4921\n",
      "2019-01-14T17:47:19.307539, step: 57, loss: 0.9654921889305115, acc: 0.6484, auc: 0.655, precision: 0.6964, recall: 0.5821\n",
      "2019-01-14T17:47:19.524122, step: 58, loss: 1.0411745309829712, acc: 0.6094, auc: 0.6415, precision: 0.6076, recall: 0.7164\n",
      "2019-01-14T17:47:19.744521, step: 59, loss: 0.9557716846466064, acc: 0.6406, auc: 0.6576, precision: 0.641, recall: 0.7353\n",
      "2019-01-14T17:47:19.979376, step: 60, loss: 0.8291990160942078, acc: 0.6719, auc: 0.7184, precision: 0.717, recall: 0.5846\n",
      "2019-01-14T17:47:20.213484, step: 61, loss: 0.8010590672492981, acc: 0.6719, auc: 0.7659, precision: 0.7576, recall: 0.4237\n",
      "2019-01-14T17:47:20.433030, step: 62, loss: 0.9844754934310913, acc: 0.6641, auc: 0.6857, precision: 0.7879, recall: 0.4194\n",
      "2019-01-14T17:47:20.655114, step: 63, loss: 0.8961812257766724, acc: 0.6562, auc: 0.7194, precision: 0.6567, recall: 0.6769\n",
      "2019-01-14T17:47:20.885896, step: 64, loss: 0.9049630165100098, acc: 0.6328, auc: 0.7351, precision: 0.5904, recall: 0.7903\n",
      "2019-01-14T17:47:21.106351, step: 65, loss: 0.5634601712226868, acc: 0.7812, auc: 0.8293, precision: 0.7966, recall: 0.746\n",
      "2019-01-14T17:47:21.340578, step: 66, loss: 0.7733883857727051, acc: 0.6719, auc: 0.7725, precision: 0.814, recall: 0.5072\n",
      "2019-01-14T17:47:21.570932, step: 67, loss: 0.6055219769477844, acc: 0.7344, auc: 0.8652, precision: 0.878, recall: 0.5538\n",
      "2019-01-14T17:47:21.801027, step: 68, loss: 0.6780505180358887, acc: 0.7188, auc: 0.7913, precision: 0.7541, recall: 0.6866\n",
      "2019-01-14T17:47:22.029320, step: 69, loss: 0.7097949981689453, acc: 0.7578, auc: 0.8172, precision: 0.7349, recall: 0.8714\n",
      "2019-01-14T17:47:22.262385, step: 70, loss: 0.7564873695373535, acc: 0.7266, auc: 0.7845, precision: 0.7143, recall: 0.8451\n",
      "2019-01-14T17:47:22.492204, step: 71, loss: 0.6744638085365295, acc: 0.7031, auc: 0.8078, precision: 0.7885, recall: 0.6029\n",
      "2019-01-14T17:47:22.715547, step: 72, loss: 0.6290930509567261, acc: 0.7578, auc: 0.8319, precision: 0.8163, recall: 0.6452\n",
      "2019-01-14T17:47:22.960082, step: 73, loss: 0.6341339349746704, acc: 0.7266, auc: 0.8169, precision: 0.7895, recall: 0.5263\n",
      "2019-01-14T17:47:23.182018, step: 74, loss: 0.49785786867141724, acc: 0.75, auc: 0.8696, precision: 0.8103, recall: 0.6912\n",
      "2019-01-14T17:47:23.412339, step: 75, loss: 0.5522749423980713, acc: 0.7344, auc: 0.8468, precision: 0.7812, recall: 0.7143\n",
      "2019-01-14T17:47:23.634594, step: 76, loss: 0.48126420378685, acc: 0.7969, auc: 0.8857, precision: 0.7973, recall: 0.8429\n",
      "2019-01-14T17:47:23.876783, step: 77, loss: 0.7185623645782471, acc: 0.7812, auc: 0.8297, precision: 0.7391, recall: 0.8361\n",
      "2019-01-14T17:47:24.100589, step: 78, loss: 0.6334874629974365, acc: 0.6797, auc: 0.873, precision: 0.8333, recall: 0.5072\n",
      "2019-01-14T17:47:24.327253, step: 79, loss: 0.6047064065933228, acc: 0.7969, auc: 0.8674, precision: 0.88, recall: 0.6875\n",
      "2019-01-14T17:47:24.555435, step: 80, loss: 0.5342386960983276, acc: 0.7812, auc: 0.8705, precision: 0.8065, recall: 0.7576\n",
      "2019-01-14T17:47:24.786215, step: 81, loss: 0.6819456815719604, acc: 0.8203, auc: 0.8735, precision: 0.7424, recall: 0.8909\n",
      "2019-01-14T17:47:25.025641, step: 82, loss: 0.6928894519805908, acc: 0.7812, auc: 0.8429, precision: 0.7538, recall: 0.8033\n",
      "2019-01-14T17:47:25.255710, step: 83, loss: 0.6198532581329346, acc: 0.8203, auc: 0.8772, precision: 0.875, recall: 0.7119\n",
      "2019-01-14T17:47:25.482701, step: 84, loss: 0.6522030830383301, acc: 0.7188, auc: 0.8835, precision: 0.8235, recall: 0.4828\n",
      "2019-01-14T17:47:25.714129, step: 85, loss: 0.5772627592086792, acc: 0.8047, auc: 0.8674, precision: 0.807, recall: 0.7667\n",
      "2019-01-14T17:47:25.929578, step: 86, loss: 0.8886159658432007, acc: 0.7969, auc: 0.8391, precision: 0.6912, recall: 0.9038\n",
      "2019-01-14T17:47:26.146614, step: 87, loss: 0.6829491257667542, acc: 0.7891, auc: 0.8595, precision: 0.8194, recall: 0.8082\n",
      "2019-01-14T17:47:26.375378, step: 88, loss: 0.46635162830352783, acc: 0.8281, auc: 0.9211, precision: 0.9231, recall: 0.7273\n",
      "2019-01-14T17:47:26.605352, step: 89, loss: 0.5954897999763489, acc: 0.7891, auc: 0.8727, precision: 0.9, recall: 0.72\n",
      "2019-01-14T17:47:26.840105, step: 90, loss: 0.46447819471359253, acc: 0.7969, auc: 0.8975, precision: 0.8475, recall: 0.7463\n",
      "2019-01-14T17:47:27.062040, step: 91, loss: 0.6209449768066406, acc: 0.7969, auc: 0.8581, precision: 0.7419, recall: 0.8214\n",
      "2019-01-14T17:47:27.293949, step: 92, loss: 0.5911248922348022, acc: 0.8203, auc: 0.8844, precision: 0.7627, recall: 0.8333\n",
      "2019-01-14T17:47:27.527148, step: 93, loss: 0.47271597385406494, acc: 0.8359, auc: 0.9124, precision: 0.9423, recall: 0.7313\n",
      "2019-01-14T17:47:27.750983, step: 94, loss: 0.43947094678878784, acc: 0.8047, auc: 0.9126, precision: 0.9149, recall: 0.6719\n",
      "2019-01-14T17:47:27.986625, step: 95, loss: 0.34385281801223755, acc: 0.8359, auc: 0.9282, precision: 0.8814, recall: 0.7879\n",
      "2019-01-14T17:47:28.216736, step: 96, loss: 0.49884018301963806, acc: 0.8516, auc: 0.9101, precision: 0.8116, recall: 0.9032\n",
      "2019-01-14T17:47:28.444454, step: 97, loss: 0.9715480804443359, acc: 0.7266, auc: 0.8216, precision: 0.6447, recall: 0.8596\n",
      "2019-01-14T17:47:28.669102, step: 98, loss: 0.646484375, acc: 0.8047, auc: 0.8904, precision: 0.9167, recall: 0.6769\n",
      "2019-01-14T17:47:28.889340, step: 99, loss: 0.4897024631500244, acc: 0.8203, auc: 0.9313, precision: 0.9444, recall: 0.6182\n",
      "2019-01-14T17:47:29.110321, step: 100, loss: 0.6445068120956421, acc: 0.8281, auc: 0.8718, precision: 0.925, recall: 0.6607\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-14T17:47:38.052782, step: 100, loss: 0.4927764603724846, acc: 0.8243256410256412, auc: 0.9054974358974357, precision: 0.8074358974358974, recall: 0.8587153846153845\n",
      "2019-01-14T17:47:38.267294, step: 101, loss: 0.6378569602966309, acc: 0.8047, auc: 0.8796, precision: 0.7778, recall: 0.8615\n",
      "2019-01-14T17:47:38.486141, step: 102, loss: 0.689957857131958, acc: 0.7969, auc: 0.8988, precision: 0.697, recall: 0.8846\n",
      "2019-01-14T17:47:38.708786, step: 103, loss: 0.5570914149284363, acc: 0.8047, auc: 0.8889, precision: 0.8621, recall: 0.7463\n",
      "2019-01-14T17:47:38.934067, step: 104, loss: 0.660875141620636, acc: 0.7812, auc: 0.8881, precision: 0.8772, recall: 0.7042\n",
      "2019-01-14T17:47:39.155304, step: 105, loss: 0.5929709672927856, acc: 0.7266, auc: 0.8971, precision: 0.8491, recall: 0.625\n",
      "2019-01-14T17:47:39.374979, step: 106, loss: 0.6518077254295349, acc: 0.8125, auc: 0.8696, precision: 0.7895, recall: 0.8824\n",
      "2019-01-14T17:47:39.594123, step: 107, loss: 0.8777315020561218, acc: 0.7578, auc: 0.8283, precision: 0.7237, recall: 0.8462\n",
      "2019-01-14T17:47:39.813992, step: 108, loss: 0.7176794409751892, acc: 0.7812, auc: 0.8442, precision: 0.7465, recall: 0.8413\n",
      "2019-01-14T17:47:40.040386, step: 109, loss: 0.7109244465827942, acc: 0.7109, auc: 0.8805, precision: 0.8723, recall: 0.5694\n",
      "2019-01-14T17:47:40.269163, step: 110, loss: 0.5282332301139832, acc: 0.8359, auc: 0.9091, precision: 0.9302, recall: 0.6897\n",
      "2019-01-14T17:47:40.491488, step: 111, loss: 0.5347272753715515, acc: 0.7969, auc: 0.8889, precision: 0.8889, recall: 0.6557\n",
      "2019-01-14T17:47:40.713983, step: 112, loss: 0.8017709255218506, acc: 0.7656, auc: 0.8629, precision: 0.6849, recall: 0.8772\n",
      "2019-01-14T17:47:40.933408, step: 113, loss: 0.6584199666976929, acc: 0.8203, auc: 0.8839, precision: 0.76, recall: 0.9194\n",
      "2019-01-14T17:47:41.152867, step: 114, loss: 0.6302224397659302, acc: 0.7812, auc: 0.8677, precision: 0.85, recall: 0.7286\n",
      "2019-01-14T17:47:41.361871, step: 115, loss: 0.8613229990005493, acc: 0.7734, auc: 0.8766, precision: 0.9512, recall: 0.5909\n",
      "2019-01-14T17:47:41.592136, step: 116, loss: 0.6691520810127258, acc: 0.7578, auc: 0.8839, precision: 0.9574, recall: 0.6081\n",
      "2019-01-14T17:47:41.822698, step: 117, loss: 0.7549316883087158, acc: 0.7578, auc: 0.8681, precision: 0.7125, recall: 0.8769\n",
      "2019-01-14T17:47:42.051259, step: 118, loss: 0.9239593148231506, acc: 0.7578, auc: 0.8708, precision: 0.7065, recall: 0.942\n",
      "2019-01-14T17:47:42.288718, step: 119, loss: 0.36969447135925293, acc: 0.875, auc: 0.9289, precision: 0.8846, recall: 0.9079\n",
      "2019-01-14T17:47:42.518315, step: 120, loss: 0.9097288846969604, acc: 0.6953, auc: 0.8233, precision: 0.8372, recall: 0.5294\n",
      "2019-01-14T17:47:42.745534, step: 121, loss: 0.5696455836296082, acc: 0.7812, auc: 0.9074, precision: 0.8846, recall: 0.6765\n",
      "2019-01-14T17:47:42.980067, step: 122, loss: 0.6574018001556396, acc: 0.7812, auc: 0.8513, precision: 0.7903, recall: 0.7656\n",
      "2019-01-14T17:47:43.201086, step: 123, loss: 0.8381141424179077, acc: 0.7578, auc: 0.8245, precision: 0.7333, recall: 0.8333\n",
      "2019-01-14T17:47:43.440836, step: 124, loss: 0.5410323143005371, acc: 0.8125, auc: 0.9012, precision: 0.8594, recall: 0.7857\n",
      "2019-01-14T17:47:43.673594, step: 125, loss: 0.45745086669921875, acc: 0.7891, auc: 0.9094, precision: 0.7606, recall: 0.8438\n",
      "2019-01-14T17:47:43.893098, step: 126, loss: 0.6214828491210938, acc: 0.8047, auc: 0.8708, precision: 0.8387, recall: 0.7761\n",
      "2019-01-14T17:47:44.113958, step: 127, loss: 0.5079634785652161, acc: 0.8438, auc: 0.8975, precision: 0.9123, recall: 0.7761\n",
      "2019-01-14T17:47:44.338280, step: 128, loss: 0.7344415187835693, acc: 0.7188, auc: 0.8545, precision: 0.8182, recall: 0.6338\n",
      "2019-01-14T17:47:44.562244, step: 129, loss: 0.7566214799880981, acc: 0.7422, auc: 0.83, precision: 0.7885, recall: 0.6508\n",
      "2019-01-14T17:47:44.783633, step: 130, loss: 0.6115850806236267, acc: 0.7656, auc: 0.8657, precision: 0.7246, recall: 0.8197\n",
      "2019-01-14T17:47:45.013300, step: 131, loss: 0.8886637687683105, acc: 0.7656, auc: 0.8363, precision: 0.7317, recall: 0.8824\n",
      "2019-01-14T17:47:45.233495, step: 132, loss: 0.6112560629844666, acc: 0.8125, auc: 0.8866, precision: 0.913, recall: 0.6774\n",
      "2019-01-14T17:47:45.460050, step: 133, loss: 0.6025018692016602, acc: 0.7812, auc: 0.8854, precision: 0.7872, recall: 0.6727\n",
      "2019-01-14T17:47:45.679668, step: 134, loss: 0.3023849427700043, acc: 0.8438, auc: 0.9443, precision: 0.881, recall: 0.7115\n",
      "2019-01-14T17:47:45.901793, step: 135, loss: 0.6539618372917175, acc: 0.7812, auc: 0.8662, precision: 0.8214, recall: 0.7188\n",
      "2019-01-14T17:47:46.129655, step: 136, loss: 0.7431955933570862, acc: 0.7969, auc: 0.8663, precision: 0.7683, recall: 0.9\n",
      "2019-01-14T17:47:46.351543, step: 137, loss: 0.34635937213897705, acc: 0.875, auc: 0.9478, precision: 0.8438, recall: 0.9\n",
      "2019-01-14T17:47:46.594362, step: 138, loss: 0.4279009997844696, acc: 0.8359, auc: 0.9266, precision: 0.8704, recall: 0.7705\n",
      "2019-01-14T17:47:46.821134, step: 139, loss: 0.5139654874801636, acc: 0.8203, auc: 0.9075, precision: 0.8929, recall: 0.7463\n",
      "2019-01-14T17:47:47.042371, step: 140, loss: 0.325947105884552, acc: 0.8594, auc: 0.9439, precision: 0.8723, recall: 0.7736\n",
      "2019-01-14T17:47:47.261285, step: 141, loss: 0.3517540991306305, acc: 0.875, auc: 0.9427, precision: 0.8462, recall: 0.9016\n",
      "2019-01-14T17:47:47.482269, step: 142, loss: 0.6582828760147095, acc: 0.8047, auc: 0.8611, precision: 0.75, recall: 0.7925\n",
      "2019-01-14T17:47:47.701997, step: 143, loss: 0.574030339717865, acc: 0.7812, auc: 0.8889, precision: 0.8491, recall: 0.6923\n",
      "2019-01-14T17:47:47.912665, step: 144, loss: 0.45921164751052856, acc: 0.8438, auc: 0.9163, precision: 0.7966, recall: 0.8545\n",
      "2019-01-14T17:47:48.130585, step: 145, loss: 0.5069040060043335, acc: 0.7969, auc: 0.8926, precision: 0.8654, recall: 0.7031\n",
      "2019-01-14T17:47:48.352527, step: 146, loss: 0.6343206763267517, acc: 0.7969, auc: 0.9027, precision: 0.9375, recall: 0.6618\n",
      "2019-01-14T17:47:48.573665, step: 147, loss: 0.6528367400169373, acc: 0.7812, auc: 0.8631, precision: 0.8333, recall: 0.7895\n",
      "2019-01-14T17:47:48.792680, step: 148, loss: 0.44041699171066284, acc: 0.8047, auc: 0.9187, precision: 0.75, recall: 0.8644\n",
      "2019-01-14T17:47:49.014096, step: 149, loss: 0.6948171257972717, acc: 0.7344, auc: 0.8911, precision: 0.6579, recall: 0.8621\n",
      "2019-01-14T17:47:49.226163, step: 150, loss: 0.43737226724624634, acc: 0.7891, auc: 0.9168, precision: 0.8704, recall: 0.7015\n",
      "2019-01-14T17:47:49.447686, step: 151, loss: 0.4454326629638672, acc: 0.7656, auc: 0.94, precision: 0.8571, recall: 0.5455\n",
      "2019-01-14T17:47:49.668807, step: 152, loss: 0.5659304857254028, acc: 0.7734, auc: 0.9138, precision: 0.9362, recall: 0.6286\n",
      "2019-01-14T17:47:49.900389, step: 153, loss: 0.630021870136261, acc: 0.8125, auc: 0.8801, precision: 0.803, recall: 0.8281\n",
      "2019-01-14T17:47:50.126749, step: 154, loss: 0.5643057227134705, acc: 0.8047, auc: 0.8953, precision: 0.7746, recall: 0.8594\n",
      "2019-01-14T17:47:50.359476, step: 155, loss: 0.6505475044250488, acc: 0.7891, auc: 0.9201, precision: 0.6986, recall: 0.9107\n",
      "2019-01-14T17:47:50.578173, step: 156, loss: 0.41551101207733154, acc: 0.8438, auc: 0.9321, precision: 0.9259, recall: 0.7576\n",
      "start training model\n",
      "2019-01-14T17:47:50.836807, step: 157, loss: 0.5630733966827393, acc: 0.8203, auc: 0.9225, precision: 0.9245, recall: 0.7206\n",
      "2019-01-14T17:47:51.055432, step: 158, loss: 0.44435766339302063, acc: 0.8203, auc: 0.9543, precision: 0.9388, recall: 0.697\n",
      "2019-01-14T17:47:51.278296, step: 159, loss: 0.25286126136779785, acc: 0.9062, auc: 0.9661, precision: 0.8939, recall: 0.9219\n",
      "2019-01-14T17:47:51.511449, step: 160, loss: 0.34598439931869507, acc: 0.8828, auc: 0.9604, precision: 0.8514, recall: 0.9403\n",
      "2019-01-14T17:47:51.732606, step: 161, loss: 0.557623028755188, acc: 0.8438, auc: 0.9416, precision: 0.7654, recall: 0.9841\n",
      "2019-01-14T17:47:51.954243, step: 162, loss: 0.2916775941848755, acc: 0.9062, auc: 0.9685, precision: 0.8269, recall: 0.9348\n",
      "2019-01-14T17:47:52.175523, step: 163, loss: 0.7556991577148438, acc: 0.7109, auc: 0.9479, precision: 0.9583, recall: 0.3898\n",
      "2019-01-14T17:47:52.393972, step: 164, loss: 0.49771857261657715, acc: 0.8125, auc: 0.9783, precision: 1.0, recall: 0.6308\n",
      "2019-01-14T17:47:52.630696, step: 165, loss: 0.44911667704582214, acc: 0.8438, auc: 0.9301, precision: 0.8636, recall: 0.8382\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-14T17:47:52.856468, step: 166, loss: 0.8000973463058472, acc: 0.7656, auc: 0.9108, precision: 0.6951, recall: 0.9194\n",
      "2019-01-14T17:47:53.083006, step: 167, loss: 0.5640276074409485, acc: 0.8359, auc: 0.9384, precision: 0.7733, recall: 0.9355\n",
      "2019-01-14T17:47:53.302194, step: 168, loss: 0.35207679867744446, acc: 0.8594, auc: 0.9422, precision: 0.8955, recall: 0.8451\n",
      "2019-01-14T17:47:53.516404, step: 169, loss: 0.3525891900062561, acc: 0.8203, auc: 0.9538, precision: 0.8704, recall: 0.746\n",
      "2019-01-14T17:47:53.733768, step: 170, loss: 0.42336422204971313, acc: 0.8125, auc: 0.9831, precision: 1.0, recall: 0.6066\n",
      "2019-01-14T17:47:53.944170, step: 171, loss: 0.26315364241600037, acc: 0.8828, auc: 0.9655, precision: 0.9388, recall: 0.7931\n",
      "2019-01-14T17:47:54.174560, step: 172, loss: 0.5387764573097229, acc: 0.8125, auc: 0.9114, precision: 0.8028, recall: 0.8507\n",
      "2019-01-14T17:47:54.409961, step: 173, loss: 0.4923793077468872, acc: 0.8516, auc: 0.9399, precision: 0.8228, recall: 0.9286\n",
      "2019-01-14T17:47:54.636657, step: 174, loss: 0.3801705241203308, acc: 0.8672, auc: 0.9422, precision: 0.84, recall: 0.9265\n",
      "2019-01-14T17:47:54.854803, step: 175, loss: 0.33921563625335693, acc: 0.875, auc: 0.9436, precision: 0.8636, recall: 0.8906\n",
      "2019-01-14T17:47:55.067292, step: 176, loss: 0.4366544187068939, acc: 0.8594, auc: 0.9582, precision: 0.9787, recall: 0.7302\n",
      "2019-01-14T17:47:55.287281, step: 177, loss: 0.3529517352581024, acc: 0.8672, auc: 0.9701, precision: 0.9683, recall: 0.8026\n",
      "2019-01-14T17:47:55.512125, step: 178, loss: 0.3158065676689148, acc: 0.8594, auc: 0.9521, precision: 0.9355, recall: 0.8056\n",
      "2019-01-14T17:47:55.729748, step: 179, loss: 0.30528563261032104, acc: 0.8594, auc: 0.9579, precision: 0.8226, recall: 0.8793\n",
      "2019-01-14T17:47:55.956451, step: 180, loss: 0.44621869921684265, acc: 0.8672, auc: 0.9556, precision: 0.7895, recall: 0.9\n",
      "2019-01-14T17:47:56.189205, step: 181, loss: 0.38547754287719727, acc: 0.875, auc: 0.9492, precision: 0.8551, recall: 0.9077\n",
      "2019-01-14T17:47:56.405495, step: 182, loss: 0.3483528196811676, acc: 0.9062, auc: 0.9632, precision: 0.9828, recall: 0.8382\n",
      "2019-01-14T17:47:56.644460, step: 183, loss: 0.5670741200447083, acc: 0.8047, auc: 0.9142, precision: 0.8727, recall: 0.7273\n",
      "2019-01-14T17:47:56.864964, step: 184, loss: 0.3555258512496948, acc: 0.8906, auc: 0.9531, precision: 0.9153, recall: 0.8571\n",
      "2019-01-14T17:47:57.098344, step: 185, loss: 0.2207947075366974, acc: 0.8984, auc: 0.9755, precision: 0.9701, recall: 0.8553\n",
      "2019-01-14T17:47:57.332443, step: 186, loss: 0.24743075668811798, acc: 0.8594, auc: 0.9676, precision: 0.9032, recall: 0.8235\n",
      "2019-01-14T17:47:57.562162, step: 187, loss: 0.6089886426925659, acc: 0.8281, auc: 0.9211, precision: 0.7895, recall: 0.9091\n",
      "2019-01-14T17:47:57.783227, step: 188, loss: 0.33888113498687744, acc: 0.875, auc: 0.9725, precision: 0.8308, recall: 0.9153\n",
      "2019-01-14T17:47:58.001314, step: 189, loss: 0.3948846459388733, acc: 0.8438, auc: 0.9451, precision: 0.9091, recall: 0.7692\n",
      "2019-01-14T17:47:58.233632, step: 190, loss: 0.15009069442749023, acc: 0.9375, auc: 0.9866, precision: 0.9655, recall: 0.9032\n",
      "2019-01-14T17:47:58.460386, step: 191, loss: 0.3997736871242523, acc: 0.8828, auc: 0.9522, precision: 0.9688, recall: 0.8267\n",
      "2019-01-14T17:47:58.701376, step: 192, loss: 0.3278076648712158, acc: 0.875, auc: 0.9479, precision: 0.9167, recall: 0.8333\n",
      "2019-01-14T17:47:58.934271, step: 193, loss: 0.36473768949508667, acc: 0.8828, auc: 0.9538, precision: 0.8788, recall: 0.8923\n",
      "2019-01-14T17:47:59.160712, step: 194, loss: 0.14746665954589844, acc: 0.9219, auc: 0.9873, precision: 0.9206, recall: 0.9206\n",
      "2019-01-14T17:47:59.381434, step: 195, loss: 0.5238224267959595, acc: 0.8125, auc: 0.916, precision: 0.806, recall: 0.8308\n",
      "2019-01-14T17:47:59.598039, step: 196, loss: 0.2885211408138275, acc: 0.8984, auc: 0.9589, precision: 0.9038, recall: 0.8545\n",
      "2019-01-14T17:47:59.815078, step: 197, loss: 0.21847739815711975, acc: 0.9141, auc: 0.9736, precision: 0.8833, recall: 0.9298\n",
      "2019-01-14T17:48:00.073985, step: 198, loss: 0.32016170024871826, acc: 0.8672, auc: 0.9533, precision: 0.9412, recall: 0.7742\n",
      "2019-01-14T17:48:00.295488, step: 199, loss: 0.4213046133518219, acc: 0.8203, auc: 0.9448, precision: 0.9216, recall: 0.7121\n",
      "2019-01-14T17:48:00.527543, step: 200, loss: 0.43530339002609253, acc: 0.8359, auc: 0.9269, precision: 0.8475, recall: 0.8065\n",
      "\n",
      "Evaluation:\n",
      "2019-01-14T17:48:08.912416, step: 200, loss: 0.42642788130503434, acc: 0.8473564102564101, auc: 0.9276153846153844, precision: 0.8511461538461538, recall: 0.848576923076923\n",
      "2019-01-14T17:48:09.128450, step: 201, loss: 0.3003205955028534, acc: 0.8984, auc: 0.9608, precision: 0.9014, recall: 0.9143\n",
      "2019-01-14T17:48:09.341859, step: 202, loss: 0.2514224946498871, acc: 0.9062, auc: 0.9741, precision: 0.8689, recall: 0.9298\n",
      "2019-01-14T17:48:09.555973, step: 203, loss: 0.2068299800157547, acc: 0.9062, auc: 0.9772, precision: 0.9286, recall: 0.8667\n",
      "2019-01-14T17:48:09.773089, step: 204, loss: 0.29068148136138916, acc: 0.8672, auc: 0.9621, precision: 0.8929, recall: 0.8197\n",
      "2019-01-14T17:48:09.997134, step: 205, loss: 0.2837989926338196, acc: 0.8828, auc: 0.9649, precision: 0.9091, recall: 0.8696\n",
      "2019-01-14T17:48:10.216621, step: 206, loss: 0.3127215504646301, acc: 0.8906, auc: 0.9579, precision: 0.9123, recall: 0.8525\n",
      "2019-01-14T17:48:10.429669, step: 207, loss: 0.30464673042297363, acc: 0.8594, auc: 0.9585, precision: 0.9, recall: 0.8182\n",
      "2019-01-14T17:48:10.647467, step: 208, loss: 0.2651321291923523, acc: 0.9062, auc: 0.9639, precision: 0.9254, recall: 0.8986\n",
      "2019-01-14T17:48:10.866652, step: 209, loss: 0.3348333239555359, acc: 0.8906, auc: 0.9517, precision: 0.9091, recall: 0.8824\n",
      "2019-01-14T17:48:11.083037, step: 210, loss: 0.42928582429885864, acc: 0.8828, auc: 0.938, precision: 0.8462, recall: 0.9167\n",
      "2019-01-14T17:48:11.302156, step: 211, loss: 0.3105262815952301, acc: 0.8828, auc: 0.9547, precision: 0.9194, recall: 0.8507\n",
      "2019-01-14T17:48:11.523229, step: 212, loss: 0.32854098081588745, acc: 0.8594, auc: 0.9536, precision: 0.9074, recall: 0.7903\n",
      "2019-01-14T17:48:11.731156, step: 213, loss: 0.3132610321044922, acc: 0.8984, auc: 0.9569, precision: 0.9444, recall: 0.8361\n",
      "2019-01-14T17:48:11.957834, step: 214, loss: 0.3586476445198059, acc: 0.8828, auc: 0.9492, precision: 0.9123, recall: 0.8387\n",
      "2019-01-14T17:48:12.169705, step: 215, loss: 0.4883171319961548, acc: 0.8438, auc: 0.923, precision: 0.8704, recall: 0.7833\n",
      "2019-01-14T17:48:12.385080, step: 216, loss: 0.43098336458206177, acc: 0.8281, auc: 0.9277, precision: 0.8235, recall: 0.8485\n",
      "2019-01-14T17:48:12.599752, step: 217, loss: 0.49132806062698364, acc: 0.8594, auc: 0.9232, precision: 0.8308, recall: 0.8852\n",
      "2019-01-14T17:48:12.818995, step: 218, loss: 0.4049844741821289, acc: 0.8516, auc: 0.956, precision: 0.9444, recall: 0.7612\n",
      "2019-01-14T17:48:13.037288, step: 219, loss: 0.4682174026966095, acc: 0.8438, auc: 0.917, precision: 0.8667, recall: 0.7358\n",
      "2019-01-14T17:48:13.249954, step: 220, loss: 0.4897547960281372, acc: 0.8281, auc: 0.9165, precision: 0.8852, recall: 0.7826\n",
      "2019-01-14T17:48:13.459476, step: 221, loss: 0.32293564081192017, acc: 0.8438, auc: 0.9471, precision: 0.8254, recall: 0.8525\n",
      "2019-01-14T17:48:13.688070, step: 222, loss: 0.3665812611579895, acc: 0.8594, auc: 0.9485, precision: 0.8387, recall: 0.8667\n",
      "2019-01-14T17:48:13.906069, step: 223, loss: 0.3917844295501709, acc: 0.875, auc: 0.9365, precision: 0.9273, recall: 0.8095\n",
      "2019-01-14T17:48:14.126428, step: 224, loss: 0.327958345413208, acc: 0.8359, auc: 0.9605, precision: 0.9032, recall: 0.7887\n",
      "2019-01-14T17:48:14.337955, step: 225, loss: 0.3045070171356201, acc: 0.8828, auc: 0.9489, precision: 0.873, recall: 0.8871\n",
      "2019-01-14T17:48:14.571322, step: 226, loss: 0.3635100722312927, acc: 0.8906, auc: 0.9466, precision: 0.8929, recall: 0.8621\n",
      "2019-01-14T17:48:14.800745, step: 227, loss: 0.3522961139678955, acc: 0.8672, auc: 0.9452, precision: 0.8519, recall: 0.8364\n",
      "2019-01-14T17:48:15.026932, step: 228, loss: 0.4671018421649933, acc: 0.8203, auc: 0.9323, precision: 0.931, recall: 0.7397\n",
      "2019-01-14T17:48:15.269945, step: 229, loss: 0.3279983401298523, acc: 0.8672, auc: 0.9502, precision: 0.8852, recall: 0.8438\n",
      "2019-01-14T17:48:15.503718, step: 230, loss: 0.2748516798019409, acc: 0.8438, auc: 0.966, precision: 0.8286, recall: 0.8788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-14T17:48:15.723874, step: 231, loss: 0.3040640652179718, acc: 0.8984, auc: 0.9582, precision: 0.8857, recall: 0.9254\n",
      "2019-01-14T17:48:15.950867, step: 232, loss: 0.31216657161712646, acc: 0.8984, auc: 0.9485, precision: 0.9322, recall: 0.8594\n",
      "2019-01-14T17:48:16.179284, step: 233, loss: 0.45784682035446167, acc: 0.8125, auc: 0.9295, precision: 0.8776, recall: 0.7049\n",
      "2019-01-14T17:48:16.407975, step: 234, loss: 0.33681797981262207, acc: 0.8672, auc: 0.9514, precision: 0.9091, recall: 0.8065\n",
      "2019-01-14T17:48:16.618809, step: 235, loss: 0.4005366861820221, acc: 0.8125, auc: 0.9291, precision: 0.82, recall: 0.7321\n",
      "2019-01-14T17:48:16.837597, step: 236, loss: 0.3651196360588074, acc: 0.8594, auc: 0.9466, precision: 0.8906, recall: 0.8382\n",
      "2019-01-14T17:48:17.053995, step: 237, loss: 0.4534922242164612, acc: 0.8359, auc: 0.9279, precision: 0.8254, recall: 0.8387\n",
      "2019-01-14T17:48:17.282156, step: 238, loss: 0.2123047262430191, acc: 0.9062, auc: 0.9714, precision: 0.9062, recall: 0.9062\n",
      "2019-01-14T17:48:17.502170, step: 239, loss: 0.2988378703594208, acc: 0.8906, auc: 0.9557, precision: 0.9123, recall: 0.8525\n",
      "2019-01-14T17:48:17.742544, step: 240, loss: 0.4723268449306488, acc: 0.8672, auc: 0.9264, precision: 0.95, recall: 0.8028\n",
      "2019-01-14T17:48:17.962447, step: 241, loss: 0.3484172821044922, acc: 0.8984, auc: 0.9446, precision: 0.8776, recall: 0.86\n",
      "2019-01-14T17:48:18.214969, step: 242, loss: 0.4202881157398224, acc: 0.8516, auc: 0.9286, precision: 0.8413, recall: 0.8548\n",
      "2019-01-14T17:48:18.435581, step: 243, loss: 0.2806923985481262, acc: 0.8906, auc: 0.9611, precision: 0.875, recall: 0.9032\n",
      "2019-01-14T17:48:18.683077, step: 244, loss: 0.43595680594444275, acc: 0.8438, auc: 0.9245, precision: 0.9, recall: 0.7941\n",
      "2019-01-14T17:48:18.926851, step: 245, loss: 0.3969246745109558, acc: 0.8516, auc: 0.94, precision: 0.8868, recall: 0.7833\n",
      "2019-01-14T17:48:19.156934, step: 246, loss: 0.24590110778808594, acc: 0.9219, auc: 0.9697, precision: 0.9643, recall: 0.871\n",
      "2019-01-14T17:48:19.384195, step: 247, loss: 0.3608994483947754, acc: 0.8828, auc: 0.9489, precision: 0.8615, recall: 0.9032\n",
      "2019-01-14T17:48:19.616364, step: 248, loss: 0.6431405544281006, acc: 0.8203, auc: 0.8865, precision: 0.8382, recall: 0.8261\n",
      "2019-01-14T17:48:19.841605, step: 249, loss: 0.3854619860649109, acc: 0.875, auc: 0.9368, precision: 0.8871, recall: 0.8594\n",
      "2019-01-14T17:48:20.052181, step: 250, loss: 0.32304757833480835, acc: 0.8906, auc: 0.9547, precision: 0.9608, recall: 0.8033\n",
      "2019-01-14T17:48:20.287139, step: 251, loss: 0.3354235291481018, acc: 0.8906, auc: 0.9523, precision: 0.9231, recall: 0.8696\n",
      "2019-01-14T17:48:20.507029, step: 252, loss: 0.22181779146194458, acc: 0.9062, auc: 0.98, precision: 0.8788, recall: 0.9355\n",
      "2019-01-14T17:48:20.741401, step: 253, loss: 0.4016790986061096, acc: 0.8438, auc: 0.9414, precision: 0.8704, recall: 0.7833\n",
      "2019-01-14T17:48:20.966649, step: 254, loss: 0.408450186252594, acc: 0.8594, auc: 0.9486, precision: 0.9143, recall: 0.8421\n",
      "2019-01-14T17:48:21.192315, step: 255, loss: 0.3443254232406616, acc: 0.8984, auc: 0.9507, precision: 0.9286, recall: 0.8904\n",
      "2019-01-14T17:48:21.413225, step: 256, loss: 0.34279605746269226, acc: 0.875, auc: 0.9651, precision: 0.8333, recall: 0.9375\n",
      "2019-01-14T17:48:21.640331, step: 257, loss: 0.38599520921707153, acc: 0.8828, auc: 0.9451, precision: 0.8767, recall: 0.9143\n",
      "2019-01-14T17:48:21.869704, step: 258, loss: 0.4965353310108185, acc: 0.8359, auc: 0.9358, precision: 0.92, recall: 0.7302\n",
      "2019-01-14T17:48:22.102385, step: 259, loss: 0.348850816488266, acc: 0.8672, auc: 0.9517, precision: 0.902, recall: 0.7931\n",
      "2019-01-14T17:48:22.351814, step: 260, loss: 0.276022732257843, acc: 0.8984, auc: 0.96, precision: 0.8983, recall: 0.8833\n",
      "2019-01-14T17:48:22.576728, step: 261, loss: 0.2872648537158966, acc: 0.8984, auc: 0.9633, precision: 0.8824, recall: 0.8654\n",
      "2019-01-14T17:48:22.807107, step: 262, loss: 0.47387784719467163, acc: 0.875, auc: 0.9304, precision: 0.8676, recall: 0.8939\n",
      "2019-01-14T17:48:23.045035, step: 263, loss: 0.35726526379585266, acc: 0.8672, auc: 0.945, precision: 0.8387, recall: 0.8814\n",
      "2019-01-14T17:48:23.266842, step: 264, loss: 0.3250880837440491, acc: 0.8438, auc: 0.9543, precision: 0.8929, recall: 0.7812\n",
      "2019-01-14T17:48:23.500509, step: 265, loss: 0.28915509581565857, acc: 0.8594, auc: 0.9744, precision: 0.9787, recall: 0.7302\n",
      "2019-01-14T17:48:23.736449, step: 266, loss: 0.31586360931396484, acc: 0.8984, auc: 0.9556, precision: 0.9062, recall: 0.8923\n",
      "2019-01-14T17:48:23.959199, step: 267, loss: 0.33018216490745544, acc: 0.8672, auc: 0.9585, precision: 0.8701, recall: 0.9054\n",
      "2019-01-14T17:48:24.178409, step: 268, loss: 0.33913877606391907, acc: 0.8906, auc: 0.964, precision: 0.8684, recall: 0.9429\n",
      "2019-01-14T17:48:24.405851, step: 269, loss: 0.37474727630615234, acc: 0.8906, auc: 0.9526, precision: 0.8769, recall: 0.9048\n",
      "2019-01-14T17:48:24.633021, step: 270, loss: 0.37367796897888184, acc: 0.8594, auc: 0.949, precision: 0.873, recall: 0.8462\n",
      "2019-01-14T17:48:24.857089, step: 271, loss: 0.4946601390838623, acc: 0.8906, auc: 0.9419, precision: 0.96, recall: 0.8\n",
      "2019-01-14T17:48:25.084248, step: 272, loss: 0.5622653961181641, acc: 0.8438, auc: 0.9338, precision: 0.9245, recall: 0.7538\n",
      "2019-01-14T17:48:25.315926, step: 273, loss: 0.566378653049469, acc: 0.8359, auc: 0.9108, precision: 0.8727, recall: 0.7742\n",
      "2019-01-14T17:48:25.536445, step: 274, loss: 0.22008362412452698, acc: 0.9219, auc: 0.9877, precision: 0.8636, recall: 0.9828\n",
      "2019-01-14T17:48:25.757062, step: 275, loss: 0.5170284509658813, acc: 0.8672, auc: 0.9375, precision: 0.8333, recall: 0.9231\n",
      "2019-01-14T17:48:25.985307, step: 276, loss: 0.329298734664917, acc: 0.8906, auc: 0.9611, precision: 0.8955, recall: 0.8955\n",
      "2019-01-14T17:48:26.210748, step: 277, loss: 0.5751996040344238, acc: 0.8281, auc: 0.9272, precision: 0.8868, recall: 0.746\n",
      "2019-01-14T17:48:26.439343, step: 278, loss: 0.5103484392166138, acc: 0.8281, auc: 0.9348, precision: 0.9464, recall: 0.7361\n",
      "2019-01-14T17:48:26.666445, step: 279, loss: 0.383034348487854, acc: 0.8438, auc: 0.9355, precision: 0.8667, recall: 0.8125\n",
      "2019-01-14T17:48:26.893875, step: 280, loss: 0.45587772130966187, acc: 0.8438, auc: 0.9479, precision: 0.7887, recall: 0.918\n",
      "2019-01-14T17:48:27.110672, step: 281, loss: 0.2593415379524231, acc: 0.8984, auc: 0.973, precision: 0.873, recall: 0.9167\n",
      "2019-01-14T17:48:27.329056, step: 282, loss: 0.2866750657558441, acc: 0.8906, auc: 0.9619, precision: 0.9091, recall: 0.8475\n",
      "2019-01-14T17:48:27.561026, step: 283, loss: 0.22277110815048218, acc: 0.9141, auc: 0.9799, precision: 0.9831, recall: 0.8529\n",
      "2019-01-14T17:48:27.788107, step: 284, loss: 0.3741578459739685, acc: 0.8594, auc: 0.9494, precision: 0.8958, recall: 0.7679\n",
      "2019-01-14T17:48:28.008165, step: 285, loss: 0.1646760106086731, acc: 0.9219, auc: 0.9849, precision: 0.9655, recall: 0.875\n",
      "2019-01-14T17:48:28.236015, step: 286, loss: 0.30881237983703613, acc: 0.8906, auc: 0.9571, precision: 0.9091, recall: 0.8824\n",
      "2019-01-14T17:48:28.465405, step: 287, loss: 0.4900098443031311, acc: 0.8281, auc: 0.9367, precision: 0.7857, recall: 0.8871\n",
      "2019-01-14T17:48:28.695858, step: 288, loss: 0.21286621689796448, acc: 0.9141, auc: 0.9719, precision: 0.8983, recall: 0.9138\n",
      "2019-01-14T17:48:28.935156, step: 289, loss: 0.31686505675315857, acc: 0.8516, auc: 0.9553, precision: 0.9074, recall: 0.7778\n",
      "2019-01-14T17:48:29.163205, step: 290, loss: 0.35554271936416626, acc: 0.8828, auc: 0.9534, precision: 0.9661, recall: 0.8143\n",
      "2019-01-14T17:48:29.390596, step: 291, loss: 0.3043641448020935, acc: 0.8828, auc: 0.962, precision: 0.9344, recall: 0.8382\n",
      "2019-01-14T17:48:29.609808, step: 292, loss: 0.5529253482818604, acc: 0.8594, auc: 0.9289, precision: 0.8077, recall: 0.9545\n",
      "2019-01-14T17:48:29.832014, step: 293, loss: 0.3730517327785492, acc: 0.875, auc: 0.9447, precision: 0.8462, recall: 0.9016\n",
      "2019-01-14T17:48:30.060170, step: 294, loss: 0.4795049726963043, acc: 0.8125, auc: 0.9267, precision: 0.8621, recall: 0.7576\n",
      "2019-01-14T17:48:30.290102, step: 295, loss: 0.3820350170135498, acc: 0.8906, auc: 0.9521, precision: 0.9811, recall: 0.8\n",
      "2019-01-14T17:48:30.517765, step: 296, loss: 0.34464216232299805, acc: 0.875, auc: 0.9573, precision: 0.9455, recall: 0.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-14T17:48:30.741270, step: 297, loss: 0.39286500215530396, acc: 0.8359, auc: 0.9352, precision: 0.8571, recall: 0.7869\n",
      "2019-01-14T17:48:30.976573, step: 298, loss: 0.3439369797706604, acc: 0.8828, auc: 0.9445, precision: 0.8806, recall: 0.8939\n",
      "2019-01-14T17:48:31.187380, step: 299, loss: 0.3848622441291809, acc: 0.8672, auc: 0.9394, precision: 0.88, recall: 0.8919\n",
      "2019-01-14T17:48:31.419919, step: 300, loss: 0.4490929841995239, acc: 0.875, auc: 0.9197, precision: 0.8542, recall: 0.82\n",
      "\n",
      "Evaluation:\n",
      "2019-01-14T17:48:40.134860, step: 300, loss: 0.4212671281435551, acc: 0.8471589743589746, auc: 0.9291179487179488, precision: 0.8875230769230769, recall: 0.7996205128205128\n",
      "2019-01-14T17:48:40.356951, step: 301, loss: 0.3434070944786072, acc: 0.8516, auc: 0.9499, precision: 0.9348, recall: 0.7288\n",
      "2019-01-14T17:48:40.582245, step: 302, loss: 0.19932958483695984, acc: 0.9219, auc: 0.9753, precision: 0.9434, recall: 0.8772\n",
      "2019-01-14T17:48:40.817078, step: 303, loss: 0.3340737521648407, acc: 0.9062, auc: 0.9621, precision: 0.9322, recall: 0.873\n",
      "2019-01-14T17:48:41.040443, step: 304, loss: 0.36484086513519287, acc: 0.8594, auc: 0.9414, precision: 0.8281, recall: 0.8833\n",
      "2019-01-14T17:48:41.273125, step: 305, loss: 0.33451446890830994, acc: 0.8984, auc: 0.9604, precision: 0.8529, recall: 0.9508\n",
      "2019-01-14T17:48:41.520789, step: 306, loss: 0.349337637424469, acc: 0.8828, auc: 0.9441, precision: 0.8889, recall: 0.875\n",
      "2019-01-14T17:48:41.757596, step: 307, loss: 0.34234750270843506, acc: 0.8828, auc: 0.9646, precision: 0.9655, recall: 0.8116\n",
      "2019-01-14T17:48:41.988972, step: 308, loss: 0.42131930589675903, acc: 0.8594, auc: 0.9418, precision: 0.918, recall: 0.8116\n",
      "2019-01-14T17:48:42.230951, step: 309, loss: 0.25178855657577515, acc: 0.9062, auc: 0.9758, precision: 0.8657, recall: 0.9508\n",
      "2019-01-14T17:48:42.455886, step: 310, loss: 0.4506762623786926, acc: 0.875, auc: 0.9274, precision: 0.8873, recall: 0.8873\n",
      "2019-01-14T17:48:42.681443, step: 311, loss: 0.41116511821746826, acc: 0.8359, auc: 0.9324, precision: 0.8548, recall: 0.8154\n",
      "2019-01-14T17:48:42.914454, step: 312, loss: 0.3780319094657898, acc: 0.8516, auc: 0.9394, precision: 0.831, recall: 0.8939\n",
      "start training model\n",
      "2019-01-14T17:48:43.159868, step: 313, loss: 0.2101394385099411, acc: 0.8828, auc: 0.9735, precision: 0.902, recall: 0.8214\n",
      "2019-01-14T17:48:43.383383, step: 314, loss: 0.24711333215236664, acc: 0.8672, auc: 0.977, precision: 0.94, recall: 0.7705\n",
      "2019-01-14T17:48:43.615864, step: 315, loss: 0.09493962675333023, acc: 0.9531, auc: 0.9946, precision: 0.9841, recall: 0.9254\n",
      "2019-01-14T17:48:43.845130, step: 316, loss: 0.2001294195652008, acc: 0.9141, auc: 0.9775, precision: 0.9344, recall: 0.8906\n",
      "2019-01-14T17:48:44.058242, step: 317, loss: 0.3645126521587372, acc: 0.9062, auc: 0.9712, precision: 0.8514, recall: 0.9844\n",
      "2019-01-14T17:48:44.273794, step: 318, loss: 0.10923603177070618, acc: 0.9453, auc: 0.9954, precision: 0.9286, recall: 0.9701\n",
      "2019-01-14T17:48:44.485869, step: 319, loss: 0.16136127710342407, acc: 0.9297, auc: 0.9848, precision: 0.9762, recall: 0.8367\n",
      "2019-01-14T17:48:44.703910, step: 320, loss: 0.2813183665275574, acc: 0.8516, auc: 0.978, precision: 0.9688, recall: 0.7848\n",
      "2019-01-14T17:48:44.934199, step: 321, loss: 0.1990787386894226, acc: 0.9375, auc: 0.9802, precision: 0.9667, recall: 0.9062\n",
      "2019-01-14T17:48:45.173039, step: 322, loss: 0.27093568444252014, acc: 0.9062, auc: 0.9753, precision: 0.8676, recall: 0.9516\n",
      "2019-01-14T17:48:45.406020, step: 323, loss: 0.21153633296489716, acc: 0.9453, auc: 0.9799, precision: 0.9355, recall: 0.9508\n",
      "2019-01-14T17:48:45.612692, step: 324, loss: 0.26130953431129456, acc: 0.8984, auc: 0.9744, precision: 0.8689, recall: 0.9138\n",
      "2019-01-14T17:48:45.834593, step: 325, loss: 0.13771407306194305, acc: 0.9375, auc: 0.9892, precision: 0.9811, recall: 0.8814\n",
      "2019-01-14T17:48:46.055235, step: 326, loss: 0.24028965830802917, acc: 0.9141, auc: 0.9895, precision: 1.0, recall: 0.8382\n",
      "2019-01-14T17:48:46.272696, step: 327, loss: 0.2723855674266815, acc: 0.8828, auc: 0.9697, precision: 0.9444, recall: 0.8095\n",
      "2019-01-14T17:48:46.496962, step: 328, loss: 0.18823838233947754, acc: 0.9375, auc: 0.9819, precision: 0.9481, recall: 0.9481\n",
      "2019-01-14T17:48:46.719218, step: 329, loss: 0.20198093354701996, acc: 0.9141, auc: 0.9883, precision: 0.8889, recall: 0.9552\n",
      "2019-01-14T17:48:46.948741, step: 330, loss: 0.24635174870491028, acc: 0.9062, auc: 0.9768, precision: 0.8873, recall: 0.9403\n",
      "2019-01-14T17:48:47.185419, step: 331, loss: 0.16421958804130554, acc: 0.9531, auc: 0.987, precision: 0.9667, recall: 0.9355\n",
      "2019-01-14T17:48:47.428052, step: 332, loss: 0.21217907965183258, acc: 0.9062, auc: 0.9841, precision: 0.9545, recall: 0.8077\n",
      "2019-01-14T17:48:47.647635, step: 333, loss: 0.14789597690105438, acc: 0.9062, auc: 0.9922, precision: 0.9821, recall: 0.8333\n",
      "2019-01-14T17:48:47.865031, step: 334, loss: 0.2296522557735443, acc: 0.9141, auc: 0.9753, precision: 0.9104, recall: 0.9242\n",
      "2019-01-14T17:48:48.106948, step: 335, loss: 0.1655350625514984, acc: 0.9609, auc: 0.9874, precision: 0.9322, recall: 0.9821\n",
      "2019-01-14T17:48:48.347277, step: 336, loss: 0.18646496534347534, acc: 0.9531, auc: 0.9832, precision: 0.9538, recall: 0.9538\n",
      "2019-01-14T17:48:48.577950, step: 337, loss: 0.08526556193828583, acc: 0.9453, auc: 0.9954, precision: 0.9683, recall: 0.9242\n",
      "2019-01-14T17:48:48.812303, step: 338, loss: 0.1704397201538086, acc: 0.9062, auc: 0.9854, precision: 0.9483, recall: 0.8594\n",
      "2019-01-14T17:48:49.036951, step: 339, loss: 0.2619638442993164, acc: 0.9141, auc: 0.9685, precision: 0.9655, recall: 0.8615\n",
      "2019-01-14T17:48:49.269332, step: 340, loss: 0.13081933557987213, acc: 0.9375, auc: 0.9907, precision: 0.9688, recall: 0.9118\n",
      "2019-01-14T17:48:49.494129, step: 341, loss: 0.17193447053432465, acc: 0.9531, auc: 0.9839, precision: 0.9474, recall: 0.9474\n",
      "2019-01-14T17:48:49.708297, step: 342, loss: 0.270384281873703, acc: 0.9297, auc: 0.9882, precision: 0.8548, recall: 1.0\n",
      "2019-01-14T17:48:49.922348, step: 343, loss: 0.06662768870592117, acc: 0.9609, auc: 0.9976, precision: 0.9828, recall: 0.9344\n",
      "2019-01-14T17:48:50.161759, step: 344, loss: 0.21210244297981262, acc: 0.9062, auc: 0.9828, precision: 0.9434, recall: 0.8475\n",
      "2019-01-14T17:48:50.385005, step: 345, loss: 0.3846447765827179, acc: 0.8281, auc: 0.9905, precision: 0.9756, recall: 0.6557\n",
      "2019-01-14T17:48:50.631114, step: 346, loss: 0.20113712549209595, acc: 0.9297, auc: 0.9828, precision: 0.9474, recall: 0.9\n",
      "2019-01-14T17:48:50.841326, step: 347, loss: 0.28109830617904663, acc: 0.9297, auc: 0.9785, precision: 0.8955, recall: 0.9677\n",
      "2019-01-14T17:48:51.062984, step: 348, loss: 0.1773434579372406, acc: 0.9453, auc: 0.9922, precision: 0.9091, recall: 0.9836\n",
      "2019-01-14T17:48:51.283925, step: 349, loss: 0.13641315698623657, acc: 0.9531, auc: 0.9934, precision: 0.9385, recall: 0.9683\n",
      "2019-01-14T17:48:51.524233, step: 350, loss: 0.14141228795051575, acc: 0.9609, auc: 0.9888, precision: 0.9839, recall: 0.9385\n",
      "2019-01-14T17:48:51.770064, step: 351, loss: 0.18361540138721466, acc: 0.9297, auc: 0.9853, precision: 0.9649, recall: 0.8871\n",
      "2019-01-14T17:48:51.992512, step: 352, loss: 0.2098415344953537, acc: 0.9219, auc: 0.9834, precision: 0.9643, recall: 0.871\n",
      "2019-01-14T17:48:52.244138, step: 353, loss: 0.1458675116300583, acc: 0.9453, auc: 0.9888, precision: 0.9836, recall: 0.9091\n",
      "2019-01-14T17:48:52.459856, step: 354, loss: 0.10882613062858582, acc: 0.9609, auc: 0.9955, precision: 0.9467, recall: 0.9861\n",
      "2019-01-14T17:48:52.669630, step: 355, loss: 0.1627233922481537, acc: 0.9453, auc: 0.9897, precision: 0.9155, recall: 0.9848\n",
      "2019-01-14T17:48:52.892870, step: 356, loss: 0.11363448202610016, acc: 0.9688, auc: 0.9921, precision: 0.9649, recall: 0.9649\n",
      "2019-01-14T17:48:53.108259, step: 357, loss: 0.20254334807395935, acc: 0.9219, auc: 0.9858, precision: 0.9623, recall: 0.8644\n",
      "2019-01-14T17:48:53.337534, step: 358, loss: 0.2289659082889557, acc: 0.9062, auc: 0.977, precision: 0.9667, recall: 0.8529\n",
      "2019-01-14T17:48:53.568996, step: 359, loss: 0.15928907692432404, acc: 0.9219, auc: 0.9866, precision: 0.8939, recall: 0.9516\n",
      "2019-01-14T17:48:53.791061, step: 360, loss: 0.23175790905952454, acc: 0.9219, auc: 0.9761, precision: 0.9516, recall: 0.8939\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-14T17:48:54.020210, step: 361, loss: 0.1536075621843338, acc: 0.9453, auc: 0.9875, precision: 0.9688, recall: 0.9254\n",
      "2019-01-14T17:48:54.252109, step: 362, loss: 0.18127477169036865, acc: 0.9219, auc: 0.9834, precision: 0.9333, recall: 0.9032\n",
      "2019-01-14T17:48:54.497824, step: 363, loss: 0.07278373837471008, acc: 0.9766, auc: 0.9973, precision: 0.9643, recall: 0.9818\n",
      "2019-01-14T17:48:54.734685, step: 364, loss: 0.3534259498119354, acc: 0.8984, auc: 0.9567, precision: 0.9254, recall: 0.8857\n",
      "2019-01-14T17:48:54.980994, step: 365, loss: 0.19245077669620514, acc: 0.9219, auc: 0.9824, precision: 0.9167, recall: 0.9167\n",
      "2019-01-14T17:48:55.213934, step: 366, loss: 0.2418077141046524, acc: 0.9453, auc: 0.9772, precision: 0.9167, recall: 0.9851\n",
      "2019-01-14T17:48:55.457011, step: 367, loss: 0.16079755127429962, acc: 0.9297, auc: 0.9872, precision: 0.9692, recall: 0.9\n",
      "2019-01-14T17:48:55.693042, step: 368, loss: 0.16722367703914642, acc: 0.9453, auc: 0.9936, precision: 1.0, recall: 0.9\n",
      "2019-01-14T17:48:55.921148, step: 369, loss: 0.1811898797750473, acc: 0.9297, auc: 0.9827, precision: 0.9365, recall: 0.9219\n",
      "2019-01-14T17:48:56.165244, step: 370, loss: 0.04367963224649429, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9851\n",
      "2019-01-14T17:48:56.408634, step: 371, loss: 0.17012685537338257, acc: 0.9453, auc: 0.9853, precision: 0.9189, recall: 0.9855\n",
      "2019-01-14T17:48:56.641256, step: 372, loss: 0.12214964628219604, acc: 0.9609, auc: 0.9918, precision: 1.0, recall: 0.9296\n",
      "2019-01-14T17:48:56.876719, step: 373, loss: 0.07812996208667755, acc: 0.9609, auc: 0.9962, precision: 0.9804, recall: 0.9259\n",
      "2019-01-14T17:48:57.095718, step: 374, loss: 0.22826334834098816, acc: 0.9219, auc: 0.9802, precision: 0.9048, recall: 0.9344\n",
      "2019-01-14T17:48:57.315188, step: 375, loss: 0.09253197908401489, acc: 0.9766, auc: 0.9941, precision: 0.9857, recall: 0.9718\n",
      "2019-01-14T17:48:57.546094, step: 376, loss: 0.19851289689540863, acc: 0.9297, auc: 0.9805, precision: 0.9655, recall: 0.8889\n",
      "2019-01-14T17:48:57.785316, step: 377, loss: 0.20541256666183472, acc: 0.9219, auc: 0.977, precision: 0.931, recall: 0.9\n",
      "2019-01-14T17:48:58.006217, step: 378, loss: 0.1586911827325821, acc: 0.9609, auc: 0.9875, precision: 0.9365, recall: 0.9833\n",
      "2019-01-14T17:48:58.233752, step: 379, loss: 0.20268532633781433, acc: 0.9062, auc: 0.9844, precision: 0.8596, recall: 0.9245\n",
      "2019-01-14T17:48:58.462109, step: 380, loss: 0.14091594517230988, acc: 0.9141, auc: 0.9917, precision: 0.9643, recall: 0.8571\n",
      "2019-01-14T17:48:58.691598, step: 381, loss: 0.10671772807836533, acc: 0.9453, auc: 0.9944, precision: 0.9839, recall: 0.9104\n",
      "2019-01-14T17:48:58.928823, step: 382, loss: 0.21974754333496094, acc: 0.9141, auc: 0.9887, precision: 0.9846, recall: 0.8649\n",
      "2019-01-14T17:48:59.169671, step: 383, loss: 0.2400553971529007, acc: 0.9062, auc: 0.9758, precision: 0.8955, recall: 0.9231\n",
      "2019-01-14T17:48:59.395623, step: 384, loss: 0.286517471075058, acc: 0.9219, auc: 0.9872, precision: 0.8438, recall: 1.0\n",
      "2019-01-14T17:48:59.619484, step: 385, loss: 0.2593473792076111, acc: 0.9297, auc: 0.973, precision: 0.9412, recall: 0.9275\n",
      "2019-01-14T17:48:59.847453, step: 386, loss: 0.20650146901607513, acc: 0.9297, auc: 0.9814, precision: 0.9825, recall: 0.875\n",
      "2019-01-14T17:49:00.075216, step: 387, loss: 0.16470207273960114, acc: 0.9375, auc: 0.9854, precision: 0.9245, recall: 0.9245\n",
      "2019-01-14T17:49:00.311298, step: 388, loss: 0.28613901138305664, acc: 0.8984, auc: 0.9675, precision: 0.9412, recall: 0.8276\n",
      "2019-01-14T17:49:00.529257, step: 389, loss: 0.2098827064037323, acc: 0.8984, auc: 0.9817, precision: 0.9455, recall: 0.8387\n",
      "2019-01-14T17:49:00.761468, step: 390, loss: 0.2608264088630676, acc: 0.9141, auc: 0.9745, precision: 0.8987, recall: 0.9595\n",
      "2019-01-14T17:49:00.977682, step: 391, loss: 0.1975698173046112, acc: 0.9375, auc: 0.9819, precision: 0.9412, recall: 0.9412\n",
      "2019-01-14T17:49:01.207571, step: 392, loss: 0.21365617215633392, acc: 0.9219, auc: 0.9829, precision: 0.9077, recall: 0.9365\n",
      "2019-01-14T17:49:01.420790, step: 393, loss: 0.1903250813484192, acc: 0.9219, auc: 0.9861, precision: 0.9423, recall: 0.875\n",
      "2019-01-14T17:49:01.660719, step: 394, loss: 0.2303491234779358, acc: 0.8984, auc: 0.9908, precision: 0.9836, recall: 0.8333\n",
      "2019-01-14T17:49:01.903913, step: 395, loss: 0.2751834988594055, acc: 0.9297, auc: 0.9732, precision: 0.963, recall: 0.8814\n",
      "2019-01-14T17:49:02.121031, step: 396, loss: 0.19603978097438812, acc: 0.9141, auc: 0.9814, precision: 0.9231, recall: 0.9351\n",
      "2019-01-14T17:49:02.345555, step: 397, loss: 0.29146599769592285, acc: 0.9219, auc: 0.9867, precision: 0.8448, recall: 0.98\n",
      "2019-01-14T17:49:02.570367, step: 398, loss: 0.16481754183769226, acc: 0.9531, auc: 0.9899, precision: 0.9333, recall: 0.9859\n",
      "2019-01-14T17:49:02.797778, step: 399, loss: 0.19096507132053375, acc: 0.9297, auc: 0.9826, precision: 0.9344, recall: 0.9194\n",
      "2019-01-14T17:49:03.030902, step: 400, loss: 0.36145490407943726, acc: 0.8672, auc: 0.9875, precision: 1.0, recall: 0.7213\n",
      "\n",
      "Evaluation:\n",
      "2019-01-14T17:49:11.790688, step: 400, loss: 0.5752126138943893, acc: 0.8185128205128206, auc: 0.9364307692307691, precision: 0.9247410256410259, recall: 0.6973897435897435\n",
      "2019-01-14T17:49:12.023181, step: 401, loss: 0.10781864076852798, acc: 0.9375, auc: 0.9983, precision: 1.0, recall: 0.871\n",
      "2019-01-14T17:49:12.257668, step: 402, loss: 0.2543281316757202, acc: 0.9297, auc: 0.9724, precision: 0.9254, recall: 0.9394\n",
      "2019-01-14T17:49:12.480312, step: 403, loss: 0.13999933004379272, acc: 0.9688, auc: 0.9922, precision: 0.9559, recall: 0.9848\n",
      "2019-01-14T17:49:12.708843, step: 404, loss: 0.2339891940355301, acc: 0.9141, auc: 0.9773, precision: 0.8923, recall: 0.9355\n",
      "2019-01-14T17:49:12.945879, step: 405, loss: 0.09134998917579651, acc: 0.9766, auc: 0.9961, precision: 0.9836, recall: 0.9677\n",
      "2019-01-14T17:49:13.177014, step: 406, loss: 0.16077254712581635, acc: 0.9219, auc: 0.986, precision: 0.9394, recall: 0.9118\n",
      "2019-01-14T17:49:13.388943, step: 407, loss: 0.20586702227592468, acc: 0.9141, auc: 0.9902, precision: 1.0, recall: 0.8358\n",
      "2019-01-14T17:49:13.625952, step: 408, loss: 0.23856061697006226, acc: 0.9062, auc: 0.9777, precision: 1.0, recall: 0.8033\n",
      "2019-01-14T17:49:13.863688, step: 409, loss: 0.23937596380710602, acc: 0.9297, auc: 0.9764, precision: 0.9286, recall: 0.942\n",
      "2019-01-14T17:49:14.096199, step: 410, loss: 0.17948132753372192, acc: 0.9531, auc: 0.9912, precision: 0.9189, recall: 1.0\n",
      "2019-01-14T17:49:14.347119, step: 411, loss: 0.269931435585022, acc: 0.9375, auc: 0.9826, precision: 0.9103, recall: 0.9861\n",
      "2019-01-14T17:49:14.576459, step: 412, loss: 0.10917113721370697, acc: 0.9609, auc: 0.9924, precision: 0.9577, recall: 0.9714\n",
      "2019-01-14T17:49:14.814116, step: 413, loss: 0.19165660440921783, acc: 0.8906, auc: 0.9912, precision: 0.9787, recall: 0.7797\n",
      "2019-01-14T17:49:15.049103, step: 414, loss: 0.3373866677284241, acc: 0.875, auc: 0.9842, precision: 1.0, recall: 0.7867\n",
      "2019-01-14T17:49:15.262891, step: 415, loss: 0.2976599931716919, acc: 0.8828, auc: 0.9695, precision: 0.9583, recall: 0.7797\n",
      "2019-01-14T17:49:15.487631, step: 416, loss: 0.35324031114578247, acc: 0.9141, auc: 0.9756, precision: 0.8548, recall: 0.9636\n",
      "2019-01-14T17:49:15.724984, step: 417, loss: 0.5046221017837524, acc: 0.8438, auc: 0.9856, precision: 0.7222, recall: 1.0\n",
      "2019-01-14T17:49:15.951481, step: 418, loss: 0.12276454269886017, acc: 0.9375, auc: 0.991, precision: 0.9831, recall: 0.8923\n",
      "2019-01-14T17:49:16.201762, step: 419, loss: 0.21292395889759064, acc: 0.8984, auc: 0.9931, precision: 1.0, recall: 0.8194\n",
      "2019-01-14T17:49:16.421180, step: 420, loss: 0.12961798906326294, acc: 0.9609, auc: 0.9921, precision: 0.9811, recall: 0.9286\n",
      "2019-01-14T17:49:16.650145, step: 421, loss: 0.30369994044303894, acc: 0.8906, auc: 0.977, precision: 0.9545, recall: 0.8514\n",
      "2019-01-14T17:49:16.870098, step: 422, loss: 0.25305986404418945, acc: 0.9219, auc: 0.972, precision: 0.9184, recall: 0.8824\n",
      "2019-01-14T17:49:17.097796, step: 423, loss: 0.29712215065956116, acc: 0.9141, auc: 0.9744, precision: 0.8824, recall: 0.9524\n",
      "2019-01-14T17:49:17.314554, step: 424, loss: 0.19715556502342224, acc: 0.9297, auc: 0.9843, precision: 0.9103, recall: 0.9726\n",
      "2019-01-14T17:49:17.529387, step: 425, loss: 0.24429896473884583, acc: 0.9219, auc: 0.976, precision: 0.9714, recall: 0.8947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-14T17:49:17.781186, step: 426, loss: 0.1742907613515854, acc: 0.9297, auc: 0.9857, precision: 0.9333, recall: 0.9459\n",
      "2019-01-14T17:49:18.021419, step: 427, loss: 0.1330176591873169, acc: 0.9688, auc: 0.987, precision: 1.0, recall: 0.9344\n",
      "2019-01-14T17:49:18.254058, step: 428, loss: 0.15497693419456482, acc: 0.9609, auc: 0.9902, precision: 0.9836, recall: 0.9375\n",
      "2019-01-14T17:49:18.485246, step: 429, loss: 0.1356549710035324, acc: 0.9453, auc: 0.989, precision: 0.9649, recall: 0.9167\n",
      "2019-01-14T17:49:18.715111, step: 430, loss: 0.0860230103135109, acc: 0.9531, auc: 0.9966, precision: 0.9836, recall: 0.9231\n",
      "2019-01-14T17:49:18.953429, step: 431, loss: 0.22468852996826172, acc: 0.9141, auc: 0.9855, precision: 0.875, recall: 0.9492\n",
      "2019-01-14T17:49:19.190377, step: 432, loss: 0.0572667121887207, acc: 0.9922, auc: 0.9985, precision: 1.0, recall: 0.9836\n",
      "2019-01-14T17:49:19.410091, step: 433, loss: 0.06336748600006104, acc: 0.9766, auc: 0.999, precision: 1.0, recall: 0.9524\n",
      "2019-01-14T17:49:19.649771, step: 434, loss: 0.08293062448501587, acc: 0.9688, auc: 0.997, precision: 0.9492, recall: 0.9825\n",
      "2019-01-14T17:49:19.882920, step: 435, loss: 0.08729317784309387, acc: 0.9531, auc: 0.9961, precision: 0.9836, recall: 0.9231\n",
      "2019-01-14T17:49:20.101017, step: 436, loss: 0.14556819200515747, acc: 0.9219, auc: 0.9888, precision: 0.9492, recall: 0.8889\n",
      "2019-01-14T17:49:20.338602, step: 437, loss: 0.10034123808145523, acc: 0.9609, auc: 0.9956, precision: 0.9559, recall: 0.9701\n",
      "2019-01-14T17:49:20.552081, step: 438, loss: 0.1611541360616684, acc: 0.9297, auc: 0.9868, precision: 0.9344, recall: 0.9194\n",
      "2019-01-14T17:49:20.775220, step: 439, loss: 0.24515631794929504, acc: 0.9062, auc: 0.9792, precision: 0.9615, recall: 0.8333\n",
      "2019-01-14T17:49:20.988232, step: 440, loss: 0.2046552300453186, acc: 0.9219, auc: 0.9792, precision: 0.963, recall: 0.8667\n",
      "2019-01-14T17:49:21.221648, step: 441, loss: 0.3463616371154785, acc: 0.9219, auc: 0.9733, precision: 0.8923, recall: 0.9508\n",
      "2019-01-14T17:49:21.446398, step: 442, loss: 0.20058661699295044, acc: 0.9297, auc: 0.9853, precision: 0.9265, recall: 0.9403\n",
      "2019-01-14T17:49:21.671028, step: 443, loss: 0.17728713154792786, acc: 0.9688, auc: 0.989, precision: 0.9516, recall: 0.9833\n",
      "2019-01-14T17:49:21.889299, step: 444, loss: 0.159213125705719, acc: 0.9375, auc: 0.9868, precision: 0.9649, recall: 0.9016\n",
      "2019-01-14T17:49:22.126620, step: 445, loss: 0.17710642516613007, acc: 0.9297, auc: 0.9885, precision: 0.9672, recall: 0.8939\n",
      "2019-01-14T17:49:22.344038, step: 446, loss: 0.22207501530647278, acc: 0.9141, auc: 0.9869, precision: 0.9836, recall: 0.8571\n",
      "2019-01-14T17:49:22.556333, step: 447, loss: 0.11500758677721024, acc: 0.9531, auc: 0.9933, precision: 0.9474, recall: 0.9474\n",
      "2019-01-14T17:49:22.800748, step: 448, loss: 0.21324950456619263, acc: 0.9297, auc: 0.9885, precision: 0.9028, recall: 0.9701\n",
      "2019-01-14T17:49:23.025607, step: 449, loss: 0.27805593609809875, acc: 0.9219, auc: 0.9681, precision: 0.9275, recall: 0.9275\n",
      "2019-01-14T17:49:23.253543, step: 450, loss: 0.1697952151298523, acc: 0.9375, auc: 0.986, precision: 0.9583, recall: 0.9324\n",
      "2019-01-14T17:49:23.485147, step: 451, loss: 0.07133668661117554, acc: 0.9688, auc: 0.9975, precision: 0.9714, recall: 0.9714\n",
      "2019-01-14T17:49:23.715030, step: 452, loss: 0.2711910009384155, acc: 0.8438, auc: 0.98, precision: 0.9388, recall: 0.7302\n",
      "2019-01-14T17:49:23.932816, step: 453, loss: 0.19633692502975464, acc: 0.9375, auc: 0.9846, precision: 0.9516, recall: 0.9219\n",
      "2019-01-14T17:49:24.152365, step: 454, loss: 0.27080604434013367, acc: 0.8906, auc: 0.9751, precision: 0.8923, recall: 0.8923\n",
      "2019-01-14T17:49:24.388081, step: 455, loss: 0.21663156151771545, acc: 0.9219, auc: 0.9865, precision: 0.8904, recall: 0.9701\n",
      "2019-01-14T17:49:24.613857, step: 456, loss: 0.15804360806941986, acc: 0.9219, auc: 0.9947, precision: 0.8548, recall: 0.9815\n",
      "2019-01-14T17:49:24.840385, step: 457, loss: 0.19651350378990173, acc: 0.9219, auc: 0.9843, precision: 0.963, recall: 0.8667\n",
      "2019-01-14T17:49:25.062676, step: 458, loss: 0.22914476692676544, acc: 0.9219, auc: 0.9863, precision: 1.0, recall: 0.8333\n",
      "2019-01-14T17:49:25.281092, step: 459, loss: 0.16064631938934326, acc: 0.9375, auc: 0.9904, precision: 0.9692, recall: 0.913\n",
      "2019-01-14T17:49:25.499916, step: 460, loss: 0.3429566025733948, acc: 0.9141, auc: 0.9655, precision: 0.9322, recall: 0.8871\n",
      "2019-01-14T17:49:25.712375, step: 461, loss: 0.1606668382883072, acc: 0.9375, auc: 0.9871, precision: 0.9375, recall: 0.9375\n",
      "2019-01-14T17:49:25.927212, step: 462, loss: 0.1267918348312378, acc: 0.9609, auc: 0.9961, precision: 0.9315, recall: 1.0\n",
      "2019-01-14T17:49:26.160639, step: 463, loss: 0.22798092663288116, acc: 0.9375, auc: 0.9784, precision: 0.9296, recall: 0.9565\n",
      "2019-01-14T17:49:26.382442, step: 464, loss: 0.17741072177886963, acc: 0.9141, auc: 0.987, precision: 0.9474, recall: 0.871\n",
      "2019-01-14T17:49:26.595559, step: 465, loss: 0.24185961484909058, acc: 0.8906, auc: 0.9821, precision: 0.96, recall: 0.8\n",
      "2019-01-14T17:49:26.815258, step: 466, loss: 0.1383020132780075, acc: 0.9141, auc: 0.9913, precision: 0.9697, recall: 0.8767\n",
      "2019-01-14T17:49:27.033615, step: 467, loss: 0.24286699295043945, acc: 0.9141, auc: 0.9854, precision: 0.8594, recall: 0.9649\n",
      "2019-01-14T17:49:27.249275, step: 468, loss: 0.20432956516742706, acc: 0.9062, auc: 0.9828, precision: 0.8904, recall: 0.942\n",
      "start training model\n",
      "2019-01-14T17:49:27.480519, step: 469, loss: 0.062228813767433167, acc: 0.9766, auc: 0.9968, precision: 0.9833, recall: 0.9672\n",
      "2019-01-14T17:49:27.710997, step: 470, loss: 0.07319813966751099, acc: 0.9531, auc: 0.9968, precision: 0.9697, recall: 0.9412\n",
      "2019-01-14T17:49:27.952264, step: 471, loss: 0.15415702760219574, acc: 0.9297, auc: 0.9953, precision: 0.9836, recall: 0.8824\n",
      "2019-01-14T17:49:28.181258, step: 472, loss: 0.03681448847055435, acc: 0.9766, auc: 1.0, precision: 1.0, recall: 0.95\n",
      "2019-01-14T17:49:28.410108, step: 473, loss: 0.05988898128271103, acc: 0.9609, auc: 0.9982, precision: 0.9859, recall: 0.9459\n",
      "2019-01-14T17:49:28.636075, step: 474, loss: 0.11080716550350189, acc: 0.9688, auc: 0.9954, precision: 0.9565, recall: 0.9851\n",
      "2019-01-14T17:49:28.862108, step: 475, loss: 0.0999269187450409, acc: 0.9531, auc: 0.9971, precision: 0.9375, recall: 0.9677\n",
      "2019-01-14T17:49:29.078827, step: 476, loss: 0.08900037407875061, acc: 0.9609, auc: 0.9963, precision: 0.9531, recall: 0.9683\n",
      "2019-01-14T17:49:29.307095, step: 477, loss: 0.08821973949670792, acc: 0.9766, auc: 0.9951, precision: 0.9831, recall: 0.9667\n",
      "2019-01-14T17:49:29.533283, step: 478, loss: 0.2205778807401657, acc: 0.9531, auc: 0.9841, precision: 1.0, recall: 0.9048\n",
      "2019-01-14T17:49:29.759999, step: 479, loss: 0.06032035872340202, acc: 0.9688, auc: 0.9985, precision: 1.0, recall: 0.9333\n",
      "2019-01-14T17:49:29.979964, step: 480, loss: 0.055467016994953156, acc: 0.9609, auc: 0.9988, precision: 0.9846, recall: 0.9412\n",
      "2019-01-14T17:49:30.213017, step: 481, loss: 0.08954483270645142, acc: 0.9531, auc: 0.9954, precision: 0.9538, recall: 0.9538\n",
      "2019-01-14T17:49:30.438307, step: 482, loss: 0.0690976232290268, acc: 0.9766, auc: 0.9976, precision: 0.9839, recall: 0.9683\n",
      "2019-01-14T17:49:30.646933, step: 483, loss: 0.06289497762918472, acc: 0.9766, auc: 0.999, precision: 0.9655, recall: 0.9825\n",
      "2019-01-14T17:49:30.875677, step: 484, loss: 0.06797502934932709, acc: 0.9766, auc: 0.9975, precision: 0.9667, recall: 0.9831\n",
      "2019-01-14T17:49:31.104494, step: 485, loss: 0.06418859213590622, acc: 0.9844, auc: 0.9983, precision: 0.9821, recall: 0.9821\n",
      "2019-01-14T17:49:31.324360, step: 486, loss: 0.1427057981491089, acc: 0.9453, auc: 0.996, precision: 1.0, recall: 0.9054\n",
      "2019-01-14T17:49:31.546030, step: 487, loss: 0.10501386225223541, acc: 0.9609, auc: 0.9961, precision: 0.9828, recall: 0.9344\n",
      "2019-01-14T17:49:31.779492, step: 488, loss: 0.07542464137077332, acc: 0.9688, auc: 0.9975, precision: 0.9848, recall: 0.9559\n",
      "2019-01-14T17:49:32.010555, step: 489, loss: 0.04762769117951393, acc: 0.9766, auc: 0.9993, precision: 0.9836, recall: 0.9677\n",
      "2019-01-14T17:49:32.246464, step: 490, loss: 0.11745420843362808, acc: 0.9688, auc: 0.9939, precision: 0.9692, recall: 0.9692\n",
      "2019-01-14T17:49:32.475345, step: 491, loss: 0.10395630449056625, acc: 0.9688, auc: 0.9985, precision: 0.9437, recall: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-14T17:49:32.708282, step: 492, loss: 0.09467790275812149, acc: 0.9609, auc: 0.9946, precision: 0.9661, recall: 0.95\n",
      "2019-01-14T17:49:32.934878, step: 493, loss: 0.09664319455623627, acc: 0.9609, auc: 0.9963, precision: 1.0, recall: 0.9286\n",
      "2019-01-14T17:49:33.166555, step: 494, loss: 0.04713602736592293, acc: 0.9688, auc: 0.9988, precision: 0.9672, recall: 0.9672\n",
      "2019-01-14T17:49:33.393717, step: 495, loss: 0.12330402433872223, acc: 0.9453, auc: 0.9946, precision: 0.9846, recall: 0.9143\n",
      "2019-01-14T17:49:33.629357, step: 496, loss: 0.096030592918396, acc: 0.9609, auc: 0.9941, precision: 0.9697, recall: 0.9552\n",
      "2019-01-14T17:49:33.852996, step: 497, loss: 0.23285940289497375, acc: 0.9297, auc: 0.9872, precision: 0.9178, recall: 0.9571\n",
      "2019-01-14T17:49:34.081027, step: 498, loss: 0.11852406710386276, acc: 0.9688, auc: 0.9956, precision: 0.9545, recall: 0.9844\n",
      "2019-01-14T17:49:34.306041, step: 499, loss: 0.09674812853336334, acc: 0.9609, auc: 0.9963, precision: 0.9701, recall: 0.9559\n",
      "2019-01-14T17:49:34.533873, step: 500, loss: 0.12887823581695557, acc: 0.9375, auc: 0.9943, precision: 0.9697, recall: 0.9143\n",
      "\n",
      "Evaluation:\n",
      "2019-01-14T17:49:43.301358, step: 500, loss: 0.6105476228090433, acc: 0.8395512820512824, auc: 0.9359461538461537, precision: 0.904984615384615, recall: 0.7610179487179486\n",
      "2019-01-14T17:49:43.519007, step: 501, loss: 0.05785185843706131, acc: 0.9688, auc: 0.9998, precision: 1.0, recall: 0.9365\n",
      "2019-01-14T17:49:43.749927, step: 502, loss: 0.08955918252468109, acc: 0.9688, auc: 0.9966, precision: 0.9701, recall: 0.9701\n",
      "2019-01-14T17:49:43.979504, step: 503, loss: 0.0467909574508667, acc: 0.9766, auc: 0.9985, precision: 0.9828, recall: 0.9661\n",
      "2019-01-14T17:49:44.209482, step: 504, loss: 0.03624090924859047, acc: 0.9766, auc: 0.9998, precision: 1.0, recall: 0.9589\n",
      "2019-01-14T17:49:44.446047, step: 505, loss: 0.04891229793429375, acc: 0.9844, auc: 0.9978, precision: 1.0, recall: 0.971\n",
      "2019-01-14T17:49:44.671250, step: 506, loss: 0.18096068501472473, acc: 0.9609, auc: 0.9845, precision: 0.9355, recall: 0.9831\n",
      "2019-01-14T17:49:44.907451, step: 507, loss: 0.08857405930757523, acc: 0.9609, auc: 0.9962, precision: 0.95, recall: 0.987\n",
      "2019-01-14T17:49:45.126573, step: 508, loss: 0.1342979520559311, acc: 0.9609, auc: 0.9919, precision: 0.9833, recall: 0.9365\n",
      "2019-01-14T17:49:45.362669, step: 509, loss: 0.01697457954287529, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9831\n",
      "2019-01-14T17:49:45.592815, step: 510, loss: 0.12745176255702972, acc: 0.9531, auc: 0.9936, precision: 0.9661, recall: 0.9344\n",
      "2019-01-14T17:49:45.827110, step: 511, loss: 0.036314330995082855, acc: 0.9766, auc: 0.9995, precision: 0.9839, recall: 0.9683\n",
      "2019-01-14T17:49:46.053961, step: 512, loss: 0.0757194459438324, acc: 0.9609, auc: 0.9988, precision: 1.0, recall: 0.9286\n",
      "2019-01-14T17:49:46.296016, step: 513, loss: 0.024483008310198784, acc: 0.9922, auc: 0.9998, precision: 1.0, recall: 0.9855\n",
      "2019-01-14T17:49:46.519853, step: 514, loss: 0.0911829024553299, acc: 0.9609, auc: 0.9973, precision: 0.95, recall: 0.9661\n",
      "2019-01-14T17:49:46.764074, step: 515, loss: 0.10568224638700485, acc: 0.9609, auc: 0.9951, precision: 0.9706, recall: 0.9565\n",
      "2019-01-14T17:49:46.976749, step: 516, loss: 0.1698235124349594, acc: 0.9219, auc: 0.9893, precision: 0.8814, recall: 0.9455\n",
      "2019-01-14T17:49:47.208331, step: 517, loss: 0.054560158401727676, acc: 0.9688, auc: 0.9985, precision: 1.0, recall: 0.9412\n",
      "2019-01-14T17:49:47.438427, step: 518, loss: 0.05354668200016022, acc: 0.9844, auc: 0.9985, precision: 0.9831, recall: 0.9831\n",
      "2019-01-14T17:49:47.665895, step: 519, loss: 0.1934034526348114, acc: 0.9688, auc: 0.9846, precision: 0.9726, recall: 0.9726\n",
      "2019-01-14T17:49:47.893407, step: 520, loss: 0.09063337743282318, acc: 0.9766, auc: 0.9953, precision: 1.0, recall: 0.9492\n",
      "2019-01-14T17:49:48.124422, step: 521, loss: 0.10564815253019333, acc: 0.9375, auc: 0.993, precision: 0.9423, recall: 0.9074\n",
      "2019-01-14T17:49:48.350410, step: 522, loss: 0.10689105093479156, acc: 0.9531, auc: 0.9968, precision: 1.0, recall: 0.9104\n",
      "2019-01-14T17:49:48.584730, step: 523, loss: 0.09628810733556747, acc: 0.9531, auc: 0.9956, precision: 0.9516, recall: 0.9516\n",
      "2019-01-14T17:49:48.812499, step: 524, loss: 0.13680584728717804, acc: 0.9609, auc: 0.9911, precision: 0.9649, recall: 0.9483\n",
      "2019-01-14T17:49:49.038535, step: 525, loss: 0.05820152163505554, acc: 0.9766, auc: 0.9988, precision: 0.9706, recall: 0.9851\n",
      "2019-01-14T17:49:49.267214, step: 526, loss: 0.14333359897136688, acc: 0.9688, auc: 0.9926, precision: 0.9365, recall: 1.0\n",
      "2019-01-14T17:49:49.504793, step: 527, loss: 0.05454007908701897, acc: 0.9766, auc: 0.9983, precision: 1.0, recall: 0.95\n",
      "2019-01-14T17:49:49.724752, step: 528, loss: 0.1525498330593109, acc: 0.9375, auc: 0.9953, precision: 0.9692, recall: 0.913\n",
      "2019-01-14T17:49:49.960688, step: 529, loss: 0.17103175818920135, acc: 0.9375, auc: 0.9956, precision: 1.0, recall: 0.8689\n",
      "2019-01-14T17:49:50.189743, step: 530, loss: 0.02646462991833687, acc: 0.9922, auc: 1.0, precision: 0.9804, recall: 1.0\n",
      "2019-01-14T17:49:50.416733, step: 531, loss: 0.143789604306221, acc: 0.9453, auc: 0.9956, precision: 0.9231, recall: 0.9677\n",
      "2019-01-14T17:49:50.654301, step: 532, loss: 0.030754465609788895, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:49:50.883424, step: 533, loss: 0.05909473076462746, acc: 0.9766, auc: 0.9983, precision: 0.9701, recall: 0.9848\n",
      "2019-01-14T17:49:51.105572, step: 534, loss: 0.03413190320134163, acc: 0.9922, auc: 0.9993, precision: 0.9846, recall: 1.0\n",
      "2019-01-14T17:49:51.333716, step: 535, loss: 0.13644064962863922, acc: 0.9453, auc: 0.9968, precision: 1.0, recall: 0.8772\n",
      "2019-01-14T17:49:51.561436, step: 536, loss: 0.12104248255491257, acc: 0.9453, auc: 0.998, precision: 1.0, recall: 0.8871\n",
      "2019-01-14T17:49:51.803669, step: 537, loss: 0.043487437069416046, acc: 0.9922, auc: 0.999, precision: 1.0, recall: 0.9857\n",
      "2019-01-14T17:49:52.032199, step: 538, loss: 0.0947398915886879, acc: 0.9844, auc: 0.997, precision: 0.9661, recall: 1.0\n",
      "2019-01-14T17:49:52.275783, step: 539, loss: 0.1486930251121521, acc: 0.9609, auc: 0.9988, precision: 0.9333, recall: 1.0\n",
      "2019-01-14T17:49:52.503298, step: 540, loss: 0.2127683311700821, acc: 0.9531, auc: 0.9822, precision: 0.9385, recall: 0.9683\n",
      "2019-01-14T17:49:52.733728, step: 541, loss: 0.07052626460790634, acc: 0.9609, auc: 0.9985, precision: 1.0, recall: 0.9194\n",
      "2019-01-14T17:49:52.954342, step: 542, loss: 0.0783102735877037, acc: 0.9609, auc: 0.997, precision: 0.9714, recall: 0.9577\n",
      "2019-01-14T17:49:53.189073, step: 543, loss: 0.08882463723421097, acc: 0.9688, auc: 0.9951, precision: 0.9839, recall: 0.9531\n",
      "2019-01-14T17:49:53.423683, step: 544, loss: 0.10235010087490082, acc: 0.9688, auc: 0.9961, precision: 0.9833, recall: 0.9516\n",
      "2019-01-14T17:49:53.639079, step: 545, loss: 0.05860604718327522, acc: 0.9844, auc: 0.9985, precision: 0.9831, recall: 0.9831\n",
      "2019-01-14T17:49:53.867247, step: 546, loss: 0.15927937626838684, acc: 0.9453, auc: 0.99, precision: 0.9516, recall: 0.9365\n",
      "2019-01-14T17:49:54.081473, step: 547, loss: 0.04534099996089935, acc: 0.9688, auc: 0.9988, precision: 0.971, recall: 0.971\n",
      "2019-01-14T17:49:54.300079, step: 548, loss: 0.03223596513271332, acc: 0.9922, auc: 0.9993, precision: 1.0, recall: 0.9836\n",
      "2019-01-14T17:49:54.519603, step: 549, loss: 0.2013043761253357, acc: 0.9375, auc: 0.9868, precision: 0.9661, recall: 0.9048\n",
      "2019-01-14T17:49:54.754735, step: 550, loss: 0.029433658346533775, acc: 0.9922, auc: 0.9988, precision: 1.0, recall: 0.9833\n",
      "2019-01-14T17:49:54.995721, step: 551, loss: 0.07653500884771347, acc: 0.9766, auc: 0.998, precision: 0.9538, recall: 1.0\n",
      "2019-01-14T17:49:55.228300, step: 552, loss: 0.19389215111732483, acc: 0.9688, auc: 0.9862, precision: 0.963, recall: 0.963\n",
      "2019-01-14T17:49:55.460872, step: 553, loss: 0.06875532865524292, acc: 0.9531, auc: 0.997, precision: 0.9851, recall: 0.9296\n",
      "2019-01-14T17:49:55.686200, step: 554, loss: 0.06393986195325851, acc: 0.9766, auc: 0.9975, precision: 0.9851, recall: 0.9706\n",
      "2019-01-14T17:49:55.935983, step: 555, loss: 0.06948652118444443, acc: 0.9609, auc: 0.9963, precision: 0.9831, recall: 0.9355\n",
      "2019-01-14T17:49:56.153788, step: 556, loss: 0.07934026420116425, acc: 0.9609, auc: 0.9956, precision: 0.9545, recall: 0.9692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-14T17:49:56.388829, step: 557, loss: 0.10243150591850281, acc: 0.9609, auc: 0.9953, precision: 0.9825, recall: 0.9333\n",
      "2019-01-14T17:49:56.606168, step: 558, loss: 0.07792036235332489, acc: 0.9766, auc: 0.9963, precision: 0.9844, recall: 0.9692\n",
      "2019-01-14T17:49:56.835873, step: 559, loss: 0.1341637820005417, acc: 0.9375, auc: 0.9931, precision: 0.9286, recall: 0.9559\n",
      "2019-01-14T17:49:57.074243, step: 560, loss: 0.04373582825064659, acc: 0.9844, auc: 0.998, precision: 1.0, recall: 0.9649\n",
      "2019-01-14T17:49:57.308828, step: 561, loss: 0.033208828419446945, acc: 0.9766, auc: 0.9998, precision: 1.0, recall: 0.95\n",
      "2019-01-14T17:49:57.536016, step: 562, loss: 0.07756949216127396, acc: 0.9844, auc: 0.9963, precision: 1.0, recall: 0.9697\n",
      "2019-01-14T17:49:57.781486, step: 563, loss: 0.07305958122015, acc: 0.9844, auc: 0.9971, precision: 0.9846, recall: 0.9846\n",
      "2019-01-14T17:49:58.002428, step: 564, loss: 0.09172281622886658, acc: 0.9453, auc: 0.9958, precision: 0.9661, recall: 0.9194\n",
      "2019-01-14T17:49:58.239939, step: 565, loss: 0.11383651196956635, acc: 0.9922, auc: 0.9898, precision: 0.9863, recall: 1.0\n",
      "2019-01-14T17:49:58.471359, step: 566, loss: 0.14508847892284393, acc: 0.9531, auc: 0.9917, precision: 0.9538, recall: 0.9538\n",
      "2019-01-14T17:49:58.699114, step: 567, loss: 0.0487712025642395, acc: 0.9844, auc: 0.999, precision: 0.9846, recall: 0.9846\n",
      "2019-01-14T17:49:58.928195, step: 568, loss: 0.07851941883563995, acc: 0.9688, auc: 0.9978, precision: 0.95, recall: 0.9828\n",
      "2019-01-14T17:49:59.145760, step: 569, loss: 0.1002316027879715, acc: 0.9297, auc: 0.9957, precision: 0.9853, recall: 0.8933\n",
      "2019-01-14T17:49:59.350867, step: 570, loss: 0.09940129518508911, acc: 0.9609, auc: 0.9963, precision: 1.0, recall: 0.9219\n",
      "2019-01-14T17:49:59.579115, step: 571, loss: 0.08411531895399094, acc: 0.9609, auc: 0.9966, precision: 0.9821, recall: 0.9322\n",
      "2019-01-14T17:49:59.815125, step: 572, loss: 0.03687123954296112, acc: 0.9844, auc: 0.9998, precision: 0.9836, recall: 0.9836\n",
      "2019-01-14T17:50:00.059497, step: 573, loss: 0.14943340420722961, acc: 0.9531, auc: 0.9923, precision: 0.9342, recall: 0.9861\n",
      "2019-01-14T17:50:00.286299, step: 574, loss: 0.08366694301366806, acc: 0.9688, auc: 0.9983, precision: 0.9688, recall: 0.9688\n",
      "2019-01-14T17:50:00.517192, step: 575, loss: 0.03501484915614128, acc: 0.9844, auc: 0.9995, precision: 0.9655, recall: 1.0\n",
      "2019-01-14T17:50:00.749827, step: 576, loss: 0.035549089312553406, acc: 0.9922, auc: 0.9993, precision: 1.0, recall: 0.9851\n",
      "2019-01-14T17:50:00.976038, step: 577, loss: 0.10283157229423523, acc: 0.9609, auc: 0.9957, precision: 0.9804, recall: 0.9259\n",
      "2019-01-14T17:50:01.204168, step: 578, loss: 0.07926221936941147, acc: 0.9531, auc: 0.9998, precision: 1.0, recall: 0.8909\n",
      "2019-01-14T17:50:01.424391, step: 579, loss: 0.11624117195606232, acc: 0.9609, auc: 0.9966, precision: 1.0, recall: 0.9242\n",
      "2019-01-14T17:50:01.654097, step: 580, loss: 0.12812307476997375, acc: 0.9531, auc: 0.9946, precision: 0.9688, recall: 0.9394\n",
      "2019-01-14T17:50:01.872134, step: 581, loss: 0.1856633871793747, acc: 0.9453, auc: 0.9922, precision: 0.9143, recall: 0.9846\n",
      "2019-01-14T17:50:02.090639, step: 582, loss: 0.05489996820688248, acc: 0.9844, auc: 1.0, precision: 0.971, recall: 1.0\n",
      "2019-01-14T17:50:02.324271, step: 583, loss: 0.05938009172677994, acc: 0.9688, auc: 0.9978, precision: 0.9844, recall: 0.9545\n",
      "2019-01-14T17:50:02.550797, step: 584, loss: 0.1849684715270996, acc: 0.9375, auc: 0.9908, precision: 0.9123, recall: 0.9455\n",
      "2019-01-14T17:50:02.769507, step: 585, loss: 0.03632740676403046, acc: 0.9766, auc: 0.9998, precision: 1.0, recall: 0.9589\n",
      "2019-01-14T17:50:02.992906, step: 586, loss: 0.08145976066589355, acc: 0.9609, auc: 0.9985, precision: 1.0, recall: 0.9254\n",
      "2019-01-14T17:50:03.211899, step: 587, loss: 0.06373797357082367, acc: 0.9844, auc: 0.9972, precision: 0.9804, recall: 0.9804\n",
      "2019-01-14T17:50:03.439091, step: 588, loss: 0.12919379770755768, acc: 0.9453, auc: 0.9963, precision: 1.0, recall: 0.8889\n",
      "2019-01-14T17:50:03.676513, step: 589, loss: 0.07713685184717178, acc: 0.9844, auc: 0.9951, precision: 1.0, recall: 0.9692\n",
      "2019-01-14T17:50:03.899943, step: 590, loss: 0.06273362040519714, acc: 0.9844, auc: 1.0, precision: 0.9623, recall: 1.0\n",
      "2019-01-14T17:50:04.117639, step: 591, loss: 0.10869081318378448, acc: 0.9688, auc: 0.9923, precision: 0.9423, recall: 0.98\n",
      "2019-01-14T17:50:04.333786, step: 592, loss: 0.10347607731819153, acc: 0.9531, auc: 0.9961, precision: 0.9296, recall: 0.9851\n",
      "2019-01-14T17:50:04.551368, step: 593, loss: 0.08243384957313538, acc: 0.9531, auc: 0.9961, precision: 0.9692, recall: 0.9403\n",
      "2019-01-14T17:50:04.777383, step: 594, loss: 0.13150374591350555, acc: 0.9453, auc: 0.9966, precision: 1.0, recall: 0.8889\n",
      "2019-01-14T17:50:05.007612, step: 595, loss: 0.0769684687256813, acc: 0.9609, auc: 0.9985, precision: 0.9836, recall: 0.9375\n",
      "2019-01-14T17:50:05.240180, step: 596, loss: 0.02226741425693035, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9839\n",
      "2019-01-14T17:50:05.467871, step: 597, loss: 0.053288765251636505, acc: 0.9688, auc: 0.9988, precision: 0.9828, recall: 0.95\n",
      "2019-01-14T17:50:05.684356, step: 598, loss: 0.2942465841770172, acc: 0.9375, auc: 0.9845, precision: 0.9032, recall: 0.9655\n",
      "2019-01-14T17:50:05.905280, step: 599, loss: 0.06711460649967194, acc: 0.9766, auc: 0.9975, precision: 0.9672, recall: 0.9833\n",
      "2019-01-14T17:50:06.135486, step: 600, loss: 0.05148608237504959, acc: 0.9688, auc: 0.999, precision: 0.9531, recall: 0.9839\n",
      "\n",
      "Evaluation:\n",
      "2019-01-14T17:50:14.711771, step: 600, loss: 0.5716800506298358, acc: 0.8595743589743591, auc: 0.9389410256410256, precision: 0.8855153846153845, recall: 0.8319512820512821\n",
      "2019-01-14T17:50:14.933383, step: 601, loss: 0.0714096799492836, acc: 0.9609, auc: 0.9972, precision: 0.9623, recall: 0.9444\n",
      "2019-01-14T17:50:15.165787, step: 602, loss: 0.03525566682219505, acc: 0.9844, auc: 0.9995, precision: 1.0, recall: 0.9697\n",
      "2019-01-14T17:50:15.391036, step: 603, loss: 0.13562770187854767, acc: 0.9531, auc: 0.9929, precision: 0.9821, recall: 0.9167\n",
      "2019-01-14T17:50:15.630118, step: 604, loss: 0.08267715573310852, acc: 0.9609, auc: 0.998, precision: 1.0, recall: 0.9324\n",
      "2019-01-14T17:50:15.861519, step: 605, loss: 0.07345232367515564, acc: 0.9688, auc: 0.9973, precision: 0.9692, recall: 0.9692\n",
      "2019-01-14T17:50:16.087170, step: 606, loss: 0.05402194708585739, acc: 0.9766, auc: 0.9983, precision: 1.0, recall: 0.9589\n",
      "2019-01-14T17:50:16.313027, step: 607, loss: 0.14573198556900024, acc: 0.9453, auc: 0.996, precision: 0.9221, recall: 0.9861\n",
      "2019-01-14T17:50:16.529143, step: 608, loss: 0.16498489677906036, acc: 0.9453, auc: 0.9888, precision: 0.9538, recall: 0.9394\n",
      "2019-01-14T17:50:16.760393, step: 609, loss: 0.058717310428619385, acc: 0.9609, auc: 0.9976, precision: 0.9672, recall: 0.9516\n",
      "2019-01-14T17:50:16.987696, step: 610, loss: 0.13924911618232727, acc: 0.9375, auc: 0.9936, precision: 0.9692, recall: 0.913\n",
      "2019-01-14T17:50:17.206289, step: 611, loss: 0.04684913158416748, acc: 0.9844, auc: 0.999, precision: 1.0, recall: 0.9649\n",
      "2019-01-14T17:50:17.438766, step: 612, loss: 0.09484368562698364, acc: 0.9609, auc: 0.998, precision: 1.0, recall: 0.9296\n",
      "2019-01-14T17:50:17.668711, step: 613, loss: 0.10099001228809357, acc: 0.9688, auc: 0.9951, precision: 0.9692, recall: 0.9692\n",
      "2019-01-14T17:50:17.900047, step: 614, loss: 0.20753102004528046, acc: 0.9609, auc: 0.9843, precision: 0.95, recall: 0.9661\n",
      "2019-01-14T17:50:18.113326, step: 615, loss: 0.08353008329868317, acc: 0.9688, auc: 0.9975, precision: 0.9589, recall: 0.9859\n",
      "2019-01-14T17:50:18.342703, step: 616, loss: 0.09194410592317581, acc: 0.9688, auc: 0.9971, precision: 0.9552, recall: 0.9846\n",
      "2019-01-14T17:50:18.572520, step: 617, loss: 0.09938704967498779, acc: 0.9688, auc: 0.9945, precision: 0.9636, recall: 0.9636\n",
      "2019-01-14T17:50:18.806785, step: 618, loss: 0.05091654136776924, acc: 0.9844, auc: 0.9998, precision: 1.0, recall: 0.9688\n",
      "2019-01-14T17:50:19.039856, step: 619, loss: 0.21170277893543243, acc: 0.9219, auc: 0.99, precision: 0.9833, recall: 0.8676\n",
      "2019-01-14T17:50:19.265435, step: 620, loss: 0.06400949507951736, acc: 0.9609, auc: 1.0, precision: 1.0, recall: 0.9306\n",
      "2019-01-14T17:50:19.487018, step: 621, loss: 0.09159719944000244, acc: 0.9688, auc: 0.9958, precision: 0.9559, recall: 0.9848\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-14T17:50:19.707427, step: 622, loss: 0.23207639157772064, acc: 0.9141, auc: 0.988, precision: 0.8767, recall: 0.9697\n",
      "2019-01-14T17:50:19.956101, step: 623, loss: 0.08539487421512604, acc: 0.9766, auc: 0.9977, precision: 0.9625, recall: 1.0\n",
      "2019-01-14T17:50:20.192656, step: 624, loss: 0.1541038155555725, acc: 0.9609, auc: 0.9931, precision: 0.9306, recall: 1.0\n",
      "start training model\n",
      "2019-01-14T17:50:20.443719, step: 625, loss: 0.021729256957769394, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9697\n",
      "2019-01-14T17:50:20.662505, step: 626, loss: 0.08656376600265503, acc: 0.9453, auc: 0.9995, precision: 1.0, recall: 0.8971\n",
      "2019-01-14T17:50:20.883076, step: 627, loss: 0.07446549832820892, acc: 0.9531, auc: 0.9997, precision: 1.0, recall: 0.8868\n",
      "2019-01-14T17:50:21.119185, step: 628, loss: 0.07693914324045181, acc: 0.9609, auc: 1.0, precision: 1.0, recall: 0.918\n",
      "2019-01-14T17:50:21.336545, step: 629, loss: 0.011212693527340889, acc: 0.9922, auc: 1.0, precision: 0.9831, recall: 1.0\n",
      "2019-01-14T17:50:21.554203, step: 630, loss: 0.014388229697942734, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:50:21.764351, step: 631, loss: 0.10901117324829102, acc: 0.9688, auc: 0.9995, precision: 0.942, recall: 1.0\n",
      "2019-01-14T17:50:21.973572, step: 632, loss: 0.1132623553276062, acc: 0.9688, auc: 0.999, precision: 0.9444, recall: 1.0\n",
      "2019-01-14T17:50:22.197289, step: 633, loss: 0.06318138539791107, acc: 0.9844, auc: 1.0, precision: 0.9714, recall: 1.0\n",
      "2019-01-14T17:50:22.431403, step: 634, loss: 0.05116787552833557, acc: 0.9766, auc: 0.9995, precision: 1.0, recall: 0.9559\n",
      "2019-01-14T17:50:22.653632, step: 635, loss: 0.052992090582847595, acc: 0.9688, auc: 1.0, precision: 1.0, recall: 0.9375\n",
      "2019-01-14T17:50:22.888469, step: 636, loss: 0.15302534401416779, acc: 0.9297, auc: 1.0, precision: 1.0, recall: 0.8475\n",
      "2019-01-14T17:50:23.107280, step: 637, loss: 0.040510114282369614, acc: 0.9844, auc: 0.999, precision: 0.9848, recall: 0.9848\n",
      "2019-01-14T17:50:23.334611, step: 638, loss: 0.003712322562932968, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:50:23.556715, step: 639, loss: 0.023609990254044533, acc: 0.9922, auc: 0.9995, precision: 0.9853, recall: 1.0\n",
      "2019-01-14T17:50:23.771974, step: 640, loss: 0.11062918603420258, acc: 0.9688, auc: 0.998, precision: 0.9385, recall: 1.0\n",
      "2019-01-14T17:50:24.001566, step: 641, loss: 0.07532238960266113, acc: 0.9688, auc: 0.9973, precision: 0.9692, recall: 0.9692\n",
      "2019-01-14T17:50:24.230354, step: 642, loss: 0.03671339154243469, acc: 0.9766, auc: 1.0, precision: 0.9571, recall: 1.0\n",
      "2019-01-14T17:50:24.456819, step: 643, loss: 0.027873819693922997, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9722\n",
      "2019-01-14T17:50:24.671495, step: 644, loss: 0.014526271261274815, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9855\n",
      "2019-01-14T17:50:24.918035, step: 645, loss: 0.05893441662192345, acc: 0.9766, auc: 0.9995, precision: 1.0, recall: 0.9508\n",
      "2019-01-14T17:50:25.147940, step: 646, loss: 0.035348065197467804, acc: 0.9766, auc: 0.999, precision: 0.9836, recall: 0.9677\n",
      "2019-01-14T17:50:25.379401, step: 647, loss: 0.04491277039051056, acc: 0.9688, auc: 0.9995, precision: 1.0, recall: 0.9355\n",
      "2019-01-14T17:50:25.623855, step: 648, loss: 0.015368321910500526, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9818\n",
      "2019-01-14T17:50:25.866402, step: 649, loss: 0.03395797312259674, acc: 0.9922, auc: 0.9993, precision: 0.9855, recall: 1.0\n",
      "2019-01-14T17:50:26.111329, step: 650, loss: 0.09125285595655441, acc: 0.9766, auc: 0.9978, precision: 0.9595, recall: 1.0\n",
      "2019-01-14T17:50:26.347695, step: 651, loss: 0.07222501188516617, acc: 0.9766, auc: 0.9995, precision: 0.9538, recall: 1.0\n",
      "2019-01-14T17:50:26.584606, step: 652, loss: 0.0491173192858696, acc: 0.9844, auc: 0.9993, precision: 0.9836, recall: 0.9836\n",
      "2019-01-14T17:50:26.819328, step: 653, loss: 0.03646184504032135, acc: 0.9922, auc: 0.9995, precision: 1.0, recall: 0.9859\n",
      "2019-01-14T17:50:27.040596, step: 654, loss: 0.02174697071313858, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9818\n",
      "2019-01-14T17:50:27.282903, step: 655, loss: 0.04363973066210747, acc: 0.9688, auc: 0.9993, precision: 0.9841, recall: 0.9538\n",
      "2019-01-14T17:50:27.516253, step: 656, loss: 0.053204990923404694, acc: 0.9844, auc: 0.9995, precision: 1.0, recall: 0.9722\n",
      "2019-01-14T17:50:27.736090, step: 657, loss: 0.05762989819049835, acc: 0.9688, auc: 0.9985, precision: 0.9857, recall: 0.9583\n",
      "2019-01-14T17:50:27.962865, step: 658, loss: 0.026287443935871124, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:50:28.195499, step: 659, loss: 0.03415917605161667, acc: 0.9844, auc: 1.0, precision: 0.9667, recall: 1.0\n",
      "2019-01-14T17:50:28.409706, step: 660, loss: 0.10597042739391327, acc: 0.9609, auc: 0.9985, precision: 0.9385, recall: 0.9839\n",
      "2019-01-14T17:50:28.620144, step: 661, loss: 0.05595985800027847, acc: 0.9688, auc: 1.0, precision: 1.0, recall: 0.9344\n",
      "2019-01-14T17:50:28.834390, step: 662, loss: 0.055231332778930664, acc: 0.9844, auc: 0.9983, precision: 1.0, recall: 0.9655\n",
      "2019-01-14T17:50:29.052555, step: 663, loss: 0.020161211490631104, acc: 0.9922, auc: 1.0, precision: 0.9821, recall: 1.0\n",
      "2019-01-14T17:50:29.268559, step: 664, loss: 0.013134622015058994, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:50:29.492527, step: 665, loss: 0.08616117388010025, acc: 0.9609, auc: 0.9973, precision: 1.0, recall: 0.9167\n",
      "2019-01-14T17:50:29.711869, step: 666, loss: 0.00897934939712286, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:50:29.923994, step: 667, loss: 0.013419616967439651, acc: 0.9922, auc: 1.0, precision: 0.9841, recall: 1.0\n",
      "2019-01-14T17:50:30.153828, step: 668, loss: 0.013780863024294376, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:50:30.374412, step: 669, loss: 0.03609609231352806, acc: 0.9844, auc: 0.9993, precision: 1.0, recall: 0.9643\n",
      "2019-01-14T17:50:30.601116, step: 670, loss: 0.05160076916217804, acc: 0.9766, auc: 0.9995, precision: 0.9667, recall: 0.9831\n",
      "2019-01-14T17:50:30.830781, step: 671, loss: 0.06914395093917847, acc: 0.9766, auc: 0.998, precision: 0.9701, recall: 0.9848\n",
      "2019-01-14T17:50:31.070558, step: 672, loss: 0.0179560836404562, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.973\n",
      "2019-01-14T17:50:31.291623, step: 673, loss: 0.023504681885242462, acc: 0.9844, auc: 0.9995, precision: 0.9841, recall: 0.9841\n",
      "2019-01-14T17:50:31.507720, step: 674, loss: 0.020407570526003838, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.971\n",
      "2019-01-14T17:50:31.743461, step: 675, loss: 0.07829391956329346, acc: 0.9766, auc: 0.9975, precision: 1.0, recall: 0.9483\n",
      "2019-01-14T17:50:31.979238, step: 676, loss: 0.08131639659404755, acc: 0.9609, auc: 0.999, precision: 1.0, recall: 0.9242\n",
      "2019-01-14T17:50:32.218363, step: 677, loss: 0.027561983093619347, acc: 0.9922, auc: 0.9995, precision: 0.9853, recall: 1.0\n",
      "2019-01-14T17:50:32.451416, step: 678, loss: 0.09726762771606445, acc: 0.9688, auc: 0.9998, precision: 0.9344, recall: 1.0\n",
      "2019-01-14T17:50:32.680057, step: 679, loss: 0.05382310971617699, acc: 0.9766, auc: 0.9983, precision: 0.9821, recall: 0.9649\n",
      "2019-01-14T17:50:32.924668, step: 680, loss: 0.010733166709542274, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:50:33.154574, step: 681, loss: 0.014005651697516441, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:50:33.405064, step: 682, loss: 0.038786690682172775, acc: 0.9922, auc: 0.9995, precision: 0.9844, recall: 1.0\n",
      "2019-01-14T17:50:33.629226, step: 683, loss: 0.02586362697184086, acc: 0.9922, auc: 0.9998, precision: 1.0, recall: 0.9861\n",
      "2019-01-14T17:50:33.850518, step: 684, loss: 0.01620871014893055, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:50:34.074367, step: 685, loss: 0.03922143951058388, acc: 0.9766, auc: 0.9995, precision: 1.0, recall: 0.9531\n",
      "2019-01-14T17:50:34.301827, step: 686, loss: 0.0233262050896883, acc: 0.9844, auc: 0.9998, precision: 1.0, recall: 0.971\n",
      "2019-01-14T17:50:34.509977, step: 687, loss: 0.04373424872756004, acc: 0.9844, auc: 0.999, precision: 0.9833, recall: 0.9833\n",
      "2019-01-14T17:50:34.723375, step: 688, loss: 0.03629011660814285, acc: 0.9922, auc: 0.9998, precision: 1.0, recall: 0.9836\n",
      "2019-01-14T17:50:34.955154, step: 689, loss: 0.06802885979413986, acc: 0.9766, auc: 0.9976, precision: 0.9697, recall: 0.9846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-14T17:50:35.179874, step: 690, loss: 0.03820259869098663, acc: 0.9844, auc: 0.9993, precision: 0.9825, recall: 0.9825\n",
      "2019-01-14T17:50:35.403766, step: 691, loss: 0.05595526471734047, acc: 0.9844, auc: 0.9983, precision: 0.9836, recall: 0.9836\n",
      "2019-01-14T17:50:35.631695, step: 692, loss: 0.019621308892965317, acc: 0.9922, auc: 0.9998, precision: 1.0, recall: 0.9833\n",
      "2019-01-14T17:50:35.861515, step: 693, loss: 0.011102136224508286, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:50:36.090577, step: 694, loss: 0.0880637988448143, acc: 0.9766, auc: 0.9961, precision: 0.9851, recall: 0.9706\n",
      "2019-01-14T17:50:36.323094, step: 695, loss: 0.015291526913642883, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9853\n",
      "2019-01-14T17:50:36.547090, step: 696, loss: 0.03869440779089928, acc: 0.9766, auc: 0.9993, precision: 0.9848, recall: 0.9701\n",
      "2019-01-14T17:50:36.777760, step: 697, loss: 0.02513662353157997, acc: 0.9922, auc: 1.0, precision: 0.9846, recall: 1.0\n",
      "2019-01-14T17:50:37.013667, step: 698, loss: 0.03363240510225296, acc: 0.9844, auc: 0.9995, precision: 0.9828, recall: 0.9828\n",
      "2019-01-14T17:50:37.231516, step: 699, loss: 0.08828845620155334, acc: 0.9688, auc: 0.999, precision: 1.0, recall: 0.9385\n",
      "2019-01-14T17:50:37.467389, step: 700, loss: 0.02859525755047798, acc: 0.9844, auc: 0.9995, precision: 1.0, recall: 0.9722\n",
      "\n",
      "Evaluation:\n",
      "2019-01-14T17:50:46.177837, step: 700, loss: 0.617643932501475, acc: 0.8704000000000002, auc: 0.941628205128205, precision: 0.8659564102564101, recall: 0.8772435897435897\n",
      "2019-01-14T17:50:46.386877, step: 701, loss: 0.05594303458929062, acc: 0.9766, auc: 0.9995, precision: 0.9667, recall: 0.9831\n",
      "2019-01-14T17:50:46.617748, step: 702, loss: 0.02391897514462471, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9859\n",
      "2019-01-14T17:50:46.833940, step: 703, loss: 0.0753873810172081, acc: 0.9766, auc: 0.9968, precision: 0.9831, recall: 0.9667\n",
      "2019-01-14T17:50:47.065150, step: 704, loss: 0.03757935017347336, acc: 0.9922, auc: 1.0, precision: 0.9831, recall: 1.0\n",
      "2019-01-14T17:50:47.291349, step: 705, loss: 0.09126241505146027, acc: 0.9766, auc: 0.9975, precision: 0.9643, recall: 0.9818\n",
      "2019-01-14T17:50:47.526822, step: 706, loss: 0.04776833578944206, acc: 0.9766, auc: 0.9988, precision: 1.0, recall: 0.9516\n",
      "2019-01-14T17:50:47.739526, step: 707, loss: 0.008879604749381542, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9844\n",
      "2019-01-14T17:50:47.967294, step: 708, loss: 0.059039607644081116, acc: 0.9688, auc: 0.999, precision: 0.9844, recall: 0.9545\n",
      "2019-01-14T17:50:48.185877, step: 709, loss: 0.04131004586815834, acc: 0.9766, auc: 1.0, precision: 1.0, recall: 0.9577\n",
      "2019-01-14T17:50:48.396851, step: 710, loss: 0.030615150928497314, acc: 0.9844, auc: 0.9998, precision: 1.0, recall: 0.9692\n",
      "2019-01-14T17:50:48.626103, step: 711, loss: 0.046289898455142975, acc: 0.9766, auc: 0.9988, precision: 0.9859, recall: 0.9722\n",
      "2019-01-14T17:50:48.861397, step: 712, loss: 0.03223846107721329, acc: 0.9922, auc: 1.0, precision: 0.9836, recall: 1.0\n",
      "2019-01-14T17:50:49.088921, step: 713, loss: 0.015174061059951782, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:50:49.339366, step: 714, loss: 0.032524578273296356, acc: 0.9844, auc: 1.0, precision: 0.9722, recall: 1.0\n",
      "2019-01-14T17:50:49.565703, step: 715, loss: 0.009405975230038166, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:50:49.810020, step: 716, loss: 0.025381818413734436, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9714\n",
      "2019-01-14T17:50:50.030746, step: 717, loss: 0.05417897552251816, acc: 0.9766, auc: 0.9985, precision: 1.0, recall: 0.9464\n",
      "2019-01-14T17:50:50.259642, step: 718, loss: 0.05286417901515961, acc: 0.9766, auc: 0.998, precision: 0.9855, recall: 0.9714\n",
      "2019-01-14T17:50:50.478190, step: 719, loss: 0.0391780324280262, acc: 0.9766, auc: 0.9993, precision: 0.971, recall: 0.9853\n",
      "2019-01-14T17:50:50.700851, step: 720, loss: 0.05461098626255989, acc: 0.9922, auc: 0.9985, precision: 1.0, recall: 0.9841\n",
      "2019-01-14T17:50:50.918130, step: 721, loss: 0.06679759919643402, acc: 0.9688, auc: 0.9973, precision: 0.9559, recall: 0.9848\n",
      "2019-01-14T17:50:51.131769, step: 722, loss: 0.061706025153398514, acc: 0.9844, auc: 0.9969, precision: 0.987, recall: 0.987\n",
      "2019-01-14T17:50:51.363260, step: 723, loss: 0.025640513747930527, acc: 0.9688, auc: 1.0, precision: 1.0, recall: 0.9375\n",
      "2019-01-14T17:50:51.609234, step: 724, loss: 0.03693787008523941, acc: 0.9766, auc: 0.9995, precision: 0.9831, recall: 0.9667\n",
      "2019-01-14T17:50:51.829636, step: 725, loss: 0.02590053901076317, acc: 0.9922, auc: 0.9995, precision: 1.0, recall: 0.9848\n",
      "2019-01-14T17:50:52.052270, step: 726, loss: 0.024681763723492622, acc: 0.9844, auc: 0.9995, precision: 0.9825, recall: 0.9825\n",
      "2019-01-14T17:50:52.282562, step: 727, loss: 0.04017336294054985, acc: 0.9766, auc: 0.9993, precision: 0.9821, recall: 0.9649\n",
      "2019-01-14T17:50:52.504207, step: 728, loss: 0.028527144342660904, acc: 0.9922, auc: 0.9995, precision: 1.0, recall: 0.9861\n",
      "2019-01-14T17:50:52.729865, step: 729, loss: 0.07514525949954987, acc: 0.9688, auc: 0.9946, precision: 0.9655, recall: 0.9655\n",
      "2019-01-14T17:50:52.943967, step: 730, loss: 0.05015082284808159, acc: 0.9766, auc: 0.999, precision: 0.9672, recall: 0.9833\n",
      "2019-01-14T17:50:53.171615, step: 731, loss: 0.04442020133137703, acc: 0.9844, auc: 0.9995, precision: 0.9839, recall: 0.9839\n",
      "2019-01-14T17:50:53.390391, step: 732, loss: 0.03767445683479309, acc: 0.9844, auc: 0.999, precision: 0.9855, recall: 0.9855\n",
      "2019-01-14T17:50:53.607231, step: 733, loss: 0.008301927708089352, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9773\n",
      "2019-01-14T17:50:53.827990, step: 734, loss: 0.04244364798069, acc: 0.9766, auc: 1.0, precision: 1.0, recall: 0.9516\n",
      "2019-01-14T17:50:54.044311, step: 735, loss: 0.06938637048006058, acc: 0.9766, auc: 0.9966, precision: 0.9855, recall: 0.9714\n",
      "2019-01-14T17:50:54.257364, step: 736, loss: 0.10519789159297943, acc: 0.9844, auc: 0.9937, precision: 0.9701, recall: 1.0\n",
      "2019-01-14T17:50:54.477756, step: 737, loss: 0.04220999404788017, acc: 0.9922, auc: 0.9983, precision: 0.9846, recall: 1.0\n",
      "2019-01-14T17:50:54.700587, step: 738, loss: 0.028959475457668304, acc: 0.9844, auc: 0.9995, precision: 1.0, recall: 0.9672\n",
      "2019-01-14T17:50:54.942898, step: 739, loss: 0.05461884289979935, acc: 0.9766, auc: 0.9998, precision: 1.0, recall: 0.95\n",
      "2019-01-14T17:50:55.172726, step: 740, loss: 0.05773137882351875, acc: 0.9844, auc: 0.9978, precision: 0.9828, recall: 0.9828\n",
      "2019-01-14T17:50:55.397552, step: 741, loss: 0.04784128814935684, acc: 0.9844, auc: 0.9988, precision: 0.9839, recall: 0.9839\n",
      "2019-01-14T17:50:55.636156, step: 742, loss: 0.06552014499902725, acc: 0.9688, auc: 0.9975, precision: 0.9821, recall: 0.9483\n",
      "2019-01-14T17:50:55.879170, step: 743, loss: 0.07093087583780289, acc: 0.9844, auc: 0.999, precision: 0.9706, recall: 1.0\n",
      "2019-01-14T17:50:56.116466, step: 744, loss: 0.036903999745845795, acc: 0.9844, auc: 1.0, precision: 0.9697, recall: 1.0\n",
      "2019-01-14T17:50:56.359304, step: 745, loss: 0.03777092695236206, acc: 0.9766, auc: 0.999, precision: 0.9718, recall: 0.9857\n",
      "2019-01-14T17:50:56.601281, step: 746, loss: 0.0507456511259079, acc: 0.9844, auc: 0.9985, precision: 0.9844, recall: 0.9844\n",
      "2019-01-14T17:50:56.830765, step: 747, loss: 0.009357050992548466, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9844\n",
      "2019-01-14T17:50:57.059145, step: 748, loss: 0.041963234543800354, acc: 0.9844, auc: 0.9995, precision: 1.0, recall: 0.9697\n",
      "2019-01-14T17:50:57.291154, step: 749, loss: 0.0865100622177124, acc: 0.9609, auc: 0.9976, precision: 0.9839, recall: 0.9385\n",
      "2019-01-14T17:50:57.521045, step: 750, loss: 0.02170318365097046, acc: 0.9922, auc: 0.9998, precision: 1.0, recall: 0.9859\n",
      "2019-01-14T17:50:57.740069, step: 751, loss: 0.023558489978313446, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:50:57.967571, step: 752, loss: 0.1113373339176178, acc: 0.9688, auc: 0.9958, precision: 0.9661, recall: 0.9661\n",
      "2019-01-14T17:50:58.190555, step: 753, loss: 0.04023926705121994, acc: 0.9844, auc: 1.0, precision: 0.9688, recall: 1.0\n",
      "2019-01-14T17:50:58.426353, step: 754, loss: 0.07251350581645966, acc: 0.9766, auc: 0.9998, precision: 0.9565, recall: 1.0\n",
      "2019-01-14T17:50:58.662950, step: 755, loss: 0.01868106797337532, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-14T17:50:58.880630, step: 756, loss: 0.09634710103273392, acc: 0.9609, auc: 0.9988, precision: 1.0, recall: 0.9194\n",
      "2019-01-14T17:50:59.112959, step: 757, loss: 0.08904361724853516, acc: 0.9609, auc: 0.9985, precision: 1.0, recall: 0.9231\n",
      "2019-01-14T17:50:59.328292, step: 758, loss: 0.03636818751692772, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9565\n",
      "2019-01-14T17:50:59.555107, step: 759, loss: 0.03502330556511879, acc: 0.9844, auc: 0.9995, precision: 0.9863, recall: 0.9863\n",
      "2019-01-14T17:50:59.768379, step: 760, loss: 0.07004719227552414, acc: 0.9766, auc: 0.9995, precision: 0.9583, recall: 1.0\n",
      "2019-01-14T17:51:00.010785, step: 761, loss: 0.05639171600341797, acc: 0.9844, auc: 1.0, precision: 0.9667, recall: 1.0\n",
      "2019-01-14T17:51:00.231916, step: 762, loss: 0.08650539815425873, acc: 0.9688, auc: 0.9995, precision: 0.931, recall: 1.0\n",
      "2019-01-14T17:51:00.465083, step: 763, loss: 0.047184303402900696, acc: 0.9766, auc: 0.9988, precision: 0.971, recall: 0.9853\n",
      "2019-01-14T17:51:00.690477, step: 764, loss: 0.1083020344376564, acc: 0.9531, auc: 0.9995, precision: 1.0, recall: 0.9178\n",
      "2019-01-14T17:51:00.924036, step: 765, loss: 0.18424341082572937, acc: 0.9219, auc: 0.9983, precision: 1.0, recall: 0.8413\n",
      "2019-01-14T17:51:01.157495, step: 766, loss: 0.07515160739421844, acc: 0.9688, auc: 0.9995, precision: 1.0, recall: 0.9467\n",
      "2019-01-14T17:51:01.388889, step: 767, loss: 0.06363318860530853, acc: 0.9844, auc: 0.9975, precision: 0.9853, recall: 0.9853\n",
      "2019-01-14T17:51:01.618375, step: 768, loss: 0.22731545567512512, acc: 0.9375, auc: 0.998, precision: 0.8873, recall: 1.0\n",
      "2019-01-14T17:51:01.846810, step: 769, loss: 0.16477593779563904, acc: 0.9531, auc: 0.9993, precision: 0.9091, recall: 1.0\n",
      "2019-01-14T17:51:02.053325, step: 770, loss: 0.0575205460190773, acc: 0.9844, auc: 0.999, precision: 0.9851, recall: 0.9851\n",
      "2019-01-14T17:51:02.282634, step: 771, loss: 0.045195795595645905, acc: 0.9688, auc: 0.9998, precision: 1.0, recall: 0.9365\n",
      "2019-01-14T17:51:02.513081, step: 772, loss: 0.06116772070527077, acc: 0.9531, auc: 0.9997, precision: 1.0, recall: 0.9189\n",
      "2019-01-14T17:51:02.742115, step: 773, loss: 0.09785345941781998, acc: 0.9688, auc: 1.0, precision: 1.0, recall: 0.9333\n",
      "2019-01-14T17:51:02.952616, step: 774, loss: 0.10268498957157135, acc: 0.9531, auc: 0.9985, precision: 1.0, recall: 0.9077\n",
      "2019-01-14T17:51:03.168585, step: 775, loss: 0.05523693561553955, acc: 0.9922, auc: 0.9985, precision: 1.0, recall: 0.9836\n",
      "2019-01-14T17:51:03.399380, step: 776, loss: 0.055571649223566055, acc: 0.9688, auc: 0.9982, precision: 0.973, recall: 0.973\n",
      "2019-01-14T17:51:03.643539, step: 777, loss: 0.26329168677330017, acc: 0.9531, auc: 0.998, precision: 0.9032, recall: 1.0\n",
      "2019-01-14T17:51:03.859321, step: 778, loss: 0.1091175228357315, acc: 0.9609, auc: 0.9998, precision: 0.9231, recall: 1.0\n",
      "2019-01-14T17:51:04.095863, step: 779, loss: 0.14555184543132782, acc: 0.9609, auc: 0.9965, precision: 0.918, recall: 1.0\n",
      "2019-01-14T17:51:04.304054, step: 780, loss: 0.049580011516809464, acc: 0.9609, auc: 1.0, precision: 1.0, recall: 0.9219\n",
      "start training model\n",
      "2019-01-14T17:51:04.551217, step: 781, loss: 0.05507316812872887, acc: 0.9688, auc: 1.0, precision: 1.0, recall: 0.9365\n",
      "2019-01-14T17:51:04.796837, step: 782, loss: 0.14095956087112427, acc: 0.9219, auc: 1.0, precision: 1.0, recall: 0.8305\n",
      "2019-01-14T17:51:05.026641, step: 783, loss: 0.05160747468471527, acc: 0.9766, auc: 1.0, precision: 1.0, recall: 0.95\n",
      "2019-01-14T17:51:05.250560, step: 784, loss: 0.002740392694249749, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:51:05.468542, step: 785, loss: 0.027648571878671646, acc: 0.9922, auc: 1.0, precision: 0.9828, recall: 1.0\n",
      "2019-01-14T17:51:05.681351, step: 786, loss: 0.008186688646674156, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:51:05.895374, step: 787, loss: 0.09036225080490112, acc: 0.9766, auc: 0.999, precision: 0.9508, recall: 1.0\n",
      "2019-01-14T17:51:06.109834, step: 788, loss: 0.07545238733291626, acc: 0.9844, auc: 1.0, precision: 0.9706, recall: 1.0\n",
      "2019-01-14T17:51:06.327261, step: 789, loss: 0.042630281299352646, acc: 0.9844, auc: 1.0, precision: 0.973, recall: 1.0\n",
      "2019-01-14T17:51:06.553832, step: 790, loss: 0.004839378874748945, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:51:06.778082, step: 791, loss: 0.02715226076543331, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9661\n",
      "2019-01-14T17:51:06.994806, step: 792, loss: 0.11243438720703125, acc: 0.9531, auc: 0.9995, precision: 1.0, recall: 0.9143\n",
      "2019-01-14T17:51:07.221848, step: 793, loss: 0.030296333134174347, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9863\n",
      "2019-01-14T17:51:07.441121, step: 794, loss: 0.09612983465194702, acc: 0.9766, auc: 0.9993, precision: 1.0, recall: 0.9492\n",
      "2019-01-14T17:51:07.654206, step: 795, loss: 0.007954178377985954, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9836\n",
      "2019-01-14T17:51:07.882565, step: 796, loss: 0.06553660333156586, acc: 0.9844, auc: 0.9988, precision: 0.971, recall: 1.0\n",
      "2019-01-14T17:51:08.098211, step: 797, loss: 0.08868774771690369, acc: 0.9766, auc: 0.9998, precision: 0.9545, recall: 1.0\n",
      "2019-01-14T17:51:08.327879, step: 798, loss: 0.06699331104755402, acc: 0.9844, auc: 0.9998, precision: 0.9706, recall: 1.0\n",
      "2019-01-14T17:51:08.565007, step: 799, loss: 0.009903556667268276, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9839\n",
      "2019-01-14T17:51:08.796374, step: 800, loss: 0.009362022392451763, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9853\n",
      "\n",
      "Evaluation:\n",
      "2019-01-14T17:51:17.513383, step: 800, loss: 0.6931714186301599, acc: 0.8701948717948721, auc: 0.9421230769230772, precision: 0.8843076923076921, recall: 0.85714358974359\n",
      "2019-01-14T17:51:17.736840, step: 801, loss: 0.007776979822665453, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:51:17.969600, step: 802, loss: 0.01881607063114643, acc: 0.9922, auc: 0.9998, precision: 0.9836, recall: 1.0\n",
      "2019-01-14T17:51:18.193257, step: 803, loss: 0.06096275523304939, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9692\n",
      "2019-01-14T17:51:18.422336, step: 804, loss: 0.0031655249185860157, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:51:18.638241, step: 805, loss: 0.009708598256111145, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:51:18.860033, step: 806, loss: 0.021126728504896164, acc: 0.9922, auc: 0.9995, precision: 1.0, recall: 0.9846\n",
      "2019-01-14T17:51:19.077934, step: 807, loss: 0.02082301676273346, acc: 0.9844, auc: 0.9998, precision: 0.9846, recall: 0.9846\n",
      "2019-01-14T17:51:19.281944, step: 808, loss: 0.01673935540020466, acc: 0.9922, auc: 1.0, precision: 0.9853, recall: 1.0\n",
      "2019-01-14T17:51:19.521800, step: 809, loss: 0.011489847674965858, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:51:19.747297, step: 810, loss: 0.09670352935791016, acc: 0.9844, auc: 0.9956, precision: 0.971, recall: 1.0\n",
      "2019-01-14T17:51:19.969332, step: 811, loss: 0.05395675078034401, acc: 0.9766, auc: 0.999, precision: 0.9841, recall: 0.9688\n",
      "2019-01-14T17:51:20.187721, step: 812, loss: 0.019130295142531395, acc: 0.9922, auc: 0.9998, precision: 1.0, recall: 0.9855\n",
      "2019-01-14T17:51:20.409500, step: 813, loss: 0.0077588846907019615, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:51:20.634435, step: 814, loss: 0.02201204188168049, acc: 0.9922, auc: 0.9998, precision: 0.9841, recall: 1.0\n",
      "2019-01-14T17:51:20.867466, step: 815, loss: 0.028138136491179466, acc: 0.9766, auc: 1.0, precision: 1.0, recall: 0.9577\n",
      "2019-01-14T17:51:21.100467, step: 816, loss: 0.04742113873362541, acc: 0.9922, auc: 0.999, precision: 1.0, recall: 0.9839\n",
      "2019-01-14T17:51:21.347232, step: 817, loss: 0.0019225911237299442, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:51:21.582856, step: 818, loss: 0.027593230828642845, acc: 0.9844, auc: 0.9995, precision: 0.9851, recall: 0.9851\n",
      "2019-01-14T17:51:21.789451, step: 819, loss: 0.03272272273898125, acc: 0.9922, auc: 0.9995, precision: 0.9844, recall: 1.0\n",
      "2019-01-14T17:51:21.993969, step: 820, loss: 0.04687826335430145, acc: 0.9766, auc: 0.999, precision: 1.0, recall: 0.9531\n",
      "2019-01-14T17:51:22.217642, step: 821, loss: 0.011748308315873146, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-14T17:51:22.437933, step: 822, loss: 0.029562678188085556, acc: 0.9766, auc: 1.0, precision: 0.9559, recall: 1.0\n",
      "2019-01-14T17:51:22.660857, step: 823, loss: 0.002724599791690707, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:51:22.879426, step: 824, loss: 0.0132761774584651, acc: 0.9922, auc: 1.0, precision: 0.9833, recall: 1.0\n",
      "2019-01-14T17:51:23.112322, step: 825, loss: 0.010938242077827454, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:51:23.336209, step: 826, loss: 0.054853394627571106, acc: 0.9844, auc: 0.9985, precision: 1.0, recall: 0.9697\n",
      "2019-01-14T17:51:23.555658, step: 827, loss: 0.05215667560696602, acc: 0.9922, auc: 0.9985, precision: 1.0, recall: 0.9855\n",
      "2019-01-14T17:51:23.764672, step: 828, loss: 0.0931725949048996, acc: 0.9609, auc: 0.9978, precision: 0.9831, recall: 0.9355\n",
      "2019-01-14T17:51:23.971273, step: 829, loss: 0.050857458263635635, acc: 0.9844, auc: 0.9987, precision: 0.9868, recall: 0.9868\n",
      "2019-01-14T17:51:24.202784, step: 830, loss: 0.1088319793343544, acc: 0.9688, auc: 0.9963, precision: 0.9355, recall: 1.0\n",
      "2019-01-14T17:51:24.435595, step: 831, loss: 0.011167029850184917, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:51:24.648465, step: 832, loss: 0.03605392575263977, acc: 0.9844, auc: 0.9995, precision: 0.9839, recall: 0.9839\n",
      "2019-01-14T17:51:24.870872, step: 833, loss: 0.02900058403611183, acc: 0.9922, auc: 0.9998, precision: 1.0, recall: 0.9846\n",
      "2019-01-14T17:51:25.084113, step: 834, loss: 0.03210723400115967, acc: 0.9766, auc: 1.0, precision: 1.0, recall: 0.9589\n",
      "2019-01-14T17:51:25.305739, step: 835, loss: 0.013415452092885971, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:51:25.551542, step: 836, loss: 0.01940780319273472, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9868\n",
      "2019-01-14T17:51:25.776734, step: 837, loss: 0.035839810967445374, acc: 0.9844, auc: 0.9995, precision: 1.0, recall: 0.9672\n",
      "2019-01-14T17:51:26.008797, step: 838, loss: 0.013453777879476547, acc: 0.9922, auc: 1.0, precision: 0.9857, recall: 1.0\n",
      "2019-01-14T17:51:26.225809, step: 839, loss: 0.013908585533499718, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:51:26.458233, step: 840, loss: 0.022375497967004776, acc: 0.9922, auc: 1.0, precision: 0.9844, recall: 1.0\n",
      "2019-01-14T17:51:26.679981, step: 841, loss: 0.04994641989469528, acc: 0.9922, auc: 0.999, precision: 0.9828, recall: 1.0\n",
      "2019-01-14T17:51:26.904723, step: 842, loss: 0.024370983242988586, acc: 0.9844, auc: 0.9998, precision: 0.9853, recall: 0.9853\n",
      "2019-01-14T17:51:27.127478, step: 843, loss: 0.06398724019527435, acc: 0.9766, auc: 0.9977, precision: 1.0, recall: 0.9595\n",
      "2019-01-14T17:51:27.345901, step: 844, loss: 0.04198319837450981, acc: 0.9688, auc: 0.9983, precision: 0.9841, recall: 0.9538\n",
      "2019-01-14T17:51:27.573158, step: 845, loss: 0.08063770830631256, acc: 0.9766, auc: 0.999, precision: 1.0, recall: 0.9565\n",
      "2019-01-14T17:51:27.789564, step: 846, loss: 0.010917897336184978, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:51:28.014039, step: 847, loss: 0.006849667988717556, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:51:28.240816, step: 848, loss: 0.007087612058967352, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:51:28.470142, step: 849, loss: 0.054108813405036926, acc: 0.9922, auc: 0.9995, precision: 0.9855, recall: 1.0\n",
      "2019-01-14T17:51:28.690562, step: 850, loss: 0.035086434334516525, acc: 0.9844, auc: 0.9993, precision: 0.9846, recall: 0.9846\n",
      "2019-01-14T17:51:28.895547, step: 851, loss: 0.053442344069480896, acc: 0.9844, auc: 0.9988, precision: 0.9649, recall: 1.0\n",
      "2019-01-14T17:51:29.106876, step: 852, loss: 0.016060709953308105, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.973\n",
      "2019-01-14T17:51:29.355095, step: 853, loss: 0.006212824955582619, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:51:29.590264, step: 854, loss: 0.0026197261177003384, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:51:29.814930, step: 855, loss: 0.014677555300295353, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9861\n",
      "2019-01-14T17:51:30.044255, step: 856, loss: 0.006410412490367889, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:51:30.270466, step: 857, loss: 0.03881171718239784, acc: 0.9844, auc: 0.999, precision: 1.0, recall: 0.9677\n",
      "2019-01-14T17:51:30.489241, step: 858, loss: 0.017999067902565002, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9841\n",
      "2019-01-14T17:51:30.715046, step: 859, loss: 0.06442154198884964, acc: 0.9844, auc: 0.998, precision: 0.9828, recall: 0.9828\n",
      "2019-01-14T17:51:30.941954, step: 860, loss: 0.026900649070739746, acc: 0.9844, auc: 0.9998, precision: 0.9701, recall: 1.0\n",
      "2019-01-14T17:51:31.167365, step: 861, loss: 0.015126852318644524, acc: 0.9922, auc: 1.0, precision: 0.9851, recall: 1.0\n",
      "2019-01-14T17:51:31.408157, step: 862, loss: 0.0022241193801164627, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:51:31.633822, step: 863, loss: 0.04220585897564888, acc: 0.9844, auc: 0.9993, precision: 0.9841, recall: 0.9841\n",
      "2019-01-14T17:51:31.857838, step: 864, loss: 0.0320272296667099, acc: 0.9766, auc: 1.0, precision: 1.0, recall: 0.95\n",
      "2019-01-14T17:51:32.089928, step: 865, loss: 0.01368825975805521, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9839\n",
      "2019-01-14T17:51:32.315066, step: 866, loss: 0.005740610882639885, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:51:32.538333, step: 867, loss: 0.11695193499326706, acc: 0.9688, auc: 0.9951, precision: 0.9524, recall: 0.9836\n",
      "2019-01-14T17:51:32.773065, step: 868, loss: 0.05994417890906334, acc: 0.9766, auc: 0.9993, precision: 1.0, recall: 0.9508\n",
      "2019-01-14T17:51:33.003050, step: 869, loss: 0.013924427330493927, acc: 0.9922, auc: 1.0, precision: 0.9825, recall: 1.0\n",
      "2019-01-14T17:51:33.224404, step: 870, loss: 0.04331066086888313, acc: 0.9922, auc: 0.9988, precision: 1.0, recall: 0.9836\n",
      "2019-01-14T17:51:33.459103, step: 871, loss: 0.023353783413767815, acc: 0.9844, auc: 0.9995, precision: 1.0, recall: 0.9688\n",
      "2019-01-14T17:51:33.688429, step: 872, loss: 0.03787028416991234, acc: 0.9922, auc: 0.999, precision: 0.9846, recall: 1.0\n",
      "2019-01-14T17:51:33.906095, step: 873, loss: 0.07213522493839264, acc: 0.9844, auc: 0.9973, precision: 0.9846, recall: 0.9846\n",
      "2019-01-14T17:51:34.131119, step: 874, loss: 0.02181827649474144, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9683\n",
      "2019-01-14T17:51:34.378504, step: 875, loss: 0.010354084894061089, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:51:34.605452, step: 876, loss: 0.002085618209093809, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:51:34.823552, step: 877, loss: 0.020581096410751343, acc: 0.9922, auc: 0.9998, precision: 1.0, recall: 0.9836\n",
      "2019-01-14T17:51:35.047871, step: 878, loss: 0.012952207587659359, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:51:35.274030, step: 879, loss: 0.053742460906505585, acc: 0.9844, auc: 0.9983, precision: 0.9825, recall: 0.9825\n",
      "2019-01-14T17:51:35.491326, step: 880, loss: 0.024334454908967018, acc: 0.9922, auc: 1.0, precision: 0.9833, recall: 1.0\n",
      "2019-01-14T17:51:35.717428, step: 881, loss: 0.01458423025906086, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9855\n",
      "2019-01-14T17:51:35.940692, step: 882, loss: 0.041773721575737, acc: 0.9844, auc: 0.9995, precision: 1.0, recall: 0.9714\n",
      "2019-01-14T17:51:36.167215, step: 883, loss: 0.1257815957069397, acc: 0.9297, auc: 0.9976, precision: 0.9815, recall: 0.8689\n",
      "2019-01-14T17:51:36.385111, step: 884, loss: 0.0042941030114889145, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:51:36.617765, step: 885, loss: 0.023454882204532623, acc: 0.9922, auc: 1.0, precision: 0.9841, recall: 1.0\n",
      "2019-01-14T17:51:36.843974, step: 886, loss: 0.02333119697868824, acc: 0.9844, auc: 1.0, precision: 0.9706, recall: 1.0\n",
      "2019-01-14T17:51:37.066859, step: 887, loss: 0.05356394499540329, acc: 0.9844, auc: 1.0, precision: 0.9688, recall: 1.0\n",
      "2019-01-14T17:51:37.293210, step: 888, loss: 0.04424683749675751, acc: 0.9766, auc: 0.9995, precision: 0.9683, recall: 0.9839\n",
      "2019-01-14T17:51:37.515123, step: 889, loss: 0.009153762832283974, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-14T17:51:37.746117, step: 890, loss: 0.10842294245958328, acc: 0.9688, auc: 0.9953, precision: 0.9655, recall: 0.9655\n",
      "2019-01-14T17:51:37.970105, step: 891, loss: 0.007198004983365536, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:51:38.189669, step: 892, loss: 0.08174813538789749, acc: 0.9609, auc: 0.9998, precision: 1.0, recall: 0.9296\n",
      "2019-01-14T17:51:38.405802, step: 893, loss: 0.0496695376932621, acc: 0.9766, auc: 0.999, precision: 0.9848, recall: 0.9701\n",
      "2019-01-14T17:51:38.627713, step: 894, loss: 0.0040099117904901505, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:51:38.851995, step: 895, loss: 0.005448808893561363, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:51:39.095798, step: 896, loss: 0.005965918768197298, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:51:39.328194, step: 897, loss: 0.020417947322130203, acc: 0.9922, auc: 1.0, precision: 0.9833, recall: 1.0\n",
      "2019-01-14T17:51:39.554613, step: 898, loss: 0.07518845051527023, acc: 0.9922, auc: 0.9963, precision: 1.0, recall: 0.9863\n",
      "2019-01-14T17:51:39.791591, step: 899, loss: 0.04931128770112991, acc: 0.9922, auc: 0.9998, precision: 0.9839, recall: 1.0\n",
      "2019-01-14T17:51:40.019739, step: 900, loss: 0.01776382513344288, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9701\n",
      "\n",
      "Evaluation:\n",
      "2019-01-14T17:51:48.687769, step: 900, loss: 0.6778483306750273, acc: 0.8643897435897435, auc: 0.9402410256410255, precision: 0.8710717948717945, recall: 0.859261538461538\n",
      "2019-01-14T17:51:48.901274, step: 901, loss: 0.005775548983365297, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:51:49.144577, step: 902, loss: 0.04160971939563751, acc: 0.9844, auc: 0.9988, precision: 0.9861, recall: 0.9861\n",
      "2019-01-14T17:51:49.361737, step: 903, loss: 0.057558443397283554, acc: 0.9766, auc: 0.999, precision: 1.0, recall: 0.9545\n",
      "2019-01-14T17:51:49.587992, step: 904, loss: 0.005955347325652838, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:51:49.825531, step: 905, loss: 0.007592807989567518, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:51:50.032890, step: 906, loss: 0.016936451196670532, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9844\n",
      "2019-01-14T17:51:50.260056, step: 907, loss: 0.02202458865940571, acc: 0.9922, auc: 0.9995, precision: 1.0, recall: 0.9836\n",
      "2019-01-14T17:51:50.483907, step: 908, loss: 0.07015077769756317, acc: 0.9922, auc: 0.999, precision: 0.9844, recall: 1.0\n",
      "2019-01-14T17:51:50.698949, step: 909, loss: 0.023955998942255974, acc: 0.9844, auc: 1.0, precision: 0.9701, recall: 1.0\n",
      "2019-01-14T17:51:50.910185, step: 910, loss: 0.09342886507511139, acc: 0.9688, auc: 0.9973, precision: 0.9726, recall: 0.9726\n",
      "2019-01-14T17:51:51.127225, step: 911, loss: 0.0628352165222168, acc: 0.9844, auc: 0.9973, precision: 0.9846, recall: 0.9846\n",
      "2019-01-14T17:51:51.355165, step: 912, loss: 0.0029620605055242777, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:51:51.574446, step: 913, loss: 0.017315050587058067, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9831\n",
      "2019-01-14T17:51:51.793559, step: 914, loss: 0.023616407066583633, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9661\n",
      "2019-01-14T17:51:52.009875, step: 915, loss: 0.03387444466352463, acc: 0.9766, auc: 1.0, precision: 1.0, recall: 0.9412\n",
      "2019-01-14T17:51:52.228675, step: 916, loss: 0.012550113722682, acc: 0.9922, auc: 1.0, precision: 0.9833, recall: 1.0\n",
      "2019-01-14T17:51:52.453880, step: 917, loss: 0.008131816051900387, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:51:52.680114, step: 918, loss: 0.03953175991773605, acc: 0.9844, auc: 0.9995, precision: 0.9818, recall: 0.9818\n",
      "2019-01-14T17:51:52.898739, step: 919, loss: 0.0037420918233692646, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:51:53.147091, step: 920, loss: 0.04877248406410217, acc: 0.9922, auc: 0.999, precision: 0.9859, recall: 1.0\n",
      "2019-01-14T17:51:53.373998, step: 921, loss: 0.018976999446749687, acc: 0.9922, auc: 1.0, precision: 0.9825, recall: 1.0\n",
      "2019-01-14T17:51:53.605314, step: 922, loss: 0.0591130256652832, acc: 0.9844, auc: 0.9978, precision: 0.9818, recall: 0.9818\n",
      "2019-01-14T17:51:53.842130, step: 923, loss: 0.022349197417497635, acc: 0.9844, auc: 1.0, precision: 0.9714, recall: 1.0\n",
      "2019-01-14T17:51:54.063259, step: 924, loss: 0.02585822343826294, acc: 0.9766, auc: 1.0, precision: 1.0, recall: 0.95\n",
      "2019-01-14T17:51:54.271939, step: 925, loss: 0.07421558350324631, acc: 0.9688, auc: 0.9983, precision: 1.0, recall: 0.9375\n",
      "2019-01-14T17:51:54.479827, step: 926, loss: 0.026074957102537155, acc: 0.9922, auc: 0.9997, precision: 1.0, recall: 0.98\n",
      "2019-01-14T17:51:54.691867, step: 927, loss: 0.012725243344902992, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:51:54.903492, step: 928, loss: 0.027373526245355606, acc: 0.9922, auc: 1.0, precision: 0.9839, recall: 1.0\n",
      "2019-01-14T17:51:55.118434, step: 929, loss: 0.05690137296915054, acc: 0.9922, auc: 0.9998, precision: 0.9848, recall: 1.0\n",
      "2019-01-14T17:51:55.335863, step: 930, loss: 0.08729302138090134, acc: 0.9688, auc: 0.999, precision: 0.9437, recall: 1.0\n",
      "2019-01-14T17:51:55.581266, step: 931, loss: 0.02564772218465805, acc: 0.9922, auc: 1.0, precision: 0.9836, recall: 1.0\n",
      "2019-01-14T17:51:55.803323, step: 932, loss: 0.013646218925714493, acc: 0.9922, auc: 0.9998, precision: 0.9848, recall: 1.0\n",
      "2019-01-14T17:51:56.018260, step: 933, loss: 0.15090590715408325, acc: 0.9531, auc: 0.9993, precision: 1.0, recall: 0.9104\n",
      "2019-01-14T17:51:56.234432, step: 934, loss: 0.037661805748939514, acc: 0.9609, auc: 1.0, precision: 1.0, recall: 0.9194\n",
      "2019-01-14T17:51:56.454840, step: 935, loss: 0.019436707720160484, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9833\n",
      "2019-01-14T17:51:56.693472, step: 936, loss: 0.016628533601760864, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9661\n",
      "start training model\n",
      "2019-01-14T17:51:56.938421, step: 937, loss: 0.016455039381980896, acc: 0.9922, auc: 1.0, precision: 0.9853, recall: 1.0\n",
      "2019-01-14T17:51:57.152724, step: 938, loss: 0.018810076639056206, acc: 0.9922, auc: 1.0, precision: 0.9853, recall: 1.0\n",
      "2019-01-14T17:51:57.381255, step: 939, loss: 0.05786912515759468, acc: 0.9766, auc: 1.0, precision: 0.9492, recall: 1.0\n",
      "2019-01-14T17:51:57.605767, step: 940, loss: 0.04157944396138191, acc: 0.9844, auc: 1.0, precision: 0.9718, recall: 1.0\n",
      "2019-01-14T17:51:57.862267, step: 941, loss: 0.01458407286554575, acc: 0.9922, auc: 1.0, precision: 0.9836, recall: 1.0\n",
      "2019-01-14T17:51:58.094430, step: 942, loss: 0.012371988035738468, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:51:58.326028, step: 943, loss: 0.005325167439877987, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:51:58.558073, step: 944, loss: 0.025172658264636993, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9697\n",
      "2019-01-14T17:51:58.791842, step: 945, loss: 0.04487788304686546, acc: 0.9766, auc: 1.0, precision: 1.0, recall: 0.9565\n",
      "2019-01-14T17:51:59.033691, step: 946, loss: 0.04043789207935333, acc: 0.9766, auc: 1.0, precision: 1.0, recall: 0.9531\n",
      "2019-01-14T17:51:59.260402, step: 947, loss: 0.004618972074240446, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:51:59.521138, step: 948, loss: 0.007371728774160147, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:51:59.751102, step: 949, loss: 0.01133772637695074, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:51:59.971328, step: 950, loss: 0.006072242744266987, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9848\n",
      "2019-01-14T17:52:00.197197, step: 951, loss: 0.062159549444913864, acc: 0.9766, auc: 1.0, precision: 0.9516, recall: 1.0\n",
      "2019-01-14T17:52:00.406635, step: 952, loss: 0.02167089655995369, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:52:00.620234, step: 953, loss: 0.017827127128839493, acc: 0.9922, auc: 1.0, precision: 0.9833, recall: 1.0\n",
      "2019-01-14T17:52:00.842947, step: 954, loss: 0.012378377839922905, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:52:01.070941, step: 955, loss: 0.1261962205171585, acc: 0.9531, auc: 0.9978, precision: 0.9836, recall: 0.9231\n",
      "2019-01-14T17:52:01.296279, step: 956, loss: 0.0027527438942342997, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-14T17:52:01.511644, step: 957, loss: 0.0099489726126194, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9815\n",
      "2019-01-14T17:52:01.741592, step: 958, loss: 0.00902460515499115, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9833\n",
      "2019-01-14T17:52:01.973775, step: 959, loss: 0.062183186411857605, acc: 0.9766, auc: 1.0, precision: 1.0, recall: 0.9634\n",
      "2019-01-14T17:52:02.195465, step: 960, loss: 0.007686220109462738, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:52:02.405427, step: 961, loss: 0.02452467568218708, acc: 0.9844, auc: 0.9995, precision: 0.9857, recall: 0.9857\n",
      "2019-01-14T17:52:02.616875, step: 962, loss: 0.012330184690654278, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9833\n",
      "2019-01-14T17:52:02.828759, step: 963, loss: 0.0058378009125590324, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:52:03.046172, step: 964, loss: 0.005541097838431597, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:52:03.260871, step: 965, loss: 0.05404333025217056, acc: 0.9922, auc: 0.9998, precision: 0.9846, recall: 1.0\n",
      "2019-01-14T17:52:03.473761, step: 966, loss: 0.005015999544411898, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:52:03.676368, step: 967, loss: 0.0053868587128818035, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:52:03.888199, step: 968, loss: 0.008310436271131039, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:52:04.110409, step: 969, loss: 0.024655696004629135, acc: 0.9922, auc: 0.9995, precision: 1.0, recall: 0.9844\n",
      "2019-01-14T17:52:04.325319, step: 970, loss: 0.012083952315151691, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:52:04.549322, step: 971, loss: 0.001553447567857802, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:52:04.762875, step: 972, loss: 0.010982065461575985, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9815\n",
      "2019-01-14T17:52:04.987726, step: 973, loss: 0.028265733271837234, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9706\n",
      "2019-01-14T17:52:05.216474, step: 974, loss: 0.0014079975662752986, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:52:05.422003, step: 975, loss: 0.013131724670529366, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9853\n",
      "2019-01-14T17:52:05.639266, step: 976, loss: 0.05927741155028343, acc: 0.9844, auc: 0.9993, precision: 1.0, recall: 0.9706\n",
      "2019-01-14T17:52:05.879663, step: 977, loss: 0.0019260849803686142, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:52:06.113667, step: 978, loss: 0.012887880206108093, acc: 0.9922, auc: 1.0, precision: 0.9855, recall: 1.0\n",
      "2019-01-14T17:52:06.340490, step: 979, loss: 0.0024437166284769773, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:52:06.570406, step: 980, loss: 0.01294547226279974, acc: 0.9922, auc: 1.0, precision: 0.9846, recall: 1.0\n",
      "2019-01-14T17:52:06.808613, step: 981, loss: 0.009611045941710472, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:52:07.029005, step: 982, loss: 0.015552307479083538, acc: 0.9922, auc: 1.0, precision: 0.9853, recall: 1.0\n",
      "2019-01-14T17:52:07.253778, step: 983, loss: 0.01588711328804493, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:52:07.469515, step: 984, loss: 0.007162461988627911, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:52:07.691890, step: 985, loss: 0.00737175066024065, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:52:07.901631, step: 986, loss: 0.006587330251932144, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:52:08.127738, step: 987, loss: 0.00291678449138999, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:52:08.355725, step: 988, loss: 0.01823076605796814, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9853\n",
      "2019-01-14T17:52:08.573469, step: 989, loss: 0.007115846034139395, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:52:08.802593, step: 990, loss: 0.04232574999332428, acc: 0.9922, auc: 0.9995, precision: 1.0, recall: 0.9848\n",
      "2019-01-14T17:52:09.022639, step: 991, loss: 0.007493363693356514, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9841\n",
      "2019-01-14T17:52:09.251426, step: 992, loss: 0.013832440599799156, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9831\n",
      "2019-01-14T17:52:09.480041, step: 993, loss: 0.03918412700295448, acc: 0.9922, auc: 0.9988, precision: 1.0, recall: 0.9846\n",
      "2019-01-14T17:52:09.708119, step: 994, loss: 0.005972742103040218, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:52:09.922340, step: 995, loss: 0.05865287408232689, acc: 0.9922, auc: 0.9998, precision: 0.9846, recall: 1.0\n",
      "2019-01-14T17:52:10.131196, step: 996, loss: 0.06657840311527252, acc: 0.9766, auc: 1.0, precision: 0.9474, recall: 1.0\n",
      "2019-01-14T17:52:10.342813, step: 997, loss: 0.0073959920555353165, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:52:10.563221, step: 998, loss: 0.00224289414472878, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:52:10.788446, step: 999, loss: 0.019689073786139488, acc: 0.9922, auc: 1.0, precision: 0.9844, recall: 1.0\n",
      "2019-01-14T17:52:11.009524, step: 1000, loss: 0.004370260983705521, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "\n",
      "Evaluation:\n",
      "2019-01-14T17:52:19.592373, step: 1000, loss: 0.8445010147033594, acc: 0.8543743589743591, auc: 0.9407743589743589, precision: 0.9065615384615384, recall: 0.7948897435897434\n",
      "2019-01-14T17:52:19.799851, step: 1001, loss: 0.059171780943870544, acc: 0.9844, auc: 0.9995, precision: 1.0, recall: 0.9667\n",
      "2019-01-14T17:52:20.025674, step: 1002, loss: 0.06456480920314789, acc: 0.9688, auc: 0.9993, precision: 1.0, recall: 0.9298\n",
      "2019-01-14T17:52:20.230431, step: 1003, loss: 0.0056862495839595795, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:52:20.446831, step: 1004, loss: 0.005421421490609646, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:52:20.657336, step: 1005, loss: 0.01148221641778946, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9828\n",
      "2019-01-14T17:52:20.873168, step: 1006, loss: 0.026155071333050728, acc: 0.9922, auc: 0.9998, precision: 0.9825, recall: 1.0\n",
      "2019-01-14T17:52:21.098513, step: 1007, loss: 0.003126095049083233, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:52:21.313759, step: 1008, loss: 0.018578248098492622, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:52:21.543522, step: 1009, loss: 0.02573280595242977, acc: 0.9922, auc: 1.0, precision: 0.9863, recall: 1.0\n",
      "2019-01-14T17:52:21.760873, step: 1010, loss: 0.004155140370130539, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:52:21.986584, step: 1011, loss: 0.07755338400602341, acc: 0.9844, auc: 0.9963, precision: 0.9831, recall: 0.9831\n",
      "2019-01-14T17:52:22.206750, step: 1012, loss: 0.004725989885628223, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:52:22.435208, step: 1013, loss: 0.06905509531497955, acc: 0.9688, auc: 0.999, precision: 1.0, recall: 0.9429\n",
      "2019-01-14T17:52:22.666094, step: 1014, loss: 0.0215398408472538, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9853\n",
      "2019-01-14T17:52:22.897355, step: 1015, loss: 0.01945362240076065, acc: 0.9922, auc: 0.9998, precision: 1.0, recall: 0.9836\n",
      "2019-01-14T17:52:23.115413, step: 1016, loss: 0.025392869487404823, acc: 0.9922, auc: 1.0, precision: 0.9859, recall: 1.0\n",
      "2019-01-14T17:52:23.331831, step: 1017, loss: 0.03370770812034607, acc: 0.9922, auc: 0.9997, precision: 0.9872, recall: 1.0\n",
      "2019-01-14T17:52:23.561059, step: 1018, loss: 0.006840006448328495, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:52:23.799700, step: 1019, loss: 0.006367592141032219, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:52:24.036174, step: 1020, loss: 0.016874190419912338, acc: 0.9922, auc: 1.0, precision: 0.9855, recall: 1.0\n",
      "2019-01-14T17:52:24.265179, step: 1021, loss: 0.0071730418130755424, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:52:24.490553, step: 1022, loss: 0.014074797742068768, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9851\n",
      "2019-01-14T17:52:24.715302, step: 1023, loss: 0.011283007450401783, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9846\n",
      "2019-01-14T17:52:24.935095, step: 1024, loss: 0.0083351731300354, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-14T17:52:25.149118, step: 1025, loss: 0.008733012713491917, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:52:25.385243, step: 1026, loss: 0.027062248438596725, acc: 0.9844, auc: 1.0, precision: 0.9706, recall: 1.0\n",
      "2019-01-14T17:52:25.616235, step: 1027, loss: 0.0046532172709703445, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:52:25.838807, step: 1028, loss: 0.03544287383556366, acc: 0.9844, auc: 0.9995, precision: 1.0, recall: 0.9688\n",
      "2019-01-14T17:52:26.065718, step: 1029, loss: 0.014310315251350403, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9706\n",
      "2019-01-14T17:52:26.293706, step: 1030, loss: 0.01604507490992546, acc: 0.9922, auc: 0.9998, precision: 1.0, recall: 0.9833\n",
      "2019-01-14T17:52:26.531587, step: 1031, loss: 0.008811568841338158, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9865\n",
      "2019-01-14T17:52:26.759148, step: 1032, loss: 0.03759196028113365, acc: 0.9844, auc: 0.9993, precision: 0.9853, recall: 0.9853\n",
      "2019-01-14T17:52:26.974940, step: 1033, loss: 0.0239715576171875, acc: 0.9922, auc: 1.0, precision: 0.9836, recall: 1.0\n",
      "2019-01-14T17:52:27.193556, step: 1034, loss: 0.01660795509815216, acc: 0.9922, auc: 0.9998, precision: 1.0, recall: 0.9848\n",
      "2019-01-14T17:52:27.411526, step: 1035, loss: 0.013888362795114517, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:52:27.637274, step: 1036, loss: 0.003257334465160966, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:52:27.868623, step: 1037, loss: 0.00510647427290678, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:52:28.086929, step: 1038, loss: 0.0016870589461177588, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:52:28.306190, step: 1039, loss: 0.053600460290908813, acc: 0.9766, auc: 0.9973, precision: 1.0, recall: 0.9552\n",
      "2019-01-14T17:52:28.525471, step: 1040, loss: 0.031317416578531265, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9649\n",
      "2019-01-14T17:52:28.759885, step: 1041, loss: 0.047957174479961395, acc: 0.9844, auc: 0.9995, precision: 1.0, recall: 0.9726\n",
      "2019-01-14T17:52:28.972954, step: 1042, loss: 0.003076449502259493, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:52:29.194020, step: 1043, loss: 0.004875555634498596, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:52:29.423548, step: 1044, loss: 0.007384146098047495, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:52:29.651847, step: 1045, loss: 0.027828194200992584, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:52:29.877746, step: 1046, loss: 0.027753477916121483, acc: 0.9922, auc: 0.9995, precision: 0.9836, recall: 1.0\n",
      "2019-01-14T17:52:30.098740, step: 1047, loss: 0.0018305627163499594, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:52:30.314961, step: 1048, loss: 0.0037740194238722324, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:52:30.540012, step: 1049, loss: 0.020958926528692245, acc: 0.9922, auc: 0.9995, precision: 1.0, recall: 0.9851\n",
      "2019-01-14T17:52:30.781190, step: 1050, loss: 0.004331617150455713, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:52:31.008018, step: 1051, loss: 0.009810857474803925, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9683\n",
      "2019-01-14T17:52:31.235225, step: 1052, loss: 0.006578605622053146, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9815\n",
      "2019-01-14T17:52:31.466999, step: 1053, loss: 0.02553962916135788, acc: 0.9844, auc: 0.9998, precision: 0.9861, recall: 0.9861\n",
      "2019-01-14T17:52:31.673251, step: 1054, loss: 0.022932425141334534, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9706\n",
      "2019-01-14T17:52:31.898315, step: 1055, loss: 0.022641632705926895, acc: 0.9844, auc: 0.9998, precision: 1.0, recall: 0.9661\n",
      "2019-01-14T17:52:32.128506, step: 1056, loss: 0.02231232076883316, acc: 0.9922, auc: 1.0, precision: 0.9848, recall: 1.0\n",
      "2019-01-14T17:52:32.350845, step: 1057, loss: 0.03128769248723984, acc: 0.9922, auc: 1.0, precision: 0.9831, recall: 1.0\n",
      "2019-01-14T17:52:32.580618, step: 1058, loss: 0.0014004084514454007, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:52:32.806493, step: 1059, loss: 0.007003392558544874, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:52:33.034506, step: 1060, loss: 0.009615598246455193, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:52:33.271919, step: 1061, loss: 0.015832001343369484, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.971\n",
      "2019-01-14T17:52:33.491045, step: 1062, loss: 0.011112215928733349, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9833\n",
      "2019-01-14T17:52:33.730942, step: 1063, loss: 0.01937868446111679, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9833\n",
      "2019-01-14T17:52:33.950928, step: 1064, loss: 0.01513579674065113, acc: 0.9922, auc: 0.9998, precision: 1.0, recall: 0.9839\n",
      "2019-01-14T17:52:34.174816, step: 1065, loss: 0.0006823285366408527, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:52:34.398639, step: 1066, loss: 0.01896105334162712, acc: 0.9922, auc: 1.0, precision: 0.9863, recall: 1.0\n",
      "2019-01-14T17:52:34.622527, step: 1067, loss: 0.013275695964694023, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:52:34.852652, step: 1068, loss: 0.0034959670156240463, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:52:35.077510, step: 1069, loss: 0.022259533405303955, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9818\n",
      "2019-01-14T17:52:35.287071, step: 1070, loss: 0.007553104776889086, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:52:35.495342, step: 1071, loss: 0.0021831572521477938, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:52:35.710492, step: 1072, loss: 0.01534426398575306, acc: 0.9922, auc: 0.9998, precision: 1.0, recall: 0.9853\n",
      "2019-01-14T17:52:35.924942, step: 1073, loss: 0.020393988117575645, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9851\n",
      "2019-01-14T17:52:36.141629, step: 1074, loss: 0.027606895193457603, acc: 0.9844, auc: 0.9998, precision: 0.9831, recall: 0.9831\n",
      "2019-01-14T17:52:36.353456, step: 1075, loss: 0.0036842762492597103, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:52:36.567303, step: 1076, loss: 0.014623111113905907, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:52:36.779989, step: 1077, loss: 0.0019677418749779463, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:52:36.996801, step: 1078, loss: 0.04489154741168022, acc: 0.9922, auc: 0.9997, precision: 0.9796, recall: 1.0\n",
      "2019-01-14T17:52:37.209776, step: 1079, loss: 0.005015294533222914, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:52:37.423210, step: 1080, loss: 0.031930722296237946, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9697\n",
      "2019-01-14T17:52:37.639881, step: 1081, loss: 0.046710655093193054, acc: 0.9844, auc: 0.9993, precision: 1.0, recall: 0.9672\n",
      "2019-01-14T17:52:37.858420, step: 1082, loss: 0.0037530488334596157, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:52:38.067246, step: 1083, loss: 0.017486456781625748, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9848\n",
      "2019-01-14T17:52:38.272966, step: 1084, loss: 0.005232465453445911, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:52:38.479620, step: 1085, loss: 0.004059930797666311, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:52:38.693494, step: 1086, loss: 0.03279653564095497, acc: 0.9922, auc: 0.9998, precision: 0.9821, recall: 1.0\n",
      "2019-01-14T17:52:38.914165, step: 1087, loss: 0.0017897062934935093, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:52:39.152019, step: 1088, loss: 0.03169297054409981, acc: 0.9844, auc: 0.9995, precision: 1.0, recall: 0.9667\n",
      "2019-01-14T17:52:39.368444, step: 1089, loss: 0.01732843555510044, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9848\n",
      "2019-01-14T17:52:39.603020, step: 1090, loss: 0.007673915941268206, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:52:39.832403, step: 1091, loss: 0.0027622038032859564, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:52:40.069143, step: 1092, loss: 0.011428018100559711, acc: 0.9922, auc: 1.0, precision: 0.9855, recall: 1.0\n",
      "start training model\n",
      "2019-01-14T17:52:40.296310, step: 1093, loss: 0.004569463897496462, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-14T17:52:40.509942, step: 1094, loss: 0.003191557480022311, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:52:40.728098, step: 1095, loss: 0.004878893960267305, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:52:40.973438, step: 1096, loss: 0.0013069197302684188, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:52:41.202724, step: 1097, loss: 0.0025355801917612553, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:52:41.429715, step: 1098, loss: 0.036744166165590286, acc: 0.9922, auc: 0.9998, precision: 0.9846, recall: 1.0\n",
      "2019-01-14T17:52:41.669302, step: 1099, loss: 0.0013865218497812748, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:52:41.883849, step: 1100, loss: 0.018916785717010498, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "\n",
      "Evaluation:\n",
      "2019-01-14T17:52:50.385112, step: 1100, loss: 0.8329841219461881, acc: 0.8511589743589746, auc: 0.9395435897435894, precision: 0.9077897435897433, recall: 0.7854205128205128\n",
      "2019-01-14T17:52:50.590889, step: 1101, loss: 0.002987203188240528, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:52:50.811209, step: 1102, loss: 0.0403175987303257, acc: 0.9922, auc: 0.9998, precision: 1.0, recall: 0.9833\n",
      "2019-01-14T17:52:51.027654, step: 1103, loss: 0.011617938056588173, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9848\n",
      "2019-01-14T17:52:51.238009, step: 1104, loss: 0.019016394391655922, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.963\n",
      "2019-01-14T17:52:51.450156, step: 1105, loss: 0.008619074709713459, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9839\n",
      "2019-01-14T17:52:51.675037, step: 1106, loss: 0.03389023244380951, acc: 0.9688, auc: 1.0, precision: 1.0, recall: 0.9394\n",
      "2019-01-14T17:52:51.897331, step: 1107, loss: 0.01958065666258335, acc: 0.9922, auc: 1.0, precision: 0.9857, recall: 1.0\n",
      "2019-01-14T17:52:52.112364, step: 1108, loss: 0.011850983835756779, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:52:52.337291, step: 1109, loss: 0.053390324115753174, acc: 0.9688, auc: 1.0, precision: 0.9444, recall: 1.0\n",
      "2019-01-14T17:52:52.551816, step: 1110, loss: 0.0330546610057354, acc: 0.9922, auc: 1.0, precision: 0.9836, recall: 1.0\n",
      "2019-01-14T17:52:52.766605, step: 1111, loss: 0.02500470168888569, acc: 0.9844, auc: 0.9998, precision: 0.9833, recall: 0.9833\n",
      "2019-01-14T17:52:52.992474, step: 1112, loss: 0.007531789597123861, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9828\n",
      "2019-01-14T17:52:53.218549, step: 1113, loss: 0.04242159426212311, acc: 0.9766, auc: 1.0, precision: 1.0, recall: 0.9434\n",
      "2019-01-14T17:52:53.440311, step: 1114, loss: 0.0633845180273056, acc: 0.9766, auc: 0.9993, precision: 1.0, recall: 0.9508\n",
      "2019-01-14T17:52:53.649841, step: 1115, loss: 0.0067207771353423595, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9867\n",
      "2019-01-14T17:52:53.876466, step: 1116, loss: 0.0013636562507599592, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:52:54.112699, step: 1117, loss: 0.0029885428957641125, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:52:54.358370, step: 1118, loss: 0.008775823749601841, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:52:54.572662, step: 1119, loss: 0.0033896814566105604, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:52:54.783230, step: 1120, loss: 0.0007276212563738227, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:52:54.999298, step: 1121, loss: 0.0020007069688290358, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:52:55.213479, step: 1122, loss: 0.0022464734502136707, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:52:55.437882, step: 1123, loss: 0.003055116394534707, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:52:55.652213, step: 1124, loss: 0.0009590727859176695, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:52:55.864823, step: 1125, loss: 0.00032439763890579343, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:52:56.093193, step: 1126, loss: 0.0025750931818038225, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:52:56.324383, step: 1127, loss: 0.0004275953979231417, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:52:56.551466, step: 1128, loss: 0.008921585977077484, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:52:56.780248, step: 1129, loss: 0.01429008785635233, acc: 0.9922, auc: 1.0, precision: 0.9831, recall: 1.0\n",
      "2019-01-14T17:52:57.012193, step: 1130, loss: 0.0037007424980401993, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:52:57.231588, step: 1131, loss: 0.0064041935838758945, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:52:57.457806, step: 1132, loss: 0.017911501228809357, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9688\n",
      "2019-01-14T17:52:57.681877, step: 1133, loss: 0.002241864800453186, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:52:57.904159, step: 1134, loss: 0.00470236549153924, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:52:58.119433, step: 1135, loss: 0.007256225682795048, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:52:58.351012, step: 1136, loss: 0.0011578965932130814, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:52:58.565903, step: 1137, loss: 0.02981562167406082, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9667\n",
      "2019-01-14T17:52:58.791264, step: 1138, loss: 0.0117820268496871, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9821\n",
      "2019-01-14T17:52:59.010799, step: 1139, loss: 0.015176192857325077, acc: 0.9922, auc: 1.0, precision: 0.9851, recall: 1.0\n",
      "2019-01-14T17:52:59.202769, step: 1140, loss: 0.006037916988134384, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:52:59.410164, step: 1141, loss: 0.0038738136645406485, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:52:59.639067, step: 1142, loss: 0.005367593374103308, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:52:59.854486, step: 1143, loss: 0.0014845725381746888, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:00.076888, step: 1144, loss: 0.035054922103881836, acc: 0.9922, auc: 0.9998, precision: 0.9859, recall: 1.0\n",
      "2019-01-14T17:53:00.299833, step: 1145, loss: 0.0009036817355081439, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:00.519633, step: 1146, loss: 0.007621577940881252, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:00.741960, step: 1147, loss: 0.0014154600212350488, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:00.957795, step: 1148, loss: 0.004307689610868692, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:01.183875, step: 1149, loss: 0.009129321202635765, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:01.406543, step: 1150, loss: 0.07687858492136002, acc: 0.9688, auc: 1.0, precision: 1.0, recall: 0.9412\n",
      "2019-01-14T17:53:01.640906, step: 1151, loss: 0.003554908325895667, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:01.861338, step: 1152, loss: 0.0006660863291472197, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:02.102614, step: 1153, loss: 0.004042797721922398, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:02.321058, step: 1154, loss: 0.002288089832291007, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:02.525501, step: 1155, loss: 0.03599671274423599, acc: 0.9922, auc: 1.0, precision: 0.9836, recall: 1.0\n",
      "2019-01-14T17:53:02.730615, step: 1156, loss: 0.0039842696860432625, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:02.944964, step: 1157, loss: 0.007811082527041435, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:03.164952, step: 1158, loss: 0.009030485525727272, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:03.378804, step: 1159, loss: 0.018073132261633873, acc: 0.9844, auc: 0.9998, precision: 0.9857, recall: 0.9857\n",
      "2019-01-14T17:53:03.593004, step: 1160, loss: 0.020687013864517212, acc: 0.9766, auc: 1.0, precision: 1.0, recall: 0.9516\n",
      "2019-01-14T17:53:03.835023, step: 1161, loss: 0.003586858045309782, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-14T17:53:04.064896, step: 1162, loss: 0.010928714647889137, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9863\n",
      "2019-01-14T17:53:04.283584, step: 1163, loss: 0.009460726752877235, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9844\n",
      "2019-01-14T17:53:04.502288, step: 1164, loss: 0.007874562405049801, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9844\n",
      "2019-01-14T17:53:04.729052, step: 1165, loss: 0.0027987794019281864, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:04.945679, step: 1166, loss: 0.0034523876383900642, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:05.158519, step: 1167, loss: 0.0008425549603998661, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:05.381525, step: 1168, loss: 0.007672950159758329, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:05.611310, step: 1169, loss: 0.02851824089884758, acc: 0.9922, auc: 1.0, precision: 0.9863, recall: 1.0\n",
      "2019-01-14T17:53:05.819291, step: 1170, loss: 0.03050411120057106, acc: 0.9922, auc: 0.9997, precision: 0.9872, recall: 1.0\n",
      "2019-01-14T17:53:06.029591, step: 1171, loss: 0.01962749846279621, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9828\n",
      "2019-01-14T17:53:06.242872, step: 1172, loss: 0.005189173389226198, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:06.468002, step: 1173, loss: 0.020950809121131897, acc: 0.9922, auc: 0.9998, precision: 1.0, recall: 0.9833\n",
      "2019-01-14T17:53:06.686671, step: 1174, loss: 0.013627101667225361, acc: 0.9922, auc: 1.0, precision: 0.9853, recall: 1.0\n",
      "2019-01-14T17:53:06.902321, step: 1175, loss: 0.020616281777620316, acc: 0.9922, auc: 1.0, precision: 0.9836, recall: 1.0\n",
      "2019-01-14T17:53:07.118252, step: 1176, loss: 0.024949731305241585, acc: 0.9844, auc: 0.9998, precision: 1.0, recall: 0.9688\n",
      "2019-01-14T17:53:07.335737, step: 1177, loss: 0.011638601310551167, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9855\n",
      "2019-01-14T17:53:07.570839, step: 1178, loss: 0.0839955061674118, acc: 0.9766, auc: 0.9998, precision: 1.0, recall: 0.9508\n",
      "2019-01-14T17:53:07.780339, step: 1179, loss: 0.0006368192844092846, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:07.991578, step: 1180, loss: 0.029916027560830116, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9722\n",
      "2019-01-14T17:53:08.218297, step: 1181, loss: 0.006640812382102013, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:08.435350, step: 1182, loss: 0.015349128283560276, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:08.649871, step: 1183, loss: 0.07432249933481216, acc: 0.9766, auc: 1.0, precision: 0.9545, recall: 1.0\n",
      "2019-01-14T17:53:08.867492, step: 1184, loss: 0.014089643023908138, acc: 0.9922, auc: 1.0, precision: 0.9855, recall: 1.0\n",
      "2019-01-14T17:53:09.086609, step: 1185, loss: 0.03730297088623047, acc: 0.9844, auc: 1.0, precision: 0.9661, recall: 1.0\n",
      "2019-01-14T17:53:09.321365, step: 1186, loss: 0.011326340958476067, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9836\n",
      "2019-01-14T17:53:09.557588, step: 1187, loss: 0.012574625201523304, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9855\n",
      "2019-01-14T17:53:09.769150, step: 1188, loss: 0.022352952510118484, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9643\n",
      "2019-01-14T17:53:09.973976, step: 1189, loss: 0.11643460392951965, acc: 0.9609, auc: 1.0, precision: 1.0, recall: 0.9231\n",
      "2019-01-14T17:53:10.187745, step: 1190, loss: 0.016732459887862206, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9623\n",
      "2019-01-14T17:53:10.403506, step: 1191, loss: 0.004257057327777147, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:10.625031, step: 1192, loss: 0.01032089814543724, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:10.849782, step: 1193, loss: 0.009386029094457626, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:11.078131, step: 1194, loss: 0.06390374898910522, acc: 0.9766, auc: 1.0, precision: 0.9595, recall: 1.0\n",
      "2019-01-14T17:53:11.301748, step: 1195, loss: 0.056556202471256256, acc: 0.9688, auc: 1.0, precision: 0.9512, recall: 1.0\n",
      "2019-01-14T17:53:11.511854, step: 1196, loss: 0.03399772569537163, acc: 0.9922, auc: 1.0, precision: 0.9804, recall: 1.0\n",
      "2019-01-14T17:53:11.746598, step: 1197, loss: 0.004871725104749203, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:11.974273, step: 1198, loss: 0.00886042695492506, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:12.193673, step: 1199, loss: 0.07826980948448181, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9649\n",
      "2019-01-14T17:53:12.422401, step: 1200, loss: 0.06950343400239944, acc: 0.9609, auc: 1.0, precision: 1.0, recall: 0.9057\n",
      "\n",
      "Evaluation:\n",
      "2019-01-14T17:53:21.110003, step: 1200, loss: 1.002984772890042, acc: 0.836546153846154, auc: 0.9390589743589743, precision: 0.9143641025641028, recall: 0.745223076923077\n",
      "2019-01-14T17:53:21.328178, step: 1201, loss: 0.05841122567653656, acc: 0.9609, auc: 1.0, precision: 1.0, recall: 0.9315\n",
      "2019-01-14T17:53:21.561321, step: 1202, loss: 0.00217604311183095, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:21.778844, step: 1203, loss: 0.007590634748339653, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:21.995694, step: 1204, loss: 0.03541172295808792, acc: 0.9844, auc: 1.0, precision: 0.9706, recall: 1.0\n",
      "2019-01-14T17:53:22.223112, step: 1205, loss: 0.04059135541319847, acc: 0.9844, auc: 1.0, precision: 0.971, recall: 1.0\n",
      "2019-01-14T17:53:22.443028, step: 1206, loss: 0.038654886186122894, acc: 0.9922, auc: 1.0, precision: 0.9853, recall: 1.0\n",
      "2019-01-14T17:53:22.676939, step: 1207, loss: 0.014959042891860008, acc: 0.9922, auc: 1.0, precision: 0.9857, recall: 1.0\n",
      "2019-01-14T17:53:22.886585, step: 1208, loss: 0.03897885978221893, acc: 0.9922, auc: 0.9995, precision: 1.0, recall: 0.9851\n",
      "2019-01-14T17:53:23.122908, step: 1209, loss: 0.0019233933417126536, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:23.338228, step: 1210, loss: 0.04846881330013275, acc: 0.9766, auc: 0.9993, precision: 0.9844, recall: 0.9692\n",
      "2019-01-14T17:53:23.553258, step: 1211, loss: 0.009228779934346676, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:23.771093, step: 1212, loss: 0.017858026549220085, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9846\n",
      "2019-01-14T17:53:23.999658, step: 1213, loss: 0.02501126192510128, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9844\n",
      "2019-01-14T17:53:24.217316, step: 1214, loss: 0.004644992295652628, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:24.444639, step: 1215, loss: 0.002136651426553726, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:24.670466, step: 1216, loss: 0.010850300081074238, acc: 0.9922, auc: 1.0, precision: 0.9863, recall: 1.0\n",
      "2019-01-14T17:53:24.894622, step: 1217, loss: 0.002512529492378235, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:25.111711, step: 1218, loss: 0.008304386399686337, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:25.330003, step: 1219, loss: 0.0010150177404284477, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:25.548259, step: 1220, loss: 0.002418800024315715, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:25.768147, step: 1221, loss: 0.008970271795988083, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9833\n",
      "2019-01-14T17:53:26.001027, step: 1222, loss: 0.00150470738299191, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:26.231502, step: 1223, loss: 0.0021043706219643354, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:26.452075, step: 1224, loss: 0.006639854516834021, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:26.679072, step: 1225, loss: 0.03606651723384857, acc: 0.9844, auc: 0.9993, precision: 1.0, recall: 0.971\n",
      "2019-01-14T17:53:26.894944, step: 1226, loss: 0.014756234362721443, acc: 0.9922, auc: 0.9998, precision: 1.0, recall: 0.9825\n",
      "2019-01-14T17:53:27.126290, step: 1227, loss: 0.031269703060388565, acc: 0.9922, auc: 0.9998, precision: 0.9841, recall: 1.0\n",
      "2019-01-14T17:53:27.359179, step: 1228, loss: 0.003407294861972332, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-14T17:53:27.583158, step: 1229, loss: 0.006735920440405607, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:27.805415, step: 1230, loss: 0.02247035689651966, acc: 0.9922, auc: 1.0, precision: 0.9855, recall: 1.0\n",
      "2019-01-14T17:53:28.020289, step: 1231, loss: 0.006388053297996521, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9853\n",
      "2019-01-14T17:53:28.234148, step: 1232, loss: 0.040617913007736206, acc: 0.9922, auc: 0.9998, precision: 0.9844, recall: 1.0\n",
      "2019-01-14T17:53:28.453678, step: 1233, loss: 0.008222617208957672, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9859\n",
      "2019-01-14T17:53:28.669432, step: 1234, loss: 0.013482476584613323, acc: 0.9922, auc: 0.9998, precision: 1.0, recall: 0.9821\n",
      "2019-01-14T17:53:28.883389, step: 1235, loss: 0.006248192861676216, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:29.111260, step: 1236, loss: 0.014908703044056892, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9846\n",
      "2019-01-14T17:53:29.331358, step: 1237, loss: 0.009500795044004917, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9848\n",
      "2019-01-14T17:53:29.552649, step: 1238, loss: 0.002093355171382427, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:29.765703, step: 1239, loss: 0.003382911439985037, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:29.991191, step: 1240, loss: 0.007817322388291359, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:30.222492, step: 1241, loss: 0.002266824245452881, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:30.450871, step: 1242, loss: 0.000565664900932461, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:30.681339, step: 1243, loss: 0.012069794349372387, acc: 0.9922, auc: 1.0, precision: 0.9825, recall: 1.0\n",
      "2019-01-14T17:53:30.901426, step: 1244, loss: 0.00691292155534029, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:31.112075, step: 1245, loss: 0.003996641840785742, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:31.345037, step: 1246, loss: 0.0024194049183279276, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:31.575722, step: 1247, loss: 0.036932844668626785, acc: 0.9844, auc: 0.9998, precision: 0.9839, recall: 0.9839\n",
      "2019-01-14T17:53:31.803824, step: 1248, loss: 0.056681640446186066, acc: 0.9766, auc: 0.9988, precision: 1.0, recall: 0.9508\n",
      "start training model\n",
      "2019-01-14T17:53:32.047999, step: 1249, loss: 0.0011967377504333854, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:32.261837, step: 1250, loss: 0.0009378224494867027, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:32.485674, step: 1251, loss: 0.01013350859284401, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9846\n",
      "2019-01-14T17:53:32.714871, step: 1252, loss: 0.02226804941892624, acc: 0.9922, auc: 1.0, precision: 0.9828, recall: 1.0\n",
      "2019-01-14T17:53:32.951081, step: 1253, loss: 0.0027842323761433363, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:33.177919, step: 1254, loss: 0.03838252276182175, acc: 0.9844, auc: 0.9998, precision: 0.9831, recall: 0.9831\n",
      "2019-01-14T17:53:33.396392, step: 1255, loss: 0.0019637816585600376, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:33.609361, step: 1256, loss: 0.001389650977216661, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:33.825908, step: 1257, loss: 0.005839621182531118, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:34.056389, step: 1258, loss: 0.0002615073462948203, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:34.285890, step: 1259, loss: 0.010481167584657669, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9872\n",
      "2019-01-14T17:53:34.510922, step: 1260, loss: 0.0003917196299880743, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:34.737137, step: 1261, loss: 0.001605242840014398, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:34.958187, step: 1262, loss: 0.007586559280753136, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:35.168650, step: 1263, loss: 0.009108125232160091, acc: 0.9922, auc: 1.0, precision: 0.9846, recall: 1.0\n",
      "2019-01-14T17:53:35.385293, step: 1264, loss: 0.0005201849853619933, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:35.595922, step: 1265, loss: 0.002940742764621973, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:35.821161, step: 1266, loss: 0.003060962539166212, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:36.042557, step: 1267, loss: 0.0014757656026631594, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:36.267817, step: 1268, loss: 0.005511191673576832, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:36.487592, step: 1269, loss: 0.13619710505008698, acc: 0.9688, auc: 1.0, precision: 1.0, recall: 0.9394\n",
      "2019-01-14T17:53:36.699988, step: 1270, loss: 0.0047540501691401005, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:36.913736, step: 1271, loss: 0.01624852418899536, acc: 0.9922, auc: 1.0, precision: 0.9844, recall: 1.0\n",
      "2019-01-14T17:53:37.125713, step: 1272, loss: 0.02536103129386902, acc: 0.9922, auc: 1.0, precision: 0.9865, recall: 1.0\n",
      "2019-01-14T17:53:37.345992, step: 1273, loss: 0.024035712704062462, acc: 0.9922, auc: 1.0, precision: 0.9855, recall: 1.0\n",
      "2019-01-14T17:53:37.557611, step: 1274, loss: 0.042285338044166565, acc: 0.9766, auc: 0.9995, precision: 0.9688, recall: 0.9841\n",
      "2019-01-14T17:53:37.797994, step: 1275, loss: 0.00909924041479826, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:38.030516, step: 1276, loss: 0.018163802102208138, acc: 0.9922, auc: 1.0, precision: 0.9811, recall: 1.0\n",
      "2019-01-14T17:53:38.256960, step: 1277, loss: 0.003209311980754137, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:38.474286, step: 1278, loss: 0.004040749277919531, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:38.708598, step: 1279, loss: 0.002572176279500127, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:38.926732, step: 1280, loss: 0.03593923896551132, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.971\n",
      "2019-01-14T17:53:39.155011, step: 1281, loss: 0.002533088903874159, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:39.382548, step: 1282, loss: 0.0034748748876154423, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:39.610050, step: 1283, loss: 0.026010848581790924, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9848\n",
      "2019-01-14T17:53:39.833979, step: 1284, loss: 0.004318821709603071, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:40.057472, step: 1285, loss: 0.002204170450568199, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:40.274313, step: 1286, loss: 0.005200145300477743, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:40.497747, step: 1287, loss: 0.0017259418964385986, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:40.713121, step: 1288, loss: 0.015311925671994686, acc: 0.9922, auc: 1.0, precision: 0.9844, recall: 1.0\n",
      "2019-01-14T17:53:40.940587, step: 1289, loss: 0.0030501161236315966, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:41.160577, step: 1290, loss: 0.001142403925769031, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:41.374196, step: 1291, loss: 0.04908132180571556, acc: 0.9766, auc: 0.9995, precision: 0.971, recall: 0.9853\n",
      "2019-01-14T17:53:41.604428, step: 1292, loss: 0.0216878242790699, acc: 0.9922, auc: 1.0, precision: 0.9821, recall: 1.0\n",
      "2019-01-14T17:53:41.839589, step: 1293, loss: 0.007762694265693426, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9846\n",
      "2019-01-14T17:53:42.069689, step: 1294, loss: 0.0034838421270251274, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:42.292882, step: 1295, loss: 0.006281143054366112, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:42.519620, step: 1296, loss: 0.03131360933184624, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9863\n",
      "2019-01-14T17:53:42.745228, step: 1297, loss: 0.007750635035336018, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-14T17:53:42.966080, step: 1298, loss: 0.008322563953697681, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9828\n",
      "2019-01-14T17:53:43.179891, step: 1299, loss: 0.0007304264581762254, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:43.405649, step: 1300, loss: 0.02242034487426281, acc: 0.9922, auc: 0.9995, precision: 0.9839, recall: 1.0\n",
      "\n",
      "Evaluation:\n",
      "2019-01-14T17:53:51.850018, step: 1300, loss: 0.8448327519954779, acc: 0.8603846153846155, auc: 0.937220512820513, precision: 0.8759051282051282, recall: 0.8436948717948716\n",
      "2019-01-14T17:53:52.062701, step: 1301, loss: 0.0004344551998656243, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:52.286373, step: 1302, loss: 0.0006297562504187226, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:52.510232, step: 1303, loss: 0.004761956166476011, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:52.736799, step: 1304, loss: 0.018704865127801895, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9844\n",
      "2019-01-14T17:53:52.963803, step: 1305, loss: 0.04537799954414368, acc: 0.9844, auc: 1.0, precision: 0.9714, recall: 1.0\n",
      "2019-01-14T17:53:53.194436, step: 1306, loss: 0.02100827917456627, acc: 0.9922, auc: 1.0, precision: 0.9841, recall: 1.0\n",
      "2019-01-14T17:53:53.415657, step: 1307, loss: 0.0016899814363569021, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:53.643361, step: 1308, loss: 0.003132377751171589, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:53.861064, step: 1309, loss: 0.0017068579327315092, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:54.088656, step: 1310, loss: 0.001977198990061879, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:54.301827, step: 1311, loss: 0.039306316524744034, acc: 0.9844, auc: 0.9995, precision: 1.0, recall: 0.9655\n",
      "2019-01-14T17:53:54.514770, step: 1312, loss: 0.00718088261783123, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9851\n",
      "2019-01-14T17:53:54.732531, step: 1313, loss: 0.0017996827373281121, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:54.942605, step: 1314, loss: 0.0003259574295952916, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:55.155189, step: 1315, loss: 0.004079318605363369, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:55.365082, step: 1316, loss: 0.0007514703902415931, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:55.577309, step: 1317, loss: 0.009254706092178822, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:55.789672, step: 1318, loss: 0.004056094214320183, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:56.000309, step: 1319, loss: 0.002227878663688898, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:56.210030, step: 1320, loss: 0.017978906631469727, acc: 0.9922, auc: 1.0, precision: 0.9857, recall: 1.0\n",
      "2019-01-14T17:53:56.420154, step: 1321, loss: 0.0011490066535770893, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:56.628492, step: 1322, loss: 0.006391250994056463, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:56.861667, step: 1323, loss: 0.0037123169749975204, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:57.091539, step: 1324, loss: 0.0053077517077326775, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:57.307656, step: 1325, loss: 0.009638766758143902, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9855\n",
      "2019-01-14T17:53:57.523556, step: 1326, loss: 0.00403807545080781, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:57.734415, step: 1327, loss: 0.01760282926261425, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9828\n",
      "2019-01-14T17:53:57.947491, step: 1328, loss: 0.015356822870671749, acc: 0.9922, auc: 1.0, precision: 0.9833, recall: 1.0\n",
      "2019-01-14T17:53:58.165671, step: 1329, loss: 0.001671766396611929, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:58.373457, step: 1330, loss: 0.00260470574721694, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:58.587350, step: 1331, loss: 0.0020161375869065523, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:58.799979, step: 1332, loss: 0.0008150273934006691, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:59.041510, step: 1333, loss: 0.0031214249320328236, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:59.263061, step: 1334, loss: 0.0006355614168569446, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:59.489921, step: 1335, loss: 0.03067464381456375, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9851\n",
      "2019-01-14T17:53:59.721011, step: 1336, loss: 0.0021289971191436052, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:53:59.943859, step: 1337, loss: 0.0037319776602089405, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:00.177678, step: 1338, loss: 0.0021830396726727486, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:00.400023, step: 1339, loss: 0.013052690774202347, acc: 0.9922, auc: 1.0, precision: 0.9821, recall: 1.0\n",
      "2019-01-14T17:54:00.626333, step: 1340, loss: 0.03060082346200943, acc: 0.9922, auc: 0.9997, precision: 0.9867, recall: 1.0\n",
      "2019-01-14T17:54:00.851883, step: 1341, loss: 0.0035161911509931087, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:01.075568, step: 1342, loss: 0.00022817299759481102, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:01.305077, step: 1343, loss: 0.014828257262706757, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9692\n",
      "2019-01-14T17:54:01.529019, step: 1344, loss: 0.006164597813040018, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9841\n",
      "2019-01-14T17:54:01.755035, step: 1345, loss: 0.00491798622533679, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9839\n",
      "2019-01-14T17:54:01.972228, step: 1346, loss: 0.008748183958232403, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9848\n",
      "2019-01-14T17:54:02.193403, step: 1347, loss: 0.01077042706310749, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9853\n",
      "2019-01-14T17:54:02.416760, step: 1348, loss: 0.009575644508004189, acc: 0.9922, auc: 1.0, precision: 0.9846, recall: 1.0\n",
      "2019-01-14T17:54:02.638455, step: 1349, loss: 0.0017918251687660813, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:02.867286, step: 1350, loss: 0.01331582386046648, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:03.092934, step: 1351, loss: 0.021228846162557602, acc: 0.9922, auc: 1.0, precision: 0.9863, recall: 1.0\n",
      "2019-01-14T17:54:03.305816, step: 1352, loss: 0.008353972807526588, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:03.520211, step: 1353, loss: 0.011675457470119, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:03.755378, step: 1354, loss: 0.0005478604580275714, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:03.978577, step: 1355, loss: 0.009182626381516457, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9865\n",
      "2019-01-14T17:54:04.229947, step: 1356, loss: 0.0031581763178110123, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:04.461290, step: 1357, loss: 0.016513394191861153, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9836\n",
      "2019-01-14T17:54:04.689274, step: 1358, loss: 0.008168883621692657, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9833\n",
      "2019-01-14T17:54:04.910360, step: 1359, loss: 0.05783163756132126, acc: 0.9609, auc: 0.9998, precision: 1.0, recall: 0.9231\n",
      "2019-01-14T17:54:05.139065, step: 1360, loss: 0.0007708097109571099, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:05.366494, step: 1361, loss: 0.0028263512067496777, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:05.603823, step: 1362, loss: 0.05036146566271782, acc: 0.9844, auc: 0.9998, precision: 0.9701, recall: 1.0\n",
      "2019-01-14T17:54:05.832390, step: 1363, loss: 0.004101534374058247, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:06.051716, step: 1364, loss: 0.014210340566933155, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:06.271271, step: 1365, loss: 0.013456557877361774, acc: 0.9922, auc: 1.0, precision: 0.9851, recall: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-14T17:54:06.493783, step: 1366, loss: 0.005980116315186024, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:06.713703, step: 1367, loss: 0.018088338896632195, acc: 0.9922, auc: 1.0, precision: 0.9859, recall: 1.0\n",
      "2019-01-14T17:54:06.932027, step: 1368, loss: 0.031985241919755936, acc: 0.9844, auc: 0.9998, precision: 0.9833, recall: 0.9833\n",
      "2019-01-14T17:54:07.163188, step: 1369, loss: 0.011684699915349483, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9853\n",
      "2019-01-14T17:54:07.370305, step: 1370, loss: 0.021527869626879692, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9672\n",
      "2019-01-14T17:54:07.584448, step: 1371, loss: 0.0016484045190736651, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:07.792972, step: 1372, loss: 0.05587197095155716, acc: 0.9766, auc: 1.0, precision: 1.0, recall: 0.9545\n",
      "2019-01-14T17:54:08.031872, step: 1373, loss: 0.00252883811481297, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:08.258198, step: 1374, loss: 0.0022375755943357944, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:08.482542, step: 1375, loss: 0.013122156262397766, acc: 0.9922, auc: 1.0, precision: 0.9853, recall: 1.0\n",
      "2019-01-14T17:54:08.704824, step: 1376, loss: 0.007356657646596432, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:08.936804, step: 1377, loss: 0.005242923274636269, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:09.157828, step: 1378, loss: 0.000784990843385458, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:09.370325, step: 1379, loss: 0.022534098476171494, acc: 0.9922, auc: 1.0, precision: 0.9825, recall: 1.0\n",
      "2019-01-14T17:54:09.602685, step: 1380, loss: 0.0168265700340271, acc: 0.9922, auc: 1.0, precision: 0.9857, recall: 1.0\n",
      "2019-01-14T17:54:09.821469, step: 1381, loss: 0.001762258936651051, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:10.049264, step: 1382, loss: 0.0008902744157239795, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:10.269324, step: 1383, loss: 0.003039686009287834, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:10.498282, step: 1384, loss: 0.005276674870401621, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:10.716523, step: 1385, loss: 0.024882985278964043, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9706\n",
      "2019-01-14T17:54:10.927765, step: 1386, loss: 0.007311838213354349, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:11.147977, step: 1387, loss: 0.02607433870434761, acc: 0.9766, auc: 1.0, precision: 1.0, recall: 0.9434\n",
      "2019-01-14T17:54:11.385074, step: 1388, loss: 0.014121685177087784, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9831\n",
      "2019-01-14T17:54:11.614693, step: 1389, loss: 0.019394000992178917, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9836\n",
      "2019-01-14T17:54:11.833141, step: 1390, loss: 0.0007239790284074843, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:12.038007, step: 1391, loss: 0.032337915152311325, acc: 0.9922, auc: 1.0, precision: 0.9857, recall: 1.0\n",
      "2019-01-14T17:54:12.255199, step: 1392, loss: 0.029679790139198303, acc: 0.9922, auc: 1.0, precision: 0.9818, recall: 1.0\n",
      "2019-01-14T17:54:12.463506, step: 1393, loss: 0.022281646728515625, acc: 0.9922, auc: 0.9998, precision: 0.9846, recall: 1.0\n",
      "2019-01-14T17:54:12.689751, step: 1394, loss: 0.02857394516468048, acc: 0.9844, auc: 0.9998, precision: 0.9841, recall: 0.9841\n",
      "2019-01-14T17:54:12.902936, step: 1395, loss: 0.004544465336948633, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:13.116832, step: 1396, loss: 0.005716016981750727, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:13.327926, step: 1397, loss: 0.0015098744770511985, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:13.541226, step: 1398, loss: 0.000575262529309839, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:13.748375, step: 1399, loss: 0.04091212898492813, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9643\n",
      "2019-01-14T17:54:13.960667, step: 1400, loss: 0.018110236153006554, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9828\n",
      "\n",
      "Evaluation:\n",
      "2019-01-14T17:54:22.395768, step: 1400, loss: 0.9308623870213827, acc: 0.8565692307692307, auc: 0.9385999999999999, precision: 0.891782051282051, recall: 0.8133051282051282\n",
      "2019-01-14T17:54:22.609709, step: 1401, loss: 0.0024309689179062843, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:22.835605, step: 1402, loss: 0.004050613846629858, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:23.059298, step: 1403, loss: 0.003981826361268759, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:23.280721, step: 1404, loss: 0.0020638734567910433, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "start training model\n",
      "2019-01-14T17:54:23.509120, step: 1405, loss: 0.004538859240710735, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:23.733822, step: 1406, loss: 0.0013235184596851468, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:23.969395, step: 1407, loss: 0.008050753735005856, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:24.191859, step: 1408, loss: 0.0001431543641956523, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:24.414688, step: 1409, loss: 0.0022310034837573767, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:24.638873, step: 1410, loss: 0.0006087608635425568, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:24.854771, step: 1411, loss: 0.06549900770187378, acc: 0.9688, auc: 1.0, precision: 0.9429, recall: 1.0\n",
      "2019-01-14T17:54:25.077070, step: 1412, loss: 0.0010270936181768775, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:25.294054, step: 1413, loss: 0.0011385035468265414, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:25.518203, step: 1414, loss: 0.0026366389356553555, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:25.747967, step: 1415, loss: 0.006065191701054573, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:25.983268, step: 1416, loss: 0.005524293519556522, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9815\n",
      "2019-01-14T17:54:26.210533, step: 1417, loss: 0.008807819336652756, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9831\n",
      "2019-01-14T17:54:26.450611, step: 1418, loss: 0.02002946101129055, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9683\n",
      "2019-01-14T17:54:26.669215, step: 1419, loss: 0.02534768544137478, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9844\n",
      "2019-01-14T17:54:26.892753, step: 1420, loss: 0.0008394979522563517, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:27.114839, step: 1421, loss: 0.017300374805927277, acc: 0.9922, auc: 0.9998, precision: 0.9855, recall: 1.0\n",
      "2019-01-14T17:54:27.357268, step: 1422, loss: 0.005233921576291323, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9857\n",
      "2019-01-14T17:54:27.569978, step: 1423, loss: 0.029335379600524902, acc: 0.9922, auc: 1.0, precision: 0.9859, recall: 1.0\n",
      "2019-01-14T17:54:27.791713, step: 1424, loss: 0.0004590584721881896, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:28.012212, step: 1425, loss: 0.0009140215115621686, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:28.224710, step: 1426, loss: 0.0003332622582092881, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:28.437785, step: 1427, loss: 0.004006759263575077, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:28.657115, step: 1428, loss: 0.0009560651378706098, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:28.898134, step: 1429, loss: 0.0003088823868893087, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:29.122946, step: 1430, loss: 0.0036808806471526623, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:29.347165, step: 1431, loss: 0.0011844844557344913, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:29.562285, step: 1432, loss: 0.005796571262180805, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:29.776312, step: 1433, loss: 0.005159248132258654, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-14T17:54:29.997049, step: 1434, loss: 0.00553515600040555, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9821\n",
      "2019-01-14T17:54:30.225951, step: 1435, loss: 0.000726417638361454, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:30.461458, step: 1436, loss: 0.004697820171713829, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:30.700395, step: 1437, loss: 0.00018625099619384855, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:30.937163, step: 1438, loss: 0.0028216582722961903, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:31.171882, step: 1439, loss: 0.013424006290733814, acc: 0.9922, auc: 1.0, precision: 0.9857, recall: 1.0\n",
      "2019-01-14T17:54:31.410873, step: 1440, loss: 0.0007932220469228923, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:31.638381, step: 1441, loss: 0.006314391270279884, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9821\n",
      "2019-01-14T17:54:31.865662, step: 1442, loss: 0.006439422257244587, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9818\n",
      "2019-01-14T17:54:32.107592, step: 1443, loss: 0.006418207194656134, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:32.335740, step: 1444, loss: 0.0008332122815772891, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:32.564848, step: 1445, loss: 0.02017834782600403, acc: 0.9922, auc: 1.0, precision: 0.9839, recall: 1.0\n",
      "2019-01-14T17:54:32.788780, step: 1446, loss: 0.00021032433141954243, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:32.999586, step: 1447, loss: 0.0006226561381481588, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:33.210539, step: 1448, loss: 0.001053187414072454, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:33.419932, step: 1449, loss: 0.014994479715824127, acc: 0.9922, auc: 1.0, precision: 0.9833, recall: 1.0\n",
      "2019-01-14T17:54:33.637138, step: 1450, loss: 0.0006876331171952188, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:33.860504, step: 1451, loss: 0.025608476251363754, acc: 0.9922, auc: 1.0, precision: 0.9839, recall: 1.0\n",
      "2019-01-14T17:54:34.107086, step: 1452, loss: 0.006069217808544636, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9851\n",
      "2019-01-14T17:54:34.327663, step: 1453, loss: 0.0004923678352497518, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:34.559799, step: 1454, loss: 0.004969798028469086, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:34.798848, step: 1455, loss: 0.004765816032886505, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:35.031801, step: 1456, loss: 0.015327392145991325, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9853\n",
      "2019-01-14T17:54:35.264331, step: 1457, loss: 0.017422791570425034, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9855\n",
      "2019-01-14T17:54:35.493971, step: 1458, loss: 0.008239628747105598, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9833\n",
      "2019-01-14T17:54:35.736615, step: 1459, loss: 0.0009430474601686001, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:35.968438, step: 1460, loss: 0.0018512627575546503, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:36.192750, step: 1461, loss: 0.0003856261901091784, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:36.417472, step: 1462, loss: 0.02195652388036251, acc: 0.9844, auc: 1.0, precision: 0.9672, recall: 1.0\n",
      "2019-01-14T17:54:36.626874, step: 1463, loss: 0.010942745953798294, acc: 0.9922, auc: 1.0, precision: 0.9873, recall: 1.0\n",
      "2019-01-14T17:54:36.836428, step: 1464, loss: 0.011721579357981682, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:37.043814, step: 1465, loss: 0.010400762781500816, acc: 0.9922, auc: 1.0, precision: 0.9851, recall: 1.0\n",
      "2019-01-14T17:54:37.267572, step: 1466, loss: 0.0020358602050691843, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:37.481705, step: 1467, loss: 0.004260909277945757, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:37.717055, step: 1468, loss: 0.02779322862625122, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9821\n",
      "2019-01-14T17:54:37.962847, step: 1469, loss: 0.002719664014875889, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:38.178990, step: 1470, loss: 0.0011297150049358606, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:38.401418, step: 1471, loss: 0.02019919455051422, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9818\n",
      "2019-01-14T17:54:38.629147, step: 1472, loss: 0.0005006573046557605, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:38.862518, step: 1473, loss: 0.03456155210733414, acc: 0.9922, auc: 0.9998, precision: 1.0, recall: 0.9836\n",
      "2019-01-14T17:54:39.097361, step: 1474, loss: 0.0005135263781994581, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:39.337944, step: 1475, loss: 0.01711963303387165, acc: 0.9922, auc: 0.9998, precision: 0.9828, recall: 1.0\n",
      "2019-01-14T17:54:39.565125, step: 1476, loss: 0.00843862071633339, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9863\n",
      "2019-01-14T17:54:39.787157, step: 1477, loss: 0.01270008273422718, acc: 0.9922, auc: 1.0, precision: 0.9868, recall: 1.0\n",
      "2019-01-14T17:54:39.994621, step: 1478, loss: 0.01520461030304432, acc: 0.9922, auc: 1.0, precision: 0.9844, recall: 1.0\n",
      "2019-01-14T17:54:40.215098, step: 1479, loss: 0.0018355234060436487, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:40.425690, step: 1480, loss: 0.00020196015248075128, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:40.647975, step: 1481, loss: 0.020142657682299614, acc: 0.9922, auc: 1.0, precision: 0.9861, recall: 1.0\n",
      "2019-01-14T17:54:40.881119, step: 1482, loss: 0.009670333936810493, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9853\n",
      "2019-01-14T17:54:41.107398, step: 1483, loss: 0.0023651262745261192, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:41.327816, step: 1484, loss: 0.00029533784254454076, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:41.547927, step: 1485, loss: 0.0003758244274649769, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:41.759939, step: 1486, loss: 0.0028011922258883715, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:41.971879, step: 1487, loss: 0.015349295921623707, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9833\n",
      "2019-01-14T17:54:42.193307, step: 1488, loss: 0.002801362657919526, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:42.424975, step: 1489, loss: 0.00045423503615893424, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:42.655407, step: 1490, loss: 0.0011113106738775969, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:42.877934, step: 1491, loss: 0.0008607716299593449, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:43.101558, step: 1492, loss: 0.0005912799970246851, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:43.326375, step: 1493, loss: 0.003361772745847702, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:43.538268, step: 1494, loss: 0.0022450373508036137, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:43.752310, step: 1495, loss: 0.0034815073013305664, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:43.976200, step: 1496, loss: 0.0008538946858607233, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:44.213650, step: 1497, loss: 0.0005236417055130005, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:44.431683, step: 1498, loss: 0.03318369761109352, acc: 0.9922, auc: 1.0, precision: 0.9833, recall: 1.0\n",
      "2019-01-14T17:54:44.648724, step: 1499, loss: 0.0021322527900338173, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:44.865566, step: 1500, loss: 0.0024643223732709885, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "\n",
      "Evaluation:\n",
      "2019-01-14T17:54:53.410523, step: 1500, loss: 0.9492368820386056, acc: 0.857971794871795, auc: 0.939674358974359, precision: 0.8969461538461538, recall: 0.8120307692307694\n",
      "2019-01-14T17:54:53.629408, step: 1501, loss: 0.0008910332689993083, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-14T17:54:53.849653, step: 1502, loss: 0.01015173178166151, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9839\n",
      "2019-01-14T17:54:54.074385, step: 1503, loss: 0.012614555656909943, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9701\n",
      "2019-01-14T17:54:54.296632, step: 1504, loss: 0.0005226375651545823, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:54.523309, step: 1505, loss: 0.0016432186821475625, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:54.753028, step: 1506, loss: 0.023906569927930832, acc: 0.9922, auc: 1.0, precision: 0.9831, recall: 1.0\n",
      "2019-01-14T17:54:54.975920, step: 1507, loss: 0.006000826135277748, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:55.205114, step: 1508, loss: 0.0051110973581671715, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:55.426481, step: 1509, loss: 0.02013910748064518, acc: 0.9844, auc: 0.9998, precision: 0.9846, recall: 0.9846\n",
      "2019-01-14T17:54:55.652688, step: 1510, loss: 0.002643466927111149, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:55.858849, step: 1511, loss: 0.001217999728396535, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:56.068403, step: 1512, loss: 0.0022382603492587805, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:56.270399, step: 1513, loss: 0.0036801036912947893, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:56.479025, step: 1514, loss: 0.04134322702884674, acc: 0.9844, auc: 1.0, precision: 0.9677, recall: 1.0\n",
      "2019-01-14T17:54:56.693983, step: 1515, loss: 0.009663287550210953, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9839\n",
      "2019-01-14T17:54:56.906444, step: 1516, loss: 0.003347192658111453, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:57.117284, step: 1517, loss: 0.0015689891297370195, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:57.330797, step: 1518, loss: 0.01340486854314804, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9841\n",
      "2019-01-14T17:54:57.535904, step: 1519, loss: 0.017272500321269035, acc: 0.9922, auc: 1.0, precision: 0.9839, recall: 1.0\n",
      "2019-01-14T17:54:57.747857, step: 1520, loss: 0.00017458708316553384, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:57.973095, step: 1521, loss: 0.00031811383087188005, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:58.200380, step: 1522, loss: 0.0019411112880334258, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:58.422727, step: 1523, loss: 0.029561595991253853, acc: 0.9922, auc: 0.9993, precision: 0.9846, recall: 1.0\n",
      "2019-01-14T17:54:58.642209, step: 1524, loss: 0.006110210437327623, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:58.844889, step: 1525, loss: 0.006174545269459486, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:59.070792, step: 1526, loss: 0.025606684386730194, acc: 0.9844, auc: 0.9998, precision: 1.0, recall: 0.9655\n",
      "2019-01-14T17:54:59.303436, step: 1527, loss: 0.00456260796636343, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:59.525606, step: 1528, loss: 0.005670579150319099, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:59.732167, step: 1529, loss: 0.003794239368289709, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:54:59.953438, step: 1530, loss: 0.021538814529776573, acc: 0.9922, auc: 1.0, precision: 0.9859, recall: 1.0\n",
      "2019-01-14T17:55:00.170577, step: 1531, loss: 0.0011813226155936718, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:55:00.389987, step: 1532, loss: 0.012697465717792511, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:55:00.616944, step: 1533, loss: 0.0004066512919962406, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:55:00.835095, step: 1534, loss: 0.005161610431969166, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:55:01.051635, step: 1535, loss: 0.0023737342562526464, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:55:01.272606, step: 1536, loss: 0.05199837684631348, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9636\n",
      "2019-01-14T17:55:01.508384, step: 1537, loss: 0.00048739174962975085, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:55:01.734986, step: 1538, loss: 0.02667490392923355, acc: 0.9844, auc: 0.9998, precision: 1.0, recall: 0.9688\n",
      "2019-01-14T17:55:01.964526, step: 1539, loss: 0.0004163662379141897, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:55:02.181498, step: 1540, loss: 0.009832898154854774, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:55:02.401714, step: 1541, loss: 0.005448455456644297, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9846\n",
      "2019-01-14T17:55:02.615985, step: 1542, loss: 0.006749190390110016, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:55:02.833227, step: 1543, loss: 0.019186032935976982, acc: 0.9922, auc: 1.0, precision: 0.9839, recall: 1.0\n",
      "2019-01-14T17:55:03.047952, step: 1544, loss: 0.012036778964102268, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:55:03.268723, step: 1545, loss: 0.017501553520560265, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9855\n",
      "2019-01-14T17:55:03.490057, step: 1546, loss: 0.004286317154765129, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:55:03.709188, step: 1547, loss: 0.005150876007974148, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:55:03.925911, step: 1548, loss: 0.02394849620759487, acc: 0.9922, auc: 0.9998, precision: 0.9836, recall: 1.0\n",
      "2019-01-14T17:55:04.184569, step: 1549, loss: 0.0036016737576574087, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:55:04.420816, step: 1550, loss: 0.008887376636266708, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9855\n",
      "2019-01-14T17:55:04.644340, step: 1551, loss: 0.01019950583577156, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:55:04.856060, step: 1552, loss: 0.05382286757230759, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9714\n",
      "2019-01-14T17:55:05.086298, step: 1553, loss: 0.002557199215516448, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:55:05.296142, step: 1554, loss: 0.0010711868526414037, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:55:05.521410, step: 1555, loss: 0.0006041104206815362, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:55:05.749587, step: 1556, loss: 0.0016908639809116721, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:55:05.970574, step: 1557, loss: 0.01492438092827797, acc: 0.9922, auc: 1.0, precision: 0.9868, recall: 1.0\n",
      "2019-01-14T17:55:06.193523, step: 1558, loss: 0.019210124388337135, acc: 0.9922, auc: 1.0, precision: 0.9836, recall: 1.0\n",
      "2019-01-14T17:55:06.416530, step: 1559, loss: 0.002787647768855095, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:55:06.637432, step: 1560, loss: 0.0011252086842432618, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "\n",
    "# 生成训练集和验证集\n",
    "trainReviews = data.trainReviews\n",
    "trainLabels = data.trainLabels\n",
    "evalReviews = data.evalReviews\n",
    "evalLabels = data.evalLabels\n",
    "\n",
    "wordEmbedding = data.wordEmbedding\n",
    "\n",
    "embeddedPosition = fixedPositionEmbedding(config.batchSize, config.sequenceLength)\n",
    "\n",
    "# 定义计算图\n",
    "with tf.Graph().as_default():\n",
    "\n",
    "    session_conf = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
    "    session_conf.gpu_options.allow_growth=True\n",
    "    session_conf.gpu_options.per_process_gpu_memory_fraction = 0.9  # 配置gpu占用率  \n",
    "\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    \n",
    "    # 定义会话\n",
    "    with sess.as_default():\n",
    "        transformer = Transformer(config, wordEmbedding)\n",
    "        \n",
    "        globalStep = tf.Variable(0, name=\"globalStep\", trainable=False)\n",
    "        # 定义优化函数，传入学习速率参数\n",
    "        optimizer = tf.train.AdamOptimizer(config.training.learningRate)\n",
    "        # 计算梯度,得到梯度和变量\n",
    "        gradsAndVars = optimizer.compute_gradients(transformer.loss)\n",
    "        # 将梯度应用到变量下，生成训练器\n",
    "        trainOp = optimizer.apply_gradients(gradsAndVars, global_step=globalStep)\n",
    "        \n",
    "        # 用summary绘制tensorBoard\n",
    "        gradSummaries = []\n",
    "        for g, v in gradsAndVars:\n",
    "            if g is not None:\n",
    "                tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "        \n",
    "        outDir = os.path.abspath(os.path.join(os.path.curdir, \"summarys\"))\n",
    "        print(\"Writing to {}\\n\".format(outDir))\n",
    "        \n",
    "        lossSummary = tf.summary.scalar(\"loss\", transformer.loss)\n",
    "        summaryOp = tf.summary.merge_all()\n",
    "        \n",
    "        trainSummaryDir = os.path.join(outDir, \"train\")\n",
    "        trainSummaryWriter = tf.summary.FileWriter(trainSummaryDir, sess.graph)\n",
    "        \n",
    "        evalSummaryDir = os.path.join(outDir, \"eval\")\n",
    "        evalSummaryWriter = tf.summary.FileWriter(evalSummaryDir, sess.graph)\n",
    "        \n",
    "        \n",
    "        # 初始化所有变量\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=5)\n",
    "        \n",
    "        # 保存模型的一种方式，保存为pb文件\n",
    "#         builder = tf.saved_model.builder.SavedModelBuilder(\"../model/Transformer/savedModel\")\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        def trainStep(batchX, batchY):\n",
    "            \"\"\"\n",
    "            训练函数\n",
    "            \"\"\"   \n",
    "            feed_dict = {\n",
    "              transformer.inputX: batchX,\n",
    "              transformer.inputY: batchY,\n",
    "              transformer.dropoutKeepProb: config.model.dropoutKeepProb,\n",
    "              transformer.embeddedPosition: embeddedPosition\n",
    "            }\n",
    "            _, summary, step, loss, predictions, binaryPreds = sess.run(\n",
    "                [trainOp, summaryOp, globalStep, transformer.loss, transformer.predictions, transformer.binaryPreds],\n",
    "                feed_dict)\n",
    "            timeStr = datetime.datetime.now().isoformat()\n",
    "            acc, auc, precision, recall = genMetrics(batchY, predictions, binaryPreds)\n",
    "            print(\"{}, step: {}, loss: {}, acc: {}, auc: {}, precision: {}, recall: {}\".format(timeStr, step, loss, acc, auc, precision, recall))\n",
    "            trainSummaryWriter.add_summary(summary, step)\n",
    "\n",
    "        def devStep(batchX, batchY):\n",
    "            \"\"\"\n",
    "            验证函数\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              transformer.inputX: batchX,\n",
    "              transformer.inputY: batchY,\n",
    "              transformer.dropoutKeepProb: 1.0,\n",
    "              transformer.embeddedPosition: embeddedPosition\n",
    "            }\n",
    "            summary, step, loss, predictions, binaryPreds = sess.run(\n",
    "                [summaryOp, globalStep, transformer.loss, transformer.predictions, transformer.binaryPreds],\n",
    "                feed_dict)\n",
    "            \n",
    "            acc, auc, precision, recall = genMetrics(batchY, predictions, binaryPreds)\n",
    "            \n",
    "            evalSummaryWriter.add_summary(summary, step)\n",
    "            \n",
    "            return loss, acc, auc, precision, recall\n",
    "        \n",
    "        for i in range(config.training.epoches):\n",
    "            # 训练模型\n",
    "            print(\"start training model\")\n",
    "            for batchTrain in nextBatch(trainReviews, trainLabels, config.batchSize):\n",
    "                trainStep(batchTrain[0], batchTrain[1])\n",
    "\n",
    "                currentStep = tf.train.global_step(sess, globalStep) \n",
    "                if currentStep % config.training.evaluateEvery == 0:\n",
    "                    print(\"\\nEvaluation:\")\n",
    "                    \n",
    "                    losses = []\n",
    "                    accs = []\n",
    "                    aucs = []\n",
    "                    precisions = []\n",
    "                    recalls = []\n",
    "                    \n",
    "                    for batchEval in nextBatch(evalReviews, evalLabels, config.batchSize):\n",
    "                        loss, acc, auc, precision, recall = devStep(batchEval[0], batchEval[1])\n",
    "                        losses.append(loss)\n",
    "                        accs.append(acc)\n",
    "                        aucs.append(auc)\n",
    "                        precisions.append(precision)\n",
    "                        recalls.append(recall)\n",
    "                        \n",
    "                    time_str = datetime.datetime.now().isoformat()\n",
    "                    print(\"{}, step: {}, loss: {}, acc: {}, auc: {}, precision: {}, recall: {}\".format(time_str, currentStep, mean(losses), \n",
    "                                                                                                       mean(accs), mean(aucs), mean(precisions),\n",
    "                                                                                                       mean(recalls)))\n",
    "                    \n",
    "#                 if currentStep % config.training.checkpointEvery == 0:\n",
    "#                     # 保存模型的另一种方法，保存checkpoint文件\n",
    "#                     path = saver.save(sess, \"../model/Transformer/model/my-model\", global_step=currentStep)\n",
    "#                     print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "                    \n",
    "#         inputs = {\"inputX\": tf.saved_model.utils.build_tensor_info(transformer.inputX),\n",
    "#                   \"keepProb\": tf.saved_model.utils.build_tensor_info(transformer.dropoutKeepProb)}\n",
    "\n",
    "#         outputs = {\"binaryPreds\": tf.saved_model.utils.build_tensor_info(transformer.binaryPreds)}\n",
    "\n",
    "#         prediction_signature = tf.saved_model.signature_def_utils.build_signature_def(inputs=inputs, outputs=outputs,\n",
    "#                                                                                       method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME)\n",
    "#         legacy_init_op = tf.group(tf.tables_initializer(), name=\"legacy_init_op\")\n",
    "#         builder.add_meta_graph_and_variables(sess, [tf.saved_model.tag_constants.SERVING],\n",
    "#                                             signature_def_map={\"predict\": prediction_signature}, legacy_init_op=legacy_init_op)\n",
    "\n",
    "#         builder.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
