{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import json\n",
    "import threading\n",
    "\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from math import sqrt\n",
    "\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置参数\n",
    "\n",
    "class TrainingConfig(object):\n",
    "    epoches = 5\n",
    "    evaluateEvery = 100\n",
    "    checkpointEvery = 100\n",
    "    learningRate = 0.001\n",
    "    \n",
    "class ModelConfig(object):\n",
    "    embeddingSize = 200\n",
    "    \n",
    "    hiddenSizes = [128]  # LSTM结构的神经元个数\n",
    "    \n",
    "    dropoutKeepProb = 0.5\n",
    "    l2RegLambda = 0.0\n",
    "    epsilon = 5\n",
    "    \n",
    "class Config(object):\n",
    "    sequenceLength = 200  # 取了所有序列长度的均值\n",
    "    batchSize = 128\n",
    "    \n",
    "    dataSource = \"../data/preProcess/labeledTrain.csv\"\n",
    "    \n",
    "    stopWordSource = \"../data/english\"\n",
    "    \n",
    "    numClasses = 2\n",
    "    \n",
    "    rate = 0.8  # 训练集的比例\n",
    "    \n",
    "    training = TrainingConfig()\n",
    "    \n",
    "    model = ModelConfig()\n",
    "\n",
    "    \n",
    "# 实例化配置参数对象\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据预处理的类，生成训练集和测试集\n",
    "\n",
    "class Dataset(object):\n",
    "    def __init__(self, config):\n",
    "        self._dataSource = config.dataSource\n",
    "        self._stopWordSource = config.stopWordSource  \n",
    "        \n",
    "        self._sequenceLength = config.sequenceLength  # 每条输入的序列处理为定长\n",
    "        self._embeddingSize = config.model.embeddingSize\n",
    "        self._batchSize = config.batchSize\n",
    "        self._rate = config.rate\n",
    "        \n",
    "        self._stopWordDict = {}\n",
    "        \n",
    "        self.trainReviews = []\n",
    "        self.trainLabels = []\n",
    "        \n",
    "        self.evalReviews = []\n",
    "        self.evalLabels = []\n",
    "        \n",
    "        self.wordEmbedding =None\n",
    "        \n",
    "        self.indexFreqs = []  # 统计词空间中的词在出现在多少个review中\n",
    "        \n",
    "        self._wordToIndex = {}\n",
    "        self._indexToWord = {}\n",
    "        \n",
    "    def _readData(self, filePath):\n",
    "        \"\"\"\n",
    "        从csv文件中读取数据集\n",
    "        \"\"\"\n",
    "        \n",
    "        df = pd.read_csv(filePath)\n",
    "        labels = df[\"sentiment\"].tolist()\n",
    "        review = df[\"review\"].tolist()\n",
    "        reviews = [line.strip().split() for line in review]\n",
    "\n",
    "        return reviews, labels\n",
    "\n",
    "    def _reviewProcess(self, review, sequenceLength, wordToIndex):\n",
    "        \"\"\"\n",
    "        将数据集中的每条评论用index表示\n",
    "        wordToIndex中“pad”对应的index为0\n",
    "        \"\"\"\n",
    "        \n",
    "        reviewVec = np.zeros((sequenceLength))\n",
    "        sequenceLen = sequenceLength\n",
    "        \n",
    "        # 判断当前的序列是否小于定义的固定序列长度\n",
    "        if len(review) < sequenceLength:\n",
    "            sequenceLen = len(review)\n",
    "            \n",
    "        for i in range(sequenceLen):\n",
    "            if review[i] in wordToIndex:\n",
    "                reviewVec[i] = wordToIndex[review[i]]\n",
    "            else:\n",
    "                reviewVec[i] = wordToIndex[\"UNK\"]\n",
    "\n",
    "        return reviewVec\n",
    "\n",
    "    def _genTrainEvalData(self, x, y, rate):\n",
    "        \"\"\"\n",
    "        生成训练集和验证集\n",
    "        \"\"\"\n",
    "        \n",
    "        reviews = []\n",
    "        labels = []\n",
    "        \n",
    "        # 遍历所有的文本，将文本中的词转换成index表示\n",
    "        for i in range(len(x)):\n",
    "            reviewVec = self._reviewProcess(x[i], self._sequenceLength, self._wordToIndex)\n",
    "            reviews.append(reviewVec)\n",
    "            \n",
    "            labels.append([y[i]])\n",
    "            \n",
    "        trainIndex = int(len(x) * rate)\n",
    "        \n",
    "        trainReviews = np.asarray(reviews[:trainIndex], dtype=\"int64\")\n",
    "        trainLabels = np.array(labels[:trainIndex], dtype=\"float32\")\n",
    "        \n",
    "        evalReviews = np.asarray(reviews[trainIndex:], dtype=\"int64\")\n",
    "        evalLabels = np.array(labels[trainIndex:], dtype=\"float32\")\n",
    "\n",
    "        return trainReviews, trainLabels, evalReviews, evalLabels\n",
    "        \n",
    "    def _genVocabulary(self, reviews):\n",
    "        \"\"\"\n",
    "        生成词向量和词汇-索引映射字典，可以用全数据集\n",
    "        \"\"\"\n",
    "        \n",
    "        allWords = [word for review in reviews for word in review]\n",
    "        \n",
    "        # 去掉停用词\n",
    "        subWords = [word for word in allWords if word not in self.stopWordDict]\n",
    "        \n",
    "        wordCount = Counter(subWords)  # 统计词频\n",
    "        sortWordCount = sorted(wordCount.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # 去除低频词\n",
    "        words = [item[0] for item in sortWordCount if item[1] >= 5]\n",
    "        \n",
    "        vocab, wordEmbedding = self._getWordEmbedding(words)\n",
    "        self.wordEmbedding = wordEmbedding\n",
    "        \n",
    "        self._wordToIndex = dict(zip(vocab, list(range(len(vocab)))))\n",
    "        self._indexToWord = dict(zip(list(range(len(vocab))), vocab))\n",
    "        \n",
    "        # 得到逆词频\n",
    "        self._getWordIndexFreq(vocab, reviews)\n",
    "        \n",
    "        # 将词汇-索引映射表保存为json数据，之后做inference时直接加载来处理数据\n",
    "        with open(\"../data/wordJson/wordToIndex.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self._wordToIndex, f)\n",
    "        \n",
    "        with open(\"../data/wordJson/indexToWord.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self._indexToWord, f)\n",
    "            \n",
    "    def _getWordEmbedding(self, words):\n",
    "        \"\"\"\n",
    "        按照我们的数据集中的单词取出预训练好的word2vec中的词向量\n",
    "        \"\"\"\n",
    "        \n",
    "        wordVec = gensim.models.KeyedVectors.load_word2vec_format(\"../word2vec/word2Vec.bin\", binary=True)\n",
    "        vocab = []\n",
    "        wordEmbedding = []\n",
    "        \n",
    "        # 添加 \"pad\" 和 \"UNK\", \n",
    "        vocab.append(\"pad\")\n",
    "        vocab.append(\"UNK\")\n",
    "        wordEmbedding.append(np.zeros(self._embeddingSize))\n",
    "        wordEmbedding.append(np.random.randn(self._embeddingSize))\n",
    "        \n",
    "        for word in words:\n",
    "            try:\n",
    "                vector = wordVec.wv[word]\n",
    "                vocab.append(word)\n",
    "                wordEmbedding.append(vector)\n",
    "            except:\n",
    "                print(word + \"不存在于词向量中\")\n",
    "                \n",
    "        return vocab, np.array(wordEmbedding)\n",
    "    \n",
    "    def _getWordIndexFreq(self, vocab, reviews):\n",
    "        \"\"\"\n",
    "        统计词汇空间中各个词出现在多少个文本中\n",
    "        \"\"\"\n",
    "        reviewDicts = [dict(zip(review, range(len(review)))) for review in reviews]\n",
    "        indexFreqs = [0] * len(vocab)\n",
    "        for word in vocab:\n",
    "            count = 0\n",
    "            for review in reviewDicts:\n",
    "                if word in review:\n",
    "                    count += 1\n",
    "            indexFreqs[self._wordToIndex[word]] = count\n",
    "        \n",
    "        self.indexFreqs = indexFreqs\n",
    "    \n",
    "    def _readStopWord(self, stopWordPath):\n",
    "        \"\"\"\n",
    "        读取停用词\n",
    "        \"\"\"\n",
    "        \n",
    "        with open(stopWordPath, \"r\") as f:\n",
    "            stopWords = f.read()\n",
    "            stopWordList = stopWords.splitlines()\n",
    "            # 将停用词用列表的形式生成，之后查找停用词时会比较快\n",
    "            self.stopWordDict = dict(zip(stopWordList, list(range(len(stopWordList)))))\n",
    "            \n",
    "    def dataGen(self):\n",
    "        \"\"\"\n",
    "        初始化训练集和验证集\n",
    "        \"\"\"\n",
    "        \n",
    "        # 初始化停用词\n",
    "        self._readStopWord(self._stopWordSource)\n",
    "        \n",
    "        # 初始化数据集\n",
    "        reviews, labels = self._readData(self._dataSource)\n",
    "        \n",
    "        # 初始化词汇-索引映射表和词向量矩阵\n",
    "        self._genVocabulary(reviews)\n",
    "        \n",
    "        # 初始化训练集和测试集\n",
    "        trainReviews, trainLabels, evalReviews, evalLabels = self._genTrainEvalData(reviews, labels, self._rate)\n",
    "        self.trainReviews = trainReviews\n",
    "        self.trainLabels = trainLabels\n",
    "        \n",
    "        self.evalReviews = evalReviews\n",
    "        self.evalLabels = evalLabels\n",
    "        \n",
    "        \n",
    "data = Dataset(config)\n",
    "data.dataGen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape: (20000, 200)\n",
      "train label shape: (20000, 1)\n",
      "eval data shape: (5000, 200)\n"
     ]
    }
   ],
   "source": [
    "print(\"train data shape: {}\".format(data.trainReviews.shape))\n",
    "print(\"train label shape: {}\".format(data.trainLabels.shape))\n",
    "print(\"eval data shape: {}\".format(data.evalReviews.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31983, 200)\n"
     ]
    }
   ],
   "source": [
    "print(data.wordEmbedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出batch数据集\n",
    "\n",
    "def nextBatch(x, y, batchSize):\n",
    "        \"\"\"\n",
    "        生成batch数据集，用生成器的方式输出\n",
    "        \"\"\"\n",
    "    \n",
    "        perm = np.arange(len(x))\n",
    "        np.random.shuffle(perm)\n",
    "        x = x[perm]\n",
    "        y = y[perm]\n",
    "        \n",
    "        numBatches = len(x) // batchSize\n",
    "\n",
    "        for i in range(numBatches):\n",
    "            start = i * batchSize\n",
    "            end = start + batchSize\n",
    "            batchX = np.array(x[start: end], dtype=\"int64\")\n",
    "            batchY = np.array(y[start: end], dtype=\"float32\")\n",
    "            \n",
    "            yield batchX, batchY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建模型\n",
    "class AdversarialLSTM(object):\n",
    "    \"\"\"\n",
    "    Text CNN 用于文本分类\n",
    "    \"\"\"\n",
    "    def __init__(self, config, wordEmbedding, indexFreqs):\n",
    "\n",
    "        # 定义模型的输入\n",
    "        self.inputX = tf.placeholder(tf.int32, [None, config.sequenceLength], name=\"inputX\")\n",
    "        self.inputY = tf.placeholder(tf.float32, [None, 1], name=\"inputY\")\n",
    "        \n",
    "        self.dropoutKeepProb = tf.placeholder(tf.float32, name=\"dropoutKeepProb\")\n",
    "        self.config = config\n",
    "        \n",
    "        # 根据词的频率计算权重\n",
    "        indexFreqs[0], indexFreqs[1] = 20000, 10000\n",
    "        weights = tf.cast(tf.reshape(indexFreqs / tf.reduce_sum(indexFreqs), [1, len(indexFreqs)]), dtype=tf.float32)\n",
    "        \n",
    "        # 词嵌入层\n",
    "        with tf.name_scope(\"embedding\"):\n",
    "\n",
    "            # 利用词频计算新的词嵌入矩阵\n",
    "            normWordEmbedding = self._normalize(tf.cast(wordEmbedding, dtype=tf.float32, name=\"word2vec\"), weights)\n",
    "            \n",
    "            # 利用词嵌入矩阵将输入的数据中的词转换成词向量，维度[batch_size, sequence_length, embedding_size]\n",
    "            self.embeddedWords = tf.nn.embedding_lookup(normWordEmbedding, self.inputX)\n",
    "            \n",
    "         # 计算二元交叉熵损失 \n",
    "        with tf.name_scope(\"loss\"):\n",
    "            with tf.variable_scope(\"Bi-LSTM\", reuse=None):\n",
    "                self.predictions = self._Bi_LSTMAttention(self.embeddedWords)\n",
    "                self.binaryPreds = tf.cast(tf.greater_equal(self.predictions, 0.5), tf.float32, name=\"binaryPreds\")\n",
    "                losses = tf.nn.sigmoid_cross_entropy_with_logits(logits=self.predictions, labels=self.inputY)\n",
    "                loss = tf.reduce_mean(losses)\n",
    "        \n",
    "        with tf.name_scope(\"perturLoss\"):\n",
    "            with tf.variable_scope(\"Bi-LSTM\", reuse=True):\n",
    "                perturWordEmbedding = self._addPerturbation(self.embeddedWords, loss)\n",
    "                perturPredictions = self._Bi_LSTMAttention(perturWordEmbedding)\n",
    "                perturLosses = tf.nn.sigmoid_cross_entropy_with_logits(logits=perturPredictions, labels=self.inputY)\n",
    "                perturLoss = tf.reduce_mean(perturLosses)\n",
    "        \n",
    "        self.loss = loss + perturLoss\n",
    "            \n",
    "    def _Bi_LSTMAttention(self, embeddedWords):\n",
    "        \"\"\"\n",
    "        Bi-LSTM + Attention 的模型结构\n",
    "        \"\"\"\n",
    "        \n",
    "        config = self.config\n",
    "        # 定义两层双向LSTM的模型结构\n",
    "        with tf.name_scope(\"Bi-LSTM\"):\n",
    "            fwHiddenLayers = []\n",
    "            bwHiddenLayers = []\n",
    "            for idx, hiddenSize in enumerate(config.model.hiddenSizes):\n",
    "                with tf.name_scope(\"Bi-LSTM\" + str(idx)):\n",
    "                    # 定义前向LSTM结构\n",
    "                    lstmFwCell = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(num_units=hiddenSize, state_is_tuple=True),\n",
    "                                                                 output_keep_prob=self.dropoutKeepProb)\n",
    "                    # 定义反向LSTM结构\n",
    "                    lstmBwCell = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(num_units=hiddenSize, state_is_tuple=True),\n",
    "                                                                 output_keep_prob=self.dropoutKeepProb)\n",
    "\n",
    "                fwHiddenLayers.append(lstmFwCell)\n",
    "                bwHiddenLayers.append(lstmBwCell)\n",
    "\n",
    "            # 实现多层的LSTM结构， state_is_tuple=True，则状态会以元祖的形式组合(h, c)，否则列向拼接\n",
    "            fwMultiLstm = tf.nn.rnn_cell.MultiRNNCell(cells=fwHiddenLayers, state_is_tuple=True)\n",
    "            bwMultiLstm = tf.nn.rnn_cell.MultiRNNCell(cells=bwHiddenLayers, state_is_tuple=True)\n",
    "\n",
    "            # 采用动态rnn，可以动态的输入序列的长度，若没有输入，则取序列的全长\n",
    "            # outputs是一个元祖(output_fw, output_bw)，其中两个元素的维度都是[batch_size, max_time, hidden_size],fw和bw的hidden_size一样\n",
    "            # self.current_state 是最终的状态，二元组(state_fw, state_bw)，state_fw=[batch_size, s]，s是一个元祖(h, c)\n",
    "            outputs, self.current_state = tf.nn.bidirectional_dynamic_rnn(fwMultiLstm, bwMultiLstm, embeddedWords, dtype=tf.float32)\n",
    "        \n",
    "        # 在Bi-LSTM+Attention的论文中，将前向和后向的输出相加\n",
    "        with tf.name_scope(\"Attention\"):\n",
    "            H = outputs[0] + outputs[1]\n",
    "\n",
    "            # 得到Attention的输出\n",
    "            output = self._attention(H)\n",
    "            outputSize = config.model.hiddenSizes[-1]\n",
    "        \n",
    "        # 全连接层的输出\n",
    "        with tf.name_scope(\"output\"):\n",
    "            outputW = tf.get_variable(\n",
    "                \"outputW\",\n",
    "                shape=[outputSize, 1],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "            outputB= tf.Variable(tf.constant(0.1, shape=[1]), name=\"outputB\")\n",
    "            predictions = tf.nn.xw_plus_b(output, outputW, outputB, name=\"predictions\")\n",
    "            \n",
    "        return predictions\n",
    "    \n",
    "    def _attention(self, H):\n",
    "        \"\"\"\n",
    "        利用Attention机制得到句子的向量表示\n",
    "        \"\"\"\n",
    "        # 获得最后一层LSTM的神经元数量\n",
    "        hiddenSize = config.model.hiddenSizes[-1]\n",
    "        \n",
    "        # 初始化一个权重向量，是可训练的参数\n",
    "        W = tf.Variable(tf.random_normal([hiddenSize], stddev=0.1))\n",
    "        \n",
    "        # 对Bi-LSTM的输出用激活函数做非线性转换\n",
    "        M = tf.tanh(H)\n",
    "        \n",
    "        # 对W和M做矩阵运算，W=[batch_size, time_step, hidden_size]，计算前做维度转换成[batch_size * time_step, hidden_size]\n",
    "        # newM = [batch_size, time_step, 1]，每一个时间步的输出由向量转换成一个数字\n",
    "        newM = tf.matmul(tf.reshape(M, [-1, hiddenSize]), tf.reshape(W, [-1, 1]))\n",
    "        \n",
    "        # 对newM做维度转换成[batch_size, time_step]\n",
    "        restoreM = tf.reshape(newM, [-1, config.sequenceLength])\n",
    "        \n",
    "        # 用softmax做归一化处理[batch_size, time_step]\n",
    "        self.alpha = tf.nn.softmax(restoreM)\n",
    "        \n",
    "        # 利用求得的alpha的值对H进行加权求和，用矩阵运算直接操作\n",
    "        r = tf.matmul(tf.transpose(H, [0, 2, 1]), tf.reshape(self.alpha, [-1, config.sequenceLength, 1]))\n",
    "        \n",
    "        # 将三维压缩成二维sequeezeR=[batch_size, hidden_size]\n",
    "        sequeezeR = tf.squeeze(r)\n",
    "        \n",
    "        sentenceRepren = tf.tanh(sequeezeR)\n",
    "        \n",
    "        # 对Attention的输出可以做dropout处理\n",
    "        output = tf.nn.dropout(sentenceRepren, self.dropoutKeepProb)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def _normalize(self, wordEmbedding, weights):\n",
    "        \"\"\"\n",
    "        对word embedding 结合权重做标准化处理\n",
    "        \"\"\"\n",
    "        \n",
    "        mean = tf.matmul(weights, wordEmbedding)\n",
    "        print(mean)\n",
    "        powWordEmbedding = tf.pow(wordEmbedding - mean, 2.)\n",
    "        \n",
    "        var = tf.matmul(weights, powWordEmbedding)\n",
    "        print(var)\n",
    "        stddev = tf.sqrt(1e-6 + var)\n",
    "        \n",
    "        return (wordEmbedding - mean) / stddev\n",
    "    \n",
    "    def _addPerturbation(self, embedded, loss):\n",
    "        \"\"\"\n",
    "        添加波动到word embedding\n",
    "        \"\"\"\n",
    "        grad, = tf.gradients(\n",
    "            loss,\n",
    "            embedded,\n",
    "            aggregation_method=tf.AggregationMethod.EXPERIMENTAL_ACCUMULATE_N)\n",
    "        grad = tf.stop_gradient(grad)\n",
    "        perturb = self._scaleL2(grad, self.config.model.epsilon)\n",
    "        return embedded + perturb\n",
    "    \n",
    "    def _scaleL2(self, x, norm_length):\n",
    "        # shape(x) = (batch, num_timesteps, d)\n",
    "        # Divide x by max(abs(x)) for a numerically stable L2 norm.\n",
    "        # 2norm(x) = a * 2norm(x/a)\n",
    "        # Scale over the full sequence, dims (1, 2)\n",
    "        alpha = tf.reduce_max(tf.abs(x), (1, 2), keep_dims=True) + 1e-12\n",
    "        l2_norm = alpha * tf.sqrt(\n",
    "            tf.reduce_sum(tf.pow(x / alpha, 2), (1, 2), keep_dims=True) + 1e-6)\n",
    "        x_unit = x / l2_norm\n",
    "        return norm_length * x_unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义性能指标函数\n",
    "\n",
    "def mean(item):\n",
    "    return sum(item) / len(item)\n",
    "\n",
    "\n",
    "def genMetrics(trueY, predY, binaryPredY):\n",
    "    \"\"\"\n",
    "    生成acc和auc值\n",
    "    \"\"\"\n",
    "    auc = roc_auc_score(trueY, predY)\n",
    "    accuracy = accuracy_score(trueY, binaryPredY)\n",
    "    precision = precision_score(trueY, binaryPredY)\n",
    "    recall = recall_score(trueY, binaryPredY)\n",
    "    \n",
    "    return round(accuracy, 4), round(auc, 4), round(precision, 4), round(recall, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"embedding/MatMul:0\", shape=(1, 200), dtype=float32)\n",
      "Tensor(\"embedding/MatMul_1:0\", shape=(1, 200), dtype=float32)\n",
      "INFO:tensorflow:Summary name Bi-LSTM/bidirectional_rnn/fw/multi_rnn_cell/cell_0/lstm_cell/kernel:0/grad/hist is illegal; using Bi-LSTM/bidirectional_rnn/fw/multi_rnn_cell/cell_0/lstm_cell/kernel_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name Bi-LSTM/bidirectional_rnn/fw/multi_rnn_cell/cell_0/lstm_cell/kernel:0/grad/sparsity is illegal; using Bi-LSTM/bidirectional_rnn/fw/multi_rnn_cell/cell_0/lstm_cell/kernel_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name Bi-LSTM/bidirectional_rnn/fw/multi_rnn_cell/cell_0/lstm_cell/bias:0/grad/hist is illegal; using Bi-LSTM/bidirectional_rnn/fw/multi_rnn_cell/cell_0/lstm_cell/bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name Bi-LSTM/bidirectional_rnn/fw/multi_rnn_cell/cell_0/lstm_cell/bias:0/grad/sparsity is illegal; using Bi-LSTM/bidirectional_rnn/fw/multi_rnn_cell/cell_0/lstm_cell/bias_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name Bi-LSTM/bidirectional_rnn/bw/multi_rnn_cell/cell_0/lstm_cell/kernel:0/grad/hist is illegal; using Bi-LSTM/bidirectional_rnn/bw/multi_rnn_cell/cell_0/lstm_cell/kernel_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name Bi-LSTM/bidirectional_rnn/bw/multi_rnn_cell/cell_0/lstm_cell/kernel:0/grad/sparsity is illegal; using Bi-LSTM/bidirectional_rnn/bw/multi_rnn_cell/cell_0/lstm_cell/kernel_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name Bi-LSTM/bidirectional_rnn/bw/multi_rnn_cell/cell_0/lstm_cell/bias:0/grad/hist is illegal; using Bi-LSTM/bidirectional_rnn/bw/multi_rnn_cell/cell_0/lstm_cell/bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name Bi-LSTM/bidirectional_rnn/bw/multi_rnn_cell/cell_0/lstm_cell/bias:0/grad/sparsity is illegal; using Bi-LSTM/bidirectional_rnn/bw/multi_rnn_cell/cell_0/lstm_cell/bias_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name loss/Bi-LSTM/Attention/Variable:0/grad/hist is illegal; using loss/Bi-LSTM/Attention/Variable_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name loss/Bi-LSTM/Attention/Variable:0/grad/sparsity is illegal; using loss/Bi-LSTM/Attention/Variable_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name Bi-LSTM/outputW:0/grad/hist is illegal; using Bi-LSTM/outputW_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name Bi-LSTM/outputW:0/grad/sparsity is illegal; using Bi-LSTM/outputW_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name loss/Bi-LSTM/output/outputB:0/grad/hist is illegal; using loss/Bi-LSTM/output/outputB_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name loss/Bi-LSTM/output/outputB:0/grad/sparsity is illegal; using loss/Bi-LSTM/output/outputB_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name perturLoss/Bi-LSTM/Attention/Variable:0/grad/hist is illegal; using perturLoss/Bi-LSTM/Attention/Variable_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name perturLoss/Bi-LSTM/Attention/Variable:0/grad/sparsity is illegal; using perturLoss/Bi-LSTM/Attention/Variable_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name perturLoss/Bi-LSTM/output/outputB:0/grad/hist is illegal; using perturLoss/Bi-LSTM/output/outputB_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name perturLoss/Bi-LSTM/output/outputB:0/grad/sparsity is illegal; using perturLoss/Bi-LSTM/output/outputB_0/grad/sparsity instead.\n",
      "Writing to /data4T/share/jiangxinyang848/textClassifier/adversarialLSTM/summarys\n",
      "\n",
      "start training model\n",
      "2018-12-29T11:25:26.516039, step: 1, loss: 1.4640507698059082, acc: 0.5156, auc: 0.5557, precision: 0.3077, recall: 0.0702\n",
      "2018-12-29T11:25:27.237203, step: 2, loss: 2.3577380180358887, acc: 0.4453, auc: 0.5757, precision: 0.0, recall: 0.0\n",
      "2018-12-29T11:25:27.998165, step: 3, loss: 1.5854132175445557, acc: 0.4766, auc: 0.5087, precision: 0.25, recall: 0.0678\n",
      "2018-12-29T11:25:28.831898, step: 4, loss: 1.5687166452407837, acc: 0.4531, auc: 0.4955, precision: 0.4789, recall: 0.5075\n",
      "2018-12-29T11:25:29.547625, step: 5, loss: 1.5760724544525146, acc: 0.5312, auc: 0.5502, precision: 0.5248, recall: 0.8154\n",
      "2018-12-29T11:25:30.272788, step: 6, loss: 1.4389290809631348, acc: 0.5391, auc: 0.5315, precision: 0.6, recall: 0.5417\n",
      "2018-12-29T11:25:31.003043, step: 7, loss: 1.496543288230896, acc: 0.4844, auc: 0.5317, precision: 0.4808, recall: 0.3906\n",
      "2018-12-29T11:25:31.652989, step: 8, loss: 1.4257090091705322, acc: 0.5938, auc: 0.553, precision: 0.6, recall: 0.2632\n",
      "2018-12-29T11:25:32.435554, step: 9, loss: 1.3824243545532227, acc: 0.6094, auc: 0.5919, precision: 0.6154, recall: 0.1509\n",
      "2018-12-29T11:25:33.100629, step: 10, loss: 1.4998257160186768, acc: 0.5391, auc: 0.528, precision: 0.75, recall: 0.0492\n",
      "2018-12-29T11:25:33.799305, step: 11, loss: 1.6011183261871338, acc: 0.4453, auc: 0.4779, precision: 0.5, recall: 0.0141\n",
      "2018-12-29T11:25:34.474927, step: 12, loss: 1.5228407382965088, acc: 0.4766, auc: 0.5242, precision: 0.4, recall: 0.0303\n",
      "2018-12-29T11:25:35.215842, step: 13, loss: 1.4368631839752197, acc: 0.5078, auc: 0.644, precision: 1.0, recall: 0.0308\n",
      "2018-12-29T11:25:35.901161, step: 14, loss: 1.4035074710845947, acc: 0.5547, auc: 0.6338, precision: 0.8, recall: 0.0667\n",
      "2018-12-29T11:25:36.647424, step: 15, loss: 1.359398365020752, acc: 0.5312, auc: 0.6026, precision: 0.5385, recall: 0.1148\n",
      "2018-12-29T11:25:37.387695, step: 16, loss: 1.359337329864502, acc: 0.6016, auc: 0.6342, precision: 0.6842, recall: 0.2241\n",
      "2018-12-29T11:25:38.100024, step: 17, loss: 1.372207760810852, acc: 0.5703, auc: 0.6074, precision: 0.5882, recall: 0.1724\n",
      "2018-12-29T11:25:38.740547, step: 18, loss: 1.4335768222808838, acc: 0.5391, auc: 0.6663, precision: 0.6471, recall: 0.1719\n",
      "2018-12-29T11:25:39.468285, step: 19, loss: 1.3433926105499268, acc: 0.5625, auc: 0.6927, precision: 0.5, recall: 0.1429\n",
      "2018-12-29T11:25:40.102538, step: 20, loss: 1.3945485353469849, acc: 0.5312, auc: 0.6598, precision: 0.5714, recall: 0.1905\n",
      "2018-12-29T11:25:40.841992, step: 21, loss: 1.3474922180175781, acc: 0.625, auc: 0.7253, precision: 0.8947, recall: 0.2698\n",
      "2018-12-29T11:25:41.587377, step: 22, loss: 1.4327642917633057, acc: 0.5312, auc: 0.6401, precision: 0.5294, recall: 0.1475\n",
      "2018-12-29T11:25:42.316607, step: 23, loss: 1.4009697437286377, acc: 0.5156, auc: 0.676, precision: 0.6667, recall: 0.1765\n",
      "2018-12-29T11:25:43.021233, step: 24, loss: 1.359368085861206, acc: 0.5859, auc: 0.7092, precision: 0.8667, recall: 0.2031\n",
      "2018-12-29T11:25:43.695930, step: 25, loss: 1.3635761737823486, acc: 0.5469, auc: 0.7329, precision: 0.7, recall: 0.2121\n",
      "2018-12-29T11:25:44.377402, step: 26, loss: 1.3232775926589966, acc: 0.6484, auc: 0.7232, precision: 0.7619, recall: 0.2857\n",
      "2018-12-29T11:25:45.132960, step: 27, loss: 1.2757658958435059, acc: 0.7031, auc: 0.8443, precision: 0.9565, recall: 0.3729\n",
      "2018-12-29T11:25:45.840632, step: 28, loss: 1.3830595016479492, acc: 0.6094, auc: 0.6767, precision: 0.8421, recall: 0.254\n",
      "2018-12-29T11:25:46.512716, step: 29, loss: 1.3329941034317017, acc: 0.6016, auc: 0.7786, precision: 0.8276, recall: 0.3429\n",
      "2018-12-29T11:25:47.188221, step: 30, loss: 1.3154103755950928, acc: 0.6406, auc: 0.7281, precision: 0.9259, recall: 0.3623\n",
      "2018-12-29T11:25:47.868243, step: 31, loss: 1.341256022453308, acc: 0.6016, auc: 0.7373, precision: 0.8696, recall: 0.2941\n",
      "2018-12-29T11:25:48.615990, step: 32, loss: 1.320792555809021, acc: 0.6016, auc: 0.7463, precision: 0.8571, recall: 0.2727\n",
      "2018-12-29T11:25:49.248131, step: 33, loss: 1.3670017719268799, acc: 0.5938, auc: 0.7547, precision: 0.7857, recall: 0.3235\n",
      "2018-12-29T11:25:50.028277, step: 34, loss: 1.268103003501892, acc: 0.6719, auc: 0.8587, precision: 0.9286, recall: 0.3939\n",
      "2018-12-29T11:25:50.696132, step: 35, loss: 1.3366117477416992, acc: 0.625, auc: 0.7625, precision: 0.7667, recall: 0.3594\n",
      "2018-12-29T11:25:51.428179, step: 36, loss: 1.2743265628814697, acc: 0.7109, auc: 0.7964, precision: 0.8372, recall: 0.5455\n",
      "2018-12-29T11:25:52.140576, step: 37, loss: 1.3194432258605957, acc: 0.7109, auc: 0.7856, precision: 0.7955, recall: 0.5556\n",
      "2018-12-29T11:25:52.880781, step: 38, loss: 1.2554938793182373, acc: 0.6484, auc: 0.8413, precision: 0.8333, recall: 0.3846\n",
      "2018-12-29T11:25:53.527990, step: 39, loss: 1.2569010257720947, acc: 0.6953, auc: 0.8453, precision: 0.8636, recall: 0.5352\n",
      "2018-12-29T11:25:54.171023, step: 40, loss: 1.2637468576431274, acc: 0.6797, auc: 0.7961, precision: 0.7619, recall: 0.5079\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-29T11:25:54.810604, step: 41, loss: 1.1981995105743408, acc: 0.7812, auc: 0.9126, precision: 0.9091, recall: 0.625\n",
      "2018-12-29T11:25:55.524222, step: 42, loss: 1.3127529621124268, acc: 0.7031, auc: 0.8044, precision: 0.7872, recall: 0.5692\n",
      "2018-12-29T11:25:56.195082, step: 43, loss: 1.2615294456481934, acc: 0.6797, auc: 0.8284, precision: 0.7949, recall: 0.4844\n",
      "2018-12-29T11:25:56.916487, step: 44, loss: 1.2160320281982422, acc: 0.7266, auc: 0.8519, precision: 0.9286, recall: 0.4407\n",
      "2018-12-29T11:25:57.727582, step: 45, loss: 1.1920576095581055, acc: 0.7969, auc: 0.9079, precision: 0.8824, recall: 0.5769\n",
      "2018-12-29T11:25:58.464835, step: 46, loss: 1.3102998733520508, acc: 0.6328, auc: 0.8701, precision: 1.0, recall: 0.3896\n",
      "2018-12-29T11:25:59.175739, step: 47, loss: 1.2885699272155762, acc: 0.6328, auc: 0.8441, precision: 0.8966, recall: 0.3714\n",
      "2018-12-29T11:25:59.799921, step: 48, loss: 1.2007193565368652, acc: 0.7344, auc: 0.8955, precision: 0.9459, recall: 0.5224\n",
      "2018-12-29T11:26:00.575964, step: 49, loss: 1.3074872493743896, acc: 0.6484, auc: 0.8172, precision: 0.7576, recall: 0.4032\n",
      "2018-12-29T11:26:01.296663, step: 50, loss: 1.225319743156433, acc: 0.7812, auc: 0.8718, precision: 0.8909, recall: 0.6901\n",
      "2018-12-29T11:26:02.011763, step: 51, loss: 1.2927358150482178, acc: 0.7344, auc: 0.8377, precision: 0.7, recall: 0.7241\n",
      "2018-12-29T11:26:02.719842, step: 52, loss: 1.1400375366210938, acc: 0.8203, auc: 0.8911, precision: 0.9048, recall: 0.7703\n",
      "2018-12-29T11:26:03.464460, step: 53, loss: 1.1324948072433472, acc: 0.875, auc: 0.9389, precision: 0.9355, recall: 0.8286\n",
      "2018-12-29T11:26:04.193591, step: 54, loss: 1.1952999830245972, acc: 0.8047, auc: 0.894, precision: 0.8571, recall: 0.7385\n",
      "2018-12-29T11:26:04.912514, step: 55, loss: 1.21940016746521, acc: 0.7734, auc: 0.8725, precision: 0.8214, recall: 0.7077\n",
      "2018-12-29T11:26:05.660881, step: 56, loss: 1.226684331893921, acc: 0.7891, auc: 0.8651, precision: 0.7872, recall: 0.6852\n",
      "2018-12-29T11:26:06.382049, step: 57, loss: 1.2749605178833008, acc: 0.7188, auc: 0.8303, precision: 0.8372, recall: 0.5538\n",
      "2018-12-29T11:26:07.136408, step: 58, loss: 1.211984395980835, acc: 0.7031, auc: 0.8842, precision: 0.9, recall: 0.5143\n",
      "2018-12-29T11:26:07.828089, step: 59, loss: 1.0672476291656494, acc: 0.8281, auc: 0.9458, precision: 0.9318, recall: 0.6833\n",
      "2018-12-29T11:26:08.518567, step: 60, loss: 1.166893482208252, acc: 0.7969, auc: 0.8941, precision: 0.8478, recall: 0.6724\n",
      "2018-12-29T11:26:09.216218, step: 61, loss: 1.2358801364898682, acc: 0.7422, auc: 0.8642, precision: 0.875, recall: 0.6087\n",
      "2018-12-29T11:26:09.895636, step: 62, loss: 1.1660315990447998, acc: 0.7344, auc: 0.8671, precision: 0.8644, recall: 0.6623\n",
      "2018-12-29T11:26:10.561811, step: 63, loss: 1.2592778205871582, acc: 0.6875, auc: 0.8288, precision: 0.8148, recall: 0.5946\n",
      "2018-12-29T11:26:11.344951, step: 64, loss: 1.088164210319519, acc: 0.8828, auc: 0.9275, precision: 0.9016, recall: 0.8594\n",
      "2018-12-29T11:26:12.049055, step: 65, loss: 1.1342921257019043, acc: 0.8594, auc: 0.9069, precision: 0.8636, recall: 0.8636\n",
      "2018-12-29T11:26:12.791911, step: 66, loss: 1.1350888013839722, acc: 0.8047, auc: 0.8913, precision: 0.8136, recall: 0.7742\n",
      "2018-12-29T11:26:13.475992, step: 67, loss: 1.147634506225586, acc: 0.8047, auc: 0.891, precision: 0.8154, recall: 0.803\n",
      "2018-12-29T11:26:14.186284, step: 68, loss: 1.139814019203186, acc: 0.8203, auc: 0.9096, precision: 0.898, recall: 0.7097\n",
      "2018-12-29T11:26:14.853980, step: 69, loss: 1.125493049621582, acc: 0.8438, auc: 0.9053, precision: 0.86, recall: 0.7679\n",
      "2018-12-29T11:26:15.546504, step: 70, loss: 1.1262718439102173, acc: 0.8281, auc: 0.896, precision: 0.8868, recall: 0.746\n",
      "2018-12-29T11:26:16.248960, step: 71, loss: 1.070982813835144, acc: 0.8203, auc: 0.946, precision: 0.9792, recall: 0.6812\n",
      "2018-12-29T11:26:16.940590, step: 72, loss: 1.1460316181182861, acc: 0.7969, auc: 0.8987, precision: 0.8958, recall: 0.6719\n",
      "2018-12-29T11:26:17.579871, step: 73, loss: 1.1483913660049438, acc: 0.7891, auc: 0.8949, precision: 0.82, recall: 0.6949\n",
      "2018-12-29T11:26:18.297488, step: 74, loss: 1.0778827667236328, acc: 0.8125, auc: 0.9296, precision: 0.9, recall: 0.6429\n",
      "2018-12-29T11:26:19.004024, step: 75, loss: 1.1374391317367554, acc: 0.7891, auc: 0.9015, precision: 0.9024, recall: 0.6167\n",
      "2018-12-29T11:26:19.691591, step: 76, loss: 1.1126097440719604, acc: 0.7891, auc: 0.9088, precision: 0.8667, recall: 0.65\n",
      "2018-12-29T11:26:20.439667, step: 77, loss: 1.0524194240570068, acc: 0.7891, auc: 0.9353, precision: 0.875, recall: 0.6667\n",
      "2018-12-29T11:26:21.196355, step: 78, loss: 1.076711654663086, acc: 0.8281, auc: 0.9113, precision: 0.8333, recall: 0.7759\n",
      "2018-12-29T11:26:21.949545, step: 79, loss: 1.0139837265014648, acc: 0.8984, auc: 0.9574, precision: 0.9452, recall: 0.8846\n",
      "2018-12-29T11:26:22.629798, step: 80, loss: 1.0705971717834473, acc: 0.8594, auc: 0.9333, precision: 0.85, recall: 0.85\n",
      "2018-12-29T11:26:23.320354, step: 81, loss: 1.0018856525421143, acc: 0.8828, auc: 0.9443, precision: 0.942, recall: 0.8553\n",
      "2018-12-29T11:26:23.978005, step: 82, loss: 1.0945979356765747, acc: 0.8125, auc: 0.9202, precision: 0.8406, recall: 0.8169\n",
      "2018-12-29T11:26:24.761320, step: 83, loss: 1.0240399837493896, acc: 0.875, auc: 0.9442, precision: 0.9028, recall: 0.8784\n",
      "2018-12-29T11:26:25.527827, step: 84, loss: 1.1129995584487915, acc: 0.875, auc: 0.9129, precision: 0.8571, recall: 0.8571\n",
      "2018-12-29T11:26:26.167261, step: 85, loss: 1.2076585292816162, acc: 0.8516, auc: 0.8991, precision: 0.7692, recall: 0.9259\n",
      "2018-12-29T11:26:26.888401, step: 86, loss: 1.0857175588607788, acc: 0.8516, auc: 0.9267, precision: 0.9455, recall: 0.7647\n",
      "2018-12-29T11:26:27.596455, step: 87, loss: 1.1648869514465332, acc: 0.7812, auc: 0.8857, precision: 0.8679, recall: 0.6866\n",
      "2018-12-29T11:26:28.282515, step: 88, loss: 1.057469367980957, acc: 0.8516, auc: 0.9335, precision: 0.9333, recall: 0.7241\n",
      "2018-12-29T11:26:28.995657, step: 89, loss: 1.0566898584365845, acc: 0.8125, auc: 0.9365, precision: 0.9091, recall: 0.6667\n",
      "2018-12-29T11:26:29.701972, step: 90, loss: 1.1271626949310303, acc: 0.8281, auc: 0.9092, precision: 0.9592, recall: 0.7015\n",
      "2018-12-29T11:26:30.463744, step: 91, loss: 1.059477686882019, acc: 0.8281, auc: 0.9315, precision: 0.9444, recall: 0.7286\n",
      "2018-12-29T11:26:31.240054, step: 92, loss: 1.127495288848877, acc: 0.8281, auc: 0.8971, precision: 0.8333, recall: 0.7407\n",
      "2018-12-29T11:26:31.949257, step: 93, loss: 1.0770180225372314, acc: 0.8438, auc: 0.9223, precision: 0.8871, recall: 0.8088\n",
      "2018-12-29T11:26:32.673619, step: 94, loss: 1.032954216003418, acc: 0.8672, auc: 0.9255, precision: 0.8966, recall: 0.8254\n",
      "2018-12-29T11:26:33.403734, step: 95, loss: 1.0592782497406006, acc: 0.8828, auc: 0.9487, precision: 0.875, recall: 0.8889\n",
      "2018-12-29T11:26:34.102656, step: 96, loss: 1.1091171503067017, acc: 0.7969, auc: 0.9074, precision: 0.8413, recall: 0.7681\n",
      "2018-12-29T11:26:34.761796, step: 97, loss: 1.0882173776626587, acc: 0.8047, auc: 0.9109, precision: 0.8281, recall: 0.791\n",
      "2018-12-29T11:26:35.467424, step: 98, loss: 1.0686936378479004, acc: 0.8359, auc: 0.9249, precision: 0.8382, recall: 0.8507\n",
      "2018-12-29T11:26:36.221127, step: 99, loss: 1.0149579048156738, acc: 0.8828, auc: 0.9552, precision: 0.9057, recall: 0.8276\n",
      "2018-12-29T11:26:36.948422, step: 100, loss: 1.0420087575912476, acc: 0.875, auc: 0.9389, precision: 0.85, recall: 0.8793\n",
      "\n",
      "Evaluation:\n",
      "2018-12-29T11:27:05.254486, step: 100, loss: 1.0370795366011167, acc: 0.83635, auc: 0.9297526315789475, precision: 0.9107394736842104, recall: 0.7531210526315791\n",
      "2018-12-29T11:27:05.918801, step: 101, loss: 0.9878535866737366, acc: 0.875, auc: 0.9466, precision: 0.95, recall: 0.8143\n",
      "2018-12-29T11:27:06.629419, step: 102, loss: 1.223325252532959, acc: 0.7812, auc: 0.8524, precision: 0.7925, recall: 0.7119\n",
      "2018-12-29T11:27:07.329107, step: 103, loss: 1.0270872116088867, acc: 0.8203, auc: 0.9596, precision: 0.9512, recall: 0.65\n",
      "2018-12-29T11:27:08.039794, step: 104, loss: 1.0428690910339355, acc: 0.8438, auc: 0.9266, precision: 0.913, recall: 0.7241\n",
      "2018-12-29T11:27:08.783290, step: 105, loss: 1.0933517217636108, acc: 0.8125, auc: 0.9214, precision: 0.931, recall: 0.7297\n",
      "2018-12-29T11:27:09.435291, step: 106, loss: 0.9792746305465698, acc: 0.875, auc: 0.9563, precision: 0.9153, recall: 0.8308\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-29T11:27:10.163832, step: 107, loss: 1.06497061252594, acc: 0.8359, auc: 0.9115, precision: 0.8571, recall: 0.8182\n",
      "2018-12-29T11:27:10.880213, step: 108, loss: 1.1167004108428955, acc: 0.8594, auc: 0.9043, precision: 0.8947, recall: 0.8095\n",
      "2018-12-29T11:27:11.534325, step: 109, loss: 0.9497590065002441, acc: 0.8828, auc: 0.9611, precision: 0.9123, recall: 0.8387\n",
      "2018-12-29T11:27:12.216082, step: 110, loss: 1.0824297666549683, acc: 0.7969, auc: 0.9091, precision: 0.8308, recall: 0.7826\n",
      "2018-12-29T11:27:12.954587, step: 111, loss: 0.9668493866920471, acc: 0.8906, auc: 0.9548, precision: 0.9048, recall: 0.8769\n",
      "2018-12-29T11:27:13.691028, step: 112, loss: 1.1021671295166016, acc: 0.8125, auc: 0.9122, precision: 0.7458, recall: 0.8302\n",
      "2018-12-29T11:27:14.383268, step: 113, loss: 0.970019519329071, acc: 0.9062, auc: 0.9567, precision: 0.9455, recall: 0.8525\n",
      "2018-12-29T11:27:15.135760, step: 114, loss: 1.0011255741119385, acc: 0.8594, auc: 0.9244, precision: 0.9219, recall: 0.8194\n",
      "2018-12-29T11:27:15.876271, step: 115, loss: 1.0636460781097412, acc: 0.8203, auc: 0.9197, precision: 0.84, recall: 0.7368\n",
      "2018-12-29T11:27:16.592306, step: 116, loss: 0.9929778575897217, acc: 0.875, auc: 0.9343, precision: 0.9167, recall: 0.8333\n",
      "2018-12-29T11:27:17.370727, step: 117, loss: 1.0777384042739868, acc: 0.8281, auc: 0.9177, precision: 0.9091, recall: 0.6897\n",
      "2018-12-29T11:27:18.095729, step: 118, loss: 0.9856215715408325, acc: 0.8984, auc: 0.9492, precision: 0.9483, recall: 0.8462\n",
      "2018-12-29T11:27:18.783947, step: 119, loss: 1.140150547027588, acc: 0.7656, auc: 0.8755, precision: 0.8302, recall: 0.6769\n",
      "2018-12-29T11:27:19.440296, step: 120, loss: 1.0390145778656006, acc: 0.8594, auc: 0.9392, precision: 0.9123, recall: 0.8\n",
      "2018-12-29T11:27:20.167808, step: 121, loss: 0.9865484237670898, acc: 0.8438, auc: 0.9571, precision: 0.9615, recall: 0.7353\n",
      "2018-12-29T11:27:20.888176, step: 122, loss: 0.9747909903526306, acc: 0.8672, auc: 0.9312, precision: 0.8793, recall: 0.8361\n",
      "2018-12-29T11:27:21.606890, step: 123, loss: 1.112352967262268, acc: 0.8125, auc: 0.8873, precision: 0.8167, recall: 0.7903\n",
      "2018-12-29T11:27:22.285259, step: 124, loss: 0.9969594478607178, acc: 0.8594, auc: 0.9223, precision: 0.8906, recall: 0.8382\n",
      "2018-12-29T11:27:23.034896, step: 125, loss: 0.9927165508270264, acc: 0.8516, auc: 0.936, precision: 0.8814, recall: 0.8125\n",
      "2018-12-29T11:27:23.867241, step: 126, loss: 0.8918200135231018, acc: 0.875, auc: 0.9661, precision: 0.9153, recall: 0.8308\n",
      "2018-12-29T11:27:24.600406, step: 127, loss: 0.9461988210678101, acc: 0.875, auc: 0.9451, precision: 0.8971, recall: 0.8714\n",
      "2018-12-29T11:27:25.303690, step: 128, loss: 1.0002731084823608, acc: 0.8594, auc: 0.9318, precision: 0.8393, recall: 0.8393\n",
      "2018-12-29T11:27:26.039723, step: 129, loss: 1.0579993724822998, acc: 0.7969, auc: 0.9067, precision: 0.82, recall: 0.7069\n",
      "2018-12-29T11:27:26.836010, step: 130, loss: 1.0431766510009766, acc: 0.8125, auc: 0.9189, precision: 0.8868, recall: 0.7231\n",
      "2018-12-29T11:27:27.531740, step: 131, loss: 1.0266461372375488, acc: 0.7891, auc: 0.9316, precision: 0.8929, recall: 0.7042\n",
      "2018-12-29T11:27:28.292054, step: 132, loss: 1.0493299961090088, acc: 0.8281, auc: 0.9163, precision: 0.8814, recall: 0.7761\n",
      "2018-12-29T11:27:28.998160, step: 133, loss: 0.9536100625991821, acc: 0.8359, auc: 0.9368, precision: 0.9123, recall: 0.7647\n",
      "2018-12-29T11:27:29.709061, step: 134, loss: 1.1713099479675293, acc: 0.7656, auc: 0.8723, precision: 0.7869, recall: 0.7385\n",
      "2018-12-29T11:27:30.371883, step: 135, loss: 1.0097604990005493, acc: 0.8594, auc: 0.9198, precision: 0.9143, recall: 0.8421\n",
      "2018-12-29T11:27:31.136469, step: 136, loss: 1.024487018585205, acc: 0.8438, auc: 0.9312, precision: 0.8406, recall: 0.8657\n",
      "2018-12-29T11:27:31.863113, step: 137, loss: 0.922450065612793, acc: 0.8672, auc: 0.9435, precision: 0.8689, recall: 0.8548\n",
      "2018-12-29T11:27:32.588203, step: 138, loss: 1.0681548118591309, acc: 0.8125, auc: 0.8822, precision: 0.8065, recall: 0.8065\n",
      "2018-12-29T11:27:33.251137, step: 139, loss: 0.9094220399856567, acc: 0.8516, auc: 0.9217, precision: 0.8714, recall: 0.8592\n",
      "2018-12-29T11:27:33.993965, step: 140, loss: 0.9886504411697388, acc: 0.8594, auc: 0.9507, precision: 0.9, recall: 0.7759\n",
      "2018-12-29T11:27:34.710970, step: 141, loss: 0.9384813904762268, acc: 0.875, auc: 0.9396, precision: 0.9322, recall: 0.8209\n",
      "2018-12-29T11:27:35.447301, step: 142, loss: 0.9713053703308105, acc: 0.8359, auc: 0.9362, precision: 0.9464, recall: 0.7465\n",
      "2018-12-29T11:27:36.196133, step: 143, loss: 0.9148253202438354, acc: 0.8594, auc: 0.9464, precision: 0.8644, recall: 0.8361\n",
      "2018-12-29T11:27:36.903494, step: 144, loss: 0.9271124005317688, acc: 0.8984, auc: 0.9507, precision: 0.8478, recall: 0.8667\n",
      "2018-12-29T11:27:37.564491, step: 145, loss: 0.9373658895492554, acc: 0.8516, auc: 0.937, precision: 0.8814, recall: 0.8125\n",
      "2018-12-29T11:27:38.287983, step: 146, loss: 0.9369145631790161, acc: 0.8516, auc: 0.9417, precision: 0.88, recall: 0.7719\n",
      "2018-12-29T11:27:38.983040, step: 147, loss: 0.9582828283309937, acc: 0.8281, auc: 0.944, precision: 0.9302, recall: 0.678\n",
      "2018-12-29T11:27:39.694674, step: 148, loss: 0.9943444728851318, acc: 0.7812, auc: 0.8946, precision: 0.8409, recall: 0.6379\n",
      "2018-12-29T11:27:40.455456, step: 149, loss: 0.9130761623382568, acc: 0.8516, auc: 0.9483, precision: 0.9298, recall: 0.7794\n",
      "2018-12-29T11:27:41.183599, step: 150, loss: 0.9424872994422913, acc: 0.875, auc: 0.9507, precision: 0.8621, recall: 0.8621\n",
      "2018-12-29T11:27:41.915918, step: 151, loss: 0.9366401433944702, acc: 0.8281, auc: 0.9453, precision: 0.8958, recall: 0.7167\n",
      "2018-12-29T11:27:42.646483, step: 152, loss: 0.8730345368385315, acc: 0.8828, auc: 0.9603, precision: 0.8596, recall: 0.875\n",
      "2018-12-29T11:27:43.343463, step: 153, loss: 0.8249994516372681, acc: 0.9062, auc: 0.9663, precision: 1.0, recall: 0.8182\n",
      "2018-12-29T11:27:44.071530, step: 154, loss: 0.9888273477554321, acc: 0.8828, auc: 0.9404, precision: 0.8305, recall: 0.9074\n",
      "2018-12-29T11:27:44.836905, step: 155, loss: 0.7755321264266968, acc: 0.9141, auc: 0.9753, precision: 0.9062, recall: 0.9206\n",
      "start training model\n",
      "2018-12-29T11:27:45.559510, step: 156, loss: 0.9429579973220825, acc: 0.8203, auc: 0.9067, precision: 0.9259, recall: 0.7246\n",
      "2018-12-29T11:27:46.188093, step: 157, loss: 0.8289735317230225, acc: 0.9141, auc: 0.9678, precision: 0.942, recall: 0.9028\n",
      "2018-12-29T11:27:46.892343, step: 158, loss: 0.8103455901145935, acc: 0.8906, auc: 0.9665, precision: 0.9333, recall: 0.8485\n",
      "2018-12-29T11:27:47.588653, step: 159, loss: 0.8946840167045593, acc: 0.8438, auc: 0.9347, precision: 0.9516, recall: 0.7763\n",
      "2018-12-29T11:27:48.367229, step: 160, loss: 0.8571964502334595, acc: 0.8672, auc: 0.9596, precision: 0.9464, recall: 0.791\n",
      "2018-12-29T11:27:49.059430, step: 161, loss: 0.9507039785385132, acc: 0.8438, auc: 0.9157, precision: 0.8095, recall: 0.8644\n",
      "2018-12-29T11:27:49.774913, step: 162, loss: 0.8971050977706909, acc: 0.8906, auc: 0.9392, precision: 0.8929, recall: 0.8621\n",
      "2018-12-29T11:27:50.492018, step: 163, loss: 0.8189783692359924, acc: 0.9219, auc: 0.9623, precision: 0.9467, recall: 0.9221\n",
      "2018-12-29T11:27:51.232192, step: 164, loss: 0.9193650484085083, acc: 0.8594, auc: 0.9123, precision: 0.8906, recall: 0.8382\n",
      "2018-12-29T11:27:51.960142, step: 165, loss: 0.9064267873764038, acc: 0.8125, auc: 0.9259, precision: 0.8833, recall: 0.7571\n",
      "2018-12-29T11:27:52.637167, step: 166, loss: 0.8204906582832336, acc: 0.8438, auc: 0.9309, precision: 0.8971, recall: 0.8243\n",
      "2018-12-29T11:27:53.318611, step: 167, loss: 0.8840823769569397, acc: 0.8125, auc: 0.9431, precision: 0.8889, recall: 0.7273\n",
      "2018-12-29T11:27:53.986301, step: 168, loss: 0.7817932367324829, acc: 0.8594, auc: 0.9511, precision: 0.8889, recall: 0.8358\n",
      "2018-12-29T11:27:54.674144, step: 169, loss: 0.7599580883979797, acc: 0.9062, auc: 0.9597, precision: 0.9219, recall: 0.8939\n",
      "2018-12-29T11:27:55.400206, step: 170, loss: 0.7384743690490723, acc: 0.9219, auc: 0.9685, precision: 0.9412, recall: 0.9143\n",
      "2018-12-29T11:27:56.080791, step: 171, loss: 0.8154250383377075, acc: 0.8828, auc: 0.9505, precision: 0.8644, recall: 0.8793\n",
      "2018-12-29T11:27:56.724791, step: 172, loss: 0.9292895793914795, acc: 0.8125, auc: 0.8941, precision: 0.807, recall: 0.7797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-29T11:27:57.455440, step: 173, loss: 0.8252881765365601, acc: 0.8516, auc: 0.918, precision: 0.85, recall: 0.8361\n",
      "2018-12-29T11:27:58.188181, step: 174, loss: 0.9199938178062439, acc: 0.8359, auc: 0.925, precision: 0.9062, recall: 0.7945\n",
      "2018-12-29T11:27:58.929226, step: 175, loss: 0.7192668914794922, acc: 0.9062, auc: 0.9633, precision: 0.9074, recall: 0.875\n",
      "2018-12-29T11:27:59.627136, step: 176, loss: 0.8197203874588013, acc: 0.8125, auc: 0.9225, precision: 0.9375, recall: 0.6818\n",
      "2018-12-29T11:28:00.295595, step: 177, loss: 0.7617433071136475, acc: 0.8438, auc: 0.9117, precision: 0.8475, recall: 0.8197\n",
      "2018-12-29T11:28:01.027652, step: 178, loss: 0.8304474949836731, acc: 0.8438, auc: 0.9573, precision: 0.8824, recall: 0.7627\n",
      "2018-12-29T11:28:01.710981, step: 179, loss: 0.738858699798584, acc: 0.8281, auc: 0.9479, precision: 0.8679, recall: 0.7541\n",
      "2018-12-29T11:28:02.339048, step: 180, loss: 0.8300277590751648, acc: 0.8281, auc: 0.9396, precision: 0.8627, recall: 0.7458\n",
      "2018-12-29T11:28:03.047620, step: 181, loss: 0.8150252103805542, acc: 0.8594, auc: 0.9488, precision: 0.9038, recall: 0.7833\n",
      "2018-12-29T11:28:03.772538, step: 182, loss: 0.7508223056793213, acc: 0.8594, auc: 0.9473, precision: 0.8814, recall: 0.8254\n",
      "2018-12-29T11:28:04.511682, step: 183, loss: 0.7916419506072998, acc: 0.875, auc: 0.9553, precision: 0.9508, recall: 0.8169\n",
      "2018-12-29T11:28:05.314967, step: 184, loss: 0.7766319513320923, acc: 0.875, auc: 0.9648, precision: 0.8769, recall: 0.8769\n",
      "2018-12-29T11:28:06.010848, step: 185, loss: 0.7404700517654419, acc: 0.875, auc: 0.9507, precision: 0.9355, recall: 0.8286\n",
      "2018-12-29T11:28:06.771833, step: 186, loss: 0.6537479162216187, acc: 0.8594, auc: 0.9556, precision: 0.9259, recall: 0.7812\n",
      "2018-12-29T11:28:07.492264, step: 187, loss: 0.7037925124168396, acc: 0.875, auc: 0.937, precision: 0.8594, recall: 0.8871\n",
      "2018-12-29T11:28:08.151253, step: 188, loss: 0.7688455581665039, acc: 0.8594, auc: 0.9531, precision: 0.9091, recall: 0.8333\n",
      "2018-12-29T11:28:08.847486, step: 189, loss: 0.7791920900344849, acc: 0.8594, auc: 0.9437, precision: 0.8519, recall: 0.8214\n",
      "2018-12-29T11:28:09.531477, step: 190, loss: 0.853909969329834, acc: 0.8438, auc: 0.9352, precision: 0.8197, recall: 0.8475\n",
      "2018-12-29T11:28:10.222989, step: 191, loss: 0.8372411727905273, acc: 0.8594, auc: 0.9226, precision: 0.8966, recall: 0.8125\n",
      "2018-12-29T11:28:10.916597, step: 192, loss: 0.8211675882339478, acc: 0.8516, auc: 0.933, precision: 0.8649, recall: 0.8767\n",
      "2018-12-29T11:28:11.628732, step: 193, loss: 0.6430718302726746, acc: 0.9141, auc: 0.9666, precision: 0.9143, recall: 0.9275\n",
      "2018-12-29T11:28:12.366831, step: 194, loss: 0.6016651391983032, acc: 0.8594, auc: 0.9382, precision: 0.8714, recall: 0.8714\n",
      "2018-12-29T11:28:13.002473, step: 195, loss: 0.6805118322372437, acc: 0.8281, auc: 0.9323, precision: 0.8793, recall: 0.7727\n",
      "2018-12-29T11:28:13.662249, step: 196, loss: 0.6202207803726196, acc: 0.8516, auc: 0.9528, precision: 0.9216, recall: 0.7581\n",
      "2018-12-29T11:28:14.302824, step: 197, loss: 0.735047459602356, acc: 0.8594, auc: 0.9453, precision: 0.8947, recall: 0.8095\n",
      "2018-12-29T11:28:15.023737, step: 198, loss: 0.8117904663085938, acc: 0.8047, auc: 0.9328, precision: 0.8868, recall: 0.7121\n",
      "2018-12-29T11:28:15.710065, step: 199, loss: 0.5842477083206177, acc: 0.9375, auc: 0.982, precision: 1.0, recall: 0.8519\n",
      "2018-12-29T11:28:16.410959, step: 200, loss: 0.7245372533798218, acc: 0.875, auc: 0.9414, precision: 0.8871, recall: 0.8594\n",
      "\n",
      "Evaluation:\n",
      "2018-12-29T11:28:43.566208, step: 200, loss: 0.6479739484034086, acc: 0.855465789473684, auc: 0.9369394736842106, precision: 0.8962342105263156, recall: 0.8077026315789473\n",
      "2018-12-29T11:28:44.228608, step: 201, loss: 0.6271111369132996, acc: 0.8828, auc: 0.9602, precision: 0.9, recall: 0.8571\n",
      "2018-12-29T11:28:44.943698, step: 202, loss: 0.6548125743865967, acc: 0.875, auc: 0.9429, precision: 0.8889, recall: 0.8276\n",
      "2018-12-29T11:28:45.624130, step: 203, loss: 0.5897282361984253, acc: 0.8828, auc: 0.9527, precision: 0.9091, recall: 0.8333\n",
      "2018-12-29T11:28:46.354594, step: 204, loss: 0.7533381581306458, acc: 0.8984, auc: 0.9304, precision: 0.8727, recall: 0.8889\n",
      "2018-12-29T11:28:47.019604, step: 205, loss: 0.57590651512146, acc: 0.8594, auc: 0.9608, precision: 0.95, recall: 0.7917\n",
      "2018-12-29T11:28:47.739548, step: 206, loss: 0.6612141132354736, acc: 0.8125, auc: 0.915, precision: 0.875, recall: 0.7424\n",
      "2018-12-29T11:28:48.441870, step: 207, loss: 0.6328479647636414, acc: 0.8594, auc: 0.9575, precision: 0.96, recall: 0.75\n",
      "2018-12-29T11:28:49.179299, step: 208, loss: 0.7517979145050049, acc: 0.8281, auc: 0.8997, precision: 0.8421, recall: 0.7869\n",
      "2018-12-29T11:28:49.920471, step: 209, loss: 0.5411221981048584, acc: 0.8984, auc: 0.9647, precision: 0.9074, recall: 0.8596\n",
      "2018-12-29T11:28:50.602464, step: 210, loss: 0.6540431380271912, acc: 0.8672, auc: 0.9437, precision: 0.9, recall: 0.863\n",
      "2018-12-29T11:28:51.352430, step: 211, loss: 0.6515936851501465, acc: 0.8438, auc: 0.9065, precision: 0.8, recall: 0.8571\n",
      "2018-12-29T11:28:52.109915, step: 212, loss: 0.7043580412864685, acc: 0.8203, auc: 0.9113, precision: 0.8382, recall: 0.8261\n",
      "2018-12-29T11:28:52.851126, step: 213, loss: 0.6154844760894775, acc: 0.875, auc: 0.9624, precision: 0.875, recall: 0.875\n",
      "2018-12-29T11:28:53.510375, step: 214, loss: 0.48927080631256104, acc: 0.8984, auc: 0.9646, precision: 0.931, recall: 0.8571\n",
      "2018-12-29T11:28:54.249242, step: 215, loss: 0.4992271661758423, acc: 0.9219, auc: 0.9678, precision: 0.9206, recall: 0.9206\n",
      "2018-12-29T11:28:54.960093, step: 216, loss: 0.7038509845733643, acc: 0.7891, auc: 0.8942, precision: 0.8302, recall: 0.7097\n",
      "2018-12-29T11:28:55.668670, step: 217, loss: 0.567333459854126, acc: 0.8672, auc: 0.9421, precision: 0.9259, recall: 0.7937\n",
      "2018-12-29T11:28:56.407893, step: 218, loss: 0.6024163961410522, acc: 0.875, auc: 0.9379, precision: 0.9302, recall: 0.7547\n",
      "2018-12-29T11:28:57.159246, step: 219, loss: 0.6674352288246155, acc: 0.8906, auc: 0.9177, precision: 0.918, recall: 0.8615\n",
      "2018-12-29T11:28:57.827622, step: 220, loss: 0.644473135471344, acc: 0.7969, auc: 0.9363, precision: 0.8958, recall: 0.6719\n",
      "2018-12-29T11:28:58.579373, step: 221, loss: 0.5406320095062256, acc: 0.8906, auc: 0.9633, precision: 0.9333, recall: 0.8485\n",
      "2018-12-29T11:28:59.283159, step: 222, loss: 0.5432907938957214, acc: 0.875, auc: 0.9484, precision: 0.9322, recall: 0.8209\n",
      "2018-12-29T11:28:59.991033, step: 223, loss: 0.5900894403457642, acc: 0.9062, auc: 0.9504, precision: 0.8841, recall: 0.9385\n",
      "2018-12-29T11:29:00.733437, step: 224, loss: 0.5884836912155151, acc: 0.8906, auc: 0.9365, precision: 0.9038, recall: 0.8393\n",
      "2018-12-29T11:29:01.500661, step: 225, loss: 0.6178921461105347, acc: 0.8047, auc: 0.9095, precision: 0.8615, recall: 0.7778\n",
      "2018-12-29T11:29:02.263771, step: 226, loss: 0.636978030204773, acc: 0.8281, auc: 0.9209, precision: 0.8909, recall: 0.7538\n",
      "2018-12-29T11:29:03.044085, step: 227, loss: 0.53043133020401, acc: 0.8828, auc: 0.9539, precision: 0.9265, recall: 0.863\n",
      "2018-12-29T11:29:03.750885, step: 228, loss: 0.5919872522354126, acc: 0.8594, auc: 0.9402, precision: 0.7969, recall: 0.9107\n",
      "2018-12-29T11:29:04.447700, step: 229, loss: 0.589893639087677, acc: 0.8672, auc: 0.9316, precision: 0.8806, recall: 0.8676\n",
      "2018-12-29T11:29:05.148961, step: 230, loss: 0.41634100675582886, acc: 0.8906, auc: 0.9735, precision: 0.9091, recall: 0.8824\n",
      "2018-12-29T11:29:05.904504, step: 231, loss: 0.5299364328384399, acc: 0.8984, auc: 0.9408, precision: 0.95, recall: 0.8507\n",
      "2018-12-29T11:29:06.619666, step: 232, loss: 0.6668329834938049, acc: 0.8047, auc: 0.8948, precision: 0.7903, recall: 0.8033\n",
      "2018-12-29T11:29:07.363132, step: 233, loss: 0.4947383999824524, acc: 0.9062, auc: 0.9646, precision: 0.8955, recall: 0.9231\n",
      "2018-12-29T11:29:08.087769, step: 234, loss: 0.516891360282898, acc: 0.8281, auc: 0.9326, precision: 0.8387, recall: 0.8125\n",
      "2018-12-29T11:29:08.872246, step: 235, loss: 0.596439778804779, acc: 0.8359, auc: 0.9353, precision: 0.9535, recall: 0.6833\n",
      "2018-12-29T11:29:09.544666, step: 236, loss: 0.578965961933136, acc: 0.8203, auc: 0.9247, precision: 0.8525, recall: 0.7879\n",
      "2018-12-29T11:29:10.228157, step: 237, loss: 0.49928784370422363, acc: 0.8359, auc: 0.9468, precision: 0.8491, recall: 0.7759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-29T11:29:10.947932, step: 238, loss: 0.45562341809272766, acc: 0.8359, auc: 0.9331, precision: 0.8772, recall: 0.7812\n",
      "2018-12-29T11:29:11.659636, step: 239, loss: 0.4219127595424652, acc: 0.8672, auc: 0.9522, precision: 0.8909, recall: 0.8167\n",
      "2018-12-29T11:29:12.393164, step: 240, loss: 0.5501759052276611, acc: 0.8594, auc: 0.937, precision: 0.898, recall: 0.7719\n",
      "2018-12-29T11:29:13.134779, step: 241, loss: 0.5582970380783081, acc: 0.8516, auc: 0.9399, precision: 0.9107, recall: 0.7846\n",
      "2018-12-29T11:29:13.823727, step: 242, loss: 0.4210389256477356, acc: 0.8984, auc: 0.9759, precision: 0.9216, recall: 0.8393\n",
      "2018-12-29T11:29:14.504294, step: 243, loss: 0.599084198474884, acc: 0.8359, auc: 0.9339, precision: 0.9423, recall: 0.7313\n",
      "2018-12-29T11:29:15.187181, step: 244, loss: 0.4720156192779541, acc: 0.8672, auc: 0.959, precision: 0.9273, recall: 0.7969\n",
      "2018-12-29T11:29:15.844715, step: 245, loss: 0.49234265089035034, acc: 0.8828, auc: 0.9407, precision: 0.9077, recall: 0.8676\n",
      "2018-12-29T11:29:16.523661, step: 246, loss: 0.46133869886398315, acc: 0.8984, auc: 0.9535, precision: 0.9231, recall: 0.9114\n",
      "2018-12-29T11:29:17.283882, step: 247, loss: 0.4931469261646271, acc: 0.8672, auc: 0.9344, precision: 0.8548, recall: 0.8689\n",
      "2018-12-29T11:29:18.014922, step: 248, loss: 0.4291396737098694, acc: 0.8516, auc: 0.9481, precision: 0.8429, recall: 0.8806\n",
      "2018-12-29T11:29:18.720268, step: 249, loss: 0.45322996377944946, acc: 0.875, auc: 0.9395, precision: 0.9, recall: 0.8438\n",
      "2018-12-29T11:29:19.383106, step: 250, loss: 0.5046883821487427, acc: 0.8828, auc: 0.9567, precision: 0.8983, recall: 0.8548\n",
      "2018-12-29T11:29:20.114491, step: 251, loss: 0.45049160718917847, acc: 0.9062, auc: 0.9452, precision: 0.9104, recall: 0.9104\n",
      "2018-12-29T11:29:20.855797, step: 252, loss: 0.46146902441978455, acc: 0.8125, auc: 0.9172, precision: 0.807, recall: 0.7797\n",
      "2018-12-29T11:29:21.612620, step: 253, loss: 0.40664950013160706, acc: 0.8672, auc: 0.9536, precision: 0.9592, recall: 0.7581\n",
      "2018-12-29T11:29:22.328214, step: 254, loss: 0.4974796175956726, acc: 0.8359, auc: 0.922, precision: 0.8571, recall: 0.8182\n",
      "2018-12-29T11:29:22.999942, step: 255, loss: 0.49534308910369873, acc: 0.8359, auc: 0.9186, precision: 0.8154, recall: 0.8548\n",
      "2018-12-29T11:29:23.743742, step: 256, loss: 0.5366649627685547, acc: 0.8516, auc: 0.9241, precision: 0.8621, recall: 0.8197\n",
      "2018-12-29T11:29:24.432361, step: 257, loss: 0.49343520402908325, acc: 0.8516, auc: 0.9402, precision: 0.9833, recall: 0.7662\n",
      "2018-12-29T11:29:25.099357, step: 258, loss: 0.5211169123649597, acc: 0.7969, auc: 0.9282, precision: 0.92, recall: 0.6765\n",
      "2018-12-29T11:29:25.831947, step: 259, loss: 0.49873387813568115, acc: 0.8438, auc: 0.9167, precision: 0.9259, recall: 0.7576\n",
      "2018-12-29T11:29:26.515239, step: 260, loss: 0.5946881175041199, acc: 0.8516, auc: 0.907, precision: 0.7917, recall: 0.8085\n",
      "2018-12-29T11:29:27.239631, step: 261, loss: 0.4473644495010376, acc: 0.8984, auc: 0.9478, precision: 0.9661, recall: 0.8382\n",
      "2018-12-29T11:29:27.991322, step: 262, loss: 0.4114849865436554, acc: 0.8516, auc: 0.9308, precision: 0.8491, recall: 0.8036\n",
      "2018-12-29T11:29:28.660945, step: 263, loss: 0.5901920199394226, acc: 0.8125, auc: 0.9178, precision: 0.9394, recall: 0.7561\n",
      "2018-12-29T11:29:29.385946, step: 264, loss: 0.3465539813041687, acc: 0.8906, auc: 0.9631, precision: 0.8955, recall: 0.8955\n",
      "2018-12-29T11:29:30.093530, step: 265, loss: 0.4911819398403168, acc: 0.8438, auc: 0.9223, precision: 0.8889, recall: 0.7742\n",
      "2018-12-29T11:29:30.927892, step: 266, loss: 0.5215243101119995, acc: 0.8594, auc: 0.9409, precision: 0.8286, recall: 0.9062\n",
      "2018-12-29T11:29:31.651704, step: 267, loss: 0.3733794093132019, acc: 0.8672, auc: 0.9649, precision: 0.8448, recall: 0.8596\n",
      "2018-12-29T11:29:32.396627, step: 268, loss: 0.4457934498786926, acc: 0.8438, auc: 0.9328, precision: 0.8696, recall: 0.8451\n",
      "2018-12-29T11:29:33.147500, step: 269, loss: 0.45447903871536255, acc: 0.875, auc: 0.9516, precision: 0.8833, recall: 0.8548\n",
      "2018-12-29T11:29:33.860194, step: 270, loss: 0.4180870056152344, acc: 0.8516, auc: 0.9484, precision: 0.9038, recall: 0.7705\n",
      "2018-12-29T11:29:34.645647, step: 271, loss: 0.4479837417602539, acc: 0.875, auc: 0.9483, precision: 0.9483, recall: 0.8088\n",
      "2018-12-29T11:29:35.311201, step: 272, loss: 0.44651949405670166, acc: 0.8359, auc: 0.9206, precision: 0.8421, recall: 0.8\n",
      "2018-12-29T11:29:36.096466, step: 273, loss: 0.5284572839736938, acc: 0.8359, auc: 0.9113, precision: 0.8824, recall: 0.75\n",
      "2018-12-29T11:29:36.820134, step: 274, loss: 0.38915055990219116, acc: 0.8672, auc: 0.9582, precision: 0.9375, recall: 0.7627\n",
      "2018-12-29T11:29:37.496131, step: 275, loss: 0.3588967025279999, acc: 0.8906, auc: 0.9562, precision: 0.9184, recall: 0.8182\n",
      "2018-12-29T11:29:38.241903, step: 276, loss: 0.43709060549736023, acc: 0.8438, auc: 0.9267, precision: 0.8871, recall: 0.8088\n",
      "2018-12-29T11:29:38.978783, step: 277, loss: 0.3830699324607849, acc: 0.8516, auc: 0.9492, precision: 0.9074, recall: 0.7778\n",
      "2018-12-29T11:29:39.742256, step: 278, loss: 0.3689558506011963, acc: 0.875, auc: 0.9478, precision: 0.8939, recall: 0.8676\n",
      "2018-12-29T11:29:40.510898, step: 279, loss: 0.3800824284553528, acc: 0.8906, auc: 0.96, precision: 0.8525, recall: 0.9123\n",
      "2018-12-29T11:29:41.266601, step: 280, loss: 0.4609655439853668, acc: 0.8281, auc: 0.9071, precision: 0.8367, recall: 0.7455\n",
      "2018-12-29T11:29:42.060186, step: 281, loss: 0.4472658038139343, acc: 0.8594, auc: 0.9405, precision: 0.8627, recall: 0.8\n",
      "2018-12-29T11:29:42.802771, step: 282, loss: 0.3780699074268341, acc: 0.8516, auc: 0.9425, precision: 0.8676, recall: 0.8551\n",
      "2018-12-29T11:29:43.558332, step: 283, loss: 0.38025832176208496, acc: 0.875, auc: 0.9511, precision: 0.8833, recall: 0.8548\n",
      "2018-12-29T11:29:44.318905, step: 284, loss: 0.35620519518852234, acc: 0.9062, auc: 0.9485, precision: 0.9636, recall: 0.8413\n",
      "2018-12-29T11:29:45.051499, step: 285, loss: 0.36123034358024597, acc: 0.8906, auc: 0.9679, precision: 0.8596, recall: 0.8909\n",
      "2018-12-29T11:29:45.794061, step: 286, loss: 0.5518460869789124, acc: 0.8125, auc: 0.8836, precision: 0.8571, recall: 0.7826\n",
      "2018-12-29T11:29:46.568422, step: 287, loss: 0.4241921305656433, acc: 0.8672, auc: 0.9475, precision: 0.8852, recall: 0.8438\n",
      "2018-12-29T11:29:47.272873, step: 288, loss: 0.40572771430015564, acc: 0.875, auc: 0.9615, precision: 0.94, recall: 0.7833\n",
      "2018-12-29T11:29:47.972163, step: 289, loss: 0.32942259311676025, acc: 0.9297, auc: 0.9578, precision: 0.9355, recall: 0.9206\n",
      "2018-12-29T11:29:48.692381, step: 290, loss: 0.4379047751426697, acc: 0.8672, auc: 0.9417, precision: 0.9123, recall: 0.8125\n",
      "2018-12-29T11:29:49.479628, step: 291, loss: 0.4242587089538574, acc: 0.8516, auc: 0.9348, precision: 0.8852, recall: 0.8182\n",
      "2018-12-29T11:29:50.191717, step: 292, loss: 0.3290737569332123, acc: 0.9219, auc: 0.965, precision: 0.9423, recall: 0.875\n",
      "2018-12-29T11:29:50.934911, step: 293, loss: 0.4243002235889435, acc: 0.8281, auc: 0.9179, precision: 0.8167, recall: 0.8167\n",
      "2018-12-29T11:29:51.639371, step: 294, loss: 0.36432069540023804, acc: 0.8984, auc: 0.9556, precision: 0.9048, recall: 0.8906\n",
      "2018-12-29T11:29:52.371452, step: 295, loss: 0.5073206424713135, acc: 0.8203, auc: 0.9015, precision: 0.7593, recall: 0.8039\n",
      "2018-12-29T11:29:53.083339, step: 296, loss: 0.46489155292510986, acc: 0.8359, auc: 0.9292, precision: 0.8644, recall: 0.7969\n",
      "2018-12-29T11:29:53.840234, step: 297, loss: 0.41554486751556396, acc: 0.8594, auc: 0.9348, precision: 0.9355, recall: 0.8056\n",
      "2018-12-29T11:29:54.576052, step: 298, loss: 0.4308651387691498, acc: 0.8516, auc: 0.946, precision: 0.94, recall: 0.746\n",
      "2018-12-29T11:29:55.254267, step: 299, loss: 0.3484429717063904, acc: 0.8672, auc: 0.9689, precision: 0.9574, recall: 0.75\n",
      "2018-12-29T11:29:56.003103, step: 300, loss: 0.43147844076156616, acc: 0.8594, auc: 0.9362, precision: 0.9661, recall: 0.7808\n",
      "\n",
      "Evaluation:\n",
      "2018-12-29T11:30:23.258641, step: 300, loss: 0.37858848783530685, acc: 0.8680131578947368, auc: 0.9412552631578945, precision: 0.8590263157894737, recall: 0.8835184210526315\n",
      "2018-12-29T11:30:23.970199, step: 301, loss: 0.4401072561740875, acc: 0.8672, auc: 0.9411, precision: 0.8871, recall: 0.8462\n",
      "2018-12-29T11:30:24.710625, step: 302, loss: 0.47736918926239014, acc: 0.8594, auc: 0.9196, precision: 0.8393, recall: 0.8393\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-29T11:30:25.440127, step: 303, loss: 0.4258868396282196, acc: 0.8672, auc: 0.9506, precision: 0.8281, recall: 0.8983\n",
      "2018-12-29T11:30:26.159232, step: 304, loss: 0.42451977729797363, acc: 0.8672, auc: 0.9333, precision: 0.8871, recall: 0.8462\n",
      "2018-12-29T11:30:26.868744, step: 305, loss: 0.4321402311325073, acc: 0.875, auc: 0.937, precision: 0.9032, recall: 0.8485\n",
      "2018-12-29T11:30:27.652446, step: 306, loss: 0.46719440817832947, acc: 0.8047, auc: 0.9307, precision: 0.8929, recall: 0.7246\n",
      "2018-12-29T11:30:28.313798, step: 307, loss: 0.41515693068504333, acc: 0.8906, auc: 0.9484, precision: 0.8824, recall: 0.8491\n",
      "2018-12-29T11:30:28.990444, step: 308, loss: 0.563597559928894, acc: 0.7812, auc: 0.9071, precision: 0.8226, recall: 0.75\n",
      "2018-12-29T11:30:29.614762, step: 309, loss: 0.34434521198272705, acc: 0.8594, auc: 0.9535, precision: 0.9216, recall: 0.7705\n",
      "2018-12-29T11:30:30.327685, step: 310, loss: 0.42292100191116333, acc: 0.8438, auc: 0.9358, precision: 0.9, recall: 0.75\n",
      "start training model\n",
      "2018-12-29T11:30:31.048504, step: 311, loss: 0.4518040418624878, acc: 0.8672, auc: 0.9406, precision: 0.9623, recall: 0.7727\n",
      "2018-12-29T11:30:31.807158, step: 312, loss: 0.41940540075302124, acc: 0.8359, auc: 0.9352, precision: 0.8727, recall: 0.7742\n",
      "2018-12-29T11:30:32.516810, step: 313, loss: 0.4559866786003113, acc: 0.7969, auc: 0.9219, precision: 0.8723, recall: 0.6721\n",
      "2018-12-29T11:30:33.274847, step: 314, loss: 0.30559685826301575, acc: 0.8672, auc: 0.9599, precision: 0.8929, recall: 0.8197\n",
      "2018-12-29T11:30:33.994659, step: 315, loss: 0.4191547632217407, acc: 0.8594, auc: 0.9265, precision: 0.8936, recall: 0.7636\n",
      "2018-12-29T11:30:34.731428, step: 316, loss: 0.33138227462768555, acc: 0.875, auc: 0.9404, precision: 0.9178, recall: 0.8701\n",
      "2018-12-29T11:30:35.481816, step: 317, loss: 0.4982427954673767, acc: 0.8281, auc: 0.8952, precision: 0.8596, recall: 0.7778\n",
      "2018-12-29T11:30:36.212588, step: 318, loss: 0.4846257269382477, acc: 0.875, auc: 0.9461, precision: 0.9286, recall: 0.8553\n",
      "2018-12-29T11:30:36.908666, step: 319, loss: 0.30748996138572693, acc: 0.9219, auc: 0.9652, precision: 0.9032, recall: 0.9333\n",
      "2018-12-29T11:30:37.635094, step: 320, loss: 0.30951496958732605, acc: 0.9141, auc: 0.9679, precision: 0.8784, recall: 0.9701\n",
      "2018-12-29T11:30:38.326772, step: 321, loss: 0.3696300685405731, acc: 0.8906, auc: 0.9519, precision: 0.8788, recall: 0.9062\n",
      "2018-12-29T11:30:38.996144, step: 322, loss: 0.3050229549407959, acc: 0.8984, auc: 0.9601, precision: 0.9254, recall: 0.8857\n",
      "2018-12-29T11:30:39.703229, step: 323, loss: 0.2631435990333557, acc: 0.875, auc: 0.9709, precision: 0.9245, recall: 0.8033\n",
      "2018-12-29T11:30:40.355835, step: 324, loss: 0.32907935976982117, acc: 0.8594, auc: 0.9526, precision: 0.898, recall: 0.7719\n",
      "2018-12-29T11:30:41.027600, step: 325, loss: 0.3908103108406067, acc: 0.8594, auc: 0.944, precision: 0.9615, recall: 0.7576\n",
      "2018-12-29T11:30:41.783429, step: 326, loss: 0.3278782069683075, acc: 0.8516, auc: 0.9561, precision: 0.9804, recall: 0.7353\n",
      "2018-12-29T11:30:42.509666, step: 327, loss: 0.4175602197647095, acc: 0.8594, auc: 0.9323, precision: 0.9149, recall: 0.7544\n",
      "2018-12-29T11:30:43.263091, step: 328, loss: 0.2986815571784973, acc: 0.9219, auc: 0.9711, precision: 0.8983, recall: 0.9298\n",
      "2018-12-29T11:30:43.998745, step: 329, loss: 0.3344217538833618, acc: 0.8516, auc: 0.9519, precision: 0.9107, recall: 0.7846\n",
      "2018-12-29T11:30:44.728270, step: 330, loss: 0.34058764576911926, acc: 0.8672, auc: 0.9687, precision: 0.82, recall: 0.8367\n",
      "2018-12-29T11:30:45.410849, step: 331, loss: 0.35777774453163147, acc: 0.8438, auc: 0.937, precision: 0.8833, recall: 0.803\n",
      "2018-12-29T11:30:46.183754, step: 332, loss: 0.3168069124221802, acc: 0.9062, auc: 0.9568, precision: 0.9636, recall: 0.8413\n",
      "2018-12-29T11:30:46.936243, step: 333, loss: 0.38293159008026123, acc: 0.8438, auc: 0.9497, precision: 0.8723, recall: 0.7455\n",
      "2018-12-29T11:30:47.656133, step: 334, loss: 0.36134758591651917, acc: 0.8438, auc: 0.9362, precision: 0.881, recall: 0.7115\n",
      "2018-12-29T11:30:48.368750, step: 335, loss: 0.41393500566482544, acc: 0.8125, auc: 0.9206, precision: 0.8636, recall: 0.7917\n",
      "2018-12-29T11:30:49.074644, step: 336, loss: 0.40462034940719604, acc: 0.8672, auc: 0.935, precision: 0.8413, recall: 0.8833\n",
      "2018-12-29T11:30:49.767068, step: 337, loss: 0.2941783368587494, acc: 0.8984, auc: 0.9691, precision: 0.8448, recall: 0.9245\n",
      "2018-12-29T11:30:50.455681, step: 338, loss: 0.39143967628479004, acc: 0.8438, auc: 0.9352, precision: 0.8833, recall: 0.803\n",
      "2018-12-29T11:30:51.115455, step: 339, loss: 0.22996506094932556, acc: 0.8984, auc: 0.9797, precision: 0.95, recall: 0.8507\n",
      "2018-12-29T11:30:51.802866, step: 340, loss: 0.30960628390312195, acc: 0.8828, auc: 0.959, precision: 0.9375, recall: 0.8451\n",
      "2018-12-29T11:30:52.447445, step: 341, loss: 0.2991049587726593, acc: 0.8672, auc: 0.9563, precision: 0.92, recall: 0.7797\n",
      "2018-12-29T11:30:53.165051, step: 342, loss: 0.29289358854293823, acc: 0.9141, auc: 0.9705, precision: 0.9091, recall: 0.8929\n",
      "2018-12-29T11:30:53.855583, step: 343, loss: 0.2510780394077301, acc: 0.9062, auc: 0.9804, precision: 0.9298, recall: 0.8689\n",
      "2018-12-29T11:30:54.560385, step: 344, loss: 0.364304780960083, acc: 0.875, auc: 0.9442, precision: 0.9385, recall: 0.8356\n",
      "2018-12-29T11:30:55.312398, step: 345, loss: 0.30489325523376465, acc: 0.8828, auc: 0.9501, precision: 0.9074, recall: 0.8305\n",
      "2018-12-29T11:30:56.036339, step: 346, loss: 0.3632287383079529, acc: 0.8516, auc: 0.9393, precision: 0.9, recall: 0.7627\n",
      "2018-12-29T11:30:56.737621, step: 347, loss: 0.42496031522750854, acc: 0.8047, auc: 0.9165, precision: 0.8367, recall: 0.7069\n",
      "2018-12-29T11:30:57.476789, step: 348, loss: 0.30243512988090515, acc: 0.875, auc: 0.958, precision: 0.9104, recall: 0.8592\n",
      "2018-12-29T11:30:58.203086, step: 349, loss: 0.26740890741348267, acc: 0.9297, auc: 0.977, precision: 0.9118, recall: 0.9538\n",
      "2018-12-29T11:30:58.851643, step: 350, loss: 0.2753864824771881, acc: 0.9219, auc: 0.9617, precision: 0.9492, recall: 0.8889\n",
      "2018-12-29T11:30:59.603533, step: 351, loss: 0.2919871509075165, acc: 0.9062, auc: 0.9675, precision: 0.9074, recall: 0.875\n",
      "2018-12-29T11:31:00.348848, step: 352, loss: 0.2897264361381531, acc: 0.8906, auc: 0.9711, precision: 0.9219, recall: 0.8676\n",
      "2018-12-29T11:31:01.081829, step: 353, loss: 0.3385302424430847, acc: 0.875, auc: 0.9581, precision: 0.8889, recall: 0.8276\n",
      "2018-12-29T11:31:01.727595, step: 354, loss: 0.3641326129436493, acc: 0.8672, auc: 0.9523, precision: 0.9245, recall: 0.7903\n",
      "2018-12-29T11:31:02.438070, step: 355, loss: 0.32205748558044434, acc: 0.9141, auc: 0.9494, precision: 0.9231, recall: 0.8727\n",
      "2018-12-29T11:31:03.094478, step: 356, loss: 0.30586764216423035, acc: 0.8828, auc: 0.9594, precision: 0.8983, recall: 0.8548\n",
      "2018-12-29T11:31:03.783561, step: 357, loss: 0.3843953609466553, acc: 0.8516, auc: 0.943, precision: 0.9649, recall: 0.7639\n",
      "2018-12-29T11:31:04.531929, step: 358, loss: 0.3101499676704407, acc: 0.8984, auc: 0.9736, precision: 0.8871, recall: 0.9016\n",
      "2018-12-29T11:31:05.248532, step: 359, loss: 0.2206522524356842, acc: 0.9141, auc: 0.9807, precision: 0.9636, recall: 0.8548\n",
      "2018-12-29T11:31:05.871531, step: 360, loss: 0.40050211548805237, acc: 0.8438, auc: 0.9344, precision: 0.9153, recall: 0.7826\n",
      "2018-12-29T11:31:06.548324, step: 361, loss: 0.3311229944229126, acc: 0.8594, auc: 0.9421, precision: 0.8852, recall: 0.8308\n",
      "2018-12-29T11:31:07.289027, step: 362, loss: 0.27203279733657837, acc: 0.9375, auc: 0.9695, precision: 0.95, recall: 0.95\n",
      "2018-12-29T11:31:08.033624, step: 363, loss: 0.32507947087287903, acc: 0.8672, auc: 0.9498, precision: 0.9216, recall: 0.7833\n",
      "2018-12-29T11:31:08.770440, step: 364, loss: 0.30896997451782227, acc: 0.875, auc: 0.9602, precision: 0.8955, recall: 0.8696\n",
      "2018-12-29T11:31:09.476682, step: 365, loss: 0.36559033393859863, acc: 0.875, auc: 0.946, precision: 0.8636, recall: 0.8906\n",
      "2018-12-29T11:31:10.230766, step: 366, loss: 0.2919886112213135, acc: 0.8984, auc: 0.9699, precision: 0.9412, recall: 0.8767\n",
      "2018-12-29T11:31:10.956236, step: 367, loss: 0.3939644992351532, acc: 0.8516, auc: 0.9356, precision: 0.9032, recall: 0.8116\n",
      "2018-12-29T11:31:11.667274, step: 368, loss: 0.5142452120780945, acc: 0.8125, auc: 0.8879, precision: 0.8909, recall: 0.7313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-29T11:31:12.394879, step: 369, loss: 0.4882228970527649, acc: 0.875, auc: 0.918, precision: 0.918, recall: 0.8358\n",
      "2018-12-29T11:31:13.068557, step: 370, loss: 0.3717639446258545, acc: 0.875, auc: 0.9459, precision: 0.8571, recall: 0.8571\n",
      "2018-12-29T11:31:13.760617, step: 371, loss: 0.34284335374832153, acc: 0.8906, auc: 0.9415, precision: 0.9123, recall: 0.8525\n",
      "2018-12-29T11:31:14.502899, step: 372, loss: 0.38986504077911377, acc: 0.8906, auc: 0.958, precision: 0.9, recall: 0.871\n",
      "2018-12-29T11:31:15.280593, step: 373, loss: 0.35567042231559753, acc: 0.875, auc: 0.9401, precision: 0.9474, recall: 0.806\n",
      "2018-12-29T11:31:16.018266, step: 374, loss: 0.3302123546600342, acc: 0.8984, auc: 0.9551, precision: 0.9048, recall: 0.8906\n",
      "2018-12-29T11:31:16.671909, step: 375, loss: 0.27211856842041016, acc: 0.9062, auc: 0.9636, precision: 0.9355, recall: 0.8788\n",
      "2018-12-29T11:31:17.354823, step: 376, loss: 0.33506691455841064, acc: 0.8594, auc: 0.9596, precision: 0.9038, recall: 0.7833\n",
      "2018-12-29T11:31:18.055374, step: 377, loss: 0.2845861613750458, acc: 0.8984, auc: 0.9631, precision: 0.9091, recall: 0.8621\n",
      "2018-12-29T11:31:18.782794, step: 378, loss: 0.38870126008987427, acc: 0.8828, auc: 0.9311, precision: 0.9, recall: 0.8571\n",
      "2018-12-29T11:31:19.467044, step: 379, loss: 0.40559715032577515, acc: 0.8359, auc: 0.9358, precision: 0.8667, recall: 0.8\n",
      "2018-12-29T11:31:20.255375, step: 380, loss: 0.3248857855796814, acc: 0.8906, auc: 0.943, precision: 0.9344, recall: 0.8507\n",
      "2018-12-29T11:31:20.919066, step: 381, loss: 0.35571613907814026, acc: 0.8672, auc: 0.9487, precision: 0.9286, recall: 0.8\n",
      "2018-12-29T11:31:21.635932, step: 382, loss: 0.3688693046569824, acc: 0.8438, auc: 0.956, precision: 0.8909, recall: 0.7778\n",
      "2018-12-29T11:31:22.423703, step: 383, loss: 0.42903435230255127, acc: 0.8281, auc: 0.9345, precision: 0.9091, recall: 0.6897\n",
      "2018-12-29T11:31:23.143084, step: 384, loss: 0.34153443574905396, acc: 0.8828, auc: 0.9648, precision: 0.9623, recall: 0.7969\n",
      "2018-12-29T11:31:23.892719, step: 385, loss: 0.3079778552055359, acc: 0.8828, auc: 0.9601, precision: 0.9333, recall: 0.8358\n",
      "2018-12-29T11:31:24.608544, step: 386, loss: 0.28645551204681396, acc: 0.8984, auc: 0.9717, precision: 0.9032, recall: 0.8889\n",
      "2018-12-29T11:31:25.305026, step: 387, loss: 0.42112818360328674, acc: 0.8438, auc: 0.94, precision: 0.8955, recall: 0.8219\n",
      "2018-12-29T11:31:26.064587, step: 388, loss: 0.3246912956237793, acc: 0.9062, auc: 0.9602, precision: 0.8933, recall: 0.9437\n",
      "2018-12-29T11:31:26.784180, step: 389, loss: 0.3317142128944397, acc: 0.8438, auc: 0.9465, precision: 0.8095, recall: 0.8644\n",
      "2018-12-29T11:31:27.491635, step: 390, loss: 0.2944512963294983, acc: 0.8984, auc: 0.956, precision: 0.9483, recall: 0.8462\n",
      "2018-12-29T11:31:28.179306, step: 391, loss: 0.3499060273170471, acc: 0.8828, auc: 0.9532, precision: 0.9531, recall: 0.8356\n",
      "2018-12-29T11:31:28.841812, step: 392, loss: 0.4028809666633606, acc: 0.8516, auc: 0.9323, precision: 0.9155, recall: 0.8333\n",
      "2018-12-29T11:31:29.544186, step: 393, loss: 0.3825905919075012, acc: 0.8516, auc: 0.9426, precision: 0.8052, recall: 0.9394\n",
      "2018-12-29T11:31:30.282304, step: 394, loss: 0.32889309525489807, acc: 0.8828, auc: 0.9457, precision: 0.873, recall: 0.8871\n",
      "2018-12-29T11:31:30.954823, step: 395, loss: 0.3827281892299652, acc: 0.8516, auc: 0.9337, precision: 0.9322, recall: 0.7857\n",
      "2018-12-29T11:31:31.691015, step: 396, loss: 0.3912626802921295, acc: 0.8594, auc: 0.9343, precision: 0.9032, recall: 0.8235\n",
      "2018-12-29T11:31:32.425027, step: 397, loss: 0.4507269561290741, acc: 0.8516, auc: 0.9262, precision: 0.8868, recall: 0.7833\n",
      "2018-12-29T11:31:33.168705, step: 398, loss: 0.47638100385665894, acc: 0.8359, auc: 0.925, precision: 0.9655, recall: 0.7467\n",
      "2018-12-29T11:31:33.883131, step: 399, loss: 0.39191150665283203, acc: 0.8281, auc: 0.9333, precision: 0.7736, recall: 0.8039\n",
      "2018-12-29T11:31:34.542759, step: 400, loss: 0.37812912464141846, acc: 0.8359, auc: 0.9397, precision: 0.8806, recall: 0.8194\n",
      "\n",
      "Evaluation:\n",
      "2018-12-29T11:32:01.470832, step: 400, loss: 0.3371764170496087, acc: 0.8706868421052633, auc: 0.9452394736842104, precision: 0.879442105263158, recall: 0.8608\n",
      "2018-12-29T11:32:02.160035, step: 401, loss: 0.4163702428340912, acc: 0.8906, auc: 0.9528, precision: 0.9138, recall: 0.8548\n",
      "2018-12-29T11:32:02.863076, step: 402, loss: 0.3075177073478699, acc: 0.8984, auc: 0.9586, precision: 0.9, recall: 0.8852\n",
      "2018-12-29T11:32:03.599841, step: 403, loss: 0.3228810429573059, acc: 0.9141, auc: 0.9609, precision: 0.9074, recall: 0.8909\n",
      "2018-12-29T11:32:04.295818, step: 404, loss: 0.2780592441558838, acc: 0.8984, auc: 0.9708, precision: 0.9104, recall: 0.8971\n",
      "2018-12-29T11:32:05.032962, step: 405, loss: 0.28495967388153076, acc: 0.875, auc: 0.9684, precision: 0.9552, recall: 0.8312\n",
      "2018-12-29T11:32:05.796225, step: 406, loss: 0.28057625889778137, acc: 0.8906, auc: 0.9771, precision: 0.9516, recall: 0.8429\n",
      "2018-12-29T11:32:06.495975, step: 407, loss: 0.4086993336677551, acc: 0.875, auc: 0.933, precision: 0.9038, recall: 0.8103\n",
      "2018-12-29T11:32:07.202784, step: 408, loss: 0.38340693712234497, acc: 0.8672, auc: 0.9327, precision: 0.8621, recall: 0.8475\n",
      "2018-12-29T11:32:07.884433, step: 409, loss: 0.39139604568481445, acc: 0.875, auc: 0.9473, precision: 0.9231, recall: 0.8\n",
      "2018-12-29T11:32:08.568182, step: 410, loss: 0.2737715244293213, acc: 0.8906, auc: 0.962, precision: 0.8696, recall: 0.8333\n",
      "2018-12-29T11:32:09.288752, step: 411, loss: 0.3208320736885071, acc: 0.8672, auc: 0.9534, precision: 0.9259, recall: 0.7937\n",
      "2018-12-29T11:32:10.106853, step: 412, loss: 0.3951747417449951, acc: 0.8125, auc: 0.93, precision: 0.9216, recall: 0.7015\n",
      "2018-12-29T11:32:10.842926, step: 413, loss: 0.3931514322757721, acc: 0.8359, auc: 0.9348, precision: 0.8772, recall: 0.7812\n",
      "2018-12-29T11:32:11.539286, step: 414, loss: 0.3833277225494385, acc: 0.875, auc: 0.9533, precision: 0.9231, recall: 0.8451\n",
      "2018-12-29T11:32:12.298627, step: 415, loss: 0.352258563041687, acc: 0.8516, auc: 0.9459, precision: 0.8276, recall: 0.8421\n",
      "2018-12-29T11:32:12.975438, step: 416, loss: 0.3567333519458771, acc: 0.8984, auc: 0.9465, precision: 0.9464, recall: 0.8413\n",
      "2018-12-29T11:32:13.651061, step: 417, loss: 0.3417961597442627, acc: 0.8516, auc: 0.9387, precision: 0.8611, recall: 0.8732\n",
      "2018-12-29T11:32:14.299421, step: 418, loss: 0.2518039643764496, acc: 0.9062, auc: 0.9747, precision: 0.9559, recall: 0.8784\n",
      "2018-12-29T11:32:15.051040, step: 419, loss: 0.3954826295375824, acc: 0.8281, auc: 0.9289, precision: 0.8413, recall: 0.8154\n",
      "2018-12-29T11:32:15.703987, step: 420, loss: 0.3857201337814331, acc: 0.8984, auc: 0.9472, precision: 0.9077, recall: 0.8939\n",
      "2018-12-29T11:32:16.448661, step: 421, loss: 0.36938825249671936, acc: 0.8672, auc: 0.9341, precision: 0.8772, recall: 0.8333\n",
      "2018-12-29T11:32:17.223360, step: 422, loss: 0.3153018355369568, acc: 0.8906, auc: 0.9477, precision: 0.9153, recall: 0.8571\n",
      "2018-12-29T11:32:17.915755, step: 423, loss: 0.23606842756271362, acc: 0.9219, auc: 0.9799, precision: 0.9718, recall: 0.8961\n",
      "2018-12-29T11:32:18.576416, step: 424, loss: 0.313569039106369, acc: 0.875, auc: 0.9541, precision: 0.9138, recall: 0.8281\n",
      "2018-12-29T11:32:19.327897, step: 425, loss: 0.32556259632110596, acc: 0.875, auc: 0.9623, precision: 0.9348, recall: 0.7679\n",
      "2018-12-29T11:32:20.103966, step: 426, loss: 0.34023961424827576, acc: 0.9297, auc: 0.957, precision: 0.9444, recall: 0.8947\n",
      "2018-12-29T11:32:20.907527, step: 427, loss: 0.3019612431526184, acc: 0.8672, auc: 0.9516, precision: 0.871, recall: 0.8571\n",
      "2018-12-29T11:32:21.670777, step: 428, loss: 0.3967234194278717, acc: 0.8203, auc: 0.9374, precision: 0.9355, recall: 0.7532\n",
      "2018-12-29T11:32:22.320117, step: 429, loss: 0.2299816757440567, acc: 0.8906, auc: 0.9785, precision: 0.9464, recall: 0.8281\n",
      "2018-12-29T11:32:23.031035, step: 430, loss: 0.3851580023765564, acc: 0.8828, auc: 0.9442, precision: 0.9254, recall: 0.8611\n",
      "2018-12-29T11:32:23.725003, step: 431, loss: 0.31707683205604553, acc: 0.9297, auc: 0.9454, precision: 0.9412, recall: 0.8889\n",
      "2018-12-29T11:32:24.426813, step: 432, loss: 0.32819539308547974, acc: 0.8906, auc: 0.9557, precision: 0.8889, recall: 0.9143\n",
      "2018-12-29T11:32:25.140442, step: 433, loss: 0.3312932550907135, acc: 0.8594, auc: 0.9468, precision: 0.8571, recall: 0.8571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-29T11:32:25.806620, step: 434, loss: 0.31519901752471924, acc: 0.8906, auc: 0.9546, precision: 0.9661, recall: 0.8261\n",
      "2018-12-29T11:32:26.488664, step: 435, loss: 0.3918478488922119, acc: 0.8516, auc: 0.9259, precision: 0.8429, recall: 0.8806\n",
      "2018-12-29T11:32:27.184284, step: 436, loss: 0.3321840763092041, acc: 0.8672, auc: 0.946, precision: 0.9107, recall: 0.8095\n",
      "2018-12-29T11:32:27.936002, step: 437, loss: 0.2854544222354889, acc: 0.9141, auc: 0.9758, precision: 0.9074, recall: 0.8909\n",
      "2018-12-29T11:32:28.667456, step: 438, loss: 0.3089566230773926, acc: 0.8828, auc: 0.9549, precision: 0.9388, recall: 0.7931\n",
      "2018-12-29T11:32:29.383019, step: 439, loss: 0.41165462136268616, acc: 0.8594, auc: 0.9297, precision: 0.92, recall: 0.7667\n",
      "2018-12-29T11:32:30.148149, step: 440, loss: 0.3050745129585266, acc: 0.8828, auc: 0.949, precision: 0.9057, recall: 0.8276\n",
      "2018-12-29T11:32:30.871814, step: 441, loss: 0.41290482878685, acc: 0.8281, auc: 0.9306, precision: 0.9298, recall: 0.7465\n",
      "2018-12-29T11:32:31.611492, step: 442, loss: 0.33158260583877563, acc: 0.8828, auc: 0.9526, precision: 0.8636, recall: 0.9048\n",
      "2018-12-29T11:32:32.352327, step: 443, loss: 0.34178635478019714, acc: 0.8906, auc: 0.9587, precision: 0.918, recall: 0.8615\n",
      "2018-12-29T11:32:33.023737, step: 444, loss: 0.3021642565727234, acc: 0.875, auc: 0.9539, precision: 0.8971, recall: 0.8714\n",
      "2018-12-29T11:32:33.728400, step: 445, loss: 0.2172859162092209, acc: 0.9297, auc: 0.9867, precision: 0.9839, recall: 0.8841\n",
      "2018-12-29T11:32:34.364372, step: 446, loss: 0.3025835454463959, acc: 0.8672, auc: 0.9579, precision: 0.8841, recall: 0.8714\n",
      "2018-12-29T11:32:35.019396, step: 447, loss: 0.3665798604488373, acc: 0.8906, auc: 0.9416, precision: 0.9074, recall: 0.8448\n",
      "2018-12-29T11:32:35.738063, step: 448, loss: 0.3088955581188202, acc: 0.8828, auc: 0.9509, precision: 0.8788, recall: 0.8923\n",
      "2018-12-29T11:32:36.396532, step: 449, loss: 0.32123830914497375, acc: 0.9141, auc: 0.9594, precision: 0.9048, recall: 0.9194\n",
      "2018-12-29T11:32:37.120523, step: 450, loss: 0.29204297065734863, acc: 0.9062, auc: 0.9676, precision: 0.9167, recall: 0.8462\n",
      "2018-12-29T11:32:37.860469, step: 451, loss: 0.5145390033721924, acc: 0.8281, auc: 0.9256, precision: 0.8947, recall: 0.7612\n",
      "2018-12-29T11:32:38.570432, step: 452, loss: 0.3439660370349884, acc: 0.8672, auc: 0.9555, precision: 0.9298, recall: 0.803\n",
      "2018-12-29T11:32:39.288458, step: 453, loss: 0.3224811553955078, acc: 0.8828, auc: 0.9534, precision: 0.8909, recall: 0.8448\n",
      "2018-12-29T11:32:40.051435, step: 454, loss: 0.37439027428627014, acc: 0.8359, auc: 0.9225, precision: 0.8596, recall: 0.7903\n",
      "2018-12-29T11:32:40.769417, step: 455, loss: 0.29374539852142334, acc: 0.8984, auc: 0.97, precision: 0.9385, recall: 0.8714\n",
      "2018-12-29T11:32:41.540651, step: 456, loss: 0.4735279679298401, acc: 0.8438, auc: 0.9273, precision: 0.9123, recall: 0.7761\n",
      "2018-12-29T11:32:42.267863, step: 457, loss: 0.22469067573547363, acc: 0.9219, auc: 0.978, precision: 0.9608, recall: 0.8596\n",
      "2018-12-29T11:32:42.965599, step: 458, loss: 0.3830142319202423, acc: 0.8125, auc: 0.9404, precision: 0.9, recall: 0.7031\n",
      "2018-12-29T11:32:43.683804, step: 459, loss: 0.3736015260219574, acc: 0.8438, auc: 0.9425, precision: 0.8431, recall: 0.7818\n",
      "2018-12-29T11:32:44.351978, step: 460, loss: 0.34684866666793823, acc: 0.8672, auc: 0.9541, precision: 0.8261, recall: 0.9194\n",
      "2018-12-29T11:32:45.067393, step: 461, loss: 0.33006376028060913, acc: 0.8828, auc: 0.9353, precision: 0.9153, recall: 0.8438\n",
      "2018-12-29T11:32:45.715574, step: 462, loss: 0.3063316345214844, acc: 0.8672, auc: 0.9645, precision: 0.9508, recall: 0.8056\n",
      "2018-12-29T11:32:46.475116, step: 463, loss: 0.38900721073150635, acc: 0.8828, auc: 0.9388, precision: 0.9333, recall: 0.8358\n",
      "2018-12-29T11:32:47.141985, step: 464, loss: 0.39521586894989014, acc: 0.8906, auc: 0.9469, precision: 0.9206, recall: 0.8657\n",
      "2018-12-29T11:32:47.774163, step: 465, loss: 0.41156765818595886, acc: 0.875, auc: 0.9513, precision: 0.7857, recall: 0.9167\n",
      "start training model\n",
      "2018-12-29T11:32:48.477347, step: 466, loss: 0.3422580659389496, acc: 0.8672, auc: 0.9494, precision: 0.8824, recall: 0.8036\n",
      "2018-12-29T11:32:49.119528, step: 467, loss: 0.25641095638275146, acc: 0.9219, auc: 0.9746, precision: 0.9524, recall: 0.8955\n",
      "2018-12-29T11:32:49.822671, step: 468, loss: 0.2548898458480835, acc: 0.9141, auc: 0.9685, precision: 0.9474, recall: 0.871\n",
      "2018-12-29T11:32:50.528705, step: 469, loss: 0.4022406339645386, acc: 0.8594, auc: 0.9339, precision: 0.9388, recall: 0.7541\n",
      "2018-12-29T11:32:51.228030, step: 470, loss: 0.31791776418685913, acc: 0.8984, auc: 0.9426, precision: 0.9167, recall: 0.8302\n",
      "2018-12-29T11:32:51.931728, step: 471, loss: 0.28208279609680176, acc: 0.9297, auc: 0.9778, precision: 0.9844, recall: 0.8873\n",
      "2018-12-29T11:32:52.623052, step: 472, loss: 0.24391424655914307, acc: 0.9219, auc: 0.9749, precision: 0.9412, recall: 0.9143\n",
      "2018-12-29T11:32:53.269316, step: 473, loss: 0.3314470946788788, acc: 0.8984, auc: 0.9524, precision: 0.9545, recall: 0.863\n",
      "2018-12-29T11:32:53.976591, step: 474, loss: 0.3249693810939789, acc: 0.875, auc: 0.943, precision: 0.8909, recall: 0.8305\n",
      "2018-12-29T11:32:54.672746, step: 475, loss: 0.3046814501285553, acc: 0.8828, auc: 0.962, precision: 0.9091, recall: 0.8333\n",
      "2018-12-29T11:32:55.372020, step: 476, loss: 0.29977190494537354, acc: 0.9062, auc: 0.9694, precision: 0.8704, recall: 0.9038\n",
      "2018-12-29T11:32:56.108497, step: 477, loss: 0.33250999450683594, acc: 0.8359, auc: 0.9531, precision: 0.8103, recall: 0.8246\n",
      "2018-12-29T11:32:56.810565, step: 478, loss: 0.26086416840553284, acc: 0.9297, auc: 0.967, precision: 0.9394, recall: 0.9254\n",
      "2018-12-29T11:32:57.503521, step: 479, loss: 0.2272011935710907, acc: 0.9062, auc: 0.9858, precision: 0.9672, recall: 0.8551\n",
      "2018-12-29T11:32:58.309993, step: 480, loss: 0.3091204762458801, acc: 0.9062, auc: 0.9663, precision: 0.9322, recall: 0.873\n",
      "2018-12-29T11:32:58.988754, step: 481, loss: 0.33568274974823, acc: 0.875, auc: 0.9587, precision: 0.8772, recall: 0.8475\n",
      "2018-12-29T11:32:59.614339, step: 482, loss: 0.30422019958496094, acc: 0.9219, auc: 0.9632, precision: 0.94, recall: 0.8704\n",
      "2018-12-29T11:33:00.331768, step: 483, loss: 0.27677851915359497, acc: 0.8906, auc: 0.967, precision: 0.9286, recall: 0.8387\n",
      "2018-12-29T11:33:01.019981, step: 484, loss: 0.15463700890541077, acc: 0.9453, auc: 0.9943, precision: 1.0, recall: 0.8793\n",
      "2018-12-29T11:33:01.702886, step: 485, loss: 0.36098387837409973, acc: 0.875, auc: 0.9485, precision: 0.8971, recall: 0.8714\n",
      "2018-12-29T11:33:02.373921, step: 486, loss: 0.29725801944732666, acc: 0.8906, auc: 0.9563, precision: 0.918, recall: 0.8615\n",
      "2018-12-29T11:33:02.999801, step: 487, loss: 0.3459111750125885, acc: 0.9062, auc: 0.9427, precision: 0.8909, recall: 0.8909\n",
      "2018-12-29T11:33:03.727043, step: 488, loss: 0.3364393711090088, acc: 0.8984, auc: 0.946, precision: 0.9016, recall: 0.8871\n",
      "2018-12-29T11:33:04.386777, step: 489, loss: 0.2541953921318054, acc: 0.9062, auc: 0.9781, precision: 0.9577, recall: 0.8831\n",
      "2018-12-29T11:33:05.076210, step: 490, loss: 0.31280389428138733, acc: 0.8984, auc: 0.9538, precision: 0.9623, recall: 0.8226\n",
      "2018-12-29T11:33:05.808236, step: 491, loss: 0.20168395340442657, acc: 0.9062, auc: 0.9834, precision: 0.9815, recall: 0.8281\n",
      "2018-12-29T11:33:06.514444, step: 492, loss: 0.3093830943107605, acc: 0.8828, auc: 0.9631, precision: 0.9804, recall: 0.7812\n",
      "2018-12-29T11:33:07.228492, step: 493, loss: 0.4246276617050171, acc: 0.875, auc: 0.9336, precision: 0.9016, recall: 0.8462\n",
      "2018-12-29T11:33:07.910431, step: 494, loss: 0.29371702671051025, acc: 0.8828, auc: 0.9575, precision: 0.9444, recall: 0.8095\n",
      "2018-12-29T11:33:08.599372, step: 495, loss: 0.2627587914466858, acc: 0.8984, auc: 0.9812, precision: 0.8333, recall: 0.9649\n",
      "2018-12-29T11:33:09.311656, step: 496, loss: 0.24587716162204742, acc: 0.9062, auc: 0.9721, precision: 0.9355, recall: 0.8788\n",
      "2018-12-29T11:33:10.008657, step: 497, loss: 0.2927905023097992, acc: 0.8828, auc: 0.9646, precision: 0.8551, recall: 0.9219\n",
      "2018-12-29T11:33:10.724270, step: 498, loss: 0.3075374960899353, acc: 0.8828, auc: 0.959, precision: 0.8621, recall: 0.8772\n",
      "2018-12-29T11:33:11.360040, step: 499, loss: 0.3106034994125366, acc: 0.8672, auc: 0.951, precision: 0.918, recall: 0.8235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-29T11:33:12.095296, step: 500, loss: 0.28256040811538696, acc: 0.8984, auc: 0.9604, precision: 0.9322, recall: 0.8594\n",
      "\n",
      "Evaluation:\n",
      "2018-12-29T11:33:39.057464, step: 500, loss: 0.33821701297634527, acc: 0.8729394736842103, auc: 0.9466710526315791, precision: 0.8974368421052632, recall: 0.8455631578947368\n",
      "2018-12-29T11:33:39.703166, step: 501, loss: 0.2845889925956726, acc: 0.8906, auc: 0.9618, precision: 0.9259, recall: 0.8333\n",
      "2018-12-29T11:33:40.427641, step: 502, loss: 0.34236016869544983, acc: 0.875, auc: 0.9441, precision: 0.9286, recall: 0.8125\n",
      "2018-12-29T11:33:41.122354, step: 503, loss: 0.3332943320274353, acc: 0.8906, auc: 0.9577, precision: 0.8704, recall: 0.8704\n",
      "2018-12-29T11:33:41.873078, step: 504, loss: 0.32223227620124817, acc: 0.8672, auc: 0.9465, precision: 0.8889, recall: 0.8136\n",
      "2018-12-29T11:33:42.583433, step: 505, loss: 0.3783959448337555, acc: 0.8828, auc: 0.9422, precision: 0.898, recall: 0.8148\n",
      "2018-12-29T11:33:43.250335, step: 506, loss: 0.242227703332901, acc: 0.8828, auc: 0.9778, precision: 0.9636, recall: 0.803\n",
      "2018-12-29T11:33:43.950087, step: 507, loss: 0.3504135310649872, acc: 0.8672, auc: 0.9624, precision: 0.9444, recall: 0.7846\n",
      "2018-12-29T11:33:44.636565, step: 508, loss: 0.2412852793931961, acc: 0.9141, auc: 0.9745, precision: 0.9153, recall: 0.9\n",
      "2018-12-29T11:33:45.385086, step: 509, loss: 0.2246168553829193, acc: 0.9141, auc: 0.9759, precision: 0.9273, recall: 0.8793\n",
      "2018-12-29T11:33:46.111714, step: 510, loss: 0.29402780532836914, acc: 0.8594, auc: 0.9582, precision: 0.918, recall: 0.8116\n",
      "2018-12-29T11:33:46.864280, step: 511, loss: 0.24542273581027985, acc: 0.9219, auc: 0.9711, precision: 0.963, recall: 0.8667\n",
      "2018-12-29T11:33:47.611462, step: 512, loss: 0.27329221367836, acc: 0.9297, auc: 0.9648, precision: 0.9344, recall: 0.9194\n",
      "2018-12-29T11:33:48.327740, step: 513, loss: 0.27816399931907654, acc: 0.9297, auc: 0.956, precision: 0.9655, recall: 0.8889\n",
      "2018-12-29T11:33:49.056099, step: 514, loss: 0.3714415431022644, acc: 0.8281, auc: 0.9295, precision: 0.8421, recall: 0.7869\n",
      "2018-12-29T11:33:49.802979, step: 515, loss: 0.34400373697280884, acc: 0.8984, auc: 0.9473, precision: 0.913, recall: 0.9\n",
      "2018-12-29T11:33:50.491057, step: 516, loss: 0.3232930898666382, acc: 0.8984, auc: 0.9476, precision: 0.8857, recall: 0.9254\n",
      "2018-12-29T11:33:51.218454, step: 517, loss: 0.3614496886730194, acc: 0.8672, auc: 0.934, precision: 0.9104, recall: 0.8472\n",
      "2018-12-29T11:33:51.900549, step: 518, loss: 0.28003188967704773, acc: 0.8906, auc: 0.9566, precision: 0.8966, recall: 0.8667\n",
      "2018-12-29T11:33:52.622512, step: 519, loss: 0.23559634387493134, acc: 0.9219, auc: 0.9712, precision: 0.9516, recall: 0.8939\n",
      "2018-12-29T11:33:53.339384, step: 520, loss: 0.2547140121459961, acc: 0.8906, auc: 0.9631, precision: 0.931, recall: 0.8438\n",
      "2018-12-29T11:33:54.139568, step: 521, loss: 0.4293912351131439, acc: 0.8125, auc: 0.9282, precision: 0.9231, recall: 0.7059\n",
      "2018-12-29T11:33:54.842994, step: 522, loss: 0.236345112323761, acc: 0.8984, auc: 0.9722, precision: 0.9574, recall: 0.8036\n",
      "2018-12-29T11:33:55.596019, step: 523, loss: 0.2980666756629944, acc: 0.8906, auc: 0.9634, precision: 0.9623, recall: 0.8095\n",
      "2018-12-29T11:33:56.300110, step: 524, loss: 0.32612717151641846, acc: 0.9141, auc: 0.9653, precision: 0.9273, recall: 0.8793\n",
      "2018-12-29T11:33:57.058643, step: 525, loss: 0.2788635194301605, acc: 0.8594, auc: 0.9555, precision: 0.8793, recall: 0.8226\n",
      "2018-12-29T11:33:57.763538, step: 526, loss: 0.27570676803588867, acc: 0.8594, auc: 0.9635, precision: 0.9655, recall: 0.7778\n",
      "2018-12-29T11:33:58.539892, step: 527, loss: 0.4238818287849426, acc: 0.8672, auc: 0.9261, precision: 0.8254, recall: 0.8966\n",
      "2018-12-29T11:33:59.267341, step: 528, loss: 0.27051404118537903, acc: 0.8828, auc: 0.9685, precision: 0.8923, recall: 0.8788\n",
      "2018-12-29T11:33:59.945355, step: 529, loss: 0.3118276000022888, acc: 0.8828, auc: 0.9497, precision: 0.875, recall: 0.8889\n",
      "2018-12-29T11:34:00.646883, step: 530, loss: 0.2921699583530426, acc: 0.9141, auc: 0.9585, precision: 0.9054, recall: 0.9437\n",
      "2018-12-29T11:34:01.455059, step: 531, loss: 0.2922031581401825, acc: 0.8984, auc: 0.9612, precision: 0.8906, recall: 0.9048\n",
      "2018-12-29T11:34:02.229329, step: 532, loss: 0.4291330575942993, acc: 0.8359, auc: 0.9195, precision: 0.8966, recall: 0.7761\n",
      "2018-12-29T11:34:02.955257, step: 533, loss: 0.30127787590026855, acc: 0.8828, auc: 0.9531, precision: 0.9636, recall: 0.803\n",
      "2018-12-29T11:34:03.602349, step: 534, loss: 0.4167836010456085, acc: 0.8281, auc: 0.9053, precision: 0.8475, recall: 0.7937\n",
      "2018-12-29T11:34:04.352988, step: 535, loss: 0.3589453101158142, acc: 0.8672, auc: 0.9537, precision: 0.9828, recall: 0.7808\n",
      "2018-12-29T11:34:05.039769, step: 536, loss: 0.25703614950180054, acc: 0.9141, auc: 0.9671, precision: 0.9394, recall: 0.8986\n",
      "2018-12-29T11:34:05.751042, step: 537, loss: 0.3512604534626007, acc: 0.875, auc: 0.9392, precision: 0.873, recall: 0.873\n",
      "2018-12-29T11:34:06.471592, step: 538, loss: 0.3474676311016083, acc: 0.8906, auc: 0.9431, precision: 0.8824, recall: 0.9091\n",
      "2018-12-29T11:34:07.197097, step: 539, loss: 0.33916348218917847, acc: 0.8594, auc: 0.9467, precision: 0.875, recall: 0.8485\n",
      "2018-12-29T11:34:07.934977, step: 540, loss: 0.2920534908771515, acc: 0.8984, auc: 0.9517, precision: 0.9365, recall: 0.8676\n",
      "2018-12-29T11:34:08.655945, step: 541, loss: 0.4091408848762512, acc: 0.8125, auc: 0.9328, precision: 0.8871, recall: 0.7639\n",
      "2018-12-29T11:34:09.392412, step: 542, loss: 0.31153517961502075, acc: 0.8828, auc: 0.9453, precision: 0.8852, recall: 0.871\n",
      "2018-12-29T11:34:10.179793, step: 543, loss: 0.2623249292373657, acc: 0.8438, auc: 0.963, precision: 0.8939, recall: 0.8194\n",
      "2018-12-29T11:34:10.911414, step: 544, loss: 0.38137519359588623, acc: 0.875, auc: 0.9375, precision: 0.875, recall: 0.875\n",
      "2018-12-29T11:34:11.635617, step: 545, loss: 0.3964841365814209, acc: 0.8438, auc: 0.9249, precision: 0.8983, recall: 0.791\n",
      "2018-12-29T11:34:12.278099, step: 546, loss: 0.2919340133666992, acc: 0.8672, auc: 0.9501, precision: 0.9508, recall: 0.8056\n",
      "2018-12-29T11:34:12.971814, step: 547, loss: 0.3128858804702759, acc: 0.8594, auc: 0.9449, precision: 0.9032, recall: 0.8235\n",
      "2018-12-29T11:34:13.631856, step: 548, loss: 0.32119882106781006, acc: 0.875, auc: 0.9528, precision: 0.8788, recall: 0.8788\n",
      "2018-12-29T11:34:14.395950, step: 549, loss: 0.3168407678604126, acc: 0.8828, auc: 0.9488, precision: 0.9245, recall: 0.8167\n",
      "2018-12-29T11:34:15.083587, step: 550, loss: 0.38301217555999756, acc: 0.8828, auc: 0.9533, precision: 0.9107, recall: 0.8361\n",
      "2018-12-29T11:34:15.745511, step: 551, loss: 0.2535664141178131, acc: 0.8984, auc: 0.9624, precision: 0.9062, recall: 0.8923\n",
      "2018-12-29T11:34:16.470718, step: 552, loss: 0.2526816129684448, acc: 0.8984, auc: 0.9715, precision: 0.9683, recall: 0.8472\n",
      "2018-12-29T11:34:17.196577, step: 553, loss: 0.28688400983810425, acc: 0.8984, auc: 0.9571, precision: 0.9538, recall: 0.8611\n",
      "2018-12-29T11:34:17.988740, step: 554, loss: 0.34046995639801025, acc: 0.8359, auc: 0.9318, precision: 0.8246, recall: 0.8103\n",
      "2018-12-29T11:34:18.720139, step: 555, loss: 0.21097955107688904, acc: 0.9219, auc: 0.9861, precision: 0.95, recall: 0.8906\n",
      "2018-12-29T11:34:19.463926, step: 556, loss: 0.3580463230609894, acc: 0.8516, auc: 0.938, precision: 0.9298, recall: 0.7794\n",
      "2018-12-29T11:34:20.215979, step: 557, loss: 0.22793741524219513, acc: 0.9297, auc: 0.9823, precision: 0.9167, recall: 0.9322\n",
      "2018-12-29T11:34:20.959465, step: 558, loss: 0.36508411169052124, acc: 0.8438, auc: 0.9326, precision: 0.8333, recall: 0.8333\n",
      "2018-12-29T11:34:21.699348, step: 559, loss: 0.42809662222862244, acc: 0.8438, auc: 0.9241, precision: 0.8772, recall: 0.7937\n",
      "2018-12-29T11:34:22.412066, step: 560, loss: 0.29217514395713806, acc: 0.9219, auc: 0.9756, precision: 1.0, recall: 0.8462\n",
      "2018-12-29T11:34:23.227367, step: 561, loss: 0.3308987319469452, acc: 0.8984, auc: 0.9719, precision: 0.9722, recall: 0.8642\n",
      "2018-12-29T11:34:23.931339, step: 562, loss: 0.25703734159469604, acc: 0.9219, auc: 0.9708, precision: 0.9531, recall: 0.8971\n",
      "2018-12-29T11:34:24.688126, step: 563, loss: 0.2982158362865448, acc: 0.8828, auc: 0.9559, precision: 0.86, recall: 0.8431\n",
      "2018-12-29T11:34:25.411439, step: 564, loss: 0.2976212799549103, acc: 0.875, auc: 0.9506, precision: 0.9048, recall: 0.8507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-29T11:34:26.151227, step: 565, loss: 0.29564550518989563, acc: 0.875, auc: 0.9492, precision: 0.9298, recall: 0.8154\n",
      "2018-12-29T11:34:26.888838, step: 566, loss: 0.3555968701839447, acc: 0.8672, auc: 0.9572, precision: 0.9583, recall: 0.7541\n",
      "2018-12-29T11:34:27.616099, step: 567, loss: 0.25651612877845764, acc: 0.8984, auc: 0.9741, precision: 0.9038, recall: 0.8545\n",
      "2018-12-29T11:34:28.309029, step: 568, loss: 0.2928429841995239, acc: 0.9062, auc: 0.9649, precision: 0.9692, recall: 0.863\n",
      "2018-12-29T11:34:29.036520, step: 569, loss: 0.3691571354866028, acc: 0.8594, auc: 0.9261, precision: 0.9048, recall: 0.8261\n",
      "2018-12-29T11:34:29.690702, step: 570, loss: 0.2767903506755829, acc: 0.9062, auc: 0.9777, precision: 0.9016, recall: 0.9016\n",
      "2018-12-29T11:34:30.432824, step: 571, loss: 0.3685939311981201, acc: 0.875, auc: 0.9468, precision: 0.9, recall: 0.8438\n",
      "2018-12-29T11:34:31.199999, step: 572, loss: 0.2842484712600708, acc: 0.8672, auc: 0.9627, precision: 0.9057, recall: 0.8\n",
      "2018-12-29T11:34:31.957470, step: 573, loss: 0.32099276781082153, acc: 0.8906, auc: 0.9537, precision: 0.9242, recall: 0.8714\n",
      "2018-12-29T11:34:32.772315, step: 574, loss: 0.346670925617218, acc: 0.8828, auc: 0.9511, precision: 0.9219, recall: 0.8551\n",
      "2018-12-29T11:34:33.472437, step: 575, loss: 0.27962374687194824, acc: 0.9062, auc: 0.9818, precision: 0.9552, recall: 0.8767\n",
      "2018-12-29T11:34:34.211721, step: 576, loss: 0.34728288650512695, acc: 0.8438, auc: 0.9391, precision: 0.8889, recall: 0.8116\n",
      "2018-12-29T11:34:34.926343, step: 577, loss: 0.20252971351146698, acc: 0.9219, auc: 0.9819, precision: 0.9143, recall: 0.9412\n",
      "2018-12-29T11:34:35.610878, step: 578, loss: 0.313336580991745, acc: 0.8984, auc: 0.9523, precision: 0.8955, recall: 0.9091\n",
      "2018-12-29T11:34:36.354987, step: 579, loss: 0.34908217191696167, acc: 0.875, auc: 0.9526, precision: 0.873, recall: 0.873\n",
      "2018-12-29T11:34:37.063017, step: 580, loss: 0.35185497999191284, acc: 0.8828, auc: 0.9372, precision: 0.9, recall: 0.8873\n",
      "2018-12-29T11:34:37.807865, step: 581, loss: 0.27221783995628357, acc: 0.8984, auc: 0.9673, precision: 0.9216, recall: 0.8393\n",
      "2018-12-29T11:34:38.521035, step: 582, loss: 0.29992109537124634, acc: 0.875, auc: 0.9533, precision: 0.8966, recall: 0.8387\n",
      "2018-12-29T11:34:39.274865, step: 583, loss: 0.30543169379234314, acc: 0.8984, auc: 0.9489, precision: 0.9783, recall: 0.7895\n",
      "2018-12-29T11:34:39.995826, step: 584, loss: 0.2629368007183075, acc: 0.9062, auc: 0.9833, precision: 0.96, recall: 0.8276\n",
      "2018-12-29T11:34:40.700268, step: 585, loss: 0.24672436714172363, acc: 0.8828, auc: 0.9799, precision: 0.9423, recall: 0.8033\n",
      "2018-12-29T11:34:41.389434, step: 586, loss: 0.3373990058898926, acc: 0.8359, auc: 0.9455, precision: 0.9245, recall: 0.7424\n",
      "2018-12-29T11:34:42.111862, step: 587, loss: 0.2623586356639862, acc: 0.8984, auc: 0.9741, precision: 0.9649, recall: 0.8333\n",
      "2018-12-29T11:34:42.867846, step: 588, loss: 0.24291715025901794, acc: 0.9062, auc: 0.9799, precision: 0.9661, recall: 0.8507\n",
      "2018-12-29T11:34:43.655429, step: 589, loss: 0.30195337533950806, acc: 0.8828, auc: 0.9563, precision: 0.8871, recall: 0.873\n",
      "2018-12-29T11:34:44.314168, step: 590, loss: 0.2531864047050476, acc: 0.8906, auc: 0.9735, precision: 0.8594, recall: 0.9167\n",
      "2018-12-29T11:34:45.080112, step: 591, loss: 0.3232887387275696, acc: 0.8828, auc: 0.9587, precision: 0.8226, recall: 0.9273\n",
      "2018-12-29T11:34:45.772137, step: 592, loss: 0.4047704041004181, acc: 0.875, auc: 0.9351, precision: 0.8036, recall: 0.9\n",
      "2018-12-29T11:34:46.563280, step: 593, loss: 0.3088255226612091, acc: 0.8672, auc: 0.9505, precision: 0.8772, recall: 0.8333\n",
      "2018-12-29T11:34:47.312220, step: 594, loss: 0.27615007758140564, acc: 0.875, auc: 0.9671, precision: 0.9524, recall: 0.8219\n",
      "2018-12-29T11:34:48.033780, step: 595, loss: 0.24591338634490967, acc: 0.9219, auc: 0.9899, precision: 1.0, recall: 0.8592\n",
      "2018-12-29T11:34:48.783303, step: 596, loss: 0.32026588916778564, acc: 0.8672, auc: 0.96, precision: 0.98, recall: 0.7538\n",
      "2018-12-29T11:34:49.502526, step: 597, loss: 0.33829551935195923, acc: 0.8672, auc: 0.9485, precision: 0.9492, recall: 0.8\n",
      "2018-12-29T11:34:50.211577, step: 598, loss: 0.34657156467437744, acc: 0.8438, auc: 0.958, precision: 0.8644, recall: 0.8095\n",
      "2018-12-29T11:34:50.983878, step: 599, loss: 0.4520208239555359, acc: 0.8438, auc: 0.9308, precision: 0.8889, recall: 0.7742\n",
      "2018-12-29T11:34:51.672725, step: 600, loss: 0.3030475676059723, acc: 0.8672, auc: 0.9507, precision: 0.9275, recall: 0.8421\n",
      "\n",
      "Evaluation:\n",
      "2018-12-29T11:35:19.037400, step: 600, loss: 0.34566404670476913, acc: 0.8752026315789474, auc: 0.9465447368421053, precision: 0.8745499999999998, recall: 0.8803210526315791\n",
      "2018-12-29T11:35:19.767766, step: 601, loss: 0.39068299531936646, acc: 0.8828, auc: 0.9449, precision: 0.871, recall: 0.8852\n",
      "2018-12-29T11:35:20.479735, step: 602, loss: 0.23732343316078186, acc: 0.8906, auc: 0.9726, precision: 0.8772, recall: 0.8772\n",
      "2018-12-29T11:35:21.195042, step: 603, loss: 0.2558523118495941, acc: 0.9141, auc: 0.9729, precision: 0.9104, recall: 0.9242\n",
      "2018-12-29T11:35:21.895102, step: 604, loss: 0.21387887001037598, acc: 0.9062, auc: 0.9749, precision: 0.9324, recall: 0.9079\n",
      "2018-12-29T11:35:22.595647, step: 605, loss: 0.358893483877182, acc: 0.9062, auc: 0.9644, precision: 0.8983, recall: 0.8983\n",
      "2018-12-29T11:35:23.340040, step: 606, loss: 0.2535315454006195, acc: 0.9219, auc: 0.9778, precision: 0.8947, recall: 0.9273\n",
      "2018-12-29T11:35:24.057318, step: 607, loss: 0.22018584609031677, acc: 0.9219, auc: 0.9786, precision: 0.9778, recall: 0.8302\n",
      "2018-12-29T11:35:24.795481, step: 608, loss: 0.28016483783721924, acc: 0.8984, auc: 0.9724, precision: 0.9032, recall: 0.8889\n",
      "2018-12-29T11:35:25.531764, step: 609, loss: 0.30037662386894226, acc: 0.9062, auc: 0.9609, precision: 0.8983, recall: 0.8983\n",
      "2018-12-29T11:35:26.286545, step: 610, loss: 0.38853251934051514, acc: 0.8359, auc: 0.9428, precision: 0.9286, recall: 0.7536\n",
      "2018-12-29T11:35:27.047634, step: 611, loss: 0.2706289291381836, acc: 0.8828, auc: 0.9772, precision: 0.9412, recall: 0.8\n",
      "2018-12-29T11:35:27.757397, step: 612, loss: 0.31653860211372375, acc: 0.875, auc: 0.9509, precision: 0.9273, recall: 0.8095\n",
      "2018-12-29T11:35:28.452972, step: 613, loss: 0.2899046838283539, acc: 0.875, auc: 0.9662, precision: 0.9091, recall: 0.8197\n",
      "2018-12-29T11:35:29.097524, step: 614, loss: 0.2921687662601471, acc: 0.8906, auc: 0.9579, precision: 0.8983, recall: 0.8689\n",
      "2018-12-29T11:35:29.747073, step: 615, loss: 0.3352522850036621, acc: 0.8594, auc: 0.943, precision: 0.9219, recall: 0.8194\n",
      "2018-12-29T11:35:30.403668, step: 616, loss: 0.28560397028923035, acc: 0.8594, auc: 0.9578, precision: 0.8621, recall: 0.8333\n",
      "2018-12-29T11:35:31.035722, step: 617, loss: 0.3713977336883545, acc: 0.8828, auc: 0.937, precision: 0.8571, recall: 0.9231\n",
      "2018-12-29T11:35:31.650757, step: 618, loss: 0.31235459446907043, acc: 0.9297, auc: 0.9558, precision: 0.8889, recall: 0.9846\n",
      "2018-12-29T11:35:32.354727, step: 619, loss: 0.308803528547287, acc: 0.8594, auc: 0.9525, precision: 0.9375, recall: 0.75\n",
      "2018-12-29T11:35:33.076783, step: 620, loss: 0.29426294565200806, acc: 0.8828, auc: 0.9519, precision: 0.9298, recall: 0.8281\n",
      "start training model\n",
      "2018-12-29T11:35:33.837984, step: 621, loss: 0.3204057216644287, acc: 0.8828, auc: 0.9491, precision: 0.9259, recall: 0.8197\n",
      "2018-12-29T11:35:34.505770, step: 622, loss: 0.26576128602027893, acc: 0.8984, auc: 0.968, precision: 0.9322, recall: 0.8594\n",
      "2018-12-29T11:35:35.223977, step: 623, loss: 0.4023670256137848, acc: 0.875, auc: 0.9325, precision: 0.8704, recall: 0.8393\n",
      "2018-12-29T11:35:35.984189, step: 624, loss: 0.25556591153144836, acc: 0.8594, auc: 0.9681, precision: 0.9048, recall: 0.8261\n",
      "2018-12-29T11:35:36.711238, step: 625, loss: 0.2872379720211029, acc: 0.875, auc: 0.9624, precision: 0.8679, recall: 0.8364\n",
      "2018-12-29T11:35:37.403138, step: 626, loss: 0.22902865707874298, acc: 0.9141, auc: 0.9772, precision: 0.9692, recall: 0.875\n",
      "2018-12-29T11:35:38.168424, step: 627, loss: 0.2754606604576111, acc: 0.9062, auc: 0.9643, precision: 0.9153, recall: 0.8852\n",
      "2018-12-29T11:35:38.875452, step: 628, loss: 0.2942849397659302, acc: 0.8594, auc: 0.955, precision: 0.9, recall: 0.8182\n",
      "2018-12-29T11:35:39.596597, step: 629, loss: 0.18587785959243774, acc: 0.9609, auc: 0.9848, precision: 0.9661, recall: 0.95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-29T11:35:40.376526, step: 630, loss: 0.2638986110687256, acc: 0.8906, auc: 0.9623, precision: 0.9273, recall: 0.8361\n",
      "2018-12-29T11:35:41.093155, step: 631, loss: 0.2970955967903137, acc: 0.9062, auc: 0.9459, precision: 0.9275, recall: 0.9014\n",
      "2018-12-29T11:35:41.828061, step: 632, loss: 0.3524641692638397, acc: 0.8906, auc: 0.9436, precision: 0.9423, recall: 0.8167\n",
      "2018-12-29T11:35:42.524684, step: 633, loss: 0.19946140050888062, acc: 0.8906, auc: 0.9903, precision: 0.9848, recall: 0.8333\n",
      "2018-12-29T11:35:43.216848, step: 634, loss: 0.2845154106616974, acc: 0.875, auc: 0.9624, precision: 0.9107, recall: 0.8226\n",
      "2018-12-29T11:35:43.926645, step: 635, loss: 0.30302852392196655, acc: 0.8594, auc: 0.9519, precision: 0.8548, recall: 0.8548\n",
      "2018-12-29T11:35:44.626818, step: 636, loss: 0.274955689907074, acc: 0.8516, auc: 0.9582, precision: 0.9138, recall: 0.791\n",
      "2018-12-29T11:35:45.327882, step: 637, loss: 0.3288918137550354, acc: 0.8828, auc: 0.9618, precision: 0.9104, recall: 0.8714\n",
      "2018-12-29T11:35:46.035540, step: 638, loss: 0.3044511079788208, acc: 0.9062, auc: 0.96, precision: 0.9184, recall: 0.8491\n",
      "2018-12-29T11:35:46.823907, step: 639, loss: 0.2109486162662506, acc: 0.8984, auc: 0.9835, precision: 0.9672, recall: 0.8429\n",
      "2018-12-29T11:35:47.583633, step: 640, loss: 0.2327679544687271, acc: 0.9375, auc: 0.9704, precision: 0.9344, recall: 0.9344\n",
      "2018-12-29T11:35:48.324195, step: 641, loss: 0.3108381927013397, acc: 0.9219, auc: 0.9571, precision: 0.8906, recall: 0.95\n",
      "2018-12-29T11:35:49.074616, step: 642, loss: 0.3136661648750305, acc: 0.9141, auc: 0.9461, precision: 0.9041, recall: 0.9429\n",
      "2018-12-29T11:35:49.831866, step: 643, loss: 0.21714815497398376, acc: 0.9062, auc: 0.9775, precision: 0.9322, recall: 0.873\n",
      "2018-12-29T11:35:50.536159, step: 644, loss: 0.38664060831069946, acc: 0.8438, auc: 0.9241, precision: 0.873, recall: 0.8209\n",
      "2018-12-29T11:35:51.320006, step: 645, loss: 0.22357258200645447, acc: 0.9141, auc: 0.9734, precision: 0.9661, recall: 0.8636\n",
      "2018-12-29T11:35:51.996425, step: 646, loss: 0.2714076340198517, acc: 0.8984, auc: 0.9624, precision: 0.9688, recall: 0.8493\n",
      "2018-12-29T11:35:52.687724, step: 647, loss: 0.2618582248687744, acc: 0.8672, auc: 0.9655, precision: 0.9153, recall: 0.8182\n",
      "2018-12-29T11:35:53.397090, step: 648, loss: 0.24169757962226868, acc: 0.8984, auc: 0.9745, precision: 0.9273, recall: 0.85\n",
      "2018-12-29T11:35:54.055699, step: 649, loss: 0.19720114767551422, acc: 0.9375, auc: 0.9874, precision: 0.9412, recall: 0.9057\n",
      "2018-12-29T11:35:54.823655, step: 650, loss: 0.19431118667125702, acc: 0.9453, auc: 0.9798, precision: 0.9701, recall: 0.9286\n",
      "2018-12-29T11:35:55.611142, step: 651, loss: 0.28428518772125244, acc: 0.8906, auc: 0.9584, precision: 0.9123, recall: 0.8525\n",
      "2018-12-29T11:35:56.304019, step: 652, loss: 0.1773250848054886, acc: 0.9219, auc: 0.9812, precision: 0.9333, recall: 0.9032\n",
      "2018-12-29T11:35:56.999010, step: 653, loss: 0.2962522804737091, acc: 0.9062, auc: 0.9531, precision: 0.95, recall: 0.8636\n",
      "2018-12-29T11:35:57.655938, step: 654, loss: 0.28832030296325684, acc: 0.8984, auc: 0.9569, precision: 0.9355, recall: 0.8657\n",
      "2018-12-29T11:35:58.361346, step: 655, loss: 0.3114365041255951, acc: 0.8906, auc: 0.9521, precision: 0.9265, recall: 0.875\n",
      "2018-12-29T11:35:59.093795, step: 656, loss: 0.23865893483161926, acc: 0.9219, auc: 0.977, precision: 0.9444, recall: 0.9189\n",
      "2018-12-29T11:35:59.860640, step: 657, loss: 0.289567232131958, acc: 0.8906, auc: 0.9606, precision: 0.875, recall: 0.875\n",
      "2018-12-29T11:36:00.606636, step: 658, loss: 0.2715015113353729, acc: 0.9141, auc: 0.9778, precision: 0.9206, recall: 0.9062\n",
      "2018-12-29T11:36:01.350441, step: 659, loss: 0.21135076880455017, acc: 0.8984, auc: 0.9796, precision: 0.9348, recall: 0.8113\n",
      "2018-12-29T11:36:01.995724, step: 660, loss: 0.23094184696674347, acc: 0.9062, auc: 0.9711, precision: 0.9245, recall: 0.8596\n",
      "2018-12-29T11:36:02.775387, step: 661, loss: 0.29290008544921875, acc: 0.8828, auc: 0.9573, precision: 0.9583, recall: 0.7797\n",
      "2018-12-29T11:36:03.470079, step: 662, loss: 0.3642662465572357, acc: 0.8984, auc: 0.9601, precision: 0.9219, recall: 0.8806\n",
      "2018-12-29T11:36:04.175054, step: 663, loss: 0.3007940948009491, acc: 0.8516, auc: 0.9541, precision: 0.8909, recall: 0.7903\n",
      "2018-12-29T11:36:04.920897, step: 664, loss: 0.4057193994522095, acc: 0.8516, auc: 0.9237, precision: 0.8621, recall: 0.8197\n",
      "2018-12-29T11:36:05.652651, step: 665, loss: 0.17711634933948517, acc: 0.9375, auc: 0.9846, precision: 0.9538, recall: 0.9254\n",
      "2018-12-29T11:36:06.364973, step: 666, loss: 0.34343934059143066, acc: 0.9219, auc: 0.9707, precision: 0.8955, recall: 0.9524\n",
      "2018-12-29T11:36:07.043523, step: 667, loss: 0.29832640290260315, acc: 0.8828, auc: 0.9645, precision: 0.8955, recall: 0.8824\n",
      "2018-12-29T11:36:07.772930, step: 668, loss: 0.23339900374412537, acc: 0.9375, auc: 0.984, precision: 0.9552, recall: 0.9275\n",
      "2018-12-29T11:36:08.411249, step: 669, loss: 0.28049370646476746, acc: 0.9297, auc: 0.9712, precision: 0.9516, recall: 0.9077\n",
      "2018-12-29T11:36:09.140209, step: 670, loss: 0.30306538939476013, acc: 0.9297, auc: 0.9646, precision: 0.9107, recall: 0.9273\n",
      "2018-12-29T11:36:09.907268, step: 671, loss: 0.2117430567741394, acc: 0.9141, auc: 0.9743, precision: 0.9016, recall: 0.9167\n",
      "2018-12-29T11:36:10.540368, step: 672, loss: 0.440350741147995, acc: 0.8516, auc: 0.9282, precision: 0.8644, recall: 0.8226\n",
      "2018-12-29T11:36:11.287408, step: 673, loss: 0.385945200920105, acc: 0.8203, auc: 0.9529, precision: 0.92, recall: 0.7077\n",
      "2018-12-29T11:36:11.957220, step: 674, loss: 0.2659289538860321, acc: 0.8984, auc: 0.979, precision: 0.9623, recall: 0.8226\n",
      "2018-12-29T11:36:12.698412, step: 675, loss: 0.2636673152446747, acc: 0.9141, auc: 0.9709, precision: 0.9836, recall: 0.8571\n",
      "2018-12-29T11:36:13.375811, step: 676, loss: 0.2833719253540039, acc: 0.8984, auc: 0.977, precision: 0.9552, recall: 0.8649\n",
      "2018-12-29T11:36:14.059669, step: 677, loss: 0.31544798612594604, acc: 0.8984, auc: 0.9655, precision: 0.9455, recall: 0.8387\n",
      "2018-12-29T11:36:14.775984, step: 678, loss: 0.31490907073020935, acc: 0.9141, auc: 0.9523, precision: 0.9286, recall: 0.8814\n",
      "2018-12-29T11:36:15.547859, step: 679, loss: 0.31836801767349243, acc: 0.9062, auc: 0.9526, precision: 0.931, recall: 0.871\n",
      "2018-12-29T11:36:16.270862, step: 680, loss: 0.2509036064147949, acc: 0.9375, auc: 0.9662, precision: 0.9286, recall: 0.9559\n",
      "2018-12-29T11:36:17.066690, step: 681, loss: 0.32833331823349, acc: 0.9219, auc: 0.952, precision: 0.9324, recall: 0.9324\n",
      "2018-12-29T11:36:17.772337, step: 682, loss: 0.27257615327835083, acc: 0.9141, auc: 0.9665, precision: 0.8939, recall: 0.9365\n",
      "2018-12-29T11:36:18.480944, step: 683, loss: 0.24797102808952332, acc: 0.9141, auc: 0.9699, precision: 0.9298, recall: 0.8833\n",
      "2018-12-29T11:36:19.191322, step: 684, loss: 0.3191837668418884, acc: 0.8984, auc: 0.9499, precision: 0.9538, recall: 0.8611\n",
      "2018-12-29T11:36:19.883262, step: 685, loss: 0.26411280035972595, acc: 0.8984, auc: 0.9713, precision: 0.8833, recall: 0.8983\n",
      "2018-12-29T11:36:20.607614, step: 686, loss: 0.24545753002166748, acc: 0.9062, auc: 0.9753, precision: 0.9483, recall: 0.8594\n",
      "2018-12-29T11:36:21.307365, step: 687, loss: 0.30388784408569336, acc: 0.8672, auc: 0.944, precision: 0.931, recall: 0.806\n",
      "2018-12-29T11:36:22.020640, step: 688, loss: 0.18631349503993988, acc: 0.9688, auc: 0.9877, precision: 1.0, recall: 0.9111\n",
      "2018-12-29T11:36:22.750801, step: 689, loss: 0.31077808141708374, acc: 0.8438, auc: 0.9605, precision: 0.9111, recall: 0.7193\n",
      "2018-12-29T11:36:23.395746, step: 690, loss: 0.2453741729259491, acc: 0.9219, auc: 0.9812, precision: 0.9672, recall: 0.8806\n",
      "2018-12-29T11:36:24.020722, step: 691, loss: 0.20897169411182404, acc: 0.9453, auc: 0.9814, precision: 0.9649, recall: 0.9167\n",
      "2018-12-29T11:36:24.663003, step: 692, loss: 0.22100341320037842, acc: 0.9453, auc: 0.9792, precision: 0.9649, recall: 0.9167\n",
      "2018-12-29T11:36:25.399429, step: 693, loss: 0.4212692081928253, acc: 0.8281, auc: 0.9316, precision: 0.9016, recall: 0.7746\n",
      "2018-12-29T11:36:26.119741, step: 694, loss: 0.3980620503425598, acc: 0.8594, auc: 0.9431, precision: 0.8448, recall: 0.8448\n",
      "2018-12-29T11:36:26.929044, step: 695, loss: 0.2606492340564728, acc: 0.9141, auc: 0.9665, precision: 0.9483, recall: 0.873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-29T11:36:27.564139, step: 696, loss: 0.23871512711048126, acc: 0.8828, auc: 0.9726, precision: 0.9444, recall: 0.8095\n",
      "2018-12-29T11:36:28.271945, step: 697, loss: 0.30430859327316284, acc: 0.9141, auc: 0.9643, precision: 0.918, recall: 0.9032\n",
      "2018-12-29T11:36:28.947855, step: 698, loss: 0.37159526348114014, acc: 0.8359, auc: 0.9591, precision: 1.0, recall: 0.7\n",
      "2018-12-29T11:36:29.679913, step: 699, loss: 0.23637986183166504, acc: 0.8828, auc: 0.9774, precision: 0.9846, recall: 0.8205\n",
      "2018-12-29T11:36:30.346117, step: 700, loss: 0.232436865568161, acc: 0.9141, auc: 0.9769, precision: 0.8889, recall: 0.9057\n",
      "\n",
      "Evaluation:\n",
      "2018-12-29T11:36:57.695813, step: 700, loss: 0.3329193944993772, acc: 0.8754105263157891, auc: 0.9468947368421052, precision: 0.8791763157894735, recall: 0.8737921052631578\n",
      "2018-12-29T11:36:58.423105, step: 701, loss: 0.20016704499721527, acc: 0.9375, auc: 0.986, precision: 0.931, recall: 0.931\n",
      "2018-12-29T11:36:59.161040, step: 702, loss: 0.23768414556980133, acc: 0.9062, auc: 0.9721, precision: 0.8889, recall: 0.918\n",
      "2018-12-29T11:36:59.903529, step: 703, loss: 0.3387446403503418, acc: 0.9375, auc: 0.9617, precision: 0.9322, recall: 0.9322\n",
      "2018-12-29T11:37:00.568775, step: 704, loss: 0.3549467921257019, acc: 0.8828, auc: 0.951, precision: 0.9091, recall: 0.8333\n",
      "2018-12-29T11:37:01.258215, step: 705, loss: 0.2656177282333374, acc: 0.9141, auc: 0.9663, precision: 0.8971, recall: 0.9385\n",
      "2018-12-29T11:37:02.035523, step: 706, loss: 0.25167667865753174, acc: 0.8906, auc: 0.9661, precision: 0.9091, recall: 0.8475\n",
      "2018-12-29T11:37:02.811419, step: 707, loss: 0.28992199897766113, acc: 0.9141, auc: 0.9593, precision: 0.9153, recall: 0.9\n",
      "2018-12-29T11:37:03.556436, step: 708, loss: 0.3021240830421448, acc: 0.8906, auc: 0.9499, precision: 0.9483, recall: 0.8333\n",
      "2018-12-29T11:37:04.263006, step: 709, loss: 0.2660151720046997, acc: 0.9141, auc: 0.9664, precision: 0.9423, recall: 0.8596\n",
      "2018-12-29T11:37:04.943699, step: 710, loss: 0.22824163734912872, acc: 0.8984, auc: 0.977, precision: 0.9615, recall: 0.8197\n",
      "2018-12-29T11:37:05.708515, step: 711, loss: 0.32620084285736084, acc: 0.8594, auc: 0.9611, precision: 0.9483, recall: 0.7857\n",
      "2018-12-29T11:37:06.451758, step: 712, loss: 0.38339176774024963, acc: 0.8438, auc: 0.9373, precision: 0.86, recall: 0.7679\n",
      "2018-12-29T11:37:07.148180, step: 713, loss: 0.2890442907810211, acc: 0.8672, auc: 0.9694, precision: 0.8909, recall: 0.8167\n",
      "2018-12-29T11:37:07.818417, step: 714, loss: 0.2503384053707123, acc: 0.875, auc: 0.9687, precision: 0.9394, recall: 0.8378\n",
      "2018-12-29T11:37:08.503482, step: 715, loss: 0.3914628028869629, acc: 0.8438, auc: 0.9268, precision: 0.8451, recall: 0.8696\n",
      "2018-12-29T11:37:09.232278, step: 716, loss: 0.3495764136314392, acc: 0.8828, auc: 0.9615, precision: 0.8387, recall: 0.9123\n",
      "2018-12-29T11:37:09.970719, step: 717, loss: 0.3017849922180176, acc: 0.8984, auc: 0.9701, precision: 0.8615, recall: 0.9333\n",
      "2018-12-29T11:37:10.687665, step: 718, loss: 0.23083147406578064, acc: 0.9297, auc: 0.9772, precision: 0.9464, recall: 0.8983\n",
      "2018-12-29T11:37:11.444572, step: 719, loss: 0.31960275769233704, acc: 0.8984, auc: 0.9456, precision: 0.9074, recall: 0.8596\n",
      "2018-12-29T11:37:12.156971, step: 720, loss: 0.33103564381599426, acc: 0.8438, auc: 0.9492, precision: 0.92, recall: 0.7419\n",
      "2018-12-29T11:37:12.926787, step: 721, loss: 0.27657055854797363, acc: 0.8672, auc: 0.9768, precision: 0.9492, recall: 0.8\n",
      "2018-12-29T11:37:13.602660, step: 722, loss: 0.27838146686553955, acc: 0.8594, auc: 0.9572, precision: 0.8936, recall: 0.7636\n",
      "2018-12-29T11:37:14.324386, step: 723, loss: 0.26018840074539185, acc: 0.8984, auc: 0.9613, precision: 0.9655, recall: 0.8358\n",
      "2018-12-29T11:37:15.042806, step: 724, loss: 0.2548304796218872, acc: 0.9062, auc: 0.9618, precision: 0.9351, recall: 0.9114\n",
      "2018-12-29T11:37:15.737791, step: 725, loss: 0.37618088722229004, acc: 0.875, auc: 0.9374, precision: 0.8841, recall: 0.8841\n",
      "2018-12-29T11:37:16.504601, step: 726, loss: 0.43191155791282654, acc: 0.8281, auc: 0.9235, precision: 0.8143, recall: 0.8636\n",
      "2018-12-29T11:37:17.231639, step: 727, loss: 0.23170074820518494, acc: 0.9062, auc: 0.9792, precision: 0.9661, recall: 0.8507\n",
      "2018-12-29T11:37:17.939712, step: 728, loss: 0.2574102282524109, acc: 0.8984, auc: 0.9699, precision: 0.875, recall: 0.918\n",
      "2018-12-29T11:37:18.651742, step: 729, loss: 0.21492904424667358, acc: 0.9141, auc: 0.9756, precision: 0.9655, recall: 0.8615\n",
      "2018-12-29T11:37:19.369720, step: 730, loss: 0.2557043731212616, acc: 0.8906, auc: 0.9634, precision: 0.9508, recall: 0.8406\n",
      "2018-12-29T11:37:20.050563, step: 731, loss: 0.27014803886413574, acc: 0.875, auc: 0.969, precision: 0.9167, recall: 0.8333\n",
      "2018-12-29T11:37:20.759057, step: 732, loss: 0.20629873871803284, acc: 0.9297, auc: 0.9801, precision: 0.9811, recall: 0.8667\n",
      "2018-12-29T11:37:21.531698, step: 733, loss: 0.24967901408672333, acc: 0.9062, auc: 0.9759, precision: 0.9394, recall: 0.8857\n",
      "2018-12-29T11:37:22.248433, step: 734, loss: 0.296769380569458, acc: 0.8438, auc: 0.9581, precision: 0.931, recall: 0.7714\n",
      "2018-12-29T11:37:22.941630, step: 735, loss: 0.1799684315919876, acc: 0.9531, auc: 0.9849, precision: 0.9833, recall: 0.9219\n",
      "2018-12-29T11:37:23.638024, step: 736, loss: 0.30637624859809875, acc: 0.9141, auc: 0.9576, precision: 0.8769, recall: 0.95\n",
      "2018-12-29T11:37:24.340943, step: 737, loss: 0.27010902762413025, acc: 0.9062, auc: 0.9717, precision: 0.8889, recall: 0.8889\n",
      "2018-12-29T11:37:25.079715, step: 738, loss: 0.3535328209400177, acc: 0.8672, auc: 0.9441, precision: 0.8406, recall: 0.9062\n",
      "2018-12-29T11:37:25.792176, step: 739, loss: 0.27709388732910156, acc: 0.9297, auc: 0.9599, precision: 0.898, recall: 0.9167\n",
      "2018-12-29T11:37:26.447461, step: 740, loss: 0.42052513360977173, acc: 0.8438, auc: 0.9409, precision: 0.9821, recall: 0.7432\n",
      "2018-12-29T11:37:27.119191, step: 741, loss: 0.3187207281589508, acc: 0.8438, auc: 0.9574, precision: 0.9434, recall: 0.7463\n",
      "2018-12-29T11:37:27.804188, step: 742, loss: 0.3040878176689148, acc: 0.875, auc: 0.9675, precision: 0.9792, recall: 0.7581\n",
      "2018-12-29T11:37:28.487910, step: 743, loss: 0.35764625668525696, acc: 0.8672, auc: 0.9608, precision: 0.9538, recall: 0.8158\n",
      "2018-12-29T11:37:29.190339, step: 744, loss: 0.2575564682483673, acc: 0.9062, auc: 0.9672, precision: 0.9231, recall: 0.8955\n",
      "2018-12-29T11:37:29.911395, step: 745, loss: 0.22564107179641724, acc: 0.9141, auc: 0.9777, precision: 0.942, recall: 0.9028\n",
      "2018-12-29T11:37:30.616273, step: 746, loss: 0.21859028935432434, acc: 0.9141, auc: 0.9756, precision: 0.9355, recall: 0.8923\n",
      "2018-12-29T11:37:31.391043, step: 747, loss: 0.2883223295211792, acc: 0.9141, auc: 0.9753, precision: 0.8846, recall: 0.9718\n",
      "2018-12-29T11:37:32.056281, step: 748, loss: 0.2478243112564087, acc: 0.9219, auc: 0.9756, precision: 0.9057, recall: 0.9057\n",
      "2018-12-29T11:37:32.751586, step: 749, loss: 0.2653680741786957, acc: 0.9219, auc: 0.9699, precision: 0.918, recall: 0.918\n",
      "2018-12-29T11:37:33.455897, step: 750, loss: 0.2678735852241516, acc: 0.8984, auc: 0.9722, precision: 0.9091, recall: 0.8621\n",
      "2018-12-29T11:37:34.195926, step: 751, loss: 0.3272508978843689, acc: 0.9219, auc: 0.968, precision: 0.9333, recall: 0.9032\n",
      "2018-12-29T11:37:34.951772, step: 752, loss: 0.34927621483802795, acc: 0.9219, auc: 0.9475, precision: 0.9444, recall: 0.8793\n",
      "2018-12-29T11:37:35.656168, step: 753, loss: 0.20043843984603882, acc: 0.9609, auc: 0.9863, precision: 1.0, recall: 0.918\n",
      "2018-12-29T11:37:36.390833, step: 754, loss: 0.33013948798179626, acc: 0.8906, auc: 0.9638, precision: 0.8793, recall: 0.8793\n",
      "2018-12-29T11:37:37.139715, step: 755, loss: 0.20743100345134735, acc: 0.8984, auc: 0.9885, precision: 0.9643, recall: 0.8308\n",
      "2018-12-29T11:37:37.912037, step: 756, loss: 0.30005019903182983, acc: 0.8984, auc: 0.9777, precision: 0.95, recall: 0.8507\n",
      "2018-12-29T11:37:38.607221, step: 757, loss: 0.3093195855617523, acc: 0.9141, auc: 0.9573, precision: 0.9062, recall: 0.9206\n",
      "2018-12-29T11:37:39.295627, step: 758, loss: 0.29807907342910767, acc: 0.9141, auc: 0.9538, precision: 0.9655, recall: 0.8615\n",
      "2018-12-29T11:37:39.948819, step: 759, loss: 0.2701146900653839, acc: 0.9141, auc: 0.9628, precision: 0.9275, recall: 0.9143\n",
      "2018-12-29T11:37:40.726897, step: 760, loss: 0.2949685752391815, acc: 0.8672, auc: 0.9595, precision: 0.9254, recall: 0.8378\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-29T11:37:41.400369, step: 761, loss: 0.21439702808856964, acc: 0.9141, auc: 0.9788, precision: 0.9403, recall: 0.9\n",
      "2018-12-29T11:37:42.183186, step: 762, loss: 0.26376229524612427, acc: 0.9141, auc: 0.9844, precision: 0.9655, recall: 0.8615\n",
      "2018-12-29T11:37:42.930009, step: 763, loss: 0.37042880058288574, acc: 0.8672, auc: 0.9471, precision: 0.9, recall: 0.7895\n",
      "2018-12-29T11:37:43.639033, step: 764, loss: 0.26977697014808655, acc: 0.9062, auc: 0.957, precision: 0.9091, recall: 0.8772\n",
      "2018-12-29T11:37:44.337299, step: 765, loss: 0.3444329500198364, acc: 0.8906, auc: 0.9522, precision: 0.9516, recall: 0.8429\n",
      "2018-12-29T11:37:45.079281, step: 766, loss: 0.33014151453971863, acc: 0.875, auc: 0.9428, precision: 0.9167, recall: 0.8333\n",
      "2018-12-29T11:37:45.804334, step: 767, loss: 0.23364895582199097, acc: 0.9062, auc: 0.971, precision: 0.9254, recall: 0.8986\n",
      "2018-12-29T11:37:46.515896, step: 768, loss: 0.2611049711704254, acc: 0.9297, auc: 0.9669, precision: 0.9259, recall: 0.9091\n",
      "2018-12-29T11:37:47.268616, step: 769, loss: 0.33483394980430603, acc: 0.8828, auc: 0.9414, precision: 0.8909, recall: 0.8448\n",
      "2018-12-29T11:37:48.034886, step: 770, loss: 0.21111559867858887, acc: 0.9375, auc: 0.9812, precision: 0.9706, recall: 0.9167\n",
      "2018-12-29T11:37:48.787264, step: 771, loss: 0.20405158400535583, acc: 0.9297, auc: 0.9773, precision: 0.9455, recall: 0.8966\n",
      "2018-12-29T11:37:49.484417, step: 772, loss: 0.30740538239479065, acc: 0.875, auc: 0.9539, precision: 0.9483, recall: 0.8088\n",
      "2018-12-29T11:37:50.226855, step: 773, loss: 0.24346302449703217, acc: 0.8984, auc: 0.9681, precision: 0.96, recall: 0.8136\n",
      "2018-12-29T11:37:50.952194, step: 774, loss: 0.32369542121887207, acc: 0.8281, auc: 0.9521, precision: 0.8906, recall: 0.7917\n",
      "2018-12-29T11:37:51.666905, step: 775, loss: 0.24921470880508423, acc: 0.9141, auc: 0.9631, precision: 0.9683, recall: 0.8714\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "\n",
    "# 生成训练集和验证集\n",
    "trainReviews = data.trainReviews\n",
    "trainLabels = data.trainLabels\n",
    "evalReviews = data.evalReviews\n",
    "evalLabels = data.evalLabels\n",
    "\n",
    "wordEmbedding = data.wordEmbedding\n",
    "indexFreqs = data.indexFreqs\n",
    "\n",
    "# 定义计算图\n",
    "with tf.Graph().as_default():\n",
    "\n",
    "    session_conf = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
    "    session_conf.gpu_options.allow_growth=True\n",
    "    session_conf.gpu_options.per_process_gpu_memory_fraction = 0.9  # 配置gpu占用率  \n",
    "\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    \n",
    "    # 定义会话\n",
    "    with sess.as_default():\n",
    "        lstm = AdversarialLSTM(config, wordEmbedding, indexFreqs)\n",
    "        \n",
    "        globalStep = tf.Variable(0, name=\"globalStep\", trainable=False)\n",
    "        # 定义优化函数，传入学习速率参数\n",
    "        optimizer = tf.train.AdamOptimizer(config.training.learningRate)\n",
    "        # 计算梯度,得到梯度和变量\n",
    "        gradsAndVars = optimizer.compute_gradients(lstm.loss)\n",
    "        # 将梯度应用到变量下，生成训练器\n",
    "        trainOp = optimizer.apply_gradients(gradsAndVars, global_step=globalStep)\n",
    "        \n",
    "        # 用summary绘制tensorBoard\n",
    "        gradSummaries = []\n",
    "        for g, v in gradsAndVars:\n",
    "            if g is not None:\n",
    "                tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "        \n",
    "        outDir = os.path.abspath(os.path.join(os.path.curdir, \"summarys\"))\n",
    "        print(\"Writing to {}\\n\".format(outDir))\n",
    "        \n",
    "        lossSummary = tf.summary.scalar(\"loss\", lstm.loss)\n",
    "        summaryOp = tf.summary.merge_all()\n",
    "        \n",
    "        trainSummaryDir = os.path.join(outDir, \"train\")\n",
    "        trainSummaryWriter = tf.summary.FileWriter(trainSummaryDir, sess.graph)\n",
    "        \n",
    "        evalSummaryDir = os.path.join(outDir, \"eval\")\n",
    "        evalSummaryWriter = tf.summary.FileWriter(evalSummaryDir, sess.graph)\n",
    "        \n",
    "        \n",
    "        # 初始化所有变量\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=5)\n",
    "        \n",
    "        # 保存模型的一种方式，保存为pb文件\n",
    "        builder = tf.saved_model.builder.SavedModelBuilder(\"../model/Bi-LSTM/savedModel\")\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        def trainStep(batchX, batchY):\n",
    "            \"\"\"\n",
    "            训练函数\n",
    "            \"\"\"   \n",
    "            feed_dict = {\n",
    "              lstm.inputX: batchX,\n",
    "              lstm.inputY: batchY,\n",
    "              lstm.dropoutKeepProb: config.model.dropoutKeepProb\n",
    "            }\n",
    "            _, summary, step, loss, predictions, binaryPreds = sess.run(\n",
    "                [trainOp, summaryOp, globalStep, lstm.loss, lstm.predictions, lstm.binaryPreds],\n",
    "                feed_dict)\n",
    "            timeStr = datetime.datetime.now().isoformat()\n",
    "            acc, auc, precision, recall = genMetrics(batchY, predictions, binaryPreds)\n",
    "            print(\"{}, step: {}, loss: {}, acc: {}, auc: {}, precision: {}, recall: {}\".format(timeStr, step, loss, acc, auc, precision, recall))\n",
    "            trainSummaryWriter.add_summary(summary, step)\n",
    "\n",
    "        def devStep(batchX, batchY):\n",
    "            \"\"\"\n",
    "            验证函数\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              lstm.inputX: batchX,\n",
    "              lstm.inputY: batchY,\n",
    "              lstm.dropoutKeepProb: 1.0\n",
    "            }\n",
    "            summary, step, loss, predictions, binaryPreds = sess.run(\n",
    "                [summaryOp, globalStep, lstm.loss, lstm.predictions, lstm.binaryPreds],\n",
    "                feed_dict)\n",
    "            \n",
    "            acc, auc, precision, recall = genMetrics(batchY, predictions, binaryPreds)\n",
    "            \n",
    "            evalSummaryWriter.add_summary(summary, step)\n",
    "            \n",
    "            return loss, acc, auc, precision, recall\n",
    "        \n",
    "        for i in range(config.training.epoches):\n",
    "            # 训练模型\n",
    "            print(\"start training model\")\n",
    "            for batchTrain in nextBatch(trainReviews, trainLabels, config.batchSize):\n",
    "                trainStep(batchTrain[0], batchTrain[1])\n",
    "\n",
    "                currentStep = tf.train.global_step(sess, globalStep) \n",
    "                if currentStep % config.training.evaluateEvery == 0:\n",
    "                    print(\"\\nEvaluation:\")\n",
    "                    \n",
    "                    losses = []\n",
    "                    accs = []\n",
    "                    aucs = []\n",
    "                    precisions = []\n",
    "                    recalls = []\n",
    "                    \n",
    "                    for batchEval in nextBatch(evalReviews, evalLabels, config.batchSize):\n",
    "                        loss, acc, auc, precision, recall = devStep(batchEval[0], batchEval[1])\n",
    "                        losses.append(loss)\n",
    "                        accs.append(acc)\n",
    "                        aucs.append(auc)\n",
    "                        precisions.append(precision)\n",
    "                        recalls.append(recall)\n",
    "                        \n",
    "                    time_str = datetime.datetime.now().isoformat()\n",
    "                    print(\"{}, step: {}, loss: {}, acc: {}, auc: {}, precision: {}, recall: {}\".format(time_str, currentStep, mean(losses), \n",
    "                                                                                                       mean(accs), mean(aucs), mean(precisions),\n",
    "                                                                                                       mean(recalls)))\n",
    "                    \n",
    "                if currentStep % config.training.checkpointEvery == 0:\n",
    "                    # 保存模型的另一种方法，保存checkpoint文件\n",
    "                    path = saver.save(sess, \"../model/Bi-LSTM/model/my-model\", global_step=currentStep)\n",
    "                    print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "                    \n",
    "        inputs = {\"inputX\": tf.saved_model.utils.build_tensor_info(lstm.inputX),\n",
    "                  \"keepProb\": tf.saved_model.utils.build_tensor_info(lstm.dropoutKeepProb)}\n",
    "\n",
    "        outputs = {\"binaryPreds\": tf.saved_model.utils.build_tensor_info(lstm.binaryPreds)}\n",
    "\n",
    "        prediction_signature = tf.saved_model.signature_def_utils.build_signature_def(inputs=inputs, outputs=outputs,\n",
    "                                                                                      method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME)\n",
    "        legacy_init_op = tf.group(tf.tables_initializer(), name=\"legacy_init_op\")\n",
    "        builder.add_meta_graph_and_variables(sess, [tf.saved_model.tag_constants.SERVING],\n",
    "                                            signature_def_map={\"predict\": prediction_signature}, legacy_init_op=legacy_init_op)\n",
    "\n",
    "         builder.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
