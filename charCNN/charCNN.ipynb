{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import csv\n",
    "import json\n",
    "from math import sqrt\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参数配置\n",
    "\n",
    "class TrainingConfig(object):\n",
    "    epoches = 10\n",
    "    evaluateEvery = 100\n",
    "    checkpointEvery = 100\n",
    "    learningRate = 0.001\n",
    "    \n",
    "\n",
    "class ModelConfig(object):\n",
    "    \n",
    "    # 该列表中子列表的三个元素分别是卷积核的数量，卷积核的高度，池化的尺寸\n",
    "    convLayers = [[256, 7, 4],\n",
    "                  [256, 7, 4],\n",
    "                  [256, 3, 4]]\n",
    "#                   [256, 3, None],\n",
    "#                   [256, 3, None],\n",
    "#                   [256, 3, 3]]\n",
    "    fcLayers = [512]\n",
    "    dropoutKeepProb = 0.5\n",
    "    \n",
    "    epsilon = 1e-3  # BN层中防止分母为0而加入的极小值\n",
    "    decay = 0.999  # BN层中用来计算滑动平均的值\n",
    "    \n",
    "    \n",
    "class Config(object):\n",
    "    alphabet = \"abcdefghijklmnopqrstuvwxyz0123456789-,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\"\n",
    "#     alphabet = \"abcdefghijklmnopqrstuvwxyz0123456789\"\n",
    "    \n",
    "    sequenceLength = 1014\n",
    "    batchSize = 128\n",
    "    \n",
    "    rate = 0.8  # 训练集的比例\n",
    "    \n",
    "    dataSource = \"../data/preProcess/labeledCharTrain.csv\"\n",
    "    \n",
    "    training = TrainingConfig()\n",
    "    \n",
    "    model = ModelConfig()\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据预处理的类，生成训练集和测试集\n",
    "\n",
    "class Dataset(object):\n",
    "    def __init__(self, config):\n",
    "        self._dataSource = config.dataSource\n",
    "        \n",
    "        self._sequenceLength = config.sequenceLength\n",
    "        self._rate = config.rate\n",
    "        \n",
    "        self.trainReviews = []\n",
    "        self.trainLabels = []\n",
    "        \n",
    "        self.evalReviews = []\n",
    "        self.evalLabels = []\n",
    "        \n",
    "        self._alphabet = config.alphabet\n",
    "        self.charEmbedding =None\n",
    "        \n",
    "        self._charToIndex = {}\n",
    "        self._indexToChar = {}\n",
    "        \n",
    "    def _readData(self, filePath):\n",
    "        \"\"\"\n",
    "        从csv文件中读取数据集\n",
    "        \"\"\"\n",
    "        \n",
    "        df = pd.read_csv(filePath)\n",
    "        labels = df[\"sentiment\"].tolist()\n",
    "        review = df[\"review\"].tolist()\n",
    "        reviews = [[char for char in line if char != \" \"] for line in review]\n",
    "        \n",
    "        return reviews, labels\n",
    "\n",
    "    def _reviewProcess(self, review, sequenceLength, charToIndex):\n",
    "        \"\"\"\n",
    "        将数据集中的每条评论用index表示\n",
    "        wordToIndex中“pad”对应的index为0\n",
    "        \"\"\"\n",
    "        \n",
    "        reviewVec = np.zeros((sequenceLength))\n",
    "        sequenceLen = sequenceLength\n",
    "        \n",
    "        # 判断当前的序列是否小于定义的固定序列长度\n",
    "        if len(review) < sequenceLength:\n",
    "            sequenceLen = len(review)\n",
    "            \n",
    "        for i in range(sequenceLen):\n",
    "            if review[i] in charToIndex:\n",
    "                reviewVec[i] = charToIndex[review[i]]\n",
    "            else:\n",
    "                reviewVec[i] = charToIndex[\"UNK\"]\n",
    "\n",
    "        return reviewVec\n",
    "\n",
    "    def _genTrainEvalData(self, x, y, rate):\n",
    "        \"\"\"\n",
    "        生成训练集和验证集\n",
    "        \"\"\"\n",
    "        \n",
    "        reviews = []\n",
    "        labels = []\n",
    "        \n",
    "        # 遍历所有的文本，将文本中的词转换成index表示\n",
    "        \n",
    "        for i in range(len(x)):\n",
    "            reviewVec = self._reviewProcess(x[i], self._sequenceLength, self._charToIndex)\n",
    "            reviews.append(reviewVec)\n",
    "            \n",
    "            labels.append([y[i]])\n",
    "            \n",
    "        trainIndex = int(len(x) * rate)\n",
    "        \n",
    "        trainReviews = np.asarray(reviews[:trainIndex], dtype=\"int64\")\n",
    "        trainLabels = np.array(labels[:trainIndex], dtype=\"float32\")\n",
    "        \n",
    "        evalReviews = np.asarray(reviews[trainIndex:], dtype=\"int64\")\n",
    "        evalLabels = np.array(labels[trainIndex:], dtype=\"float32\")\n",
    "\n",
    "        return trainReviews, trainLabels, evalReviews, evalLabels\n",
    "        \n",
    "    def _genVocabulary(self, reviews):\n",
    "        \"\"\"\n",
    "        生成字符向量和字符-索引映射字典\n",
    "        \"\"\"\n",
    "        \n",
    "        chars = [char for char in self._alphabet]\n",
    "        \n",
    "        vocab, charEmbedding = self._getCharEmbedding(chars)\n",
    "        self.charEmbedding = charEmbedding\n",
    "        \n",
    "        self._charToIndex = dict(zip(vocab, list(range(len(vocab)))))\n",
    "        self._indexToChar = dict(zip(list(range(len(vocab))), vocab))\n",
    "        \n",
    "        # 将词汇-索引映射表保存为json数据，之后做inference时直接加载来处理数据\n",
    "        with open(\"../data/charJson/charToIndex.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self._charToIndex, f)\n",
    "        \n",
    "        with open(\"../data/charJson/indexToChar.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self._indexToChar, f)\n",
    "            \n",
    "    def _getCharEmbedding(self, chars):\n",
    "        \"\"\"\n",
    "        按照one的形式将字符映射成向量\n",
    "        \"\"\"\n",
    "        \n",
    "        alphabet = [\"UNK\"] + [char for char in self._alphabet]\n",
    "        vocab = [\"pad\"] + alphabet\n",
    "        charEmbedding = []\n",
    "        charEmbedding.append(np.zeros(len(alphabet), dtype=\"float32\"))\n",
    "        \n",
    "        for i, alpha in enumerate(alphabet):\n",
    "            onehot = np.zeros(len(alphabet), dtype=\"float32\")\n",
    "            \n",
    "            # 生成每个字符对应的向量\n",
    "            onehot[i] = 1\n",
    "            \n",
    "            # 生成字符嵌入的向量矩阵\n",
    "            charEmbedding.append(onehot)\n",
    "                \n",
    "        return vocab, np.array(charEmbedding)\n",
    "            \n",
    "    def dataGen(self):\n",
    "        \"\"\"\n",
    "        初始化训练集和验证集\n",
    "        \"\"\"\n",
    "        \n",
    "        # 初始化数据集\n",
    "        reviews, labels = self._readData(self._dataSource)\n",
    "        \n",
    "        # 初始化词汇-索引映射表和词向量矩阵\n",
    "        self._genVocabulary(reviews)\n",
    "        \n",
    "        # 初始化训练集和测试集\n",
    "        trainReviews, trainLabels, evalReviews, evalLabels = self._genTrainEvalData(reviews, labels, self._rate)\n",
    "        self.trainReviews = trainReviews\n",
    "        self.trainLabels = trainLabels\n",
    "        \n",
    "        self.evalReviews = evalReviews\n",
    "        self.evalLabels = evalLabels\n",
    "        \n",
    "        \n",
    "data = Dataset(config)\n",
    "data.dataGen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape: (20000, 1014)\n",
      "train label shape: (20000, 1)\n",
      "eval data shape: (5000, 1014)\n",
      "charEmbedding shape: (71, 70)\n"
     ]
    }
   ],
   "source": [
    "print(\"train data shape: {}\".format(data.trainReviews.shape))\n",
    "print(\"train label shape: {}\".format(data.trainLabels.shape))\n",
    "print(\"eval data shape: {}\".format(data.evalReviews.shape))\n",
    "print(\"charEmbedding shape: {}\".format(data.charEmbedding.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出batch数据集\n",
    "\n",
    "def nextBatch(x, y, batchSize):\n",
    "        \"\"\"\n",
    "        生成batch数据集，用生成器的方式输出\n",
    "        \"\"\"\n",
    "    \n",
    "        perm = np.arange(len(x))\n",
    "        np.random.shuffle(perm)\n",
    "        x = x[perm]\n",
    "        y = y[perm]\n",
    "        \n",
    "        numBatches = len(x) // batchSize\n",
    "\n",
    "        for i in range(numBatches):\n",
    "            start = i * batchSize\n",
    "            end = start + batchSize\n",
    "            batchX = np.array(x[start: end], dtype=\"int64\")\n",
    "            batchY = np.array(y[start: end], dtype=\"float32\")\n",
    "            \n",
    "            yield batchX, batchY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义char-CNN分类器\n",
    "\n",
    "class CharCNN(object):\n",
    "    \"\"\"\n",
    "    char-CNN用于文本分类\n",
    "    \"\"\"\n",
    "    def __init__(self, config, charEmbedding):\n",
    "        # placeholders for input, output and dropuot\n",
    "        self.inputX = tf.placeholder(tf.int32, [None, config.sequenceLength], name=\"inputX\")\n",
    "        self.inputY = tf.placeholder(tf.float32, [None, 1], name=\"inputY\")\n",
    "        self.dropoutKeepProb = tf.placeholder(tf.float32, name=\"dropoutKeepProb\")\n",
    "        self.isTraining = tf.placeholder(tf.bool, name=\"isTraining\")\n",
    "        \n",
    "        self.epsilon = config.model.epsilon\n",
    "        self.decay = config.model.decay\n",
    "        \n",
    "        # 字符嵌入\n",
    "        with tf.name_scope(\"embedding\"):\n",
    "            \n",
    "            # 利用one-hot的字符向量作为初始化词嵌入矩阵\n",
    "            self.W = tf.Variable(tf.cast(charEmbedding, dtype=tf.float32, name=\"charEmbedding\") ,name=\"W\")\n",
    "            # 获得字符嵌入\n",
    "            self.embededChars = tf.nn.embedding_lookup(self.W, self.inputX)\n",
    "            # 添加一个通道维度\n",
    "            self.embededCharsExpand = tf.expand_dims(self.embededChars, -1)\n",
    "\n",
    "        for i, cl in enumerate(config.model.convLayers):\n",
    "            print(\"开始第\" + str(i + 1) + \"卷积层的处理\")\n",
    "            # 利用命名空间name_scope来实现变量名复用\n",
    "            with tf.name_scope(\"convLayer-%s\"%(i+1)):\n",
    "                # 获取字符的向量长度\n",
    "                filterWidth = self.embededCharsExpand.get_shape()[2].value\n",
    "                \n",
    "                # filterShape = [height, width, in_channels, out_channels]\n",
    "                filterShape = [cl[1], filterWidth, 1, cl[0]]\n",
    "\n",
    "                stdv = 1 / sqrt(cl[0] * cl[1])\n",
    "                \n",
    "                # 初始化w和b的值\n",
    "                wConv = tf.Variable(tf.random_uniform(filterShape, minval=-stdv, maxval=stdv),\n",
    "                                     dtype='float32', name='w')\n",
    "                bConv = tf.Variable(tf.random_uniform(shape=[cl[0]], minval=-stdv, maxval=stdv), name='b')\n",
    "                \n",
    "#                 w_conv = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.05), name=\"w\")\n",
    "#                 b_conv = tf.Variable(tf.constant(0.1, shape=[cl[0]]), name=\"b\")\n",
    "                # 构建卷积层，可以直接将卷积核的初始化方法传入（w_conv）\n",
    "                conv = tf.nn.conv2d(self.embededCharsExpand, wConv, strides=[1, 1, 1, 1], padding=\"VALID\", name=\"conv\")\n",
    "                # 加上偏差\n",
    "                hConv = tf.nn.bias_add(conv, bConv)\n",
    "                # 可以直接加上relu函数，因为tf.nn.conv2d事实上是做了一个卷积运算，然后在这个运算结果上加上偏差，再导入到relu函数中\n",
    "                hConv = tf.nn.relu(hConv)\n",
    "                \n",
    "#                 with tf.name_scope(\"batchNormalization\"):\n",
    "#                     hConvBN = self._batchNorm(hConv)\n",
    "                \n",
    "                if cl[-1] is not None:\n",
    "                    ksizeShape = [1, cl[2], 1, 1]\n",
    "                    hPool = tf.nn.max_pool(hConv, ksize=ksizeShape, strides=ksizeShape, padding=\"VALID\", name=\"pool\")\n",
    "                else:\n",
    "                    hPool = hConv\n",
    "                    \n",
    "                print(hPool.shape)\n",
    "    \n",
    "                # 对维度进行转换，转换成卷积层的输入维度\n",
    "                self.embededCharsExpand = tf.transpose(hPool, [0, 1, 3, 2], name=\"transpose\")\n",
    "        print(self.embededCharsExpand)\n",
    "        with tf.name_scope(\"reshape\"):\n",
    "            fcDim = self.embededCharsExpand.get_shape()[1].value * self.embededCharsExpand.get_shape()[2].value\n",
    "            self.inputReshape = tf.reshape(self.embededCharsExpand, [-1, fcDim])\n",
    "        \n",
    "        # 保存的是神经元的个数[34*256, 1024, 1024]\n",
    "        weights = [fcDim] + config.model.fcLayers\n",
    "        \n",
    "        for i, fl in enumerate(config.model.fcLayers):\n",
    "            with tf.name_scope(\"fcLayer-%s\"%(i+1)):\n",
    "                print(\"开始第\" + str(i + 1) + \"全连接层的处理\")\n",
    "                stdv = 1 / sqrt(weights[i])\n",
    "                \n",
    "                # 定义全连接层的初始化方法，均匀分布初始化w和b的值\n",
    "                wFc = tf.Variable(tf.random_uniform([weights[i], fl], minval=-stdv, maxval=stdv), dtype=\"float32\", name=\"w\")\n",
    "                bFc = tf.Variable(tf.random_uniform(shape=[fl], minval=-stdv, maxval=stdv), dtype=\"float32\", name=\"b\")\n",
    "                \n",
    "#                 w_fc = tf.Variable(tf.truncated_normal([weights[i], fl], stddev=0.05), name=\"W\")\n",
    "#                 b_fc = tf.Variable(tf.constant(0.1, shape=[fl]), name=\"b\")\n",
    "                \n",
    "                self.fcInput = tf.nn.relu(tf.matmul(self.inputReshape, wFc) + bFc)\n",
    "                \n",
    "                with tf.name_scope(\"dropOut\"):\n",
    "                    self.fcInputDrop = tf.nn.dropout(self.fcInput, self.dropoutKeepProb)\n",
    "                    \n",
    "            self.inputReshape = self.fcInputDrop\n",
    "            \n",
    "        with tf.name_scope(\"outputLayer\"):\n",
    "            stdv = 1 / sqrt(weights[-1])\n",
    "            # 定义隐层到输出层的权重系数和偏差的初始化方法\n",
    "#             w_out = tf.Variable(tf.truncated_normal([fc_layers[-1], num_classes], stddev=0.1), name=\"W\")\n",
    "#             b_out = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
    "            \n",
    "            wOut = tf.Variable(tf.random_uniform([config.model.fcLayers[-1], 1], minval=-stdv, maxval=stdv), dtype=\"float32\", name=\"w\")\n",
    "            bOut = tf.Variable(tf.random_uniform(shape=[1], minval=-stdv, maxval=stdv), name=\"b\")\n",
    "            # tf.nn.xw_plus_b就是x和w的乘积加上b\n",
    "            self.predictions = tf.nn.xw_plus_b(self.inputReshape, wOut, bOut, name=\"predictions\")\n",
    "            # 进行二元分类\n",
    "            self.binaryPreds = tf.cast(tf.greater_equal(self.predictions, 0.5), tf.float32, name=\"binaryPreds\")\n",
    "            \n",
    "            \n",
    "        with tf.name_scope(\"loss\"):\n",
    "            # 定义损失函数，对预测值进行softmax，再求交叉熵。\n",
    "            \n",
    "            losses = tf.nn.sigmoid_cross_entropy_with_logits(logits=self.predictions, labels=self.inputY)\n",
    "            self.loss = tf.reduce_mean(losses)\n",
    "    \n",
    "    def _batchNorm(self, x):\n",
    "        \n",
    "        gamma = tf.Variable(tf.ones([x.get_shape()[3].value]))\n",
    "        beta = tf.Variable(tf.zeros([x.get_shape()[3].value]))\n",
    "\n",
    "        self.popMean = tf.Variable(tf.zeros([x.get_shape()[3].value]), trainable=False, name=\"popMean\")\n",
    "        self.popVariance = tf.Variable(tf.ones([x.get_shape()[3].value]), trainable=False, name=\"popVariance\")\n",
    "\n",
    "        def batchNormTraining():\n",
    "            # 一定要使用正确的维度确保计算的是每个特征图上的平均值和方差而不是整个网络节点上的统计分布值\n",
    "            batchMean, batchVariance = tf.nn.moments(x, [0, 1, 2], keep_dims=False)\n",
    "\n",
    "            decay = 0.99\n",
    "            trainMean = tf.assign(self.popMean, self.popMean*self.decay + batchMean*(1 - self.decay))\n",
    "            trainVariance = tf.assign(self.popVariance, self.popVariance*self.decay + batchVariance*(1 - self.decay))\n",
    "\n",
    "            with tf.control_dependencies([trainMean, trainVariance]):\n",
    "                return tf.nn.batch_normalization(x, batchMean, batchVariance, beta, gamma, self.epsilon)\n",
    "\n",
    "        def batchNormInference():\n",
    "            return tf.nn.batch_normalization(x, self.popMean, self.popVariance, beta, gamma, self.epsilon)\n",
    "\n",
    "        batchNormalizedOutput = tf.cond(self.isTraining, batchNormTraining, batchNormInference)\n",
    "        return tf.nn.relu(batchNormalizedOutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义性能指标函数\n",
    "\n",
    "def mean(item):\n",
    "    return sum(item) / len(item)\n",
    "\n",
    "\n",
    "def genMetrics(trueY, predY, binaryPredY):\n",
    "    \"\"\"\n",
    "    生成acc和auc值\n",
    "    \"\"\"\n",
    "    \n",
    "    auc = roc_auc_score(trueY, predY)\n",
    "    accuracy = accuracy_score(trueY, binaryPredY)\n",
    "    precision = precision_score(trueY, binaryPredY, average='macro')\n",
    "    recall = recall_score(trueY, binaryPredY, average='macro')\n",
    "    \n",
    "    return round(accuracy, 4), round(auc, 4), round(precision, 4), round(recall, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始第1卷积层的处理\n",
      "(?, 252, 1, 256)\n",
      "开始第2卷积层的处理\n",
      "(?, 61, 1, 256)\n",
      "开始第3卷积层的处理\n",
      "(?, 14, 1, 256)\n",
      "Tensor(\"convLayer-3/transpose:0\", shape=(?, 14, 256, 1), dtype=float32)\n",
      "开始第1全连接层的处理\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name convLayer-1/w:0/grad/hist is illegal; using convLayer-1/w_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name convLayer-1/w:0/grad/sparsity is illegal; using convLayer-1/w_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name convLayer-1/b:0/grad/hist is illegal; using convLayer-1/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name convLayer-1/b:0/grad/sparsity is illegal; using convLayer-1/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name convLayer-2/w:0/grad/hist is illegal; using convLayer-2/w_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name convLayer-2/w:0/grad/sparsity is illegal; using convLayer-2/w_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name convLayer-2/b:0/grad/hist is illegal; using convLayer-2/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name convLayer-2/b:0/grad/sparsity is illegal; using convLayer-2/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name convLayer-3/w:0/grad/hist is illegal; using convLayer-3/w_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name convLayer-3/w:0/grad/sparsity is illegal; using convLayer-3/w_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name convLayer-3/b:0/grad/hist is illegal; using convLayer-3/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name convLayer-3/b:0/grad/sparsity is illegal; using convLayer-3/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name fcLayer-1/w:0/grad/hist is illegal; using fcLayer-1/w_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name fcLayer-1/w:0/grad/sparsity is illegal; using fcLayer-1/w_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name fcLayer-1/b:0/grad/hist is illegal; using fcLayer-1/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name fcLayer-1/b:0/grad/sparsity is illegal; using fcLayer-1/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name outputLayer/w:0/grad/hist is illegal; using outputLayer/w_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name outputLayer/w:0/grad/sparsity is illegal; using outputLayer/w_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name outputLayer/b:0/grad/hist is illegal; using outputLayer/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name outputLayer/b:0/grad/sparsity is illegal; using outputLayer/b_0/grad/sparsity instead.\n",
      "Writing to /data4T/share/jiangxinyang848/textClassifier/charCNN/summarys\n",
      "\n",
      "start training model\n",
      "2018-12-29T17:20:08.745294, step: 1, loss: 0.6936788558959961, acc: 0.5312, auc: 0.536, precision: 0.2656, recall: 0.5\n",
      "2018-12-29T17:20:08.852246, step: 2, loss: 0.6919198036193848, acc: 0.4375, auc: 0.5097, precision: 0.2188, recall: 0.5\n",
      "2018-12-29T17:20:08.955735, step: 3, loss: 0.693166971206665, acc: 0.5078, auc: 0.5475, precision: 0.2539, recall: 0.5\n",
      "2018-12-29T17:20:09.044518, step: 4, loss: 0.6921828985214233, acc: 0.4531, auc: 0.5145, precision: 0.2266, recall: 0.5\n",
      "2018-12-29T17:20:09.142578, step: 5, loss: 0.6936560273170471, acc: 0.5234, auc: 0.4881, precision: 0.2617, recall: 0.5\n",
      "2018-12-29T17:20:09.238047, step: 6, loss: 0.6930904388427734, acc: 0.5, auc: 0.5022, precision: 0.25, recall: 0.5\n",
      "2018-12-29T17:20:09.338523, step: 7, loss: 0.6928656101226807, acc: 0.5, auc: 0.5571, precision: 0.25, recall: 0.5\n",
      "2018-12-29T17:20:09.432194, step: 8, loss: 0.6933512687683105, acc: 0.5, auc: 0.4785, precision: 0.25, recall: 0.5\n",
      "2018-12-29T17:20:09.528033, step: 9, loss: 0.6935836672782898, acc: 0.5234, auc: 0.5045, precision: 0.2617, recall: 0.5\n",
      "2018-12-29T17:20:09.631951, step: 10, loss: 0.6927999258041382, acc: 0.4844, auc: 0.5205, precision: 0.2422, recall: 0.5\n",
      "2018-12-29T17:20:09.727943, step: 11, loss: 0.6946361064910889, acc: 0.5625, auc: 0.4293, precision: 0.2812, recall: 0.5\n",
      "2018-12-29T17:20:09.826112, step: 12, loss: 0.6924072504043579, acc: 0.4453, auc: 0.4613, precision: 0.2227, recall: 0.5\n",
      "2018-12-29T17:20:09.922472, step: 13, loss: 0.6918758153915405, acc: 0.4453, auc: 0.5434, precision: 0.2227, recall: 0.5\n",
      "2018-12-29T17:20:10.018936, step: 14, loss: 0.6927357912063599, acc: 0.4844, auc: 0.5398, precision: 0.2422, recall: 0.5\n",
      "2018-12-29T17:20:10.120726, step: 15, loss: 0.6940320134162903, acc: 0.5312, auc: 0.4539, precision: 0.2656, recall: 0.5\n",
      "2018-12-29T17:20:10.230040, step: 16, loss: 0.6918733716011047, acc: 0.4531, auc: 0.5921, precision: 0.2266, recall: 0.5\n",
      "2018-12-29T17:20:10.334422, step: 17, loss: 0.6911148428916931, acc: 0.3984, auc: 0.5393, precision: 0.1992, recall: 0.5\n",
      "2018-12-29T17:20:10.427034, step: 18, loss: 0.6914733648300171, acc: 0.3984, auc: 0.4612, precision: 0.1992, recall: 0.5\n",
      "2018-12-29T17:20:10.522271, step: 19, loss: 0.694012463092804, acc: 0.5234, auc: 0.4235, precision: 0.2617, recall: 0.5\n",
      "2018-12-29T17:20:10.633934, step: 20, loss: 0.6927275657653809, acc: 0.4609, auc: 0.4547, precision: 0.2305, recall: 0.5\n",
      "2018-12-29T17:20:10.736254, step: 21, loss: 0.6942019462585449, acc: 0.5234, auc: 0.4023, precision: 0.2617, recall: 0.5\n",
      "2018-12-29T17:20:10.836367, step: 22, loss: 0.6922966241836548, acc: 0.4688, auc: 0.5505, precision: 0.2344, recall: 0.5\n",
      "2018-12-29T17:20:10.942075, step: 23, loss: 0.6933614015579224, acc: 0.4766, auc: 0.3839, precision: 0.2383, recall: 0.5\n",
      "2018-12-29T17:20:11.036027, step: 24, loss: 0.6934008598327637, acc: 0.5078, auc: 0.5065, precision: 0.2539, recall: 0.5\n",
      "2018-12-29T17:20:11.137574, step: 25, loss: 0.692190945148468, acc: 0.4531, auc: 0.4966, precision: 0.2266, recall: 0.5\n",
      "2018-12-29T17:20:11.246992, step: 26, loss: 0.6937903165817261, acc: 0.5156, auc: 0.436, precision: 0.2578, recall: 0.5\n",
      "2018-12-29T17:20:11.354066, step: 27, loss: 0.6944396495819092, acc: 0.5547, auc: 0.4828, precision: 0.2773, recall: 0.5\n",
      "2018-12-29T17:20:11.452048, step: 28, loss: 0.6913108229637146, acc: 0.4219, auc: 0.5763, precision: 0.2109, recall: 0.5\n",
      "2018-12-29T17:20:11.558365, step: 29, loss: 0.6933563947677612, acc: 0.5234, auc: 0.5503, precision: 0.2617, recall: 0.5\n",
      "2018-12-29T17:20:11.658318, step: 30, loss: 0.6934025883674622, acc: 0.5078, auc: 0.4933, precision: 0.2539, recall: 0.5\n",
      "2018-12-29T17:20:11.764814, step: 31, loss: 0.694335401058197, acc: 0.5703, auc: 0.539, precision: 0.2852, recall: 0.5\n",
      "2018-12-29T17:20:11.859878, step: 32, loss: 0.6946694254875183, acc: 0.5859, auc: 0.5303, precision: 0.293, recall: 0.5\n",
      "2018-12-29T17:20:11.966789, step: 33, loss: 0.6920750141143799, acc: 0.4609, auc: 0.5741, precision: 0.2305, recall: 0.5\n",
      "2018-12-29T17:20:12.083536, step: 34, loss: 0.6924453973770142, acc: 0.4609, auc: 0.492, precision: 0.2305, recall: 0.5\n",
      "2018-12-29T17:20:12.190680, step: 35, loss: 0.6928254961967468, acc: 0.4844, auc: 0.5337, precision: 0.2422, recall: 0.5\n",
      "2018-12-29T17:20:12.297704, step: 36, loss: 0.692455530166626, acc: 0.4609, auc: 0.5043, precision: 0.2305, recall: 0.5\n",
      "2018-12-29T17:20:12.396062, step: 37, loss: 0.6931300163269043, acc: 0.5156, auc: 0.5604, precision: 0.2578, recall: 0.5\n",
      "2018-12-29T17:20:12.493157, step: 38, loss: 0.6925010681152344, acc: 0.4609, auc: 0.4783, precision: 0.2305, recall: 0.5\n",
      "2018-12-29T17:20:12.588371, step: 39, loss: 0.692603588104248, acc: 0.4609, auc: 0.4559, precision: 0.2305, recall: 0.5\n",
      "2018-12-29T17:20:12.683414, step: 40, loss: 0.6930372714996338, acc: 0.4766, auc: 0.4419, precision: 0.2383, recall: 0.5\n",
      "2018-12-29T17:20:12.782398, step: 41, loss: 0.6933017373085022, acc: 0.5078, auc: 0.5092, precision: 0.2539, recall: 0.5\n",
      "2018-12-29T17:20:12.889821, step: 42, loss: 0.69243323802948, acc: 0.5, auc: 0.6313, precision: 0.25, recall: 0.5\n",
      "2018-12-29T17:20:12.989159, step: 43, loss: 0.6932070851325989, acc: 0.4922, auc: 0.4813, precision: 0.2461, recall: 0.5\n",
      "2018-12-29T17:20:13.098703, step: 44, loss: 0.6919212341308594, acc: 0.4453, auc: 0.554, precision: 0.2227, recall: 0.5\n",
      "2018-12-29T17:20:13.195641, step: 45, loss: 0.6931551098823547, acc: 0.5078, auc: 0.5385, precision: 0.2539, recall: 0.5\n",
      "2018-12-29T17:20:13.289403, step: 46, loss: 0.6945547461509705, acc: 0.5781, auc: 0.5521, precision: 0.2891, recall: 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-29T17:20:13.381719, step: 47, loss: 0.6948320865631104, acc: 0.5938, auc: 0.5281, precision: 0.2969, recall: 0.5\n",
      "2018-12-29T17:20:13.480149, step: 48, loss: 0.6928154230117798, acc: 0.5078, auc: 0.6037, precision: 0.2539, recall: 0.5\n",
      "2018-12-29T17:20:13.585767, step: 49, loss: 0.6941088438034058, acc: 0.5547, auc: 0.5147, precision: 0.2773, recall: 0.5\n",
      "2018-12-29T17:20:13.679185, step: 50, loss: 0.6926795840263367, acc: 0.5, auc: 0.5974, precision: 0.25, recall: 0.5\n",
      "2018-12-29T17:20:13.783856, step: 51, loss: 0.692560076713562, acc: 0.4531, auc: 0.4754, precision: 0.2266, recall: 0.5\n",
      "2018-12-29T17:20:13.890484, step: 52, loss: 0.694084107875824, acc: 0.5547, auc: 0.5028, precision: 0.2773, recall: 0.5\n",
      "2018-12-29T17:20:13.990723, step: 53, loss: 0.6924004554748535, acc: 0.4688, auc: 0.5532, precision: 0.2344, recall: 0.5\n",
      "2018-12-29T17:20:14.093485, step: 54, loss: 0.6915577054023743, acc: 0.4297, auc: 0.5841, precision: 0.2148, recall: 0.5\n",
      "2018-12-29T17:20:14.195967, step: 55, loss: 0.6930052638053894, acc: 0.4688, auc: 0.4294, precision: 0.2344, recall: 0.5\n",
      "2018-12-29T17:20:14.301816, step: 56, loss: 0.693146288394928, acc: 0.5, auc: 0.5066, precision: 0.25, recall: 0.5\n",
      "2018-12-29T17:20:14.399370, step: 57, loss: 0.694191575050354, acc: 0.5469, auc: 0.4953, precision: 0.2734, recall: 0.5\n",
      "2018-12-29T17:20:14.507055, step: 58, loss: 0.6945964097976685, acc: 0.5625, auc: 0.4382, precision: 0.2812, recall: 0.5\n",
      "2018-12-29T17:20:14.615768, step: 59, loss: 0.6938680410385132, acc: 0.5234, auc: 0.4297, precision: 0.2617, recall: 0.5\n",
      "2018-12-29T17:20:14.725959, step: 60, loss: 0.6928740739822388, acc: 0.4688, auc: 0.4471, precision: 0.2344, recall: 0.5\n",
      "2018-12-29T17:20:14.834481, step: 61, loss: 0.6933542490005493, acc: 0.5, auc: 0.4678, precision: 0.25, recall: 0.5\n",
      "2018-12-29T17:20:14.943095, step: 62, loss: 0.6927711367607117, acc: 0.4922, auc: 0.5526, precision: 0.2461, recall: 0.5\n",
      "2018-12-29T17:20:15.033287, step: 63, loss: 0.6936777234077454, acc: 0.5312, auc: 0.5105, precision: 0.2656, recall: 0.5\n",
      "2018-12-29T17:20:15.125952, step: 64, loss: 0.6931973695755005, acc: 0.4766, auc: 0.4441, precision: 0.2383, recall: 0.5\n",
      "2018-12-29T17:20:15.223435, step: 65, loss: 0.693895697593689, acc: 0.5234, auc: 0.4341, precision: 0.2617, recall: 0.5\n",
      "2018-12-29T17:20:15.321827, step: 66, loss: 0.6935745477676392, acc: 0.5156, auc: 0.4558, precision: 0.2578, recall: 0.5\n",
      "2018-12-29T17:20:15.418459, step: 67, loss: 0.6936007738113403, acc: 0.5156, auc: 0.4431, precision: 0.2578, recall: 0.5\n",
      "2018-12-29T17:20:15.524731, step: 68, loss: 0.6934263706207275, acc: 0.5156, auc: 0.4917, precision: 0.2578, recall: 0.5\n",
      "2018-12-29T17:20:15.619478, step: 69, loss: 0.6935574412345886, acc: 0.5156, auc: 0.4653, precision: 0.2578, recall: 0.5\n",
      "2018-12-29T17:20:15.723016, step: 70, loss: 0.6935362815856934, acc: 0.5391, auc: 0.5171, precision: 0.2695, recall: 0.5\n",
      "2018-12-29T17:20:15.823948, step: 71, loss: 0.6936377286911011, acc: 0.5, auc: 0.3921, precision: 0.25, recall: 0.5\n",
      "2018-12-29T17:20:15.923175, step: 72, loss: 0.6931458711624146, acc: 0.4766, auc: 0.4737, precision: 0.2383, recall: 0.5\n",
      "2018-12-29T17:20:16.038131, step: 73, loss: 0.6936770081520081, acc: 0.5156, auc: 0.457, precision: 0.2578, recall: 0.5\n",
      "2018-12-29T17:20:16.135604, step: 74, loss: 0.6931475400924683, acc: 0.5, auc: 0.4883, precision: 0.25, recall: 0.5\n",
      "2018-12-29T17:20:16.232236, step: 75, loss: 0.6929187774658203, acc: 0.4766, auc: 0.5075, precision: 0.2383, recall: 0.5\n",
      "2018-12-29T17:20:16.321616, step: 76, loss: 0.6924847960472107, acc: 0.4844, auc: 0.6114, precision: 0.2422, recall: 0.5\n",
      "2018-12-29T17:20:16.415145, step: 77, loss: 0.6929367780685425, acc: 0.4688, auc: 0.4833, precision: 0.2344, recall: 0.5\n",
      "2018-12-29T17:20:16.513526, step: 78, loss: 0.6939994692802429, acc: 0.5859, auc: 0.5582, precision: 0.293, recall: 0.5\n",
      "2018-12-29T17:20:16.603637, step: 79, loss: 0.6932151317596436, acc: 0.5078, auc: 0.5206, precision: 0.2539, recall: 0.5\n",
      "2018-12-29T17:20:16.701751, step: 80, loss: 0.6936421394348145, acc: 0.5625, auc: 0.4767, precision: 0.2812, recall: 0.5\n",
      "2018-12-29T17:20:16.800503, step: 81, loss: 0.6930181384086609, acc: 0.4297, auc: 0.4927, precision: 0.2148, recall: 0.5\n",
      "2018-12-29T17:20:16.895809, step: 82, loss: 0.6931033134460449, acc: 0.5312, auc: 0.5708, precision: 0.2656, recall: 0.5\n",
      "2018-12-29T17:20:17.002133, step: 83, loss: 0.6932121515274048, acc: 0.5469, auc: 0.5288, precision: 0.2734, recall: 0.5\n",
      "2018-12-29T17:20:17.116821, step: 84, loss: 0.6930205821990967, acc: 0.5078, auc: 0.5145, precision: 0.2539, recall: 0.5\n",
      "2018-12-29T17:20:17.224790, step: 85, loss: 0.6925804615020752, acc: 0.4922, auc: 0.5834, precision: 0.2461, recall: 0.5\n",
      "2018-12-29T17:20:17.321916, step: 86, loss: 0.6928266882896423, acc: 0.4922, auc: 0.5607, precision: 0.2461, recall: 0.5\n",
      "2018-12-29T17:20:17.419772, step: 87, loss: 0.6930443644523621, acc: 0.3984, auc: 0.4433, precision: 0.1992, recall: 0.5\n",
      "2018-12-29T17:20:17.510627, step: 88, loss: 0.6926337480545044, acc: 0.4766, auc: 0.564, precision: 0.2383, recall: 0.5\n",
      "2018-12-29T17:20:17.608664, step: 89, loss: 0.694156289100647, acc: 0.5391, auc: 0.3982, precision: 0.2695, recall: 0.5\n",
      "2018-12-29T17:20:17.707729, step: 90, loss: 0.6936748623847961, acc: 0.4844, auc: 0.3991, precision: 0.2422, recall: 0.5\n",
      "2018-12-29T17:20:17.802174, step: 91, loss: 0.6923415660858154, acc: 0.4453, auc: 0.5738, precision: 0.2227, recall: 0.5\n",
      "2018-12-29T17:20:17.898800, step: 92, loss: 0.6937656998634338, acc: 0.5391, auc: 0.4999, precision: 0.2695, recall: 0.5\n",
      "2018-12-29T17:20:18.004901, step: 93, loss: 0.6930328607559204, acc: 0.4922, auc: 0.5123, precision: 0.2461, recall: 0.5\n",
      "2018-12-29T17:20:18.116078, step: 94, loss: 0.6932017803192139, acc: 0.4922, auc: 0.494, precision: 0.2461, recall: 0.5\n",
      "2018-12-29T17:20:18.218431, step: 95, loss: 0.6928615570068359, acc: 0.4688, auc: 0.475, precision: 0.2344, recall: 0.5\n",
      "2018-12-29T17:20:18.310380, step: 96, loss: 0.6931804418563843, acc: 0.5391, auc: 0.594, precision: 0.2695, recall: 0.5\n",
      "2018-12-29T17:20:18.405118, step: 97, loss: 0.6937963962554932, acc: 0.5547, auc: 0.4744, precision: 0.2773, recall: 0.5\n",
      "2018-12-29T17:20:18.500152, step: 98, loss: 0.6931471824645996, acc: 0.4922, auc: 0.4889, precision: 0.2461, recall: 0.5\n",
      "2018-12-29T17:20:18.593060, step: 99, loss: 0.6930633783340454, acc: 0.4688, auc: 0.4978, precision: 0.2344, recall: 0.5\n",
      "2018-12-29T17:20:18.700349, step: 100, loss: 0.6926698684692383, acc: 0.4609, auc: 0.533, precision: 0.2305, recall: 0.5\n",
      "\n",
      "Evaluation:\n",
      "2018-12-29T17:20:22.552282, step: 100, loss: 0.6931109672937638, acc: 0.49439487179487185, auc: 0.49054615384615385, precision: 0.24719743589743592, recall: 0.5\n",
      "2018-12-29T17:20:22.644268, step: 101, loss: 0.6946608424186707, acc: 0.6016, auc: 0.4281, precision: 0.3008, recall: 0.5\n",
      "2018-12-29T17:20:22.736982, step: 102, loss: 0.6929951906204224, acc: 0.4844, auc: 0.5376, precision: 0.2422, recall: 0.5\n",
      "2018-12-29T17:20:22.829720, step: 103, loss: 0.6932190656661987, acc: 0.5078, auc: 0.4945, precision: 0.2539, recall: 0.5\n",
      "2018-12-29T17:20:22.925596, step: 104, loss: 0.6933408379554749, acc: 0.5781, auc: 0.474, precision: 0.2891, recall: 0.5\n",
      "2018-12-29T17:20:23.031617, step: 105, loss: 0.6925310492515564, acc: 0.5391, auc: 0.5095, precision: 0.2695, recall: 0.5\n",
      "2018-12-29T17:20:23.125734, step: 106, loss: 0.6934905648231506, acc: 0.4844, auc: 0.4739, precision: 0.2422, recall: 0.5\n",
      "2018-12-29T17:20:23.216193, step: 107, loss: 0.693138062953949, acc: 0.4844, auc: 0.5337, precision: 0.2422, recall: 0.5\n",
      "2018-12-29T17:20:23.323620, step: 108, loss: 0.693489670753479, acc: 0.5078, auc: 0.4481, precision: 0.2539, recall: 0.5\n",
      "2018-12-29T17:20:23.435418, step: 109, loss: 0.6926819086074829, acc: 0.5547, auc: 0.4927, precision: 0.2773, recall: 0.5\n",
      "2018-12-29T17:20:23.532028, step: 110, loss: 0.6933383345603943, acc: 0.5, auc: 0.4822, precision: 0.25, recall: 0.5\n",
      "2018-12-29T17:20:23.624594, step: 111, loss: 0.6931651830673218, acc: 0.4766, auc: 0.5339, precision: 0.2383, recall: 0.5\n",
      "2018-12-29T17:20:23.717812, step: 112, loss: 0.693031370639801, acc: 0.5078, auc: 0.5018, precision: 0.2539, recall: 0.5\n",
      "2018-12-29T17:20:23.818644, step: 113, loss: 0.6920655965805054, acc: 0.5391, auc: 0.5883, precision: 0.2695, recall: 0.5\n",
      "2018-12-29T17:20:23.914930, step: 114, loss: 0.6926422119140625, acc: 0.5156, auc: 0.5354, precision: 0.2578, recall: 0.5\n",
      "2018-12-29T17:20:24.007067, step: 115, loss: 0.693678081035614, acc: 0.4766, auc: 0.5026, precision: 0.2383, recall: 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-29T17:20:24.103917, step: 116, loss: 0.6937824487686157, acc: 0.4688, auc: 0.4775, precision: 0.2344, recall: 0.5\n",
      "2018-12-29T17:20:24.196825, step: 117, loss: 0.6926805973052979, acc: 0.5234, auc: 0.5566, precision: 0.2617, recall: 0.5\n",
      "2018-12-29T17:20:24.288063, step: 118, loss: 0.6930062174797058, acc: 0.5312, auc: 0.4733, precision: 0.2656, recall: 0.5\n",
      "2018-12-29T17:20:24.390876, step: 119, loss: 0.6936447620391846, acc: 0.4531, auc: 0.5567, precision: 0.2266, recall: 0.5\n",
      "2018-12-29T17:20:24.485181, step: 120, loss: 0.6934823989868164, acc: 0.4375, auc: 0.4973, precision: 0.2188, recall: 0.5\n",
      "2018-12-29T17:20:24.578189, step: 121, loss: 0.6930456161499023, acc: 0.4766, auc: 0.4837, precision: 0.2383, recall: 0.5\n",
      "2018-12-29T17:20:24.672182, step: 122, loss: 0.6933823823928833, acc: 0.5391, auc: 0.5633, precision: 0.2695, recall: 0.5\n",
      "2018-12-29T17:20:24.787839, step: 123, loss: 0.6927562952041626, acc: 0.5078, auc: 0.5665, precision: 0.2539, recall: 0.5\n",
      "2018-12-29T17:20:24.879823, step: 124, loss: 0.6931599378585815, acc: 0.5391, auc: 0.5011, precision: 0.2695, recall: 0.5\n",
      "2018-12-29T17:20:24.972305, step: 125, loss: 0.6933688521385193, acc: 0.5469, auc: 0.3909, precision: 0.2734, recall: 0.5\n",
      "2018-12-29T17:20:25.063593, step: 126, loss: 0.6944324970245361, acc: 0.4609, auc: 0.4827, precision: 0.2305, recall: 0.5\n",
      "2018-12-29T17:20:25.158092, step: 127, loss: 0.6933135986328125, acc: 0.4297, auc: 0.606, precision: 0.2148, recall: 0.5\n",
      "2018-12-29T17:20:25.267552, step: 128, loss: 0.6938719153404236, acc: 0.5312, auc: 0.4294, precision: 0.2656, recall: 0.5\n",
      "2018-12-29T17:20:25.362846, step: 129, loss: 0.6926063299179077, acc: 0.5, auc: 0.5896, precision: 0.25, recall: 0.5\n",
      "2018-12-29T17:20:25.451401, step: 130, loss: 0.6940502524375916, acc: 0.3594, auc: 0.4711, precision: 0.1797, recall: 0.5\n",
      "2018-12-29T17:20:25.559160, step: 131, loss: 0.6935721039772034, acc: 0.5, auc: 0.4541, precision: 0.25, recall: 0.5\n",
      "2018-12-29T17:20:25.666304, step: 132, loss: 0.6918027400970459, acc: 0.4609, auc: 0.522, precision: 0.2305, recall: 0.5\n",
      "2018-12-29T17:20:25.758496, step: 133, loss: 0.6925516128540039, acc: 0.5, auc: 0.6238, precision: 0.25, recall: 0.5\n",
      "2018-12-29T17:20:25.844629, step: 134, loss: 0.692143976688385, acc: 0.4688, auc: 0.4941, precision: 0.2344, recall: 0.5\n",
      "2018-12-29T17:20:25.929409, step: 135, loss: 0.691811740398407, acc: 0.4688, auc: 0.5135, precision: 0.2344, recall: 0.5\n",
      "2018-12-29T17:20:26.032201, step: 136, loss: 0.6971584558486938, acc: 0.5547, auc: 0.4267, precision: 0.2773, recall: 0.5\n",
      "2018-12-29T17:20:26.127755, step: 137, loss: 0.693015992641449, acc: 0.4922, auc: 0.4991, precision: 0.2461, recall: 0.5\n",
      "2018-12-29T17:20:26.215954, step: 138, loss: 0.6941918730735779, acc: 0.5547, auc: 0.5747, precision: 0.2773, recall: 0.5\n",
      "2018-12-29T17:20:26.300111, step: 139, loss: 0.6936376094818115, acc: 0.5703, auc: 0.5054, precision: 0.2852, recall: 0.5\n",
      "2018-12-29T17:20:26.385952, step: 140, loss: 0.6936740875244141, acc: 0.4531, auc: 0.5323, precision: 0.2266, recall: 0.5\n",
      "2018-12-29T17:20:26.480282, step: 141, loss: 0.693051815032959, acc: 0.4844, auc: 0.4924, precision: 0.2422, recall: 0.5\n",
      "2018-12-29T17:20:26.571122, step: 142, loss: 0.6930407881736755, acc: 0.5469, auc: 0.5645, precision: 0.2734, recall: 0.5\n",
      "2018-12-29T17:20:26.656982, step: 143, loss: 0.6933826804161072, acc: 0.5, auc: 0.4839, precision: 0.25, recall: 0.5\n",
      "2018-12-29T17:20:26.741069, step: 144, loss: 0.6925415396690369, acc: 0.4844, auc: 0.5714, precision: 0.2422, recall: 0.5\n",
      "2018-12-29T17:20:26.828266, step: 145, loss: 0.6931262016296387, acc: 0.5078, auc: 0.483, precision: 0.2539, recall: 0.5\n",
      "2018-12-29T17:20:26.916140, step: 146, loss: 0.6923872232437134, acc: 0.5312, auc: 0.5444, precision: 0.2656, recall: 0.5\n",
      "2018-12-29T17:20:27.009615, step: 147, loss: 0.692544162273407, acc: 0.5234, auc: 0.5246, precision: 0.2617, recall: 0.5\n",
      "2018-12-29T17:20:27.109802, step: 148, loss: 0.6904690265655518, acc: 0.5859, auc: 0.5004, precision: 0.293, recall: 0.5\n",
      "2018-12-29T17:20:27.196168, step: 149, loss: 0.6898130178451538, acc: 0.5625, auc: 0.4283, precision: 0.2812, recall: 0.5\n",
      "2018-12-29T17:20:27.296939, step: 150, loss: 0.7015949487686157, acc: 0.4609, auc: 0.3913, precision: 0.2305, recall: 0.5\n",
      "2018-12-29T17:20:27.378414, step: 151, loss: 0.6950337290763855, acc: 0.4766, auc: 0.425, precision: 0.2383, recall: 0.5\n",
      "2018-12-29T17:20:27.459887, step: 152, loss: 0.6905072331428528, acc: 0.6328, auc: 0.5353, precision: 0.3164, recall: 0.5\n",
      "2018-12-29T17:20:27.540809, step: 153, loss: 0.6932747960090637, acc: 0.4922, auc: 0.5917, precision: 0.2461, recall: 0.5\n",
      "2018-12-29T17:20:27.623610, step: 154, loss: 0.6910215616226196, acc: 0.5312, auc: 0.5547, precision: 0.2656, recall: 0.5\n",
      "2018-12-29T17:20:27.706562, step: 155, loss: 0.6950081586837769, acc: 0.4922, auc: 0.4889, precision: 0.2461, recall: 0.5\n",
      "2018-12-29T17:20:27.791793, step: 156, loss: 0.6923708319664001, acc: 0.5234, auc: 0.4808, precision: 0.2617, recall: 0.5\n",
      "start training model\n",
      "2018-12-29T17:20:28.011791, step: 157, loss: 0.6953626871109009, acc: 0.4609, auc: 0.5483, precision: 0.2305, recall: 0.5\n",
      "2018-12-29T17:20:28.092845, step: 158, loss: 0.6928276419639587, acc: 0.5156, auc: 0.4822, precision: 0.2578, recall: 0.5\n",
      "2018-12-29T17:20:28.174270, step: 159, loss: 0.6924010515213013, acc: 0.5078, auc: 0.5717, precision: 0.2539, recall: 0.5\n",
      "2018-12-29T17:20:28.256268, step: 160, loss: 0.695317804813385, acc: 0.4609, auc: 0.4883, precision: 0.2305, recall: 0.5\n",
      "2018-12-29T17:20:28.335134, step: 161, loss: 0.6926232576370239, acc: 0.5469, auc: 0.4833, precision: 0.2734, recall: 0.5\n",
      "2018-12-29T17:20:28.414496, step: 162, loss: 0.6955117583274841, acc: 0.4453, auc: 0.4791, precision: 0.2227, recall: 0.5\n",
      "2018-12-29T17:20:28.494377, step: 163, loss: 0.6928920745849609, acc: 0.4453, auc: 0.5574, precision: 0.2227, recall: 0.5\n",
      "2018-12-29T17:20:28.574390, step: 164, loss: 0.6937829256057739, acc: 0.5312, auc: 0.4885, precision: 0.2656, recall: 0.5\n",
      "2018-12-29T17:20:28.651324, step: 165, loss: 0.6935776472091675, acc: 0.4531, auc: 0.4685, precision: 0.2266, recall: 0.5\n",
      "2018-12-29T17:20:28.733889, step: 166, loss: 0.6947633624076843, acc: 0.5625, auc: 0.465, precision: 0.2812, recall: 0.5\n",
      "2018-12-29T17:20:28.815615, step: 167, loss: 0.6927758455276489, acc: 0.5234, auc: 0.5356, precision: 0.2617, recall: 0.5\n",
      "2018-12-29T17:20:28.897562, step: 168, loss: 0.6940423846244812, acc: 0.4766, auc: 0.4414, precision: 0.2383, recall: 0.5\n",
      "2018-12-29T17:20:28.994232, step: 169, loss: 0.6934146285057068, acc: 0.5156, auc: 0.4572, precision: 0.2578, recall: 0.5\n",
      "2018-12-29T17:20:29.086151, step: 170, loss: 0.6935992240905762, acc: 0.5078, auc: 0.4486, precision: 0.2539, recall: 0.5\n",
      "2018-12-29T17:20:29.169330, step: 171, loss: 0.6935482025146484, acc: 0.4922, auc: 0.4713, precision: 0.2461, recall: 0.5\n",
      "2018-12-29T17:20:29.244921, step: 172, loss: 0.6933809518814087, acc: 0.5156, auc: 0.4514, precision: 0.2578, recall: 0.5\n",
      "2018-12-29T17:20:29.330860, step: 173, loss: 0.6937329769134521, acc: 0.4609, auc: 0.4505, precision: 0.2305, recall: 0.5\n",
      "2018-12-29T17:20:29.414861, step: 174, loss: 0.6929046511650085, acc: 0.5078, auc: 0.5485, precision: 0.2539, recall: 0.5\n",
      "2018-12-29T17:20:29.493188, step: 175, loss: 0.6928102970123291, acc: 0.5, auc: 0.5674, precision: 0.25, recall: 0.5\n",
      "2018-12-29T17:20:29.589685, step: 176, loss: 0.6929088234901428, acc: 0.4141, auc: 0.4883, precision: 0.207, recall: 0.5\n",
      "2018-12-29T17:20:29.674348, step: 177, loss: 0.6938163042068481, acc: 0.4922, auc: 0.4117, precision: 0.2461, recall: 0.5\n",
      "2018-12-29T17:20:29.765721, step: 178, loss: 0.6964904069900513, acc: 0.5781, auc: 0.4154, precision: 0.2891, recall: 0.5\n",
      "2018-12-29T17:20:29.848006, step: 179, loss: 0.6929059028625488, acc: 0.4844, auc: 0.5455, precision: 0.2422, recall: 0.5\n",
      "2018-12-29T17:20:29.929389, step: 180, loss: 0.6933629512786865, acc: 0.5312, auc: 0.5282, precision: 0.2656, recall: 0.5\n",
      "2018-12-29T17:20:30.023225, step: 181, loss: 0.6937039494514465, acc: 0.5156, auc: 0.3954, precision: 0.2578, recall: 0.5\n",
      "2018-12-29T17:20:30.098868, step: 182, loss: 0.6933886408805847, acc: 0.5156, auc: 0.4399, precision: 0.2578, recall: 0.5\n",
      "2018-12-29T17:20:30.174568, step: 183, loss: 0.6928327083587646, acc: 0.5391, auc: 0.4648, precision: 0.2695, recall: 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-29T17:20:30.269838, step: 184, loss: 0.6912068724632263, acc: 0.5547, auc: 0.5135, precision: 0.2773, recall: 0.5\n",
      "2018-12-29T17:20:30.348206, step: 185, loss: 0.695985734462738, acc: 0.4766, auc: 0.5872, precision: 0.2383, recall: 0.5\n",
      "2018-12-29T17:20:30.427008, step: 186, loss: 0.6949476003646851, acc: 0.4375, auc: 0.4945, precision: 0.2188, recall: 0.5\n",
      "2018-12-29T17:20:30.507933, step: 187, loss: 0.6938637495040894, acc: 0.5391, auc: 0.4864, precision: 0.2695, recall: 0.5\n",
      "2018-12-29T17:20:30.589769, step: 188, loss: 0.6930268406867981, acc: 0.5078, auc: 0.5009, precision: 0.2539, recall: 0.5\n",
      "2018-12-29T17:20:30.678140, step: 189, loss: 0.6932500600814819, acc: 0.5, auc: 0.4932, precision: 0.25, recall: 0.5\n",
      "2018-12-29T17:20:30.757477, step: 190, loss: 0.6936135292053223, acc: 0.4922, auc: 0.4347, precision: 0.2461, recall: 0.5\n",
      "2018-12-29T17:20:30.844656, step: 191, loss: 0.6932598352432251, acc: 0.4922, auc: 0.4672, precision: 0.2461, recall: 0.5\n",
      "2018-12-29T17:20:30.925605, step: 192, loss: 0.6926400065422058, acc: 0.5078, auc: 0.6034, precision: 0.2539, recall: 0.5\n",
      "2018-12-29T17:20:30.999940, step: 193, loss: 0.693461537361145, acc: 0.4688, auc: 0.4681, precision: 0.2344, recall: 0.5\n",
      "2018-12-29T17:20:31.075321, step: 194, loss: 0.6934939622879028, acc: 0.5156, auc: 0.4853, precision: 0.2578, recall: 0.5\n",
      "2018-12-29T17:20:31.153810, step: 195, loss: 0.6928563714027405, acc: 0.5312, auc: 0.5725, precision: 0.2656, recall: 0.5\n",
      "2018-12-29T17:20:31.234559, step: 196, loss: 0.6926385164260864, acc: 0.5078, auc: 0.5299, precision: 0.2539, recall: 0.5\n",
      "2018-12-29T17:20:31.308099, step: 197, loss: 0.6925760507583618, acc: 0.5312, auc: 0.4706, precision: 0.2656, recall: 0.5\n",
      "2018-12-29T17:20:31.389305, step: 198, loss: 0.6935747265815735, acc: 0.4766, auc: 0.6024, precision: 0.2383, recall: 0.5\n",
      "2018-12-29T17:20:31.481681, step: 199, loss: 0.6913889646530151, acc: 0.5625, auc: 0.5836, precision: 0.2812, recall: 0.5\n",
      "2018-12-29T17:20:31.559832, step: 200, loss: 0.6870081424713135, acc: 0.5625, auc: 0.5432, precision: 0.2812, recall: 0.5\n",
      "\n",
      "Evaluation:\n",
      "2018-12-29T17:20:34.693260, step: 200, loss: 0.7101858059565226, acc: 0.49438974358974375, auc: 0.5205230769230769, precision: 0.24718717948717953, recall: 0.5\n",
      "2018-12-29T17:20:34.767659, step: 201, loss: 0.7088208198547363, acc: 0.4922, auc: 0.5336, precision: 0.2461, recall: 0.5\n",
      "2018-12-29T17:20:34.845767, step: 202, loss: 0.6927844285964966, acc: 0.4844, auc: 0.5181, precision: 0.2422, recall: 0.5\n",
      "2018-12-29T17:20:34.923349, step: 203, loss: 0.694316029548645, acc: 0.5312, auc: 0.4662, precision: 0.2656, recall: 0.5\n",
      "2018-12-29T17:20:35.000097, step: 204, loss: 0.69386887550354, acc: 0.4609, auc: 0.466, precision: 0.2305, recall: 0.5\n",
      "2018-12-29T17:20:35.079856, step: 205, loss: 0.6916993260383606, acc: 0.4453, auc: 0.5624, precision: 0.2227, recall: 0.5\n",
      "2018-12-29T17:20:35.159865, step: 206, loss: 0.6909056901931763, acc: 0.4453, auc: 0.5021, precision: 0.2227, recall: 0.5\n",
      "2018-12-29T17:20:35.250195, step: 207, loss: 0.694354772567749, acc: 0.5234, auc: 0.5887, precision: 0.2617, recall: 0.5\n",
      "2018-12-29T17:20:35.324146, step: 208, loss: 0.6940858364105225, acc: 0.5234, auc: 0.516, precision: 0.2617, recall: 0.5\n",
      "2018-12-29T17:20:35.403789, step: 209, loss: 0.6942614316940308, acc: 0.5703, auc: 0.5365, precision: 0.2852, recall: 0.5\n",
      "2018-12-29T17:20:35.489345, step: 210, loss: 0.6926311254501343, acc: 0.5625, auc: 0.4621, precision: 0.2812, recall: 0.5\n",
      "2018-12-29T17:20:35.570315, step: 211, loss: 0.6923154592514038, acc: 0.4922, auc: 0.5919, precision: 0.2461, recall: 0.5\n",
      "2018-12-29T17:20:35.655763, step: 212, loss: 0.6905480623245239, acc: 0.5547, auc: 0.5478, precision: 0.2773, recall: 0.5\n",
      "2018-12-29T17:20:35.739841, step: 213, loss: 0.6966057419776917, acc: 0.4766, auc: 0.4894, precision: 0.2383, recall: 0.5\n",
      "2018-12-29T17:20:35.823789, step: 214, loss: 0.6916202306747437, acc: 0.5078, auc: 0.5946, precision: 0.2539, recall: 0.5\n",
      "2018-12-29T17:20:35.903376, step: 215, loss: 0.6915431022644043, acc: 0.5312, auc: 0.5306, precision: 0.2656, recall: 0.5\n",
      "2018-12-29T17:20:35.980528, step: 216, loss: 0.6938502788543701, acc: 0.4766, auc: 0.5283, precision: 0.2383, recall: 0.5\n",
      "2018-12-29T17:20:36.074589, step: 217, loss: 0.6936827301979065, acc: 0.5, auc: 0.488, precision: 0.25, recall: 0.5\n",
      "2018-12-29T17:20:36.170291, step: 218, loss: 0.6932074427604675, acc: 0.3672, auc: 0.5566, precision: 0.1836, recall: 0.5\n",
      "2018-12-29T17:20:36.253775, step: 219, loss: 0.6922229528427124, acc: 0.4766, auc: 0.4913, precision: 0.2383, recall: 0.5\n",
      "2018-12-29T17:20:36.336999, step: 220, loss: 0.695357084274292, acc: 0.5312, auc: 0.561, precision: 0.2656, recall: 0.5\n",
      "2018-12-29T17:20:36.415746, step: 221, loss: 0.690933108329773, acc: 0.4375, auc: 0.5045, precision: 0.2188, recall: 0.5\n",
      "2018-12-29T17:20:36.494808, step: 222, loss: 0.6928157806396484, acc: 0.5156, auc: 0.5777, precision: 0.2578, recall: 0.5\n",
      "2018-12-29T17:20:36.567523, step: 223, loss: 0.6857466697692871, acc: 0.3672, auc: 0.5387, precision: 0.1836, recall: 0.5\n",
      "2018-12-29T17:20:36.639971, step: 224, loss: 0.693291187286377, acc: 0.4766, auc: 0.5013, precision: 0.2383, recall: 0.5\n",
      "2018-12-29T17:20:36.717400, step: 225, loss: 0.692436695098877, acc: 0.4922, auc: 0.5592, precision: 0.2461, recall: 0.5\n",
      "2018-12-29T17:20:36.797913, step: 226, loss: 0.6945770978927612, acc: 0.5, auc: 0.4949, precision: 0.25, recall: 0.5\n",
      "2018-12-29T17:20:36.877694, step: 227, loss: 0.6953023672103882, acc: 0.5391, auc: 0.5124, precision: 0.2695, recall: 0.5\n",
      "2018-12-29T17:20:36.973571, step: 228, loss: 0.6941295862197876, acc: 0.5312, auc: 0.5012, precision: 0.2656, recall: 0.5\n",
      "2018-12-29T17:20:37.062024, step: 229, loss: 0.6921424865722656, acc: 0.5469, auc: 0.5916, precision: 0.2734, recall: 0.5\n",
      "2018-12-29T17:20:37.141498, step: 230, loss: 0.6949753165245056, acc: 0.5312, auc: 0.4287, precision: 0.2656, recall: 0.5\n",
      "2018-12-29T17:20:37.223032, step: 231, loss: 0.6946686506271362, acc: 0.5078, auc: 0.431, precision: 0.2539, recall: 0.5\n",
      "2018-12-29T17:20:37.303821, step: 232, loss: 0.6935914754867554, acc: 0.4609, auc: 0.519, precision: 0.2305, recall: 0.5\n",
      "2018-12-29T17:20:37.382155, step: 233, loss: 0.6935775876045227, acc: 0.5078, auc: 0.4979, precision: 0.2539, recall: 0.5\n",
      "2018-12-29T17:20:37.460101, step: 234, loss: 0.69313645362854, acc: 0.4531, auc: 0.4879, precision: 0.2266, recall: 0.5\n",
      "2018-12-29T17:20:37.538966, step: 235, loss: 0.6926530003547668, acc: 0.4766, auc: 0.5104, precision: 0.2383, recall: 0.5\n",
      "2018-12-29T17:20:37.615735, step: 236, loss: 0.6945492625236511, acc: 0.5078, auc: 0.4349, precision: 0.2539, recall: 0.5\n",
      "2018-12-29T17:20:37.697023, step: 237, loss: 0.691582202911377, acc: 0.4609, auc: 0.5242, precision: 0.2305, recall: 0.5\n",
      "2018-12-29T17:20:37.773583, step: 238, loss: 0.6900269985198975, acc: 0.4531, auc: 0.5249, precision: 0.2266, recall: 0.5\n",
      "2018-12-29T17:20:37.853405, step: 239, loss: 0.690146267414093, acc: 0.4688, auc: 0.5446, precision: 0.2344, recall: 0.5\n",
      "2018-12-29T17:20:37.943798, step: 240, loss: 0.6743386387825012, acc: 0.3828, auc: 0.5823, precision: 0.1914, recall: 0.5\n",
      "2018-12-29T17:20:38.019535, step: 241, loss: 0.7631450891494751, acc: 0.4531, auc: 0.4332, precision: 0.4182, recall: 0.4586\n",
      "2018-12-29T17:20:38.113107, step: 242, loss: 0.6932827234268188, acc: 0.4844, auc: 0.5147, precision: 0.2422, recall: 0.5\n",
      "2018-12-29T17:20:38.184139, step: 243, loss: 0.687811553478241, acc: 0.4609, auc: 0.5947, precision: 0.2305, recall: 0.5\n",
      "2018-12-29T17:20:38.265911, step: 244, loss: 0.697597861289978, acc: 0.5312, auc: 0.5358, precision: 0.2656, recall: 0.5\n",
      "2018-12-29T17:20:38.341939, step: 245, loss: 0.6966381669044495, acc: 0.5312, auc: 0.5169, precision: 0.2656, recall: 0.5\n",
      "2018-12-29T17:20:38.417944, step: 246, loss: 0.6995296478271484, acc: 0.6016, auc: 0.5531, precision: 0.3008, recall: 0.5\n",
      "2018-12-29T17:20:38.498870, step: 247, loss: 0.6942747235298157, acc: 0.5469, auc: 0.453, precision: 0.2734, recall: 0.5\n",
      "2018-12-29T17:20:38.577737, step: 248, loss: 0.6914085149765015, acc: 0.5391, auc: 0.5154, precision: 0.2695, recall: 0.5\n",
      "2018-12-29T17:20:38.662570, step: 249, loss: 0.7007207870483398, acc: 0.4297, auc: 0.5059, precision: 0.2148, recall: 0.5\n",
      "2018-12-29T17:20:38.751212, step: 250, loss: 0.693489670753479, acc: 0.5312, auc: 0.5118, precision: 0.2656, recall: 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-29T17:20:38.829703, step: 251, loss: 0.693954348564148, acc: 0.4453, auc: 0.5036, precision: 0.2227, recall: 0.5\n",
      "2018-12-29T17:20:38.906244, step: 252, loss: 0.6944657564163208, acc: 0.5469, auc: 0.4855, precision: 0.2734, recall: 0.5\n",
      "2018-12-29T17:20:38.981030, step: 253, loss: 0.6930980086326599, acc: 0.4297, auc: 0.5046, precision: 0.2148, recall: 0.5\n",
      "2018-12-29T17:20:39.058927, step: 254, loss: 0.6939687132835388, acc: 0.5547, auc: 0.5246, precision: 0.2773, recall: 0.5\n",
      "2018-12-29T17:20:39.154522, step: 255, loss: 0.6924185752868652, acc: 0.5078, auc: 0.5529, precision: 0.2539, recall: 0.5\n",
      "2018-12-29T17:20:39.236660, step: 256, loss: 0.6932753920555115, acc: 0.5312, auc: 0.4824, precision: 0.2656, recall: 0.5\n",
      "2018-12-29T17:20:39.337078, step: 257, loss: 0.6939822435379028, acc: 0.4375, auc: 0.5258, precision: 0.2188, recall: 0.5\n",
      "2018-12-29T17:20:39.414235, step: 258, loss: 0.6936004757881165, acc: 0.5078, auc: 0.4684, precision: 0.2539, recall: 0.5\n",
      "2018-12-29T17:20:39.494702, step: 259, loss: 0.6940169334411621, acc: 0.5625, auc: 0.5055, precision: 0.2812, recall: 0.5\n",
      "2018-12-29T17:20:39.579360, step: 260, loss: 0.6920652389526367, acc: 0.4922, auc: 0.5883, precision: 0.2461, recall: 0.5\n",
      "2018-12-29T17:20:39.659166, step: 261, loss: 0.6927996873855591, acc: 0.5156, auc: 0.4927, precision: 0.2578, recall: 0.5\n",
      "2018-12-29T17:20:39.748615, step: 262, loss: 0.6926184892654419, acc: 0.5078, auc: 0.5333, precision: 0.2539, recall: 0.5\n",
      "2018-12-29T17:20:39.828331, step: 263, loss: 0.6963601112365723, acc: 0.4453, auc: 0.383, precision: 0.2227, recall: 0.5\n",
      "2018-12-29T17:20:39.909876, step: 264, loss: 0.6902990341186523, acc: 0.4062, auc: 0.545, precision: 0.2031, recall: 0.5\n",
      "2018-12-29T17:20:39.982366, step: 265, loss: 0.6974867582321167, acc: 0.5781, auc: 0.5611, precision: 0.2891, recall: 0.5\n",
      "2018-12-29T17:20:40.061057, step: 266, loss: 0.6919338703155518, acc: 0.4688, auc: 0.5402, precision: 0.2344, recall: 0.5\n",
      "2018-12-29T17:20:40.148907, step: 267, loss: 0.6941344738006592, acc: 0.5234, auc: 0.5016, precision: 0.2617, recall: 0.5\n",
      "2018-12-29T17:20:40.224115, step: 268, loss: 0.6918607354164124, acc: 0.5, auc: 0.5271, precision: 0.25, recall: 0.5\n",
      "2018-12-29T17:20:40.317850, step: 269, loss: 0.6932944059371948, acc: 0.5, auc: 0.5017, precision: 0.25, recall: 0.5\n",
      "2018-12-29T17:20:40.410445, step: 270, loss: 0.6929616928100586, acc: 0.5234, auc: 0.5133, precision: 0.2617, recall: 0.5\n",
      "2018-12-29T17:20:40.486401, step: 271, loss: 0.6918154358863831, acc: 0.4297, auc: 0.5843, precision: 0.2148, recall: 0.5\n",
      "2018-12-29T17:20:40.563973, step: 272, loss: 0.6938859224319458, acc: 0.5391, auc: 0.5355, precision: 0.2695, recall: 0.5\n",
      "2018-12-29T17:20:40.647222, step: 273, loss: 0.6945725083351135, acc: 0.5078, auc: 0.4615, precision: 0.2539, recall: 0.5\n",
      "2018-12-29T17:20:40.734301, step: 274, loss: 0.6910628080368042, acc: 0.4375, auc: 0.5608, precision: 0.2188, recall: 0.5\n",
      "2018-12-29T17:20:40.810113, step: 275, loss: 0.6934731006622314, acc: 0.4844, auc: 0.4892, precision: 0.2422, recall: 0.5\n",
      "2018-12-29T17:20:40.894793, step: 276, loss: 0.6927663087844849, acc: 0.5156, auc: 0.5447, precision: 0.2578, recall: 0.5\n",
      "2018-12-29T17:20:40.979388, step: 277, loss: 0.6936460733413696, acc: 0.5156, auc: 0.4844, precision: 0.2578, recall: 0.5\n",
      "2018-12-29T17:20:41.052855, step: 278, loss: 0.6933203935623169, acc: 0.4844, auc: 0.48, precision: 0.2422, recall: 0.5\n",
      "2018-12-29T17:20:41.128260, step: 279, loss: 0.6945391893386841, acc: 0.5, auc: 0.4426, precision: 0.25, recall: 0.5\n",
      "2018-12-29T17:20:41.206367, step: 280, loss: 0.6934260129928589, acc: 0.4688, auc: 0.4625, precision: 0.2344, recall: 0.5\n",
      "2018-12-29T17:20:41.287099, step: 281, loss: 0.6934057474136353, acc: 0.5156, auc: 0.5086, precision: 0.2578, recall: 0.5\n",
      "2018-12-29T17:20:41.364504, step: 282, loss: 0.6941192746162415, acc: 0.5, auc: 0.4365, precision: 0.25, recall: 0.5\n",
      "2018-12-29T17:20:41.445903, step: 283, loss: 0.6928667426109314, acc: 0.4766, auc: 0.5138, precision: 0.2383, recall: 0.5\n",
      "2018-12-29T17:20:41.540611, step: 284, loss: 0.6913060545921326, acc: 0.4453, auc: 0.6106, precision: 0.2227, recall: 0.5\n",
      "2018-12-29T17:20:41.615332, step: 285, loss: 0.6939195990562439, acc: 0.5078, auc: 0.4884, precision: 0.2539, recall: 0.5\n",
      "2018-12-29T17:20:41.697676, step: 286, loss: 0.6955479383468628, acc: 0.5547, auc: 0.4539, precision: 0.2773, recall: 0.5\n",
      "2018-12-29T17:20:41.775880, step: 287, loss: 0.6927179098129272, acc: 0.4766, auc: 0.5361, precision: 0.2383, recall: 0.5\n",
      "2018-12-29T17:20:41.855397, step: 288, loss: 0.6923980712890625, acc: 0.5078, auc: 0.58, precision: 0.2539, recall: 0.5\n",
      "2018-12-29T17:20:41.945878, step: 289, loss: 0.6934181451797485, acc: 0.5469, auc: 0.5328, precision: 0.2734, recall: 0.5\n",
      "2018-12-29T17:20:42.024182, step: 290, loss: 0.6922787427902222, acc: 0.5312, auc: 0.5059, precision: 0.2656, recall: 0.5\n",
      "2018-12-29T17:20:42.104199, step: 291, loss: 0.6883707046508789, acc: 0.5938, auc: 0.4772, precision: 0.2969, recall: 0.5\n",
      "2018-12-29T17:20:42.181870, step: 292, loss: 0.7190288305282593, acc: 0.4766, auc: 0.4473, precision: 0.2383, recall: 0.5\n",
      "2018-12-29T17:20:42.261151, step: 293, loss: 0.6916051506996155, acc: 0.5547, auc: 0.5229, precision: 0.2773, recall: 0.5\n",
      "2018-12-29T17:20:42.342391, step: 294, loss: 0.6941167116165161, acc: 0.4844, auc: 0.4968, precision: 0.2422, recall: 0.5\n",
      "2018-12-29T17:20:42.421185, step: 295, loss: 0.6928161382675171, acc: 0.4688, auc: 0.6152, precision: 0.2344, recall: 0.5\n",
      "2018-12-29T17:20:42.505737, step: 296, loss: 0.6927691698074341, acc: 0.5312, auc: 0.5145, precision: 0.2656, recall: 0.5\n",
      "2018-12-29T17:20:42.578999, step: 297, loss: 0.6910557150840759, acc: 0.5547, auc: 0.5483, precision: 0.2773, recall: 0.5\n",
      "2018-12-29T17:20:42.670747, step: 298, loss: 0.6944037675857544, acc: 0.4766, auc: 0.5241, precision: 0.2383, recall: 0.5\n",
      "2018-12-29T17:20:42.742053, step: 299, loss: 0.6935939788818359, acc: 0.5, auc: 0.4819, precision: 0.25, recall: 0.5\n",
      "2018-12-29T17:20:42.817791, step: 300, loss: 0.6905779838562012, acc: 0.5625, auc: 0.566, precision: 0.2812, recall: 0.5\n",
      "\n",
      "Evaluation:\n",
      "2018-12-29T17:20:45.753596, step: 300, loss: 0.6934188589071616, acc: 0.49438974358974364, auc: 0.6042153846153844, precision: 0.24719743589743595, recall: 0.5\n",
      "2018-12-29T17:20:45.835322, step: 301, loss: 0.6925930976867676, acc: 0.5156, auc: 0.5064, precision: 0.2578, recall: 0.5\n",
      "2018-12-29T17:20:45.927105, step: 302, loss: 0.6936360597610474, acc: 0.4922, auc: 0.5211, precision: 0.2461, recall: 0.5\n",
      "2018-12-29T17:20:46.004383, step: 303, loss: 0.6909267902374268, acc: 0.5469, auc: 0.4978, precision: 0.2734, recall: 0.5\n",
      "2018-12-29T17:20:46.082351, step: 304, loss: 0.6899089813232422, acc: 0.5156, auc: 0.6139, precision: 0.2578, recall: 0.5\n",
      "2018-12-29T17:20:46.162686, step: 305, loss: 0.6935537457466125, acc: 0.5, auc: 0.5317, precision: 0.25, recall: 0.5\n",
      "2018-12-29T17:20:46.249276, step: 306, loss: 0.6912778615951538, acc: 0.5312, auc: 0.5098, precision: 0.2656, recall: 0.5\n",
      "2018-12-29T17:20:46.333680, step: 307, loss: 0.6940704584121704, acc: 0.4844, auc: 0.5442, precision: 0.2422, recall: 0.5\n",
      "2018-12-29T17:20:46.418625, step: 308, loss: 0.6915415525436401, acc: 0.4531, auc: 0.5047, precision: 0.2266, recall: 0.5\n",
      "2018-12-29T17:20:46.513600, step: 309, loss: 0.6931381821632385, acc: 0.5312, auc: 0.6574, precision: 0.2656, recall: 0.5\n",
      "2018-12-29T17:20:46.596819, step: 310, loss: 0.6796852350234985, acc: 0.6094, auc: 0.5595, precision: 0.3047, recall: 0.5\n",
      "2018-12-29T17:20:46.671093, step: 311, loss: 0.7230671644210815, acc: 0.4766, auc: 0.5481, precision: 0.2383, recall: 0.5\n",
      "2018-12-29T17:20:46.750628, step: 312, loss: 0.6896963119506836, acc: 0.5391, auc: 0.5893, precision: 0.2695, recall: 0.5\n",
      "start training model\n",
      "2018-12-29T17:20:46.980755, step: 313, loss: 0.6949058175086975, acc: 0.4375, auc: 0.5627, precision: 0.2188, recall: 0.5\n",
      "2018-12-29T17:20:47.058187, step: 314, loss: 0.6938261389732361, acc: 0.5391, auc: 0.5291, precision: 0.2695, recall: 0.5\n",
      "2018-12-29T17:20:47.130628, step: 315, loss: 0.6934173107147217, acc: 0.4609, auc: 0.5244, precision: 0.2305, recall: 0.5\n",
      "2018-12-29T17:20:47.225915, step: 316, loss: 0.6881044507026672, acc: 0.4375, auc: 0.5521, precision: 0.2188, recall: 0.5\n",
      "2018-12-29T17:20:47.307950, step: 317, loss: 0.6891870498657227, acc: 0.4688, auc: 0.5608, precision: 0.2344, recall: 0.5\n",
      "2018-12-29T17:20:47.388501, step: 318, loss: 0.6949522495269775, acc: 0.6094, auc: 0.6044, precision: 0.3047, recall: 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-29T17:20:47.466670, step: 319, loss: 0.689606785774231, acc: 0.5234, auc: 0.5542, precision: 0.2617, recall: 0.5\n",
      "2018-12-29T17:20:47.562184, step: 320, loss: 0.6964305639266968, acc: 0.4609, auc: 0.5888, precision: 0.2305, recall: 0.5\n",
      "2018-12-29T17:20:47.646288, step: 321, loss: 0.6951625943183899, acc: 0.4766, auc: 0.4908, precision: 0.2383, recall: 0.5\n",
      "2018-12-29T17:20:47.728919, step: 322, loss: 0.6903175115585327, acc: 0.4844, auc: 0.6361, precision: 0.2422, recall: 0.5\n",
      "2018-12-29T17:20:47.805210, step: 323, loss: 0.6894237995147705, acc: 0.5547, auc: 0.5471, precision: 0.2773, recall: 0.5\n",
      "2018-12-29T17:20:47.884670, step: 324, loss: 0.7060839533805847, acc: 0.3828, auc: 0.4888, precision: 0.1914, recall: 0.5\n",
      "2018-12-29T17:20:47.957492, step: 325, loss: 0.6925733685493469, acc: 0.5391, auc: 0.5475, precision: 0.2695, recall: 0.5\n",
      "2018-12-29T17:20:48.038090, step: 326, loss: 0.6891541481018066, acc: 0.5391, auc: 0.6185, precision: 0.2695, recall: 0.5\n",
      "2018-12-29T17:20:48.131587, step: 327, loss: 0.6863321661949158, acc: 0.4922, auc: 0.6571, precision: 0.2461, recall: 0.5\n",
      "2018-12-29T17:20:48.226785, step: 328, loss: 0.6921805143356323, acc: 0.5547, auc: 0.6731, precision: 0.2773, recall: 0.5\n",
      "2018-12-29T17:20:48.317824, step: 329, loss: 0.704376757144928, acc: 0.4375, auc: 0.4886, precision: 0.2188, recall: 0.5\n",
      "2018-12-29T17:20:48.402007, step: 330, loss: 0.6905789375305176, acc: 0.4609, auc: 0.6296, precision: 0.2305, recall: 0.5\n",
      "2018-12-29T17:20:48.480785, step: 331, loss: 0.688432514667511, acc: 0.4375, auc: 0.6161, precision: 0.2188, recall: 0.5\n",
      "2018-12-29T17:20:48.559947, step: 332, loss: 0.6881313323974609, acc: 0.4688, auc: 0.5792, precision: 0.2344, recall: 0.5\n",
      "2018-12-29T17:20:48.634502, step: 333, loss: 0.6859941482543945, acc: 0.4766, auc: 0.6349, precision: 0.2383, recall: 0.5\n",
      "2018-12-29T17:20:48.708796, step: 334, loss: 0.6936750411987305, acc: 0.5312, auc: 0.4993, precision: 0.2656, recall: 0.5\n",
      "2018-12-29T17:20:48.785785, step: 335, loss: 0.6852700114250183, acc: 0.5156, auc: 0.7006, precision: 0.2578, recall: 0.5\n",
      "2018-12-29T17:20:48.878110, step: 336, loss: 0.6896844506263733, acc: 0.4688, auc: 0.5971, precision: 0.2344, recall: 0.5\n",
      "2018-12-29T17:20:48.966324, step: 337, loss: 0.6780529618263245, acc: 0.4375, auc: 0.6491, precision: 0.2188, recall: 0.5\n",
      "2018-12-29T17:20:49.041496, step: 338, loss: 0.6844169497489929, acc: 0.5938, auc: 0.6213, precision: 0.5887, recall: 0.5893\n",
      "2018-12-29T17:20:49.137480, step: 339, loss: 0.679139256477356, acc: 0.4844, auc: 0.7148, precision: 0.2422, recall: 0.5\n",
      "2018-12-29T17:20:49.226788, step: 340, loss: 0.6862838268280029, acc: 0.5312, auc: 0.66, precision: 0.2656, recall: 0.5\n",
      "2018-12-29T17:20:49.320014, step: 341, loss: 0.6849395632743835, acc: 0.5234, auc: 0.6056, precision: 0.2617, recall: 0.5\n",
      "2018-12-29T17:20:49.414310, step: 342, loss: 0.679149866104126, acc: 0.5078, auc: 0.6718, precision: 0.2539, recall: 0.5\n",
      "2018-12-29T17:20:49.497066, step: 343, loss: 0.6822819709777832, acc: 0.5391, auc: 0.6033, precision: 0.2695, recall: 0.5\n",
      "2018-12-29T17:20:49.592890, step: 344, loss: 0.6709709763526917, acc: 0.5469, auc: 0.6938, precision: 0.2734, recall: 0.5\n",
      "2018-12-29T17:20:49.686196, step: 345, loss: 0.7173505425453186, acc: 0.5625, auc: 0.5516, precision: 0.2812, recall: 0.5\n",
      "2018-12-29T17:20:49.767704, step: 346, loss: 0.6818679571151733, acc: 0.5391, auc: 0.6566, precision: 0.2695, recall: 0.5\n",
      "2018-12-29T17:20:49.843950, step: 347, loss: 0.6882936358451843, acc: 0.4453, auc: 0.6464, precision: 0.2227, recall: 0.5\n",
      "2018-12-29T17:20:49.927197, step: 348, loss: 0.6732751131057739, acc: 0.4531, auc: 0.6883, precision: 0.2283, recall: 0.4915\n",
      "2018-12-29T17:20:50.020815, step: 349, loss: 0.6693403720855713, acc: 0.5156, auc: 0.6855, precision: 0.2578, recall: 0.5\n",
      "2018-12-29T17:20:50.115406, step: 350, loss: 0.6638359427452087, acc: 0.5312, auc: 0.7174, precision: 0.7638, recall: 0.5082\n",
      "2018-12-29T17:20:50.210837, step: 351, loss: 0.7206660509109497, acc: 0.4297, auc: 0.6374, precision: 0.2148, recall: 0.5\n",
      "2018-12-29T17:20:50.294787, step: 352, loss: 0.7231007814407349, acc: 0.6094, auc: 0.6563, precision: 0.6065, recall: 0.5856\n",
      "2018-12-29T17:20:50.387651, step: 353, loss: 0.6790753602981567, acc: 0.5, auc: 0.6855, precision: 0.25, recall: 0.5\n",
      "2018-12-29T17:20:50.481876, step: 354, loss: 0.6894301176071167, acc: 0.4531, auc: 0.665, precision: 0.2266, recall: 0.5\n",
      "2018-12-29T17:20:50.567123, step: 355, loss: 0.652022123336792, acc: 0.4609, auc: 0.7932, precision: 0.2305, recall: 0.5\n",
      "2018-12-29T17:20:50.644644, step: 356, loss: 0.6691900491714478, acc: 0.4531, auc: 0.6648, precision: 0.2266, recall: 0.5\n",
      "2018-12-29T17:20:50.717572, step: 357, loss: 0.6901035308837891, acc: 0.6094, auc: 0.6631, precision: 0.6867, recall: 0.5734\n",
      "2018-12-29T17:20:50.791897, step: 358, loss: 0.6605867743492126, acc: 0.5547, auc: 0.6936, precision: 0.2773, recall: 0.5\n",
      "2018-12-29T17:20:50.873426, step: 359, loss: 0.6621873378753662, acc: 0.5234, auc: 0.7079, precision: 0.2617, recall: 0.5\n",
      "2018-12-29T17:20:50.946219, step: 360, loss: 0.6657493710517883, acc: 0.5469, auc: 0.6571, precision: 0.7717, recall: 0.5085\n",
      "2018-12-29T17:20:51.026094, step: 361, loss: 0.6753199696540833, acc: 0.5156, auc: 0.6593, precision: 0.2578, recall: 0.5\n",
      "2018-12-29T17:20:51.101961, step: 362, loss: 0.717109203338623, acc: 0.5859, auc: 0.6124, precision: 0.5725, recall: 0.5585\n",
      "2018-12-29T17:20:51.183286, step: 363, loss: 0.7086344957351685, acc: 0.4219, auc: 0.726, precision: 0.2109, recall: 0.5\n",
      "2018-12-29T17:20:51.259845, step: 364, loss: 0.6529695391654968, acc: 0.4453, auc: 0.7796, precision: 0.2227, recall: 0.5\n",
      "2018-12-29T17:20:51.340967, step: 365, loss: 0.648627519607544, acc: 0.4453, auc: 0.7032, precision: 0.4722, recall: 0.4983\n",
      "2018-12-29T17:20:51.428624, step: 366, loss: 0.6628928780555725, acc: 0.5391, auc: 0.6802, precision: 0.5997, recall: 0.5179\n",
      "2018-12-29T17:20:51.510085, step: 367, loss: 0.6554076671600342, acc: 0.5078, auc: 0.6852, precision: 0.2539, recall: 0.5\n",
      "2018-12-29T17:20:51.597217, step: 368, loss: 0.6600255966186523, acc: 0.6016, auc: 0.7297, precision: 0.7292, recall: 0.5544\n",
      "2018-12-29T17:20:51.675605, step: 369, loss: 0.7839034795761108, acc: 0.4375, auc: 0.5985, precision: 0.2188, recall: 0.5\n",
      "2018-12-29T17:20:51.765928, step: 370, loss: 0.6775966286659241, acc: 0.6406, auc: 0.7162, precision: 0.7259, recall: 0.6196\n",
      "2018-12-29T17:20:51.858937, step: 371, loss: 0.6425054669380188, acc: 0.5078, auc: 0.7836, precision: 0.2539, recall: 0.5\n",
      "2018-12-29T17:20:51.942904, step: 372, loss: 0.6454153060913086, acc: 0.4609, auc: 0.7401, precision: 0.7262, recall: 0.5141\n",
      "2018-12-29T17:20:52.020558, step: 373, loss: 0.6512559056282043, acc: 0.5938, auc: 0.7189, precision: 0.6422, recall: 0.5782\n",
      "2018-12-29T17:20:52.098983, step: 374, loss: 0.6362336874008179, acc: 0.5234, auc: 0.7488, precision: 0.7598, recall: 0.5081\n",
      "2018-12-29T17:20:52.188797, step: 375, loss: 0.6363146305084229, acc: 0.6953, auc: 0.7625, precision: 0.7178, recall: 0.6899\n",
      "2018-12-29T17:20:52.267340, step: 376, loss: 0.6760897636413574, acc: 0.5391, auc: 0.7015, precision: 0.7659, recall: 0.5164\n",
      "2018-12-29T17:20:52.358129, step: 377, loss: 0.65587317943573, acc: 0.6094, auc: 0.6791, precision: 0.6214, recall: 0.5837\n",
      "2018-12-29T17:20:52.439866, step: 378, loss: 0.6770426034927368, acc: 0.4922, auc: 0.679, precision: 0.7421, recall: 0.5149\n",
      "2018-12-29T17:20:52.535525, step: 379, loss: 0.6873314380645752, acc: 0.6562, auc: 0.6847, precision: 0.6521, recall: 0.6521\n",
      "2018-12-29T17:20:52.633315, step: 380, loss: 0.6377426981925964, acc: 0.5469, auc: 0.7291, precision: 0.2734, recall: 0.5\n",
      "2018-12-29T17:20:52.720883, step: 381, loss: 0.6599558591842651, acc: 0.4688, auc: 0.7047, precision: 0.728, recall: 0.5211\n",
      "2018-12-29T17:20:52.805185, step: 382, loss: 0.6365083456039429, acc: 0.6641, auc: 0.718, precision: 0.6623, recall: 0.6626\n",
      "2018-12-29T17:20:52.881421, step: 383, loss: 0.6284875869750977, acc: 0.5312, auc: 0.7302, precision: 0.6175, recall: 0.5243\n",
      "2018-12-29T17:20:52.973139, step: 384, loss: 0.6454917192459106, acc: 0.6172, auc: 0.6875, precision: 0.6267, recall: 0.5995\n",
      "2018-12-29T17:20:53.053339, step: 385, loss: 0.6778891086578369, acc: 0.5156, auc: 0.6404, precision: 0.754, recall: 0.5156\n",
      "2018-12-29T17:20:53.129242, step: 386, loss: 0.6652851104736328, acc: 0.6094, auc: 0.6767, precision: 0.6092, recall: 0.609\n",
      "2018-12-29T17:20:53.200392, step: 387, loss: 0.6787166595458984, acc: 0.5234, auc: 0.7011, precision: 0.7598, recall: 0.5081\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-29T17:20:53.293826, step: 388, loss: 0.677283763885498, acc: 0.6172, auc: 0.6721, precision: 0.6152, recall: 0.6123\n",
      "2018-12-29T17:20:53.372696, step: 389, loss: 0.6716902256011963, acc: 0.5156, auc: 0.6808, precision: 0.2578, recall: 0.5\n",
      "2018-12-29T17:20:53.451740, step: 390, loss: 0.6313674449920654, acc: 0.5625, auc: 0.7439, precision: 0.7083, recall: 0.5489\n",
      "2018-12-29T17:20:53.529702, step: 391, loss: 0.6330670118331909, acc: 0.5625, auc: 0.719, precision: 0.6429, recall: 0.5625\n",
      "2018-12-29T17:20:53.604950, step: 392, loss: 0.6539707183837891, acc: 0.5703, auc: 0.6577, precision: 0.6509, recall: 0.5514\n",
      "2018-12-29T17:20:53.683811, step: 393, loss: 0.6210558414459229, acc: 0.6719, auc: 0.7383, precision: 0.6754, recall: 0.6488\n",
      "2018-12-29T17:20:53.761759, step: 394, loss: 0.7332888841629028, acc: 0.5469, auc: 0.6062, precision: 0.7717, recall: 0.5085\n",
      "2018-12-29T17:20:53.836517, step: 395, loss: 0.6680490970611572, acc: 0.6562, auc: 0.7201, precision: 0.6591, recall: 0.6584\n",
      "2018-12-29T17:20:53.921504, step: 396, loss: 0.6482192277908325, acc: 0.5469, auc: 0.7352, precision: 0.2734, recall: 0.5\n",
      "2018-12-29T17:20:54.016245, step: 397, loss: 0.6041953563690186, acc: 0.5781, auc: 0.8255, precision: 0.6322, recall: 0.561\n",
      "2018-12-29T17:20:54.096599, step: 398, loss: 0.6313865184783936, acc: 0.5703, auc: 0.7285, precision: 0.7727, recall: 0.5565\n",
      "2018-12-29T17:20:54.189214, step: 399, loss: 0.6044385433197021, acc: 0.625, auc: 0.7729, precision: 0.6865, recall: 0.6102\n",
      "2018-12-29T17:20:54.267129, step: 400, loss: 0.5925140380859375, acc: 0.6953, auc: 0.7761, precision: 0.7301, recall: 0.6922\n",
      "\n",
      "Evaluation:\n",
      "2018-12-29T17:20:57.347481, step: 400, loss: 0.7207631591038827, acc: 0.5070153846153845, auc: 0.7346051282051282, precision: 0.632002564102564, recall: 0.5125205128205127\n",
      "2018-12-29T17:20:57.418948, step: 401, loss: 0.7106561660766602, acc: 0.5469, auc: 0.7001, precision: 0.7717, recall: 0.5085\n",
      "2018-12-29T17:20:57.497023, step: 402, loss: 0.733134388923645, acc: 0.5859, auc: 0.7749, precision: 0.7077, recall: 0.5859\n",
      "2018-12-29T17:20:57.572913, step: 403, loss: 0.6859290599822998, acc: 0.4688, auc: 0.6735, precision: 0.2344, recall: 0.5\n",
      "2018-12-29T17:20:57.646640, step: 404, loss: 0.6091675162315369, acc: 0.6328, auc: 0.76, precision: 0.6769, recall: 0.6434\n",
      "2018-12-29T17:20:57.726262, step: 405, loss: 0.6057948470115662, acc: 0.6484, auc: 0.7759, precision: 0.6625, recall: 0.64\n",
      "2018-12-29T17:20:57.811804, step: 406, loss: 0.6389232873916626, acc: 0.5781, auc: 0.7211, precision: 0.697, recall: 0.5719\n",
      "2018-12-29T17:20:57.892485, step: 407, loss: 0.6511263847351074, acc: 0.6875, auc: 0.7626, precision: 0.7249, recall: 0.6906\n",
      "2018-12-29T17:20:57.964068, step: 408, loss: 0.6731694340705872, acc: 0.5703, auc: 0.6266, precision: 0.5885, recall: 0.5664\n",
      "2018-12-29T17:20:58.040230, step: 409, loss: 0.6379784345626831, acc: 0.5938, auc: 0.6897, precision: 0.6275, recall: 0.534\n",
      "2018-12-29T17:20:58.120175, step: 410, loss: 0.6184289455413818, acc: 0.5781, auc: 0.7255, precision: 0.6295, recall: 0.6042\n",
      "2018-12-29T17:20:58.202878, step: 411, loss: 0.7122402191162109, acc: 0.5938, auc: 0.6447, precision: 0.6028, recall: 0.5987\n",
      "2018-12-29T17:20:58.280758, step: 412, loss: 0.6939399242401123, acc: 0.4844, auc: 0.8021, precision: 0.2422, recall: 0.5\n",
      "2018-12-29T17:20:58.363134, step: 413, loss: 0.6365666389465332, acc: 0.6016, auc: 0.6988, precision: 0.6476, recall: 0.6224\n",
      "2018-12-29T17:20:58.450980, step: 414, loss: 0.6081173419952393, acc: 0.6641, auc: 0.7702, precision: 0.7003, recall: 0.6673\n",
      "2018-12-29T17:20:58.542586, step: 415, loss: 0.5948958396911621, acc: 0.6719, auc: 0.7815, precision: 0.7378, recall: 0.6587\n",
      "2018-12-29T17:20:58.637052, step: 416, loss: 0.5626106262207031, acc: 0.6953, auc: 0.8466, precision: 0.7662, recall: 0.6779\n",
      "2018-12-29T17:20:58.727560, step: 417, loss: 0.6405200958251953, acc: 0.5703, auc: 0.7002, precision: 0.6193, recall: 0.5703\n",
      "2018-12-29T17:20:58.805052, step: 418, loss: 0.8252219557762146, acc: 0.5234, auc: 0.7092, precision: 0.6266, recall: 0.5795\n",
      "2018-12-29T17:20:58.887095, step: 419, loss: 0.7267212867736816, acc: 0.5469, auc: 0.6879, precision: 0.2734, recall: 0.5\n",
      "2018-12-29T17:20:58.973265, step: 420, loss: 0.6288303732872009, acc: 0.5938, auc: 0.7231, precision: 0.716, recall: 0.5995\n",
      "2018-12-29T17:20:59.068934, step: 421, loss: 0.5843595862388611, acc: 0.6406, auc: 0.8028, precision: 0.7186, recall: 0.6496\n",
      "2018-12-29T17:20:59.163055, step: 422, loss: 0.5935813784599304, acc: 0.7578, auc: 0.7931, precision: 0.7826, recall: 0.7416\n",
      "2018-12-29T17:20:59.243866, step: 423, loss: 0.5934784412384033, acc: 0.6094, auc: 0.7867, precision: 0.7333, recall: 0.5556\n",
      "2018-12-29T17:20:59.334913, step: 424, loss: 0.5856242775917053, acc: 0.7344, auc: 0.81, precision: 0.7421, recall: 0.7392\n",
      "2018-12-29T17:20:59.425893, step: 425, loss: 0.6225635409355164, acc: 0.5938, auc: 0.7383, precision: 0.7411, recall: 0.6056\n",
      "2018-12-29T17:20:59.506812, step: 426, loss: 0.5875405073165894, acc: 0.7109, auc: 0.8003, precision: 0.711, recall: 0.6954\n",
      "2018-12-29T17:20:59.586985, step: 427, loss: 0.6539783477783203, acc: 0.5625, auc: 0.6971, precision: 0.7231, recall: 0.5873\n",
      "2018-12-29T17:20:59.661169, step: 428, loss: 0.6080998778343201, acc: 0.6953, auc: 0.7907, precision: 0.7062, recall: 0.6934\n",
      "2018-12-29T17:20:59.736663, step: 429, loss: 0.6318212151527405, acc: 0.6172, auc: 0.7566, precision: 0.7514, recall: 0.586\n",
      "2018-12-29T17:20:59.812196, step: 430, loss: 0.6050809621810913, acc: 0.7344, auc: 0.7678, precision: 0.7364, recall: 0.735\n",
      "2018-12-29T17:20:59.899341, step: 431, loss: 0.6312961578369141, acc: 0.5859, auc: 0.7488, precision: 0.7355, recall: 0.5859\n",
      "2018-12-29T17:20:59.975700, step: 432, loss: 0.598280131816864, acc: 0.75, auc: 0.8113, precision: 0.771, recall: 0.7374\n",
      "2018-12-29T17:21:00.057786, step: 433, loss: 0.6080001592636108, acc: 0.6016, auc: 0.7524, precision: 0.7944, recall: 0.5364\n",
      "2018-12-29T17:21:00.153183, step: 434, loss: 0.5963525772094727, acc: 0.6641, auc: 0.7542, precision: 0.7008, recall: 0.6703\n",
      "2018-12-29T17:21:00.249137, step: 435, loss: 0.5829982161521912, acc: 0.75, auc: 0.7967, precision: 0.7535, recall: 0.7465\n",
      "2018-12-29T17:21:00.343348, step: 436, loss: 0.6194337010383606, acc: 0.6016, auc: 0.7812, precision: 0.7927, recall: 0.5446\n",
      "2018-12-29T17:21:00.437981, step: 437, loss: 0.7288922071456909, acc: 0.6484, auc: 0.7367, precision: 0.7135, recall: 0.6712\n",
      "2018-12-29T17:21:00.530550, step: 438, loss: 0.6712898015975952, acc: 0.5391, auc: 0.8012, precision: 0.7621, recall: 0.5317\n",
      "2018-12-29T17:21:00.619094, step: 439, loss: 0.636077880859375, acc: 0.6953, auc: 0.7287, precision: 0.6903, recall: 0.6895\n",
      "2018-12-29T17:21:00.714772, step: 440, loss: 0.6448317766189575, acc: 0.5156, auc: 0.7485, precision: 0.6915, recall: 0.5557\n",
      "2018-12-29T17:21:00.807715, step: 441, loss: 0.5582687854766846, acc: 0.75, auc: 0.8273, precision: 0.7448, recall: 0.7487\n",
      "2018-12-29T17:21:00.904140, step: 442, loss: 0.5814447402954102, acc: 0.6875, auc: 0.7936, precision: 0.7083, recall: 0.6961\n",
      "2018-12-29T17:21:00.998384, step: 443, loss: 0.6022465825080872, acc: 0.6719, auc: 0.7377, precision: 0.6795, recall: 0.6557\n",
      "2018-12-29T17:21:01.079944, step: 444, loss: 0.5886207818984985, acc: 0.5781, auc: 0.8179, precision: 0.775, recall: 0.5645\n",
      "2018-12-29T17:21:01.172627, step: 445, loss: 0.5763545036315918, acc: 0.7266, auc: 0.8155, precision: 0.7344, recall: 0.7073\n",
      "2018-12-29T17:21:01.259023, step: 446, loss: 0.5792304277420044, acc: 0.5781, auc: 0.8315, precision: 0.7611, recall: 0.6087\n",
      "2018-12-29T17:21:01.344577, step: 447, loss: 0.65753173828125, acc: 0.7031, auc: 0.8373, precision: 0.7585, recall: 0.7167\n",
      "2018-12-29T17:21:01.431546, step: 448, loss: 0.6746230125427246, acc: 0.4609, auc: 0.8201, precision: 0.7195, recall: 0.5338\n",
      "2018-12-29T17:21:01.527965, step: 449, loss: 0.583911657333374, acc: 0.7266, auc: 0.7453, precision: 0.7086, recall: 0.6938\n",
      "2018-12-29T17:21:01.602703, step: 450, loss: 0.5638977289199829, acc: 0.7656, auc: 0.8332, precision: 0.7666, recall: 0.7651\n",
      "2018-12-29T17:21:01.680256, step: 451, loss: 0.5829330682754517, acc: 0.6484, auc: 0.7717, precision: 0.6896, recall: 0.6693\n",
      "2018-12-29T17:21:01.757792, step: 452, loss: 0.598841667175293, acc: 0.7109, auc: 0.7803, precision: 0.7076, recall: 0.695\n",
      "2018-12-29T17:21:01.836708, step: 453, loss: 0.5666305422782898, acc: 0.6016, auc: 0.8259, precision: 0.7893, recall: 0.5603\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-29T17:21:01.909748, step: 454, loss: 0.5626754760742188, acc: 0.7266, auc: 0.8087, precision: 0.7264, recall: 0.7265\n",
      "2018-12-29T17:21:01.998032, step: 455, loss: 0.616855263710022, acc: 0.6406, auc: 0.7395, precision: 0.6802, recall: 0.6406\n",
      "2018-12-29T17:21:02.081335, step: 456, loss: 0.6218792200088501, acc: 0.6328, auc: 0.7218, precision: 0.6319, recall: 0.627\n",
      "2018-12-29T17:21:02.156104, step: 457, loss: 0.6189839839935303, acc: 0.6094, auc: 0.7927, precision: 0.7263, recall: 0.6094\n",
      "2018-12-29T17:21:02.235949, step: 458, loss: 0.6152245402336121, acc: 0.6875, auc: 0.8418, precision: 0.7403, recall: 0.6875\n",
      "2018-12-29T17:21:02.330491, step: 459, loss: 0.6670806407928467, acc: 0.5078, auc: 0.79, precision: 0.7375, recall: 0.5563\n",
      "2018-12-29T17:21:02.425888, step: 460, loss: 0.6149945259094238, acc: 0.75, auc: 0.8188, precision: 0.7773, recall: 0.7475\n",
      "2018-12-29T17:21:02.517981, step: 461, loss: 0.5740160942077637, acc: 0.6406, auc: 0.7894, precision: 0.7143, recall: 0.5952\n",
      "2018-12-29T17:21:02.608331, step: 462, loss: 0.5632538795471191, acc: 0.6719, auc: 0.802, precision: 0.7395, recall: 0.6719\n",
      "2018-12-29T17:21:02.684334, step: 463, loss: 0.5647758841514587, acc: 0.6562, auc: 0.7941, precision: 0.6749, recall: 0.668\n",
      "2018-12-29T17:21:02.760872, step: 464, loss: 0.6037912964820862, acc: 0.6484, auc: 0.7287, precision: 0.6572, recall: 0.6577\n",
      "2018-12-29T17:21:02.855376, step: 465, loss: 0.5374094247817993, acc: 0.7344, auc: 0.8198, precision: 0.7365, recall: 0.7344\n",
      "2018-12-29T17:21:02.939500, step: 466, loss: 0.5853073596954346, acc: 0.5781, auc: 0.8081, precision: 0.7076, recall: 0.6345\n",
      "2018-12-29T17:21:03.030713, step: 467, loss: 0.9221120476722717, acc: 0.5469, auc: 0.7726, precision: 0.7138, recall: 0.6005\n",
      "2018-12-29T17:21:03.111921, step: 468, loss: 0.5712217688560486, acc: 0.5781, auc: 0.8477, precision: 0.7805, recall: 0.5424\n",
      "start training model\n",
      "2018-12-29T17:21:03.353162, step: 469, loss: 0.5143262147903442, acc: 0.7734, auc: 0.8674, precision: 0.789, recall: 0.7752\n",
      "2018-12-29T17:21:03.453615, step: 470, loss: 0.5236585736274719, acc: 0.75, auc: 0.8326, precision: 0.7754, recall: 0.7262\n",
      "2018-12-29T17:21:03.545220, step: 471, loss: 0.6042388677597046, acc: 0.6172, auc: 0.7617, precision: 0.6927, recall: 0.6498\n",
      "2018-12-29T17:21:03.623988, step: 472, loss: 0.602291464805603, acc: 0.6953, auc: 0.8255, precision: 0.704, recall: 0.6982\n",
      "2018-12-29T17:21:03.715908, step: 473, loss: 0.5664099454879761, acc: 0.625, auc: 0.7801, precision: 0.6771, recall: 0.6329\n",
      "2018-12-29T17:21:03.791921, step: 474, loss: 0.5496244430541992, acc: 0.6875, auc: 0.7937, precision: 0.6905, recall: 0.6924\n",
      "2018-12-29T17:21:03.872814, step: 475, loss: 0.5917802453041077, acc: 0.7109, auc: 0.7676, precision: 0.7093, recall: 0.7073\n",
      "2018-12-29T17:21:03.948899, step: 476, loss: 0.6075557470321655, acc: 0.5703, auc: 0.8103, precision: 0.7273, recall: 0.6057\n",
      "2018-12-29T17:21:04.045033, step: 477, loss: 0.6881542801856995, acc: 0.6406, auc: 0.8407, precision: 0.7459, recall: 0.6598\n",
      "2018-12-29T17:21:04.123109, step: 478, loss: 0.6162552833557129, acc: 0.6016, auc: 0.7485, precision: 0.6955, recall: 0.5902\n",
      "2018-12-29T17:21:04.201815, step: 479, loss: 0.5472912788391113, acc: 0.7422, auc: 0.8408, precision: 0.7436, recall: 0.7415\n",
      "2018-12-29T17:21:04.280285, step: 480, loss: 0.524696946144104, acc: 0.6797, auc: 0.8518, precision: 0.7564, recall: 0.6797\n",
      "2018-12-29T17:21:04.369975, step: 481, loss: 0.5032471418380737, acc: 0.7734, auc: 0.8641, precision: 0.7733, recall: 0.773\n",
      "2018-12-29T17:21:04.450899, step: 482, loss: 0.507823646068573, acc: 0.7734, auc: 0.8411, precision: 0.8001, recall: 0.7476\n",
      "2018-12-29T17:21:04.551296, step: 483, loss: 0.47597014904022217, acc: 0.6719, auc: 0.8849, precision: 0.7272, recall: 0.6271\n",
      "2018-12-29T17:21:04.626869, step: 484, loss: 0.5307061076164246, acc: 0.7656, auc: 0.8548, precision: 0.7746, recall: 0.7568\n",
      "2018-12-29T17:21:04.720730, step: 485, loss: 0.5613909363746643, acc: 0.6562, auc: 0.8293, precision: 0.7564, recall: 0.6562\n",
      "2018-12-29T17:21:04.802408, step: 486, loss: 0.7071729302406311, acc: 0.6719, auc: 0.7844, precision: 0.7394, recall: 0.6836\n",
      "2018-12-29T17:21:04.896582, step: 487, loss: 0.6258444786071777, acc: 0.5625, auc: 0.7876, precision: 0.5983, recall: 0.5454\n",
      "2018-12-29T17:21:04.976423, step: 488, loss: 0.5860217213630676, acc: 0.6953, auc: 0.7909, precision: 0.6875, recall: 0.6902\n",
      "2018-12-29T17:21:05.058207, step: 489, loss: 0.5463065505027771, acc: 0.625, auc: 0.869, precision: 0.7214, recall: 0.6527\n",
      "2018-12-29T17:21:05.134900, step: 490, loss: 0.5743378400802612, acc: 0.7031, auc: 0.8074, precision: 0.7047, recall: 0.7047\n",
      "2018-12-29T17:21:05.218023, step: 491, loss: 0.4938942790031433, acc: 0.7266, auc: 0.8523, precision: 0.7539, recall: 0.7239\n",
      "2018-12-29T17:21:05.300072, step: 492, loss: 0.5328238606452942, acc: 0.7031, auc: 0.811, precision: 0.713, recall: 0.7013\n",
      "2018-12-29T17:21:05.391198, step: 493, loss: 0.5040197372436523, acc: 0.7422, auc: 0.8451, precision: 0.7642, recall: 0.7259\n",
      "2018-12-29T17:21:05.464795, step: 494, loss: 0.5429576635360718, acc: 0.6797, auc: 0.8551, precision: 0.7587, recall: 0.718\n",
      "2018-12-29T17:21:05.539992, step: 495, loss: 0.7122514247894287, acc: 0.6875, auc: 0.8198, precision: 0.7896, recall: 0.6875\n",
      "2018-12-29T17:21:05.622351, step: 496, loss: 0.5695613622665405, acc: 0.6406, auc: 0.8116, precision: 0.6979, recall: 0.6486\n",
      "2018-12-29T17:21:05.705105, step: 497, loss: 0.5583937168121338, acc: 0.7344, auc: 0.8081, precision: 0.7429, recall: 0.7344\n",
      "2018-12-29T17:21:05.801929, step: 498, loss: 0.5613211989402771, acc: 0.7266, auc: 0.7848, precision: 0.7285, recall: 0.731\n",
      "2018-12-29T17:21:05.891856, step: 499, loss: 0.5103838443756104, acc: 0.7656, auc: 0.8377, precision: 0.7754, recall: 0.7625\n",
      "2018-12-29T17:21:05.970983, step: 500, loss: 0.5053353309631348, acc: 0.6953, auc: 0.8601, precision: 0.7521, recall: 0.7155\n",
      "\n",
      "Evaluation:\n",
      "2018-12-29T17:21:09.057698, step: 500, loss: 0.5922232957986685, acc: 0.731774358974359, auc: 0.8446102564102563, precision: 0.765353846153846, recall: 0.7296589743589745\n",
      "2018-12-29T17:21:09.139096, step: 501, loss: 0.55777907371521, acc: 0.7656, auc: 0.8403, precision: 0.7833, recall: 0.7544\n",
      "2018-12-29T17:21:09.218769, step: 502, loss: 0.5253674387931824, acc: 0.6406, auc: 0.8936, precision: 0.7628, recall: 0.6608\n",
      "2018-12-29T17:21:09.299930, step: 503, loss: 0.6964516639709473, acc: 0.6875, auc: 0.7553, precision: 0.7598, recall: 0.6993\n",
      "2018-12-29T17:21:09.380561, step: 504, loss: 0.511542558670044, acc: 0.6953, auc: 0.8556, precision: 0.7536, recall: 0.7093\n",
      "2018-12-29T17:21:09.467801, step: 505, loss: 0.5425018072128296, acc: 0.7344, auc: 0.816, precision: 0.7379, recall: 0.7361\n",
      "2018-12-29T17:21:09.550774, step: 506, loss: 0.5322232246398926, acc: 0.7031, auc: 0.8141, precision: 0.7329, recall: 0.6632\n",
      "2018-12-29T17:21:09.636658, step: 507, loss: 0.5129482746124268, acc: 0.75, auc: 0.8343, precision: 0.7614, recall: 0.7237\n",
      "2018-12-29T17:21:09.717569, step: 508, loss: 0.54236900806427, acc: 0.7031, auc: 0.8198, precision: 0.7365, recall: 0.706\n",
      "2018-12-29T17:21:09.802398, step: 509, loss: 0.458308607339859, acc: 0.8047, auc: 0.8913, precision: 0.8081, recall: 0.8055\n",
      "2018-12-29T17:21:09.883843, step: 510, loss: 0.5434616804122925, acc: 0.7422, auc: 0.8138, precision: 0.7657, recall: 0.7373\n",
      "2018-12-29T17:21:09.962382, step: 511, loss: 0.46005573868751526, acc: 0.8359, auc: 0.8927, precision: 0.8383, recall: 0.8346\n",
      "2018-12-29T17:21:10.041145, step: 512, loss: 0.5423026084899902, acc: 0.7422, auc: 0.8162, precision: 0.8096, recall: 0.7045\n",
      "2018-12-29T17:21:10.127246, step: 513, loss: 0.6417233943939209, acc: 0.7109, auc: 0.7588, precision: 0.7305, recall: 0.7208\n",
      "2018-12-29T17:21:10.210749, step: 514, loss: 0.6433498859405518, acc: 0.5703, auc: 0.8415, precision: 0.7669, recall: 0.5769\n",
      "2018-12-29T17:21:10.291194, step: 515, loss: 0.6616429686546326, acc: 0.6562, auc: 0.789, precision: 0.7091, recall: 0.6601\n",
      "2018-12-29T17:21:10.368706, step: 516, loss: 0.5063484311103821, acc: 0.8047, auc: 0.8931, precision: 0.8082, recall: 0.7992\n",
      "2018-12-29T17:21:10.448316, step: 517, loss: 0.4913138151168823, acc: 0.7344, auc: 0.8566, precision: 0.7673, recall: 0.6931\n",
      "2018-12-29T17:21:10.535206, step: 518, loss: 0.581169068813324, acc: 0.6719, auc: 0.7909, precision: 0.7205, recall: 0.6911\n",
      "2018-12-29T17:21:10.609955, step: 519, loss: 0.4969821870326996, acc: 0.7891, auc: 0.8713, precision: 0.7897, recall: 0.7891\n",
      "2018-12-29T17:21:10.688279, step: 520, loss: 0.5093607902526855, acc: 0.7266, auc: 0.8604, precision: 0.7552, recall: 0.745\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-29T17:21:10.776345, step: 521, loss: 0.5626587867736816, acc: 0.7578, auc: 0.8499, precision: 0.7774, recall: 0.7578\n",
      "2018-12-29T17:21:10.869356, step: 522, loss: 0.5188784599304199, acc: 0.6328, auc: 0.8887, precision: 0.7399, recall: 0.6162\n",
      "2018-12-29T17:21:10.950539, step: 523, loss: 0.5384833216667175, acc: 0.7891, auc: 0.8635, precision: 0.8041, recall: 0.7941\n",
      "2018-12-29T17:21:11.030028, step: 524, loss: 0.4659655690193176, acc: 0.7344, auc: 0.8915, precision: 0.8045, recall: 0.7052\n",
      "2018-12-29T17:21:11.102234, step: 525, loss: 0.44121164083480835, acc: 0.7969, auc: 0.8995, precision: 0.801, recall: 0.8025\n",
      "2018-12-29T17:21:11.177329, step: 526, loss: 0.44849514961242676, acc: 0.7656, auc: 0.8806, precision: 0.7667, recall: 0.7656\n",
      "2018-12-29T17:21:11.254592, step: 527, loss: 0.49418842792510986, acc: 0.7188, auc: 0.8687, precision: 0.7583, recall: 0.751\n",
      "2018-12-29T17:21:11.331896, step: 528, loss: 0.7773374319076538, acc: 0.6406, auc: 0.8166, precision: 0.7313, recall: 0.6454\n",
      "2018-12-29T17:21:11.426681, step: 529, loss: 0.520127534866333, acc: 0.6875, auc: 0.8711, precision: 0.7598, recall: 0.6993\n",
      "2018-12-29T17:21:11.521005, step: 530, loss: 0.5137940049171448, acc: 0.7578, auc: 0.8425, precision: 0.7568, recall: 0.7582\n",
      "2018-12-29T17:21:11.614419, step: 531, loss: 0.4780154824256897, acc: 0.7812, auc: 0.8533, precision: 0.7786, recall: 0.7836\n",
      "2018-12-29T17:21:11.693194, step: 532, loss: 0.49113720655441284, acc: 0.7109, auc: 0.8262, precision: 0.7109, recall: 0.7152\n",
      "2018-12-29T17:21:11.773050, step: 533, loss: 0.4972718358039856, acc: 0.7578, auc: 0.8327, precision: 0.7554, recall: 0.7575\n",
      "2018-12-29T17:21:11.864456, step: 534, loss: 0.4822627305984497, acc: 0.7344, auc: 0.8563, precision: 0.7382, recall: 0.7425\n",
      "2018-12-29T17:21:11.959028, step: 535, loss: 0.43818432092666626, acc: 0.8203, auc: 0.873, precision: 0.818, recall: 0.819\n",
      "2018-12-29T17:21:12.050195, step: 536, loss: 0.5953457951545715, acc: 0.6406, auc: 0.7968, precision: 0.6695, recall: 0.6437\n",
      "2018-12-29T17:21:12.125989, step: 537, loss: 0.6206711530685425, acc: 0.7266, auc: 0.9328, precision: 0.8158, recall: 0.7426\n",
      "2018-12-29T17:21:12.206816, step: 538, loss: 0.6939549446105957, acc: 0.5469, auc: 0.8498, precision: 0.7126, recall: 0.5725\n",
      "2018-12-29T17:21:12.284771, step: 539, loss: 0.5783705711364746, acc: 0.7188, auc: 0.8254, precision: 0.7331, recall: 0.7167\n",
      "2018-12-29T17:21:12.366776, step: 540, loss: 0.5379766225814819, acc: 0.7656, auc: 0.8542, precision: 0.7431, recall: 0.7565\n",
      "2018-12-29T17:21:12.453724, step: 541, loss: 0.5304790139198303, acc: 0.625, auc: 0.8466, precision: 0.7364, recall: 0.6195\n",
      "2018-12-29T17:21:12.527136, step: 542, loss: 0.4933987259864807, acc: 0.7656, auc: 0.8536, precision: 0.7601, recall: 0.7573\n",
      "2018-12-29T17:21:12.618350, step: 543, loss: 0.509369969367981, acc: 0.7422, auc: 0.8728, precision: 0.8122, recall: 0.753\n",
      "2018-12-29T17:21:12.711129, step: 544, loss: 0.5824385285377502, acc: 0.7266, auc: 0.8152, precision: 0.7485, recall: 0.7266\n",
      "2018-12-29T17:21:12.805943, step: 545, loss: 0.4756297469139099, acc: 0.75, auc: 0.8746, precision: 0.8007, recall: 0.7434\n",
      "2018-12-29T17:21:12.891399, step: 546, loss: 0.49060243368148804, acc: 0.7109, auc: 0.8342, precision: 0.7018, recall: 0.698\n",
      "2018-12-29T17:21:12.974798, step: 547, loss: 0.45301610231399536, acc: 0.75, auc: 0.875, precision: 0.7771, recall: 0.75\n",
      "2018-12-29T17:21:13.067690, step: 548, loss: 0.41739410161972046, acc: 0.8203, auc: 0.8995, precision: 0.8195, recall: 0.8186\n",
      "2018-12-29T17:21:13.159709, step: 549, loss: 0.4075411856174469, acc: 0.8281, auc: 0.9145, precision: 0.8388, recall: 0.8309\n",
      "2018-12-29T17:21:13.236503, step: 550, loss: 0.40690407156944275, acc: 0.8203, auc: 0.8998, precision: 0.819, recall: 0.8145\n",
      "2018-12-29T17:21:13.318435, step: 551, loss: 0.6819580793380737, acc: 0.6328, auc: 0.8689, precision: 0.7902, recall: 0.627\n",
      "2018-12-29T17:21:13.395953, step: 552, loss: 1.1859478950500488, acc: 0.5625, auc: 0.7998, precision: 0.7778, recall: 0.5172\n",
      "2018-12-29T17:21:13.494325, step: 553, loss: 0.5348864793777466, acc: 0.7422, auc: 0.84, precision: 0.746, recall: 0.7368\n",
      "2018-12-29T17:21:13.573281, step: 554, loss: 0.49317675828933716, acc: 0.8125, auc: 0.886, precision: 0.8177, recall: 0.8115\n",
      "2018-12-29T17:21:13.655363, step: 555, loss: 0.4706268906593323, acc: 0.75, auc: 0.8908, precision: 0.792, recall: 0.7695\n",
      "2018-12-29T17:21:13.746499, step: 556, loss: 0.44214051961898804, acc: 0.8359, auc: 0.8923, precision: 0.8355, recall: 0.8359\n",
      "2018-12-29T17:21:13.839580, step: 557, loss: 0.46263256669044495, acc: 0.7812, auc: 0.8679, precision: 0.7854, recall: 0.7765\n",
      "2018-12-29T17:21:13.927319, step: 558, loss: 0.4904012382030487, acc: 0.7188, auc: 0.871, precision: 0.7526, recall: 0.7318\n",
      "2018-12-29T17:21:14.008245, step: 559, loss: 0.485213041305542, acc: 0.7266, auc: 0.8707, precision: 0.733, recall: 0.7236\n",
      "2018-12-29T17:21:14.090956, step: 560, loss: 0.4796812832355499, acc: 0.7656, auc: 0.8524, precision: 0.7623, recall: 0.7599\n",
      "2018-12-29T17:21:14.180463, step: 561, loss: 0.47712844610214233, acc: 0.7578, auc: 0.8786, precision: 0.8034, recall: 0.7608\n",
      "2018-12-29T17:21:14.274146, step: 562, loss: 0.4791337251663208, acc: 0.8047, auc: 0.8623, precision: 0.8108, recall: 0.8047\n",
      "2018-12-29T17:21:14.354428, step: 563, loss: 0.5106378793716431, acc: 0.7578, auc: 0.8659, precision: 0.7776, recall: 0.751\n",
      "2018-12-29T17:21:14.448693, step: 564, loss: 0.5326541066169739, acc: 0.7656, auc: 0.8336, precision: 0.7661, recall: 0.7666\n",
      "2018-12-29T17:21:14.547818, step: 565, loss: 0.4514818787574768, acc: 0.7734, auc: 0.8739, precision: 0.7812, recall: 0.7759\n",
      "2018-12-29T17:21:14.633296, step: 566, loss: 0.4377738833427429, acc: 0.7969, auc: 0.8838, precision: 0.7957, recall: 0.7969\n",
      "2018-12-29T17:21:14.726750, step: 567, loss: 0.4324686527252197, acc: 0.7812, auc: 0.8788, precision: 0.7864, recall: 0.7551\n",
      "2018-12-29T17:21:14.824053, step: 568, loss: 0.4246407449245453, acc: 0.7969, auc: 0.8848, precision: 0.7966, recall: 0.7951\n",
      "2018-12-29T17:21:14.897991, step: 569, loss: 0.5382212400436401, acc: 0.6562, auc: 0.8497, precision: 0.7414, recall: 0.6652\n",
      "2018-12-29T17:21:14.970174, step: 570, loss: 0.8242402672767639, acc: 0.6406, auc: 0.8322, precision: 0.7464, recall: 0.6552\n",
      "2018-12-29T17:21:15.046469, step: 571, loss: 0.49733614921569824, acc: 0.625, auc: 0.93, precision: 0.7757, recall: 0.6522\n",
      "2018-12-29T17:21:15.138369, step: 572, loss: 0.46175581216812134, acc: 0.8359, auc: 0.9027, precision: 0.8374, recall: 0.837\n",
      "2018-12-29T17:21:15.230735, step: 573, loss: 0.4406590461730957, acc: 0.7812, auc: 0.9076, precision: 0.797, recall: 0.797\n",
      "2018-12-29T17:21:15.308300, step: 574, loss: 0.5079858303070068, acc: 0.7734, auc: 0.8606, precision: 0.7852, recall: 0.7734\n",
      "2018-12-29T17:21:15.387916, step: 575, loss: 0.49156874418258667, acc: 0.7422, auc: 0.8505, precision: 0.7655, recall: 0.7346\n",
      "2018-12-29T17:21:15.473155, step: 576, loss: 0.44934016466140747, acc: 0.8203, auc: 0.8654, precision: 0.8221, recall: 0.8161\n",
      "2018-12-29T17:21:15.568119, step: 577, loss: 0.465751051902771, acc: 0.7891, auc: 0.8867, precision: 0.8021, recall: 0.7838\n",
      "2018-12-29T17:21:15.662826, step: 578, loss: 0.516435444355011, acc: 0.7812, auc: 0.8466, precision: 0.7836, recall: 0.7786\n",
      "2018-12-29T17:21:15.744643, step: 579, loss: 0.6087733507156372, acc: 0.6562, auc: 0.8103, precision: 0.7415, recall: 0.6608\n",
      "2018-12-29T17:21:15.823277, step: 580, loss: 0.6992132663726807, acc: 0.6719, auc: 0.8557, precision: 0.7273, recall: 0.7317\n",
      "2018-12-29T17:21:15.920911, step: 581, loss: 0.5754321813583374, acc: 0.5547, auc: 0.8708, precision: 0.7645, recall: 0.5547\n",
      "2018-12-29T17:21:16.000242, step: 582, loss: 0.49668967723846436, acc: 0.7734, auc: 0.8694, precision: 0.773, recall: 0.774\n",
      "2018-12-29T17:21:16.081918, step: 583, loss: 0.4624907970428467, acc: 0.7656, auc: 0.884, precision: 0.7787, recall: 0.7673\n",
      "2018-12-29T17:21:16.178784, step: 584, loss: 0.42766886949539185, acc: 0.8281, auc: 0.9052, precision: 0.8291, recall: 0.8246\n",
      "2018-12-29T17:21:16.269497, step: 585, loss: 0.44806644320487976, acc: 0.7344, auc: 0.9038, precision: 0.7677, recall: 0.7642\n",
      "2018-12-29T17:21:16.363077, step: 586, loss: 0.5754408836364746, acc: 0.7422, auc: 0.826, precision: 0.7605, recall: 0.7378\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-29T17:21:16.456430, step: 587, loss: 0.4266911745071411, acc: 0.7578, auc: 0.8944, precision: 0.7882, recall: 0.7294\n",
      "2018-12-29T17:21:16.542471, step: 588, loss: 0.3879481852054596, acc: 0.8438, auc: 0.9177, precision: 0.8438, recall: 0.8438\n",
      "2018-12-29T17:21:16.628864, step: 589, loss: 0.44019174575805664, acc: 0.7422, auc: 0.8963, precision: 0.774, recall: 0.7569\n",
      "2018-12-29T17:21:16.723651, step: 590, loss: 0.54898601770401, acc: 0.75, auc: 0.862, precision: 0.7767, recall: 0.7524\n",
      "2018-12-29T17:21:16.806085, step: 591, loss: 0.48489588499069214, acc: 0.7188, auc: 0.8862, precision: 0.771, recall: 0.7079\n",
      "2018-12-29T17:21:16.892072, step: 592, loss: 0.5554653406143188, acc: 0.7344, auc: 0.8536, precision: 0.7697, recall: 0.7475\n",
      "2018-12-29T17:21:16.980142, step: 593, loss: 0.552227795124054, acc: 0.6953, auc: 0.8485, precision: 0.7798, recall: 0.6865\n",
      "2018-12-29T17:21:17.064858, step: 594, loss: 0.4246876835823059, acc: 0.8594, auc: 0.933, precision: 0.8592, recall: 0.8592\n",
      "2018-12-29T17:21:17.146589, step: 595, loss: 0.5169105529785156, acc: 0.75, auc: 0.8522, precision: 0.8007, recall: 0.7434\n",
      "2018-12-29T17:21:17.230565, step: 596, loss: 0.4873589277267456, acc: 0.7969, auc: 0.8742, precision: 0.8127, recall: 0.7913\n",
      "2018-12-29T17:21:17.308771, step: 597, loss: 0.5514099597930908, acc: 0.6641, auc: 0.8534, precision: 0.7206, recall: 0.6969\n",
      "2018-12-29T17:21:17.393603, step: 598, loss: 0.47406136989593506, acc: 0.8203, auc: 0.9094, precision: 0.8341, recall: 0.8203\n",
      "2018-12-29T17:21:17.478980, step: 599, loss: 0.4421725869178772, acc: 0.75, auc: 0.8785, precision: 0.7513, recall: 0.7337\n",
      "2018-12-29T17:21:17.565095, step: 600, loss: 0.4529406428337097, acc: 0.75, auc: 0.8915, precision: 0.7825, recall: 0.7551\n",
      "\n",
      "Evaluation:\n",
      "2018-12-29T17:21:20.701160, step: 600, loss: 0.5280889784678434, acc: 0.7690333333333336, auc: 0.8803025641025639, precision: 0.7997974358974361, recall: 0.767702564102564\n",
      "2018-12-29T17:21:20.773412, step: 601, loss: 0.5024545192718506, acc: 0.7812, auc: 0.9054, precision: 0.815, recall: 0.7912\n",
      "2018-12-29T17:21:20.854335, step: 602, loss: 0.5481891632080078, acc: 0.6797, auc: 0.8494, precision: 0.7564, recall: 0.6797\n",
      "2018-12-29T17:21:20.933924, step: 603, loss: 0.5142451524734497, acc: 0.7969, auc: 0.8845, precision: 0.8146, recall: 0.8023\n",
      "2018-12-29T17:21:21.015589, step: 604, loss: 0.4459644556045532, acc: 0.8203, auc: 0.9021, precision: 0.8454, recall: 0.8182\n",
      "2018-12-29T17:21:21.109414, step: 605, loss: 0.4599631428718567, acc: 0.7891, auc: 0.8712, precision: 0.8067, recall: 0.7761\n",
      "2018-12-29T17:21:21.198840, step: 606, loss: 0.43441662192344666, acc: 0.7969, auc: 0.8921, precision: 0.7971, recall: 0.7971\n",
      "2018-12-29T17:21:21.281767, step: 607, loss: 0.39581099152565, acc: 0.8203, auc: 0.9076, precision: 0.842, recall: 0.7999\n",
      "2018-12-29T17:21:21.376810, step: 608, loss: 0.3853828012943268, acc: 0.8047, auc: 0.9139, precision: 0.8115, recall: 0.8145\n",
      "2018-12-29T17:21:21.455106, step: 609, loss: 0.5072341561317444, acc: 0.7891, auc: 0.8892, precision: 0.7953, recall: 0.7945\n",
      "2018-12-29T17:21:21.531286, step: 610, loss: 1.020592212677002, acc: 0.5234, auc: 0.8732, precision: 0.752, recall: 0.5379\n",
      "2018-12-29T17:21:21.608583, step: 611, loss: 0.5637779235839844, acc: 0.7344, auc: 0.8409, precision: 0.7731, recall: 0.7283\n",
      "2018-12-29T17:21:21.690764, step: 612, loss: 0.5219309329986572, acc: 0.75, auc: 0.8568, precision: 0.75, recall: 0.7502\n",
      "2018-12-29T17:21:21.771165, step: 613, loss: 0.5010495185852051, acc: 0.7578, auc: 0.8535, precision: 0.7598, recall: 0.7627\n",
      "2018-12-29T17:21:21.853686, step: 614, loss: 0.4750979244709015, acc: 0.7812, auc: 0.8845, precision: 0.7824, recall: 0.7807\n",
      "2018-12-29T17:21:21.933818, step: 615, loss: 0.43576791882514954, acc: 0.7734, auc: 0.9046, precision: 0.7979, recall: 0.7799\n",
      "2018-12-29T17:21:22.012831, step: 616, loss: 0.5469209551811218, acc: 0.75, auc: 0.7947, precision: 0.7498, recall: 0.7498\n",
      "2018-12-29T17:21:22.091200, step: 617, loss: 0.45615750551223755, acc: 0.7656, auc: 0.8616, precision: 0.7639, recall: 0.7679\n",
      "2018-12-29T17:21:22.188707, step: 618, loss: 0.41032546758651733, acc: 0.8281, auc: 0.8993, precision: 0.8251, recall: 0.8274\n",
      "2018-12-29T17:21:22.279151, step: 619, loss: 0.4648972749710083, acc: 0.8125, auc: 0.873, precision: 0.8123, recall: 0.8108\n",
      "2018-12-29T17:21:22.359250, step: 620, loss: 0.4559551775455475, acc: 0.7734, auc: 0.8995, precision: 0.8385, recall: 0.7555\n",
      "2018-12-29T17:21:22.438789, step: 621, loss: 0.3779718279838562, acc: 0.8359, auc: 0.9266, precision: 0.8436, recall: 0.8323\n",
      "2018-12-29T17:21:22.518000, step: 622, loss: 0.40180325508117676, acc: 0.7734, auc: 0.9297, precision: 0.8339, recall: 0.7701\n",
      "2018-12-29T17:21:22.602979, step: 623, loss: 0.4883508086204529, acc: 0.7734, auc: 0.8917, precision: 0.7855, recall: 0.77\n",
      "2018-12-29T17:21:22.698197, step: 624, loss: 0.5886469483375549, acc: 0.6172, auc: 0.8755, precision: 0.754, recall: 0.6228\n",
      "start training model\n",
      "2018-12-29T17:21:22.940994, step: 625, loss: 0.4630601406097412, acc: 0.8359, auc: 0.9295, precision: 0.8506, recall: 0.8441\n",
      "2018-12-29T17:21:23.033946, step: 626, loss: 0.5214693546295166, acc: 0.7422, auc: 0.8351, precision: 0.7646, recall: 0.7486\n",
      "2018-12-29T17:21:23.118063, step: 627, loss: 0.40134501457214355, acc: 0.8828, auc: 0.9321, precision: 0.8672, recall: 0.8782\n",
      "2018-12-29T17:21:23.195849, step: 628, loss: 0.49143698811531067, acc: 0.6797, auc: 0.8792, precision: 0.7699, recall: 0.6797\n",
      "2018-12-29T17:21:23.274211, step: 629, loss: 0.4427627921104431, acc: 0.8125, auc: 0.8819, precision: 0.8118, recall: 0.8127\n",
      "2018-12-29T17:21:23.365860, step: 630, loss: 0.36091548204421997, acc: 0.8672, auc: 0.9259, precision: 0.8731, recall: 0.8608\n",
      "2018-12-29T17:21:23.440592, step: 631, loss: 0.3616369366645813, acc: 0.875, auc: 0.9217, precision: 0.8755, recall: 0.874\n",
      "2018-12-29T17:21:23.517566, step: 632, loss: 0.3469530940055847, acc: 0.8672, auc: 0.9252, precision: 0.8687, recall: 0.8683\n",
      "2018-12-29T17:21:23.589731, step: 633, loss: 0.5144836902618408, acc: 0.7578, auc: 0.8817, precision: 0.7632, recall: 0.7675\n",
      "2018-12-29T17:21:23.666433, step: 634, loss: 1.1357707977294922, acc: 0.4844, auc: 0.8612, precision: 0.736, recall: 0.5217\n",
      "2018-12-29T17:21:23.741098, step: 635, loss: 0.5159772634506226, acc: 0.7734, auc: 0.8357, precision: 0.7615, recall: 0.77\n",
      "2018-12-29T17:21:23.820027, step: 636, loss: 0.5257946848869324, acc: 0.7812, auc: 0.8622, precision: 0.783, recall: 0.783\n",
      "2018-12-29T17:21:23.900828, step: 637, loss: 0.4462038278579712, acc: 0.7969, auc: 0.8981, precision: 0.8027, recall: 0.7816\n",
      "2018-12-29T17:21:23.979592, step: 638, loss: 0.4096375107765198, acc: 0.8203, auc: 0.9261, precision: 0.8294, recall: 0.8283\n",
      "2018-12-29T17:21:24.062189, step: 639, loss: 0.4198845624923706, acc: 0.8672, auc: 0.9086, precision: 0.8658, recall: 0.8699\n",
      "2018-12-29T17:21:24.160295, step: 640, loss: 0.4693340063095093, acc: 0.7422, auc: 0.8752, precision: 0.7716, recall: 0.7396\n",
      "2018-12-29T17:21:24.242361, step: 641, loss: 0.40691933035850525, acc: 0.8047, auc: 0.8896, precision: 0.8017, recall: 0.8086\n",
      "2018-12-29T17:21:24.322524, step: 642, loss: 0.4419136643409729, acc: 0.8125, auc: 0.8894, precision: 0.8103, recall: 0.8028\n",
      "2018-12-29T17:21:24.418712, step: 643, loss: 0.37743937969207764, acc: 0.8047, auc: 0.9219, precision: 0.8307, recall: 0.7847\n",
      "2018-12-29T17:21:24.514075, step: 644, loss: 0.35311436653137207, acc: 0.8438, auc: 0.925, precision: 0.8498, recall: 0.8316\n",
      "2018-12-29T17:21:24.605445, step: 645, loss: 0.3691750466823578, acc: 0.7891, auc: 0.9226, precision: 0.811, recall: 0.7891\n",
      "2018-12-29T17:21:24.693468, step: 646, loss: 0.37602657079696655, acc: 0.8281, auc: 0.9288, precision: 0.8307, recall: 0.8209\n",
      "2018-12-29T17:21:24.784255, step: 647, loss: 0.5256707072257996, acc: 0.6875, auc: 0.8769, precision: 0.7481, recall: 0.7052\n",
      "2018-12-29T17:21:24.874141, step: 648, loss: 0.6795248985290527, acc: 0.7266, auc: 0.8855, precision: 0.7951, recall: 0.7303\n",
      "2018-12-29T17:21:24.969663, step: 649, loss: 0.5666860342025757, acc: 0.5234, auc: 0.9226, precision: 0.6995, recall: 0.6006\n",
      "2018-12-29T17:21:25.053753, step: 650, loss: 0.46576589345932007, acc: 0.7891, auc: 0.8844, precision: 0.785, recall: 0.78\n",
      "2018-12-29T17:21:25.138950, step: 651, loss: 0.45515385270118713, acc: 0.7656, auc: 0.8875, precision: 0.7953, recall: 0.7917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-29T17:21:25.230248, step: 652, loss: 0.43415945768356323, acc: 0.8359, auc: 0.9054, precision: 0.8374, recall: 0.837\n",
      "2018-12-29T17:21:25.306589, step: 653, loss: 0.4390256106853485, acc: 0.7656, auc: 0.8878, precision: 0.7749, recall: 0.7749\n",
      "2018-12-29T17:21:25.391317, step: 654, loss: 0.410238116979599, acc: 0.8359, auc: 0.9043, precision: 0.8387, recall: 0.8381\n",
      "2018-12-29T17:21:25.465366, step: 655, loss: 0.3415742516517639, acc: 0.8359, auc: 0.9477, precision: 0.8537, recall: 0.8394\n",
      "2018-12-29T17:21:25.548389, step: 656, loss: 0.4052554965019226, acc: 0.8359, auc: 0.9069, precision: 0.8374, recall: 0.837\n",
      "2018-12-29T17:21:25.641641, step: 657, loss: 0.414337694644928, acc: 0.7656, auc: 0.9129, precision: 0.7869, recall: 0.7717\n",
      "2018-12-29T17:21:25.721535, step: 658, loss: 0.47989845275878906, acc: 0.8125, auc: 0.9135, precision: 0.8375, recall: 0.8167\n",
      "2018-12-29T17:21:25.800840, step: 659, loss: 0.38545680046081543, acc: 0.7812, auc: 0.9142, precision: 0.8343, recall: 0.7321\n",
      "2018-12-29T17:21:25.893074, step: 660, loss: 0.37263813614845276, acc: 0.8438, auc: 0.9166, precision: 0.8541, recall: 0.8294\n",
      "2018-12-29T17:21:25.973372, step: 661, loss: 0.3014468252658844, acc: 0.8438, auc: 0.9651, precision: 0.8538, recall: 0.8668\n",
      "2018-12-29T17:21:26.050597, step: 662, loss: 0.6983596086502075, acc: 0.7031, auc: 0.9124, precision: 0.7802, recall: 0.7149\n",
      "2018-12-29T17:21:26.133252, step: 663, loss: 0.6176267862319946, acc: 0.6016, auc: 0.862, precision: 0.71, recall: 0.577\n",
      "2018-12-29T17:21:26.210175, step: 664, loss: 0.4544212818145752, acc: 0.8047, auc: 0.8865, precision: 0.8039, recall: 0.8044\n",
      "2018-12-29T17:21:26.293161, step: 665, loss: 0.4996873736381531, acc: 0.7734, auc: 0.8419, precision: 0.7727, recall: 0.7721\n",
      "2018-12-29T17:21:26.393000, step: 666, loss: 0.33232933282852173, acc: 0.875, auc: 0.9511, precision: 0.8782, recall: 0.8597\n",
      "2018-12-29T17:21:26.476086, step: 667, loss: 0.41192862391471863, acc: 0.7891, auc: 0.9094, precision: 0.8059, recall: 0.7891\n",
      "2018-12-29T17:21:26.568795, step: 668, loss: 0.5119835138320923, acc: 0.7891, auc: 0.8674, precision: 0.7906, recall: 0.7921\n",
      "2018-12-29T17:21:26.659657, step: 669, loss: 0.3443754315376282, acc: 0.8203, auc: 0.9421, precision: 0.8485, recall: 0.8062\n",
      "2018-12-29T17:21:26.743853, step: 670, loss: 0.2911556363105774, acc: 0.9141, auc: 0.9587, precision: 0.9132, recall: 0.9141\n",
      "2018-12-29T17:21:26.832704, step: 671, loss: 0.39727452397346497, acc: 0.7344, auc: 0.9027, precision: 0.7421, recall: 0.7392\n",
      "2018-12-29T17:21:26.930700, step: 672, loss: 0.4081376791000366, acc: 0.8047, auc: 0.9031, precision: 0.8066, recall: 0.8047\n",
      "2018-12-29T17:21:27.011042, step: 673, loss: 0.378646582365036, acc: 0.8047, auc: 0.9175, precision: 0.8102, recall: 0.8111\n",
      "2018-12-29T17:21:27.093663, step: 674, loss: 0.3963606357574463, acc: 0.8125, auc: 0.9177, precision: 0.8243, recall: 0.811\n",
      "2018-12-29T17:21:27.180888, step: 675, loss: 0.646978497505188, acc: 0.6641, auc: 0.837, precision: 0.7749, recall: 0.6784\n",
      "2018-12-29T17:21:27.266563, step: 676, loss: 0.7514219284057617, acc: 0.6641, auc: 0.8728, precision: 0.7606, recall: 0.6687\n",
      "2018-12-29T17:21:27.351537, step: 677, loss: 0.45949238538742065, acc: 0.7266, auc: 0.8828, precision: 0.7844, recall: 0.73\n",
      "2018-12-29T17:21:27.435514, step: 678, loss: 0.4370647370815277, acc: 0.8047, auc: 0.8903, precision: 0.8112, recall: 0.8023\n",
      "2018-12-29T17:21:27.519121, step: 679, loss: 0.45467257499694824, acc: 0.7422, auc: 0.8725, precision: 0.751, recall: 0.7486\n",
      "2018-12-29T17:21:27.600510, step: 680, loss: 0.43110841512680054, acc: 0.8359, auc: 0.9049, precision: 0.8365, recall: 0.8338\n",
      "2018-12-29T17:21:27.676716, step: 681, loss: 0.4044744372367859, acc: 0.7969, auc: 0.9027, precision: 0.8068, recall: 0.7996\n",
      "2018-12-29T17:21:27.765825, step: 682, loss: 0.454670250415802, acc: 0.7891, auc: 0.8892, precision: 0.7917, recall: 0.7912\n",
      "2018-12-29T17:21:27.845508, step: 683, loss: 0.39973628520965576, acc: 0.7422, auc: 0.9376, precision: 0.8073, recall: 0.731\n",
      "2018-12-29T17:21:27.926489, step: 684, loss: 0.49132978916168213, acc: 0.8047, auc: 0.9003, precision: 0.8263, recall: 0.8087\n",
      "2018-12-29T17:21:28.013885, step: 685, loss: 0.44379886984825134, acc: 0.7891, auc: 0.8706, precision: 0.7916, recall: 0.7725\n",
      "2018-12-29T17:21:28.092525, step: 686, loss: 0.40304532647132874, acc: 0.8359, auc: 0.9044, precision: 0.836, recall: 0.8323\n",
      "2018-12-29T17:21:28.174054, step: 687, loss: 0.3513786196708679, acc: 0.8203, auc: 0.9382, precision: 0.8407, recall: 0.8279\n",
      "2018-12-29T17:21:28.255585, step: 688, loss: 0.541408121585846, acc: 0.7656, auc: 0.8764, precision: 0.7888, recall: 0.7634\n",
      "2018-12-29T17:21:28.334227, step: 689, loss: 0.4975771903991699, acc: 0.6094, auc: 0.9146, precision: 0.7001, recall: 0.5924\n",
      "2018-12-29T17:21:28.425483, step: 690, loss: 0.4135008752346039, acc: 0.8203, auc: 0.9278, precision: 0.8211, recall: 0.8239\n",
      "2018-12-29T17:21:28.508938, step: 691, loss: 0.3664264380931854, acc: 0.8281, auc: 0.9221, precision: 0.8304, recall: 0.8353\n",
      "2018-12-29T17:21:28.602020, step: 692, loss: 0.4177883267402649, acc: 0.8281, auc: 0.8997, precision: 0.831, recall: 0.8281\n",
      "2018-12-29T17:21:28.693320, step: 693, loss: 0.3736141324043274, acc: 0.7969, auc: 0.919, precision: 0.8126, recall: 0.7823\n",
      "2018-12-29T17:21:28.775727, step: 694, loss: 0.3991223871707916, acc: 0.8359, auc: 0.9114, precision: 0.8323, recall: 0.8306\n",
      "2018-12-29T17:21:28.868444, step: 695, loss: 0.5973317623138428, acc: 0.6875, auc: 0.9036, precision: 0.7727, recall: 0.75\n",
      "2018-12-29T17:21:28.943820, step: 696, loss: 0.6151815056800842, acc: 0.75, auc: 0.9365, precision: 0.8316, recall: 0.7538\n",
      "2018-12-29T17:21:29.020914, step: 697, loss: 0.45579129457473755, acc: 0.7734, auc: 0.8887, precision: 0.8011, recall: 0.7632\n",
      "2018-12-29T17:21:29.103860, step: 698, loss: 0.42191794514656067, acc: 0.8125, auc: 0.9051, precision: 0.8179, recall: 0.8092\n",
      "2018-12-29T17:21:29.187083, step: 699, loss: 0.35353392362594604, acc: 0.8281, auc: 0.936, precision: 0.8401, recall: 0.8281\n",
      "2018-12-29T17:21:29.271180, step: 700, loss: 0.4067646861076355, acc: 0.8281, auc: 0.899, precision: 0.8275, recall: 0.8284\n",
      "\n",
      "Evaluation:\n",
      "2018-12-29T17:21:32.575715, step: 700, loss: 0.4107741415500641, acc: 0.8139076923076924, auc: 0.8964000000000001, precision: 0.8142846153846154, recall: 0.8142051282051281\n",
      "2018-12-29T17:21:32.649763, step: 701, loss: 0.32342231273651123, acc: 0.8359, auc: 0.9401, precision: 0.8397, recall: 0.8405\n",
      "2018-12-29T17:21:32.728137, step: 702, loss: 0.3749235272407532, acc: 0.8281, auc: 0.9182, precision: 0.8281, recall: 0.8281\n",
      "2018-12-29T17:21:32.806934, step: 703, loss: 0.3450468182563782, acc: 0.8203, auc: 0.9356, precision: 0.8418, recall: 0.8262\n",
      "2018-12-29T17:21:32.888838, step: 704, loss: 0.5727943181991577, acc: 0.7812, auc: 0.8785, precision: 0.8049, recall: 0.7497\n",
      "2018-12-29T17:21:32.986443, step: 705, loss: 0.5239259004592896, acc: 0.6797, auc: 0.9062, precision: 0.8029, recall: 0.6846\n",
      "2018-12-29T17:21:33.070512, step: 706, loss: 0.414249062538147, acc: 0.8125, auc: 0.9201, precision: 0.8246, recall: 0.8094\n",
      "2018-12-29T17:21:33.152273, step: 707, loss: 0.42454057931900024, acc: 0.7812, auc: 0.893, precision: 0.7812, recall: 0.7812\n",
      "2018-12-29T17:21:33.235471, step: 708, loss: 0.36132410168647766, acc: 0.8281, auc: 0.9229, precision: 0.8375, recall: 0.8209\n",
      "2018-12-29T17:21:33.332320, step: 709, loss: 0.36083805561065674, acc: 0.8516, auc: 0.9216, precision: 0.8516, recall: 0.8516\n",
      "2018-12-29T17:21:33.411442, step: 710, loss: 0.3995993435382843, acc: 0.7891, auc: 0.896, precision: 0.7897, recall: 0.7891\n",
      "2018-12-29T17:21:33.491972, step: 711, loss: 0.3816617727279663, acc: 0.8438, auc: 0.9244, precision: 0.8426, recall: 0.8453\n",
      "2018-12-29T17:21:33.582759, step: 712, loss: 0.5592319369316101, acc: 0.625, auc: 0.8919, precision: 0.7931, recall: 0.6\n",
      "2018-12-29T17:21:33.665992, step: 713, loss: 0.5348117351531982, acc: 0.7969, auc: 0.8747, precision: 0.8251, recall: 0.7726\n",
      "2018-12-29T17:21:33.748410, step: 714, loss: 0.42129191756248474, acc: 0.8359, auc: 0.9128, precision: 0.848, recall: 0.8264\n",
      "2018-12-29T17:21:33.830670, step: 715, loss: 0.42590153217315674, acc: 0.7734, auc: 0.9097, precision: 0.8048, recall: 0.7927\n",
      "2018-12-29T17:21:33.914050, step: 716, loss: 0.39561039209365845, acc: 0.8359, auc: 0.9273, precision: 0.8406, recall: 0.8306\n",
      "2018-12-29T17:21:33.994972, step: 717, loss: 0.47566235065460205, acc: 0.7422, auc: 0.914, precision: 0.8059, recall: 0.7829\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-29T17:21:34.089165, step: 718, loss: 0.4712352752685547, acc: 0.8359, auc: 0.9457, precision: 0.8659, recall: 0.8404\n",
      "2018-12-29T17:21:34.172457, step: 719, loss: 0.37287062406539917, acc: 0.8438, auc: 0.9288, precision: 0.8643, recall: 0.8342\n",
      "2018-12-29T17:21:34.257886, step: 720, loss: 0.36074182391166687, acc: 0.8359, auc: 0.921, precision: 0.8363, recall: 0.8151\n",
      "2018-12-29T17:21:34.345730, step: 721, loss: 0.3310011327266693, acc: 0.7969, auc: 0.9441, precision: 0.8167, recall: 0.7969\n",
      "2018-12-29T17:21:34.430107, step: 722, loss: 0.3588967025279999, acc: 0.8203, auc: 0.9162, precision: 0.8218, recall: 0.8214\n",
      "2018-12-29T17:21:34.512170, step: 723, loss: 0.3649916648864746, acc: 0.8281, auc: 0.9087, precision: 0.8277, recall: 0.8277\n",
      "2018-12-29T17:21:34.598606, step: 724, loss: 0.37903839349746704, acc: 0.8359, auc: 0.923, precision: 0.8417, recall: 0.838\n",
      "2018-12-29T17:21:34.681372, step: 725, loss: 0.389498770236969, acc: 0.8359, auc: 0.9493, precision: 0.8472, recall: 0.8417\n",
      "2018-12-29T17:21:34.781206, step: 726, loss: 0.6190686225891113, acc: 0.6484, auc: 0.8679, precision: 0.7877, recall: 0.6642\n",
      "2018-12-29T17:21:34.861002, step: 727, loss: 0.554095983505249, acc: 0.7656, auc: 0.8875, precision: 0.8333, recall: 0.751\n",
      "2018-12-29T17:21:34.954022, step: 728, loss: 0.4119114279747009, acc: 0.8672, auc: 0.9268, precision: 0.876, recall: 0.8636\n",
      "2018-12-29T17:21:35.053472, step: 729, loss: 0.3536406457424164, acc: 0.875, auc: 0.9627, precision: 0.8733, recall: 0.8769\n",
      "2018-12-29T17:21:35.140229, step: 730, loss: 0.3711542785167694, acc: 0.7578, auc: 0.957, precision: 0.8244, recall: 0.7578\n",
      "2018-12-29T17:21:35.221069, step: 731, loss: 0.5039026737213135, acc: 0.7969, auc: 0.9285, precision: 0.8306, recall: 0.8091\n",
      "2018-12-29T17:21:35.299567, step: 732, loss: 0.39413878321647644, acc: 0.75, auc: 0.9197, precision: 0.8301, recall: 0.7062\n",
      "2018-12-29T17:21:35.391058, step: 733, loss: 0.4674484133720398, acc: 0.7578, auc: 0.8615, precision: 0.7762, recall: 0.7635\n",
      "2018-12-29T17:21:35.485426, step: 734, loss: 0.3261309862136841, acc: 0.8672, auc: 0.9564, precision: 0.873, recall: 0.8791\n",
      "2018-12-29T17:21:35.575068, step: 735, loss: 0.4081536531448364, acc: 0.8281, auc: 0.9174, precision: 0.8304, recall: 0.8294\n",
      "2018-12-29T17:21:35.659298, step: 736, loss: 0.48631370067596436, acc: 0.7031, auc: 0.886, precision: 0.7591, recall: 0.7135\n",
      "2018-12-29T17:21:35.742564, step: 737, loss: 0.4407959580421448, acc: 0.8281, auc: 0.9118, precision: 0.8528, recall: 0.8196\n",
      "2018-12-29T17:21:35.822989, step: 738, loss: 0.44604605436325073, acc: 0.7656, auc: 0.8794, precision: 0.7787, recall: 0.7673\n",
      "2018-12-29T17:21:35.905595, step: 739, loss: 0.38815587759017944, acc: 0.8516, auc: 0.9091, precision: 0.8564, recall: 0.8497\n",
      "2018-12-29T17:21:35.989178, step: 740, loss: 0.385638952255249, acc: 0.8594, auc: 0.9077, precision: 0.8593, recall: 0.8593\n",
      "2018-12-29T17:21:36.068949, step: 741, loss: 0.38066038489341736, acc: 0.7656, auc: 0.9209, precision: 0.8031, recall: 0.7507\n",
      "2018-12-29T17:21:36.149977, step: 742, loss: 0.3512568473815918, acc: 0.8828, auc: 0.9464, precision: 0.8995, recall: 0.87\n",
      "2018-12-29T17:21:36.232174, step: 743, loss: 0.44564831256866455, acc: 0.7656, auc: 0.9023, precision: 0.8099, recall: 0.7626\n",
      "2018-12-29T17:21:36.309874, step: 744, loss: 0.42517173290252686, acc: 0.8281, auc: 0.901, precision: 0.8296, recall: 0.827\n",
      "2018-12-29T17:21:36.389442, step: 745, loss: 0.35249197483062744, acc: 0.8359, auc: 0.9307, precision: 0.8604, recall: 0.8131\n",
      "2018-12-29T17:21:36.468678, step: 746, loss: 0.3936343193054199, acc: 0.8438, auc: 0.9077, precision: 0.8465, recall: 0.8444\n",
      "2018-12-29T17:21:36.548814, step: 747, loss: 0.3607787787914276, acc: 0.8516, auc: 0.9276, precision: 0.8568, recall: 0.8451\n",
      "2018-12-29T17:21:36.635922, step: 748, loss: 0.41519176959991455, acc: 0.7891, auc: 0.9035, precision: 0.8248, recall: 0.7864\n",
      "2018-12-29T17:21:36.718414, step: 749, loss: 0.42081618309020996, acc: 0.8281, auc: 0.9123, precision: 0.8307, recall: 0.832\n",
      "2018-12-29T17:21:36.800983, step: 750, loss: 0.5055291652679443, acc: 0.6484, auc: 0.9257, precision: 0.7897, recall: 0.6591\n",
      "2018-12-29T17:21:36.883430, step: 751, loss: 0.48950451612472534, acc: 0.8047, auc: 0.9337, precision: 0.8458, recall: 0.8127\n",
      "2018-12-29T17:21:36.962306, step: 752, loss: 0.42399299144744873, acc: 0.6875, auc: 0.9262, precision: 0.7708, recall: 0.7039\n",
      "2018-12-29T17:21:37.041113, step: 753, loss: 0.4107303321361542, acc: 0.8203, auc: 0.9024, precision: 0.8203, recall: 0.8242\n",
      "2018-12-29T17:21:37.126129, step: 754, loss: 0.313068687915802, acc: 0.8516, auc: 0.9509, precision: 0.8729, recall: 0.8497\n",
      "2018-12-29T17:21:37.203957, step: 755, loss: 0.3825452923774719, acc: 0.8125, auc: 0.9197, precision: 0.8128, recall: 0.8079\n",
      "2018-12-29T17:21:37.281878, step: 756, loss: 0.3695776164531708, acc: 0.8203, auc: 0.9172, precision: 0.8244, recall: 0.8149\n",
      "2018-12-29T17:21:37.359831, step: 757, loss: 0.5015895962715149, acc: 0.7812, auc: 0.8559, precision: 0.7855, recall: 0.7822\n",
      "2018-12-29T17:21:37.438176, step: 758, loss: 0.3910996913909912, acc: 0.7891, auc: 0.9263, precision: 0.7979, recall: 0.781\n",
      "2018-12-29T17:21:37.516889, step: 759, loss: 0.5844812393188477, acc: 0.625, auc: 0.9086, precision: 0.7556, recall: 0.6461\n",
      "2018-12-29T17:21:37.597972, step: 760, loss: 0.5272845029830933, acc: 0.7734, auc: 0.9074, precision: 0.8131, recall: 0.7762\n",
      "2018-12-29T17:21:37.681667, step: 761, loss: 0.41294658184051514, acc: 0.8203, auc: 0.9214, precision: 0.8438, recall: 0.8223\n",
      "2018-12-29T17:21:37.765902, step: 762, loss: 0.3525567352771759, acc: 0.8828, auc: 0.9469, precision: 0.882, recall: 0.8839\n",
      "2018-12-29T17:21:37.847810, step: 763, loss: 0.3373974561691284, acc: 0.8438, auc: 0.9302, precision: 0.8528, recall: 0.8425\n",
      "2018-12-29T17:21:37.928158, step: 764, loss: 0.2973822057247162, acc: 0.8828, auc: 0.955, precision: 0.884, recall: 0.8815\n",
      "2018-12-29T17:21:38.008816, step: 765, loss: 0.45622238516807556, acc: 0.7891, auc: 0.8727, precision: 0.7899, recall: 0.7924\n",
      "2018-12-29T17:21:38.100685, step: 766, loss: 0.4030585289001465, acc: 0.8594, auc: 0.9305, precision: 0.8699, recall: 0.8554\n",
      "2018-12-29T17:21:38.181934, step: 767, loss: 0.4690835773944855, acc: 0.7109, auc: 0.9227, precision: 0.8112, recall: 0.7239\n",
      "2018-12-29T17:21:38.262179, step: 768, loss: 0.402969092130661, acc: 0.8203, auc: 0.9099, precision: 0.8284, recall: 0.7944\n",
      "2018-12-29T17:21:38.351629, step: 769, loss: 0.41413986682891846, acc: 0.7656, auc: 0.8876, precision: 0.7695, recall: 0.7674\n",
      "2018-12-29T17:21:38.444012, step: 770, loss: 0.3924295902252197, acc: 0.8516, auc: 0.9051, precision: 0.8524, recall: 0.8495\n",
      "2018-12-29T17:21:38.527742, step: 771, loss: 0.34119874238967896, acc: 0.8125, auc: 0.932, precision: 0.8351, recall: 0.8062\n",
      "2018-12-29T17:21:38.613215, step: 772, loss: 0.4073714017868042, acc: 0.8125, auc: 0.9027, precision: 0.812, recall: 0.8101\n",
      "2018-12-29T17:21:38.696062, step: 773, loss: 0.48068755865097046, acc: 0.7422, auc: 0.9119, precision: 0.7946, recall: 0.7422\n",
      "2018-12-29T17:21:38.788445, step: 774, loss: 0.6407479047775269, acc: 0.7188, auc: 0.887, precision: 0.7917, recall: 0.7188\n",
      "2018-12-29T17:21:38.872281, step: 775, loss: 0.5111736059188843, acc: 0.6406, auc: 0.874, precision: 0.7982, recall: 0.6167\n",
      "2018-12-29T17:21:38.956871, step: 776, loss: 0.5002140402793884, acc: 0.7109, auc: 0.8394, precision: 0.7313, recall: 0.7109\n",
      "2018-12-29T17:21:39.066149, step: 777, loss: 0.5246877670288086, acc: 0.7188, auc: 0.8199, precision: 0.7316, recall: 0.7098\n",
      "2018-12-29T17:21:39.150715, step: 778, loss: 0.43657156825065613, acc: 0.7734, auc: 0.9022, precision: 0.7951, recall: 0.7855\n",
      "2018-12-29T17:21:39.259457, step: 779, loss: 0.41814225912094116, acc: 0.8438, auc: 0.901, precision: 0.8421, recall: 0.8472\n",
      "2018-12-29T17:21:39.353159, step: 780, loss: 0.39522242546081543, acc: 0.8203, auc: 0.9002, precision: 0.8201, recall: 0.8195\n",
      "start training model\n",
      "2018-12-29T17:21:39.588944, step: 781, loss: 0.3126888871192932, acc: 0.875, auc: 0.9535, precision: 0.8759, recall: 0.8804\n",
      "2018-12-29T17:21:39.670032, step: 782, loss: 0.3081912100315094, acc: 0.8984, auc: 0.9492, precision: 0.8985, recall: 0.8984\n",
      "2018-12-29T17:21:39.759307, step: 783, loss: 0.35861408710479736, acc: 0.8438, auc: 0.9279, precision: 0.8659, recall: 0.8223\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-29T17:21:39.845938, step: 784, loss: 0.34307408332824707, acc: 0.875, auc: 0.9623, precision: 0.8813, recall: 0.8794\n",
      "2018-12-29T17:21:39.943328, step: 785, loss: 0.4834562838077545, acc: 0.7031, auc: 0.9714, precision: 0.8137, recall: 0.7031\n",
      "2018-12-29T17:21:40.026396, step: 786, loss: 0.44238290190696716, acc: 0.8359, auc: 0.9564, precision: 0.8883, recall: 0.8091\n",
      "2018-12-29T17:21:40.111396, step: 787, loss: 0.34853047132492065, acc: 0.8594, auc: 0.935, precision: 0.8594, recall: 0.8597\n",
      "2018-12-29T17:21:40.197970, step: 788, loss: 0.43015313148498535, acc: 0.7812, auc: 0.8846, precision: 0.7867, recall: 0.7853\n",
      "2018-12-29T17:21:40.280431, step: 789, loss: 0.36676687002182007, acc: 0.8438, auc: 0.9167, precision: 0.843, recall: 0.847\n",
      "2018-12-29T17:21:40.367795, step: 790, loss: 0.347016304731369, acc: 0.875, auc: 0.9338, precision: 0.8754, recall: 0.875\n",
      "2018-12-29T17:21:40.462987, step: 791, loss: 0.4011123478412628, acc: 0.7891, auc: 0.9053, precision: 0.811, recall: 0.7891\n",
      "2018-12-29T17:21:40.557745, step: 792, loss: 0.3109181523323059, acc: 0.8672, auc: 0.9463, precision: 0.8673, recall: 0.8672\n",
      "2018-12-29T17:21:40.639891, step: 793, loss: 0.31277793645858765, acc: 0.8594, auc: 0.9397, precision: 0.861, recall: 0.8589\n",
      "2018-12-29T17:21:40.725598, step: 794, loss: 0.42102739214897156, acc: 0.8125, auc: 0.897, precision: 0.8122, recall: 0.8128\n",
      "2018-12-29T17:21:40.809013, step: 795, loss: 0.38367414474487305, acc: 0.7969, auc: 0.9287, precision: 0.8404, recall: 0.7853\n",
      "2018-12-29T17:21:40.891076, step: 796, loss: 0.37636590003967285, acc: 0.8203, auc: 0.9419, precision: 0.8302, recall: 0.8021\n",
      "2018-12-29T17:21:40.981933, step: 797, loss: 0.30280473828315735, acc: 0.8516, auc: 0.9554, precision: 0.8767, recall: 0.8392\n",
      "2018-12-29T17:21:41.066142, step: 798, loss: 0.33732870221138, acc: 0.8828, auc: 0.9345, precision: 0.8811, recall: 0.8793\n",
      "2018-12-29T17:21:41.155851, step: 799, loss: 0.35754188895225525, acc: 0.8125, auc: 0.9355, precision: 0.8473, recall: 0.81\n",
      "2018-12-29T17:21:41.237979, step: 800, loss: 0.4930829405784607, acc: 0.7891, auc: 0.9201, precision: 0.8163, recall: 0.7913\n",
      "\n",
      "Evaluation:\n",
      "2018-12-29T17:21:44.694348, step: 800, loss: 0.5341423253218333, acc: 0.6670692307692309, auc: 0.8943538461538464, precision: 0.7714641025641026, recall: 0.6717692307692306\n",
      "2018-12-29T17:21:44.774409, step: 801, loss: 0.5050770044326782, acc: 0.6094, auc: 0.9172, precision: 0.7276, recall: 0.6399\n",
      "2018-12-29T17:21:44.875585, step: 802, loss: 0.46288251876831055, acc: 0.8203, auc: 0.9235, precision: 0.8247, recall: 0.824\n",
      "2018-12-29T17:21:44.958164, step: 803, loss: 0.3882066607475281, acc: 0.8438, auc: 0.9136, precision: 0.8496, recall: 0.8427\n",
      "2018-12-29T17:21:45.055904, step: 804, loss: 0.2628585398197174, acc: 0.9062, auc: 0.9728, precision: 0.9175, recall: 0.8923\n",
      "2018-12-29T17:21:45.145965, step: 805, loss: 0.2885572910308838, acc: 0.8906, auc: 0.9543, precision: 0.8936, recall: 0.8913\n",
      "2018-12-29T17:21:45.242474, step: 806, loss: 0.28470197319984436, acc: 0.9141, auc: 0.9519, precision: 0.915, recall: 0.9141\n",
      "2018-12-29T17:21:45.328349, step: 807, loss: 0.5332965850830078, acc: 0.7422, auc: 0.9022, precision: 0.7969, recall: 0.724\n",
      "2018-12-29T17:21:45.438158, step: 808, loss: 0.5800222158432007, acc: 0.75, auc: 0.9243, precision: 0.806, recall: 0.7786\n",
      "2018-12-29T17:21:45.523928, step: 809, loss: 0.4864566922187805, acc: 0.6328, auc: 0.8877, precision: 0.7253, recall: 0.6328\n",
      "2018-12-29T17:21:45.629313, step: 810, loss: 0.3875385522842407, acc: 0.8359, auc: 0.9458, precision: 0.8664, recall: 0.8219\n",
      "2018-12-29T17:21:45.725173, step: 811, loss: 0.38087379932403564, acc: 0.8047, auc: 0.9115, precision: 0.8118, recall: 0.8093\n",
      "2018-12-29T17:21:45.836998, step: 812, loss: 0.3492441177368164, acc: 0.8281, auc: 0.9364, precision: 0.8414, recall: 0.8233\n",
      "2018-12-29T17:21:45.918772, step: 813, loss: 0.3487994074821472, acc: 0.8438, auc: 0.9294, precision: 0.8436, recall: 0.8436\n",
      "2018-12-29T17:21:46.004490, step: 814, loss: 0.427511602640152, acc: 0.8203, auc: 0.9212, precision: 0.844, recall: 0.8328\n",
      "2018-12-29T17:21:46.098132, step: 815, loss: 0.5237236618995667, acc: 0.7969, auc: 0.9414, precision: 0.8367, recall: 0.7969\n",
      "2018-12-29T17:21:46.182538, step: 816, loss: 0.4761688709259033, acc: 0.6641, auc: 0.9211, precision: 0.8028, recall: 0.6532\n",
      "2018-12-29T17:21:46.265616, step: 817, loss: 0.4050338864326477, acc: 0.8359, auc: 0.9299, precision: 0.8467, recall: 0.8346\n",
      "2018-12-29T17:21:46.346159, step: 818, loss: 0.3902701139450073, acc: 0.7812, auc: 0.9058, precision: 0.7848, recall: 0.7837\n",
      "2018-12-29T17:21:46.426996, step: 819, loss: 0.2687017023563385, acc: 0.9375, auc: 0.9626, precision: 0.9322, recall: 0.9379\n",
      "2018-12-29T17:21:46.510717, step: 820, loss: 0.35195299983024597, acc: 0.875, auc: 0.9605, precision: 0.8784, recall: 0.8821\n",
      "2018-12-29T17:21:46.595662, step: 821, loss: 0.39049339294433594, acc: 0.7656, auc: 0.9424, precision: 0.8333, recall: 0.751\n",
      "2018-12-29T17:21:46.696238, step: 822, loss: 0.4118192791938782, acc: 0.8203, auc: 0.9114, precision: 0.8221, recall: 0.8209\n",
      "2018-12-29T17:21:46.783687, step: 823, loss: 0.30355745553970337, acc: 0.8047, auc: 0.9504, precision: 0.8278, recall: 0.8047\n",
      "2018-12-29T17:21:46.866200, step: 824, loss: 0.342303067445755, acc: 0.8594, auc: 0.9358, precision: 0.8551, recall: 0.8551\n",
      "2018-12-29T17:21:46.952323, step: 825, loss: 0.37207454442977905, acc: 0.8125, auc: 0.9325, precision: 0.8365, recall: 0.8187\n",
      "2018-12-29T17:21:47.035344, step: 826, loss: 0.42238229513168335, acc: 0.8672, auc: 0.9316, precision: 0.8672, recall: 0.8716\n",
      "2018-12-29T17:21:47.117699, step: 827, loss: 0.3561985492706299, acc: 0.7734, auc: 0.9334, precision: 0.8009, recall: 0.766\n",
      "2018-12-29T17:21:47.212082, step: 828, loss: 0.3431253731250763, acc: 0.8438, auc: 0.9323, precision: 0.8532, recall: 0.8412\n",
      "2018-12-29T17:21:47.308429, step: 829, loss: 0.2705563008785248, acc: 0.8828, auc: 0.9568, precision: 0.8839, recall: 0.8803\n",
      "2018-12-29T17:21:47.394673, step: 830, loss: 0.22750039398670197, acc: 0.9062, auc: 0.9701, precision: 0.9082, recall: 0.9082\n",
      "2018-12-29T17:21:47.483729, step: 831, loss: 0.34167587757110596, acc: 0.8359, auc: 0.9316, precision: 0.8431, recall: 0.8348\n",
      "2018-12-29T17:21:47.571027, step: 832, loss: 0.613402247428894, acc: 0.7812, auc: 0.9029, precision: 0.8093, recall: 0.7881\n",
      "2018-12-29T17:21:47.653813, step: 833, loss: 0.8476622700691223, acc: 0.5469, auc: 0.8551, precision: 0.7521, recall: 0.5797\n",
      "2018-12-29T17:21:47.741302, step: 834, loss: 0.46287915110588074, acc: 0.8125, auc: 0.9194, precision: 0.8177, recall: 0.8064\n",
      "2018-12-29T17:21:47.828429, step: 835, loss: 0.4383097290992737, acc: 0.7812, auc: 0.9029, precision: 0.7751, recall: 0.7801\n",
      "2018-12-29T17:21:47.913747, step: 836, loss: 0.3122043013572693, acc: 0.8594, auc: 0.9538, precision: 0.8677, recall: 0.8606\n",
      "2018-12-29T17:21:48.000091, step: 837, loss: 0.3593568205833435, acc: 0.8359, auc: 0.9343, precision: 0.8469, recall: 0.8482\n",
      "2018-12-29T17:21:48.097193, step: 838, loss: 0.3621973991394043, acc: 0.8281, auc: 0.9233, precision: 0.8292, recall: 0.8286\n",
      "2018-12-29T17:21:48.180702, step: 839, loss: 0.3922617435455322, acc: 0.8281, auc: 0.9061, precision: 0.8294, recall: 0.8255\n",
      "2018-12-29T17:21:48.264323, step: 840, loss: 0.33486759662628174, acc: 0.8281, auc: 0.9294, precision: 0.8281, recall: 0.8281\n",
      "2018-12-29T17:21:48.350627, step: 841, loss: 0.32400643825531006, acc: 0.8984, auc: 0.9393, precision: 0.8975, recall: 0.8984\n",
      "2018-12-29T17:21:48.433850, step: 842, loss: 0.34666869044303894, acc: 0.8125, auc: 0.925, precision: 0.8147, recall: 0.8138\n",
      "2018-12-29T17:21:48.517502, step: 843, loss: 0.29088151454925537, acc: 0.8516, auc: 0.9504, precision: 0.8554, recall: 0.8524\n",
      "2018-12-29T17:21:48.600120, step: 844, loss: 0.2831992506980896, acc: 0.8672, auc: 0.9538, precision: 0.8682, recall: 0.8663\n",
      "2018-12-29T17:21:48.684385, step: 845, loss: 0.2573091387748718, acc: 0.8906, auc: 0.9627, precision: 0.8988, recall: 0.8863\n",
      "2018-12-29T17:21:48.782174, step: 846, loss: 0.4157911241054535, acc: 0.8359, auc: 0.9494, precision: 0.8487, recall: 0.8531\n",
      "2018-12-29T17:21:48.867718, step: 847, loss: 1.512291431427002, acc: 0.4844, auc: 0.9213, precision: 0.2422, recall: 0.5\n",
      "2018-12-29T17:21:48.960132, step: 848, loss: 0.5469304919242859, acc: 0.7656, auc: 0.8775, precision: 0.7778, recall: 0.7778\n",
      "2018-12-29T17:21:49.042708, step: 849, loss: 0.4819263219833374, acc: 0.7969, auc: 0.8843, precision: 0.8046, recall: 0.7912\n",
      "2018-12-29T17:21:49.131675, step: 850, loss: 0.36877936124801636, acc: 0.8594, auc: 0.9578, precision: 0.8629, recall: 0.8586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-29T17:21:49.214771, step: 851, loss: 0.3779701590538025, acc: 0.7969, auc: 0.9437, precision: 0.8062, recall: 0.8008\n",
      "2018-12-29T17:21:49.298828, step: 852, loss: 0.31206008791923523, acc: 0.8672, auc: 0.9585, precision: 0.867, recall: 0.8673\n",
      "2018-12-29T17:21:49.395270, step: 853, loss: 0.30136343836784363, acc: 0.8281, auc: 0.9611, precision: 0.8413, recall: 0.8413\n",
      "2018-12-29T17:21:49.490149, step: 854, loss: 0.28526777029037476, acc: 0.8906, auc: 0.9635, precision: 0.8929, recall: 0.8867\n",
      "2018-12-29T17:21:49.583620, step: 855, loss: 0.31343305110931396, acc: 0.8594, auc: 0.9421, precision: 0.8708, recall: 0.8507\n",
      "2018-12-29T17:21:49.667492, step: 856, loss: 0.2538902461528778, acc: 0.9141, auc: 0.9761, precision: 0.922, recall: 0.9081\n",
      "2018-12-29T17:21:49.753627, step: 857, loss: 0.3891965448856354, acc: 0.8828, auc: 0.9451, precision: 0.8828, recall: 0.8874\n",
      "2018-12-29T17:21:49.838260, step: 858, loss: 0.6719198226928711, acc: 0.6172, auc: 0.9292, precision: 0.7832, recall: 0.6172\n",
      "2018-12-29T17:21:49.931288, step: 859, loss: 0.3791157901287079, acc: 0.8438, auc: 0.926, precision: 0.8538, recall: 0.8382\n",
      "2018-12-29T17:21:50.015058, step: 860, loss: 0.4124508798122406, acc: 0.7578, auc: 0.8911, precision: 0.7688, recall: 0.7525\n",
      "2018-12-29T17:21:50.097856, step: 861, loss: 0.3205524981021881, acc: 0.8594, auc: 0.9449, precision: 0.8611, recall: 0.8569\n",
      "2018-12-29T17:21:50.182150, step: 862, loss: 0.34417247772216797, acc: 0.7734, auc: 0.9358, precision: 0.7907, recall: 0.7984\n",
      "2018-12-29T17:21:50.270577, step: 863, loss: 0.47382134199142456, acc: 0.8281, auc: 0.9221, precision: 0.8446, recall: 0.8281\n",
      "2018-12-29T17:21:50.366187, step: 864, loss: 0.4499639570713043, acc: 0.7031, auc: 0.9138, precision: 0.7983, recall: 0.6987\n",
      "2018-12-29T17:21:50.453625, step: 865, loss: 0.34804674983024597, acc: 0.8516, auc: 0.924, precision: 0.8507, recall: 0.8452\n",
      "2018-12-29T17:21:50.549289, step: 866, loss: 0.36463993787765503, acc: 0.8359, auc: 0.9159, precision: 0.8358, recall: 0.836\n",
      "2018-12-29T17:21:50.653296, step: 867, loss: 0.29487836360931396, acc: 0.8672, auc: 0.9481, precision: 0.882, recall: 0.8561\n",
      "2018-12-29T17:21:50.755554, step: 868, loss: 0.3120383620262146, acc: 0.8672, auc: 0.9414, precision: 0.868, recall: 0.8672\n",
      "2018-12-29T17:21:50.852831, step: 869, loss: 0.25749489665031433, acc: 0.9141, auc: 0.9633, precision: 0.92, recall: 0.9172\n",
      "2018-12-29T17:21:50.939223, step: 870, loss: 0.34742772579193115, acc: 0.8672, auc: 0.9433, precision: 0.8681, recall: 0.8668\n",
      "2018-12-29T17:21:51.026314, step: 871, loss: 0.46600857377052307, acc: 0.7422, auc: 0.9346, precision: 0.8281, recall: 0.7462\n",
      "2018-12-29T17:21:51.121602, step: 872, loss: 0.5080674886703491, acc: 0.8203, auc: 0.9416, precision: 0.8491, recall: 0.8248\n",
      "2018-12-29T17:21:51.218900, step: 873, loss: 0.47183337807655334, acc: 0.7109, auc: 0.9028, precision: 0.8011, recall: 0.7431\n",
      "2018-12-29T17:21:51.309298, step: 874, loss: 0.39904943108558655, acc: 0.8281, auc: 0.914, precision: 0.8296, recall: 0.827\n",
      "2018-12-29T17:21:51.390979, step: 875, loss: 0.3978017270565033, acc: 0.8047, auc: 0.8891, precision: 0.8057, recall: 0.8105\n",
      "2018-12-29T17:21:51.486405, step: 876, loss: 0.3440816402435303, acc: 0.8047, auc: 0.9478, precision: 0.834, recall: 0.8185\n",
      "2018-12-29T17:21:51.579591, step: 877, loss: 0.322257936000824, acc: 0.8828, auc: 0.9566, precision: 0.8889, recall: 0.8789\n",
      "2018-12-29T17:21:51.683198, step: 878, loss: 0.3647784888744354, acc: 0.8047, auc: 0.9175, precision: 0.8242, recall: 0.7894\n",
      "2018-12-29T17:21:51.782949, step: 879, loss: 0.2948797047138214, acc: 0.9062, auc: 0.9494, precision: 0.906, recall: 0.906\n",
      "2018-12-29T17:21:51.869402, step: 880, loss: 0.3319593071937561, acc: 0.7812, auc: 0.9688, precision: 0.8295, recall: 0.7959\n",
      "2018-12-29T17:21:51.957621, step: 881, loss: 0.5444949269294739, acc: 0.7812, auc: 0.9628, precision: 0.8406, recall: 0.7712\n",
      "2018-12-29T17:21:52.057993, step: 882, loss: 0.44924160838127136, acc: 0.6797, auc: 0.9143, precision: 0.7548, recall: 0.6562\n",
      "2018-12-29T17:21:52.152963, step: 883, loss: 0.38358592987060547, acc: 0.8203, auc: 0.9235, precision: 0.8225, recall: 0.823\n",
      "2018-12-29T17:21:52.241341, step: 884, loss: 0.4326789379119873, acc: 0.7969, auc: 0.8884, precision: 0.8013, recall: 0.7978\n",
      "2018-12-29T17:21:52.353369, step: 885, loss: 0.43600910902023315, acc: 0.7891, auc: 0.8879, precision: 0.809, recall: 0.7948\n",
      "2018-12-29T17:21:52.441627, step: 886, loss: 0.434596449136734, acc: 0.8438, auc: 0.91, precision: 0.8409, recall: 0.8433\n",
      "2018-12-29T17:21:52.527355, step: 887, loss: 0.40766245126724243, acc: 0.7891, auc: 0.9058, precision: 0.8122, recall: 0.7824\n",
      "2018-12-29T17:21:52.626546, step: 888, loss: 0.3101690411567688, acc: 0.8672, auc: 0.9453, precision: 0.8697, recall: 0.8665\n",
      "2018-12-29T17:21:52.724506, step: 889, loss: 0.27323901653289795, acc: 0.8906, auc: 0.9588, precision: 0.8902, recall: 0.8902\n",
      "2018-12-29T17:21:52.808999, step: 890, loss: 0.3654702305793762, acc: 0.8125, auc: 0.9226, precision: 0.8243, recall: 0.811\n",
      "2018-12-29T17:21:52.902552, step: 891, loss: 0.3116685748100281, acc: 0.8672, auc: 0.9462, precision: 0.8645, recall: 0.8626\n",
      "2018-12-29T17:21:52.999409, step: 892, loss: 0.4160662293434143, acc: 0.7812, auc: 0.9677, precision: 0.8526, recall: 0.7705\n",
      "2018-12-29T17:21:53.081085, step: 893, loss: 0.7016381025314331, acc: 0.7188, auc: 0.924, precision: 0.7897, recall: 0.7263\n",
      "2018-12-29T17:21:53.165961, step: 894, loss: 0.48563382029533386, acc: 0.6875, auc: 0.8779, precision: 0.75, recall: 0.6875\n",
      "2018-12-29T17:21:53.249520, step: 895, loss: 0.35429680347442627, acc: 0.8359, auc: 0.9489, precision: 0.8516, recall: 0.8326\n",
      "2018-12-29T17:21:53.335592, step: 896, loss: 0.4049859046936035, acc: 0.8047, auc: 0.9081, precision: 0.837, recall: 0.7946\n",
      "2018-12-29T17:21:53.436342, step: 897, loss: 0.38342806696891785, acc: 0.7812, auc: 0.914, precision: 0.7925, recall: 0.7885\n",
      "2018-12-29T17:21:53.526440, step: 898, loss: 0.295086532831192, acc: 0.9062, auc: 0.9567, precision: 0.9107, recall: 0.9047\n",
      "2018-12-29T17:21:53.614515, step: 899, loss: 0.3077024221420288, acc: 0.8516, auc: 0.9443, precision: 0.8591, recall: 0.8504\n",
      "2018-12-29T17:21:53.706909, step: 900, loss: 0.32821211218833923, acc: 0.8594, auc: 0.934, precision: 0.8671, recall: 0.8617\n",
      "\n",
      "Evaluation:\n",
      "2018-12-29T17:21:57.315979, step: 900, loss: 0.4099368307835016, acc: 0.8377410256410258, auc: 0.912702564102564, precision: 0.8403974358974358, recall: 0.837194871794872\n",
      "2018-12-29T17:21:57.398859, step: 901, loss: 0.2532236576080322, acc: 0.875, auc: 0.9727, precision: 0.8796, recall: 0.8695\n",
      "2018-12-29T17:21:57.490147, step: 902, loss: 0.28019607067108154, acc: 0.8438, auc: 0.9596, precision: 0.8512, recall: 0.8512\n",
      "2018-12-29T17:21:57.577778, step: 903, loss: 0.42197421193122864, acc: 0.8594, auc: 0.9426, precision: 0.8759, recall: 0.8493\n",
      "2018-12-29T17:21:57.677940, step: 904, loss: 0.4387386739253998, acc: 0.7344, auc: 0.9542, precision: 0.8365, recall: 0.7069\n",
      "2018-12-29T17:21:57.773717, step: 905, loss: 0.38465964794158936, acc: 0.8438, auc: 0.9348, precision: 0.8657, recall: 0.8457\n",
      "2018-12-29T17:21:57.856103, step: 906, loss: 0.31340914964675903, acc: 0.8438, auc: 0.947, precision: 0.8438, recall: 0.8441\n",
      "2018-12-29T17:21:57.940801, step: 907, loss: 0.35838043689727783, acc: 0.8672, auc: 0.9149, precision: 0.8671, recall: 0.8515\n",
      "2018-12-29T17:21:58.030194, step: 908, loss: 0.351626455783844, acc: 0.8359, auc: 0.9329, precision: 0.8462, recall: 0.8429\n",
      "2018-12-29T17:21:58.116662, step: 909, loss: 0.33340078592300415, acc: 0.8438, auc: 0.9304, precision: 0.8393, recall: 0.8423\n",
      "2018-12-29T17:21:58.207770, step: 910, loss: 0.3832612633705139, acc: 0.8125, auc: 0.9157, precision: 0.8158, recall: 0.8189\n",
      "2018-12-29T17:21:58.300324, step: 911, loss: 0.2734644412994385, acc: 0.8828, auc: 0.956, precision: 0.8839, recall: 0.882\n",
      "2018-12-29T17:21:58.407965, step: 912, loss: 0.2981991767883301, acc: 0.875, auc: 0.9453, precision: 0.8755, recall: 0.8747\n",
      "2018-12-29T17:21:58.494482, step: 913, loss: 0.40068599581718445, acc: 0.75, auc: 0.8984, precision: 0.7596, recall: 0.7638\n",
      "2018-12-29T17:21:58.580908, step: 914, loss: 0.5930943489074707, acc: 0.8125, auc: 0.9027, precision: 0.8508, recall: 0.7991\n",
      "2018-12-29T17:21:58.677384, step: 915, loss: 0.6033404469490051, acc: 0.5859, auc: 0.9446, precision: 0.7655, recall: 0.6103\n",
      "2018-12-29T17:21:58.764121, step: 916, loss: 0.48597604036331177, acc: 0.7812, auc: 0.8818, precision: 0.789, recall: 0.7873\n",
      "2018-12-29T17:21:58.852195, step: 917, loss: 0.33258843421936035, acc: 0.8828, auc: 0.9524, precision: 0.8848, recall: 0.8834\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-29T17:21:58.934574, step: 918, loss: 0.3270699977874756, acc: 0.8594, auc: 0.9444, precision: 0.8627, recall: 0.8663\n",
      "2018-12-29T17:21:59.015167, step: 919, loss: 0.3179062008857727, acc: 0.8828, auc: 0.9364, precision: 0.884, recall: 0.8815\n",
      "2018-12-29T17:21:59.104029, step: 920, loss: 0.29639732837677, acc: 0.8281, auc: 0.9451, precision: 0.8292, recall: 0.8286\n",
      "2018-12-29T17:21:59.192148, step: 921, loss: 0.25476375222206116, acc: 0.875, auc: 0.9651, precision: 0.8819, recall: 0.8729\n",
      "2018-12-29T17:21:59.297577, step: 922, loss: 0.33764171600341797, acc: 0.875, auc: 0.9427, precision: 0.8718, recall: 0.8747\n",
      "2018-12-29T17:21:59.391583, step: 923, loss: 0.6076272130012512, acc: 0.6875, auc: 0.9058, precision: 0.7741, recall: 0.6468\n",
      "2018-12-29T17:21:59.478158, step: 924, loss: 0.34638863801956177, acc: 0.875, auc: 0.9538, precision: 0.9059, recall: 0.8343\n",
      "2018-12-29T17:21:59.564117, step: 925, loss: 0.34512221813201904, acc: 0.8516, auc: 0.9243, precision: 0.8512, recall: 0.8525\n",
      "2018-12-29T17:21:59.658576, step: 926, loss: 0.3321790099143982, acc: 0.8594, auc: 0.9401, precision: 0.8634, recall: 0.8548\n",
      "2018-12-29T17:21:59.764370, step: 927, loss: 0.33615779876708984, acc: 0.8828, auc: 0.9406, precision: 0.8985, recall: 0.8736\n",
      "2018-12-29T17:21:59.851921, step: 928, loss: 0.3986090421676636, acc: 0.8203, auc: 0.9104, precision: 0.8204, recall: 0.8203\n",
      "2018-12-29T17:21:59.946264, step: 929, loss: 0.34692642092704773, acc: 0.8047, auc: 0.9362, precision: 0.8379, recall: 0.7889\n",
      "2018-12-29T17:22:00.035142, step: 930, loss: 0.41071054339408875, acc: 0.7969, auc: 0.9025, precision: 0.7969, recall: 0.7972\n",
      "2018-12-29T17:22:00.114756, step: 931, loss: 0.3581709563732147, acc: 0.7422, auc: 0.9691, precision: 0.8167, recall: 0.7676\n",
      "2018-12-29T17:22:00.196244, step: 932, loss: 0.41854628920555115, acc: 0.8047, auc: 0.9039, precision: 0.8146, recall: 0.7985\n",
      "2018-12-29T17:22:00.279594, step: 933, loss: 0.31113335490226746, acc: 0.8047, auc: 0.9602, precision: 0.8425, recall: 0.8021\n",
      "2018-12-29T17:22:00.369679, step: 934, loss: 0.30963021516799927, acc: 0.8828, auc: 0.9499, precision: 0.8807, recall: 0.8859\n",
      "2018-12-29T17:22:00.463252, step: 935, loss: 0.3818589448928833, acc: 0.8125, auc: 0.9235, precision: 0.85, recall: 0.802\n",
      "2018-12-29T17:22:00.552353, step: 936, loss: 0.2903502583503723, acc: 0.8984, auc: 0.9699, precision: 0.9042, recall: 0.8966\n",
      "start training model\n",
      "2018-12-29T17:22:00.794581, step: 937, loss: 0.35689955949783325, acc: 0.8047, auc: 0.9465, precision: 0.8486, recall: 0.8074\n",
      "2018-12-29T17:22:00.879625, step: 938, loss: 0.3033069372177124, acc: 0.9062, auc: 0.9809, precision: 0.9188, recall: 0.9024\n",
      "2018-12-29T17:22:00.967643, step: 939, loss: 0.39294445514678955, acc: 0.7656, auc: 0.9501, precision: 0.8257, recall: 0.7722\n",
      "2018-12-29T17:22:01.049759, step: 940, loss: 0.31397396326065063, acc: 0.8984, auc: 0.9409, precision: 0.902, recall: 0.8956\n",
      "2018-12-29T17:22:01.136608, step: 941, loss: 0.2597300410270691, acc: 0.8906, auc: 0.9649, precision: 0.8929, recall: 0.8875\n",
      "2018-12-29T17:22:01.231946, step: 942, loss: 0.36729317903518677, acc: 0.8438, auc: 0.9257, precision: 0.8532, recall: 0.8412\n",
      "2018-12-29T17:22:01.334231, step: 943, loss: 0.3840932846069336, acc: 0.8516, auc: 0.9172, precision: 0.8489, recall: 0.8502\n",
      "2018-12-29T17:22:01.430867, step: 944, loss: 0.34414932131767273, acc: 0.7578, auc: 0.9697, precision: 0.8368, recall: 0.7578\n",
      "2018-12-29T17:22:01.516942, step: 945, loss: 0.4463176429271698, acc: 0.8203, auc: 0.964, precision: 0.8722, recall: 0.8115\n",
      "2018-12-29T17:22:01.609359, step: 946, loss: 0.3423231244087219, acc: 0.75, auc: 0.9683, precision: 0.8131, recall: 0.7313\n",
      "2018-12-29T17:22:01.693721, step: 947, loss: 0.2625420391559601, acc: 0.875, auc: 0.975, precision: 0.8849, recall: 0.8804\n",
      "2018-12-29T17:22:01.796135, step: 948, loss: 0.26740872859954834, acc: 0.8672, auc: 0.9612, precision: 0.8661, recall: 0.8647\n",
      "2018-12-29T17:22:01.885480, step: 949, loss: 0.29449591040611267, acc: 0.8828, auc: 0.9516, precision: 0.9003, recall: 0.8812\n",
      "2018-12-29T17:22:01.976253, step: 950, loss: 0.3068663775920868, acc: 0.8828, auc: 0.9614, precision: 0.8848, recall: 0.8834\n",
      "2018-12-29T17:22:02.079926, step: 951, loss: 0.4364622235298157, acc: 0.7344, auc: 0.9502, precision: 0.8138, recall: 0.7304\n",
      "2018-12-29T17:22:02.184850, step: 952, loss: 0.4345702528953552, acc: 0.8125, auc: 0.9017, precision: 0.8432, recall: 0.7975\n",
      "2018-12-29T17:22:02.272302, step: 953, loss: 0.31217142939567566, acc: 0.8828, auc: 0.9483, precision: 0.8834, recall: 0.8848\n",
      "2018-12-29T17:22:02.361869, step: 954, loss: 0.3018823266029358, acc: 0.9141, auc: 0.9552, precision: 0.9129, recall: 0.914\n",
      "2018-12-29T17:22:02.448753, step: 955, loss: 0.2539924383163452, acc: 0.875, auc: 0.9634, precision: 0.8878, recall: 0.8648\n",
      "2018-12-29T17:22:02.540170, step: 956, loss: 0.2620559632778168, acc: 0.875, auc: 0.9596, precision: 0.8739, recall: 0.8768\n",
      "2018-12-29T17:22:02.629844, step: 957, loss: 0.2931849956512451, acc: 0.8984, auc: 0.9644, precision: 0.9054, recall: 0.8935\n",
      "2018-12-29T17:22:02.718582, step: 958, loss: 0.6314138174057007, acc: 0.6719, auc: 0.9315, precision: 0.7921, recall: 0.6957\n",
      "2018-12-29T17:22:02.808222, step: 959, loss: 0.6903149485588074, acc: 0.7031, auc: 0.8912, precision: 0.7773, recall: 0.747\n",
      "2018-12-29T17:22:02.902023, step: 960, loss: 0.3701803982257843, acc: 0.8438, auc: 0.9606, precision: 0.8821, recall: 0.8204\n",
      "2018-12-29T17:22:03.004178, step: 961, loss: 0.340734601020813, acc: 0.8359, auc: 0.9453, precision: 0.857, recall: 0.8321\n",
      "2018-12-29T17:22:03.091914, step: 962, loss: 0.3476009964942932, acc: 0.8828, auc: 0.9319, precision: 0.8848, recall: 0.8834\n",
      "2018-12-29T17:22:03.210631, step: 963, loss: 0.24043042957782745, acc: 0.9219, auc: 0.9766, precision: 0.9223, recall: 0.9219\n",
      "2018-12-29T17:22:03.295995, step: 964, loss: 0.250856876373291, acc: 0.8828, auc: 0.9683, precision: 0.8838, recall: 0.8824\n",
      "2018-12-29T17:22:03.385300, step: 965, loss: 0.3525950610637665, acc: 0.8438, auc: 0.9449, precision: 0.8574, recall: 0.85\n",
      "2018-12-29T17:22:03.482756, step: 966, loss: 0.39840298891067505, acc: 0.8516, auc: 0.9253, precision: 0.8562, recall: 0.8507\n",
      "2018-12-29T17:22:03.584253, step: 967, loss: 0.3898165225982666, acc: 0.75, auc: 0.9414, precision: 0.814, recall: 0.7228\n",
      "2018-12-29T17:22:03.673506, step: 968, loss: 0.41676920652389526, acc: 0.8281, auc: 0.9172, precision: 0.8281, recall: 0.8363\n",
      "2018-12-29T17:22:03.776867, step: 969, loss: 0.3401927053928375, acc: 0.8125, auc: 0.9519, precision: 0.875, recall: 0.7857\n",
      "2018-12-29T17:22:03.880847, step: 970, loss: 0.34076324105262756, acc: 0.8281, auc: 0.9314, precision: 0.8359, recall: 0.8293\n",
      "2018-12-29T17:22:03.963997, step: 971, loss: 0.3113163709640503, acc: 0.8516, auc: 0.9545, precision: 0.8516, recall: 0.8523\n",
      "2018-12-29T17:22:04.049106, step: 972, loss: 0.3179798424243927, acc: 0.8203, auc: 0.9683, precision: 0.859, recall: 0.8203\n",
      "2018-12-29T17:22:04.139737, step: 973, loss: 0.3232099115848541, acc: 0.8828, auc: 0.9444, precision: 0.8805, recall: 0.8819\n",
      "2018-12-29T17:22:04.228105, step: 974, loss: 0.3181714713573456, acc: 0.8359, auc: 0.945, precision: 0.8516, recall: 0.8326\n",
      "2018-12-29T17:22:04.314978, step: 975, loss: 0.2382836937904358, acc: 0.9375, auc: 0.9756, precision: 0.9375, recall: 0.9379\n",
      "2018-12-29T17:22:04.403467, step: 976, loss: 0.2529081702232361, acc: 0.8906, auc: 0.9606, precision: 0.8904, recall: 0.8869\n",
      "2018-12-29T17:22:04.492521, step: 977, loss: 0.2139626145362854, acc: 0.875, auc: 0.98, precision: 0.8958, recall: 0.8715\n",
      "2018-12-29T17:22:04.589560, step: 978, loss: 0.5410459637641907, acc: 0.8047, auc: 0.9403, precision: 0.8375, recall: 0.7918\n",
      "2018-12-29T17:22:04.688880, step: 979, loss: 0.6595364809036255, acc: 0.6016, auc: 0.911, precision: 0.7703, recall: 0.625\n",
      "2018-12-29T17:22:04.776180, step: 980, loss: 0.4572112560272217, acc: 0.8359, auc: 0.9484, precision: 0.8537, recall: 0.8394\n",
      "2018-12-29T17:22:04.867866, step: 981, loss: 0.32351019978523254, acc: 0.8594, auc: 0.9575, precision: 0.8646, recall: 0.8603\n",
      "2018-12-29T17:22:04.958575, step: 982, loss: 0.3101024627685547, acc: 0.8828, auc: 0.9431, precision: 0.8973, recall: 0.877\n",
      "2018-12-29T17:22:05.047101, step: 983, loss: 0.3306533098220825, acc: 0.8594, auc: 0.9324, precision: 0.8552, recall: 0.8609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-29T17:22:05.143462, step: 984, loss: 0.23827232420444489, acc: 0.8984, auc: 0.9709, precision: 0.9015, recall: 0.8971\n",
      "2018-12-29T17:22:05.245702, step: 985, loss: 0.26106828451156616, acc: 0.8984, auc: 0.9554, precision: 0.8978, recall: 0.8985\n",
      "2018-12-29T17:22:05.330551, step: 986, loss: 0.32993990182876587, acc: 0.8594, auc: 0.9444, precision: 0.8586, recall: 0.8629\n",
      "2018-12-29T17:22:05.415471, step: 987, loss: 0.43456804752349854, acc: 0.8125, auc: 0.9295, precision: 0.8474, recall: 0.8249\n",
      "2018-12-29T17:22:05.500736, step: 988, loss: 0.3520810604095459, acc: 0.9062, auc: 0.986, precision: 0.9091, recall: 0.9189\n",
      "2018-12-29T17:22:05.592030, step: 989, loss: 0.31822192668914795, acc: 0.8281, auc: 0.9522, precision: 0.8599, recall: 0.8186\n",
      "2018-12-29T17:22:05.694322, step: 990, loss: 0.3573791980743408, acc: 0.8516, auc: 0.9203, precision: 0.8524, recall: 0.8495\n",
      "2018-12-29T17:22:05.793319, step: 991, loss: 0.23328407108783722, acc: 0.9062, auc: 0.9782, precision: 0.9125, recall: 0.8964\n",
      "2018-12-29T17:22:05.892526, step: 992, loss: 0.3368750810623169, acc: 0.8203, auc: 0.9429, precision: 0.8489, recall: 0.8034\n",
      "2018-12-29T17:22:05.999547, step: 993, loss: 0.2947554588317871, acc: 0.8828, auc: 0.9547, precision: 0.884, recall: 0.8809\n",
      "2018-12-29T17:22:06.085461, step: 994, loss: 0.3452472686767578, acc: 0.8047, auc: 0.9441, precision: 0.8415, recall: 0.8047\n",
      "2018-12-29T17:22:06.195276, step: 995, loss: 0.3326021432876587, acc: 0.8828, auc: 0.9624, precision: 0.8911, recall: 0.8817\n",
      "2018-12-29T17:22:06.285242, step: 996, loss: 0.43950337171554565, acc: 0.7188, auc: 0.9137, precision: 0.8092, recall: 0.701\n",
      "2018-12-29T17:22:06.374931, step: 997, loss: 0.35376954078674316, acc: 0.8281, auc: 0.9225, precision: 0.828, recall: 0.828\n",
      "2018-12-29T17:22:06.464482, step: 998, loss: 0.3592734634876251, acc: 0.8281, auc: 0.9461, precision: 0.8373, recall: 0.8583\n",
      "2018-12-29T17:22:06.554960, step: 999, loss: 0.42309510707855225, acc: 0.8359, auc: 0.9469, precision: 0.8572, recall: 0.8522\n",
      "2018-12-29T17:22:06.641456, step: 1000, loss: 0.3429107069969177, acc: 0.8203, auc: 0.9538, precision: 0.8665, recall: 0.8\n",
      "\n",
      "Evaluation:\n",
      "2018-12-29T17:22:10.352827, step: 1000, loss: 0.3793878838037833, acc: 0.8351410256410257, auc: 0.914669230769231, precision: 0.8351461538461539, recall: 0.8352538461538463\n",
      "2018-12-29T17:22:10.440615, step: 1001, loss: 0.29270321130752563, acc: 0.9062, auc: 0.9562, precision: 0.906, recall: 0.9068\n",
      "2018-12-29T17:22:10.531449, step: 1002, loss: 0.23660019040107727, acc: 0.8984, auc: 0.9707, precision: 0.8964, recall: 0.9018\n",
      "2018-12-29T17:22:10.620349, step: 1003, loss: 0.28148987889289856, acc: 0.8672, auc: 0.9538, precision: 0.8663, recall: 0.8682\n",
      "2018-12-29T17:22:10.710473, step: 1004, loss: 0.34279194474220276, acc: 0.8281, auc: 0.9365, precision: 0.8388, recall: 0.8309\n",
      "2018-12-29T17:22:10.805450, step: 1005, loss: 0.2655062675476074, acc: 0.9141, auc: 0.9842, precision: 0.9129, recall: 0.9232\n",
      "2018-12-29T17:22:10.895728, step: 1006, loss: 0.303178995847702, acc: 0.8438, auc: 0.9733, precision: 0.8766, recall: 0.8368\n",
      "2018-12-29T17:22:10.985834, step: 1007, loss: 0.43748152256011963, acc: 0.8438, auc: 0.9133, precision: 0.854, recall: 0.8366\n",
      "2018-12-29T17:22:11.088165, step: 1008, loss: 0.31781554222106934, acc: 0.8203, auc: 0.9487, precision: 0.8454, recall: 0.8309\n",
      "2018-12-29T17:22:11.172806, step: 1009, loss: 0.2292933464050293, acc: 0.9453, auc: 0.9882, precision: 0.9449, recall: 0.9466\n",
      "2018-12-29T17:22:11.262873, step: 1010, loss: 0.26518768072128296, acc: 0.8438, auc: 0.9582, precision: 0.8574, recall: 0.8407\n",
      "2018-12-29T17:22:11.356262, step: 1011, loss: 0.2756253778934479, acc: 0.9062, auc: 0.9585, precision: 0.9062, recall: 0.9062\n",
      "2018-12-29T17:22:11.445490, step: 1012, loss: 0.3275343179702759, acc: 0.8125, auc: 0.9424, precision: 0.8432, recall: 0.7975\n",
      "2018-12-29T17:22:11.549317, step: 1013, loss: 0.31133711338043213, acc: 0.8438, auc: 0.9461, precision: 0.8473, recall: 0.8402\n",
      "2018-12-29T17:22:11.644284, step: 1014, loss: 0.4514160752296448, acc: 0.75, auc: 0.9488, precision: 0.8018, recall: 0.7685\n",
      "2018-12-29T17:22:11.731329, step: 1015, loss: 0.5441403388977051, acc: 0.7656, auc: 0.9392, precision: 0.8363, recall: 0.7386\n",
      "2018-12-29T17:22:11.816111, step: 1016, loss: 0.3854641914367676, acc: 0.8047, auc: 0.9318, precision: 0.8389, recall: 0.7725\n",
      "2018-12-29T17:22:11.910968, step: 1017, loss: 0.2659950256347656, acc: 0.8984, auc: 0.9692, precision: 0.9018, recall: 0.8964\n",
      "2018-12-29T17:22:12.007966, step: 1018, loss: 0.3124734163284302, acc: 0.875, auc: 0.9448, precision: 0.8856, recall: 0.8724\n",
      "2018-12-29T17:22:12.104821, step: 1019, loss: 0.3615425229072571, acc: 0.8438, auc: 0.9268, precision: 0.8422, recall: 0.8438\n",
      "2018-12-29T17:22:12.194843, step: 1020, loss: 0.2777683734893799, acc: 0.8594, auc: 0.9565, precision: 0.8621, recall: 0.8468\n",
      "2018-12-29T17:22:12.287196, step: 1021, loss: 0.2696808874607086, acc: 0.8906, auc: 0.9629, precision: 0.9067, recall: 0.8876\n",
      "2018-12-29T17:22:12.376806, step: 1022, loss: 0.28523609042167664, acc: 0.8906, auc: 0.9578, precision: 0.891, recall: 0.8906\n",
      "2018-12-29T17:22:12.466207, step: 1023, loss: 0.3589118719100952, acc: 0.7891, auc: 0.9363, precision: 0.8267, recall: 0.7779\n",
      "2018-12-29T17:22:12.571354, step: 1024, loss: 0.414408802986145, acc: 0.8438, auc: 0.9325, precision: 0.854, recall: 0.8478\n",
      "2018-12-29T17:22:12.670473, step: 1025, loss: 0.34985336661338806, acc: 0.75, auc: 0.9635, precision: 0.8162, recall: 0.7605\n",
      "2018-12-29T17:22:12.772584, step: 1026, loss: 0.3879871964454651, acc: 0.8281, auc: 0.9349, precision: 0.8463, recall: 0.8226\n",
      "2018-12-29T17:22:12.860612, step: 1027, loss: 0.3550645709037781, acc: 0.8281, auc: 0.9475, precision: 0.8636, recall: 0.8281\n",
      "2018-12-29T17:22:12.951552, step: 1028, loss: 0.26179978251457214, acc: 0.875, auc: 0.9635, precision: 0.8744, recall: 0.8755\n",
      "2018-12-29T17:22:13.042404, step: 1029, loss: 0.31128180027008057, acc: 0.875, auc: 0.9415, precision: 0.877, recall: 0.8718\n",
      "2018-12-29T17:22:13.138184, step: 1030, loss: 0.37871497869491577, acc: 0.7969, auc: 0.9358, precision: 0.825, recall: 0.8059\n",
      "2018-12-29T17:22:13.230850, step: 1031, loss: 0.5653773546218872, acc: 0.8047, auc: 0.9114, precision: 0.8295, recall: 0.798\n",
      "2018-12-29T17:22:13.317415, step: 1032, loss: 0.5167550444602966, acc: 0.6484, auc: 0.9279, precision: 0.7857, recall: 0.6691\n",
      "2018-12-29T17:22:13.404518, step: 1033, loss: 0.3486495912075043, acc: 0.8438, auc: 0.9439, precision: 0.843, recall: 0.847\n",
      "2018-12-29T17:22:13.505022, step: 1034, loss: 0.2599324584007263, acc: 0.8828, auc: 0.974, precision: 0.8719, recall: 0.899\n",
      "2018-12-29T17:22:13.597490, step: 1035, loss: 0.34728533029556274, acc: 0.875, auc: 0.9555, precision: 0.8706, recall: 0.8826\n",
      "2018-12-29T17:22:13.694906, step: 1036, loss: 0.3734464645385742, acc: 0.8125, auc: 0.9512, precision: 0.8636, recall: 0.8125\n",
      "2018-12-29T17:22:13.785985, step: 1037, loss: 0.31447792053222656, acc: 0.875, auc: 0.9641, precision: 0.8757, recall: 0.8779\n",
      "2018-12-29T17:22:13.888153, step: 1038, loss: 0.28163665533065796, acc: 0.8672, auc: 0.9531, precision: 0.8699, recall: 0.8658\n",
      "2018-12-29T17:22:13.973929, step: 1039, loss: 0.30065906047821045, acc: 0.8672, auc: 0.9523, precision: 0.8672, recall: 0.868\n",
      "2018-12-29T17:22:14.064763, step: 1040, loss: 0.24059796333312988, acc: 0.9062, auc: 0.9717, precision: 0.9127, recall: 0.9062\n",
      "2018-12-29T17:22:14.163768, step: 1041, loss: 0.3234553039073944, acc: 0.8516, auc: 0.9382, precision: 0.8523, recall: 0.8516\n",
      "2018-12-29T17:22:14.251455, step: 1042, loss: 0.4098222553730011, acc: 0.8125, auc: 0.9346, precision: 0.8422, recall: 0.8029\n",
      "2018-12-29T17:22:14.352580, step: 1043, loss: 0.35126179456710815, acc: 0.875, auc: 0.9264, precision: 0.8792, recall: 0.8644\n",
      "2018-12-29T17:22:14.444906, step: 1044, loss: 0.3346421718597412, acc: 0.875, auc: 0.939, precision: 0.8735, recall: 0.8735\n",
      "2018-12-29T17:22:14.533782, step: 1045, loss: 0.2463948130607605, acc: 0.9062, auc: 0.9667, precision: 0.9067, recall: 0.9075\n",
      "2018-12-29T17:22:14.630812, step: 1046, loss: 0.28669166564941406, acc: 0.8672, auc: 0.9502, precision: 0.8669, recall: 0.8611\n",
      "2018-12-29T17:22:14.731329, step: 1047, loss: 0.25944143533706665, acc: 0.8516, auc: 0.9609, precision: 0.8681, recall: 0.8482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-29T17:22:14.823575, step: 1048, loss: 0.2905362844467163, acc: 0.8984, auc: 0.9554, precision: 0.9023, recall: 0.8919\n",
      "2018-12-29T17:22:14.912062, step: 1049, loss: 0.2746005654335022, acc: 0.8672, auc: 0.9618, precision: 0.8925, recall: 0.8593\n",
      "2018-12-29T17:22:15.001621, step: 1050, loss: 0.2687516212463379, acc: 0.8906, auc: 0.9597, precision: 0.8879, recall: 0.8879\n",
      "2018-12-29T17:22:15.090808, step: 1051, loss: 0.19590535759925842, acc: 0.9297, auc: 0.9783, precision: 0.9328, recall: 0.9357\n",
      "2018-12-29T17:22:15.186147, step: 1052, loss: 0.3022181987762451, acc: 0.8594, auc: 0.96, precision: 0.8622, recall: 0.8601\n",
      "2018-12-29T17:22:15.272263, step: 1053, loss: 0.7440314292907715, acc: 0.5781, auc: 0.9145, precision: 0.7311, recall: 0.5844\n",
      "2018-12-29T17:22:15.364222, step: 1054, loss: 0.5452349781990051, acc: 0.8203, auc: 0.9128, precision: 0.8663, recall: 0.8231\n",
      "2018-12-29T17:22:15.466361, step: 1055, loss: 0.3690035343170166, acc: 0.8516, auc: 0.952, precision: 0.8601, recall: 0.8553\n",
      "2018-12-29T17:22:15.569031, step: 1056, loss: 0.32371217012405396, acc: 0.8594, auc: 0.9459, precision: 0.8647, recall: 0.869\n",
      "2018-12-29T17:22:15.664423, step: 1057, loss: 0.2847994565963745, acc: 0.8828, auc: 0.9541, precision: 0.8761, recall: 0.8787\n",
      "2018-12-29T17:22:15.751756, step: 1058, loss: 0.28578677773475647, acc: 0.8984, auc: 0.9535, precision: 0.8966, recall: 0.8998\n",
      "2018-12-29T17:22:15.849002, step: 1059, loss: 0.2607795298099518, acc: 0.8984, auc: 0.965, precision: 0.8981, recall: 0.8968\n",
      "2018-12-29T17:22:15.940016, step: 1060, loss: 0.22798317670822144, acc: 0.8594, auc: 0.9802, precision: 0.8855, recall: 0.8553\n",
      "2018-12-29T17:22:16.038327, step: 1061, loss: 0.24228304624557495, acc: 0.9375, auc: 0.9868, precision: 0.9352, recall: 0.943\n",
      "2018-12-29T17:22:16.144003, step: 1062, loss: 0.49407878518104553, acc: 0.7109, auc: 0.9604, precision: 0.815, recall: 0.7154\n",
      "2018-12-29T17:22:16.238976, step: 1063, loss: 0.5113729238510132, acc: 0.7891, auc: 0.9115, precision: 0.8081, recall: 0.7966\n",
      "2018-12-29T17:22:16.344329, step: 1064, loss: 0.3340376913547516, acc: 0.7812, auc: 0.9368, precision: 0.8068, recall: 0.7716\n",
      "2018-12-29T17:22:16.436162, step: 1065, loss: 0.3658454716205597, acc: 0.8438, auc: 0.9243, precision: 0.839, recall: 0.839\n",
      "2018-12-29T17:22:16.532248, step: 1066, loss: 0.2675246596336365, acc: 0.8672, auc: 0.9596, precision: 0.8817, recall: 0.8579\n",
      "2018-12-29T17:22:16.624125, step: 1067, loss: 0.2539181411266327, acc: 0.8672, auc: 0.9612, precision: 0.878, recall: 0.8744\n",
      "2018-12-29T17:22:16.710862, step: 1068, loss: 0.26009494066238403, acc: 0.8672, auc: 0.9701, precision: 0.8728, recall: 0.8632\n",
      "2018-12-29T17:22:16.803010, step: 1069, loss: 0.3250417113304138, acc: 0.8047, auc: 0.9729, precision: 0.8306, recall: 0.8325\n",
      "2018-12-29T17:22:16.898456, step: 1070, loss: 0.37273862957954407, acc: 0.8828, auc: 0.9455, precision: 0.8917, recall: 0.8805\n",
      "2018-12-29T17:22:16.992108, step: 1071, loss: 0.31066054105758667, acc: 0.875, auc: 0.9636, precision: 0.8949, recall: 0.8733\n",
      "2018-12-29T17:22:17.082629, step: 1072, loss: 0.3110601007938385, acc: 0.8672, auc: 0.9477, precision: 0.8673, recall: 0.867\n",
      "2018-12-29T17:22:17.189556, step: 1073, loss: 0.2456013262271881, acc: 0.8672, auc: 0.9633, precision: 0.8772, recall: 0.8507\n",
      "2018-12-29T17:22:17.283890, step: 1074, loss: 0.29316821694374084, acc: 0.8672, auc: 0.963, precision: 0.8786, recall: 0.88\n",
      "2018-12-29T17:22:17.372333, step: 1075, loss: 0.44246742129325867, acc: 0.8438, auc: 0.9521, precision: 0.8813, recall: 0.8234\n",
      "2018-12-29T17:22:17.461885, step: 1076, loss: 0.6603736877441406, acc: 0.6172, auc: 0.927, precision: 0.7812, recall: 0.6231\n",
      "2018-12-29T17:22:17.550658, step: 1077, loss: 0.35700464248657227, acc: 0.9219, auc: 0.9561, precision: 0.9233, recall: 0.9245\n",
      "2018-12-29T17:22:17.641256, step: 1078, loss: 0.3012453317642212, acc: 0.8984, auc: 0.9599, precision: 0.8998, recall: 0.8953\n",
      "2018-12-29T17:22:17.753184, step: 1079, loss: 0.2520245313644409, acc: 0.8984, auc: 0.9768, precision: 0.9058, recall: 0.8995\n",
      "2018-12-29T17:22:17.851456, step: 1080, loss: 0.3941439092159271, acc: 0.8672, auc: 0.9128, precision: 0.8681, recall: 0.8668\n",
      "2018-12-29T17:22:17.957408, step: 1081, loss: 0.2557075619697571, acc: 0.8672, auc: 0.9643, precision: 0.8658, recall: 0.8667\n",
      "2018-12-29T17:22:18.067827, step: 1082, loss: 0.3350822329521179, acc: 0.8359, auc: 0.9383, precision: 0.8582, recall: 0.8418\n",
      "2018-12-29T17:22:18.165343, step: 1083, loss: 0.26166898012161255, acc: 0.9062, auc: 0.9957, precision: 0.9091, recall: 0.9189\n",
      "2018-12-29T17:22:18.257993, step: 1084, loss: 0.33915048837661743, acc: 0.7812, auc: 0.9665, precision: 0.8394, recall: 0.7747\n",
      "2018-12-29T17:22:18.353803, step: 1085, loss: 0.2763265371322632, acc: 0.8984, auc: 0.9582, precision: 0.8983, recall: 0.8986\n",
      "2018-12-29T17:22:18.441179, step: 1086, loss: 0.2712821364402771, acc: 0.8516, auc: 0.9552, precision: 0.8566, recall: 0.8487\n",
      "2018-12-29T17:22:18.539965, step: 1087, loss: 0.2673477530479431, acc: 0.875, auc: 0.9543, precision: 0.874, recall: 0.8755\n",
      "2018-12-29T17:22:18.633960, step: 1088, loss: 0.2609981894493103, acc: 0.8984, auc: 0.9623, precision: 0.8978, recall: 0.9012\n",
      "2018-12-29T17:22:18.728714, step: 1089, loss: 0.2706471383571625, acc: 0.8906, auc: 0.9554, precision: 0.8859, recall: 0.8859\n",
      "2018-12-29T17:22:18.815784, step: 1090, loss: 0.1886938363313675, acc: 0.8984, auc: 0.9824, precision: 0.9012, recall: 0.8978\n",
      "2018-12-29T17:22:18.913981, step: 1091, loss: 0.19205716252326965, acc: 0.9531, auc: 0.9789, precision: 0.9567, recall: 0.9504\n",
      "2018-12-29T17:22:19.000754, step: 1092, loss: 0.3027307391166687, acc: 0.8594, auc: 0.9543, precision: 0.8745, recall: 0.8715\n",
      "start training model\n",
      "2018-12-29T17:22:19.233720, step: 1093, loss: 0.675921618938446, acc: 0.7578, auc: 0.9692, precision: 0.8465, recall: 0.7328\n",
      "2018-12-29T17:22:19.327506, step: 1094, loss: 0.6677077412605286, acc: 0.6172, auc: 0.9326, precision: 0.7906, recall: 0.5917\n",
      "2018-12-29T17:22:19.420502, step: 1095, loss: 0.41402918100357056, acc: 0.8828, auc: 0.9466, precision: 0.8889, recall: 0.8789\n",
      "2018-12-29T17:22:19.522973, step: 1096, loss: 0.27787166833877563, acc: 0.8906, auc: 0.9773, precision: 0.8956, recall: 0.8925\n",
      "2018-12-29T17:22:19.610606, step: 1097, loss: 0.2647954821586609, acc: 0.8828, auc: 0.9653, precision: 0.8934, recall: 0.8751\n",
      "2018-12-29T17:22:19.709088, step: 1098, loss: 0.2443787008523941, acc: 0.8828, auc: 0.9745, precision: 0.8927, recall: 0.8779\n",
      "2018-12-29T17:22:19.811666, step: 1099, loss: 0.2807654142379761, acc: 0.8672, auc: 0.9587, precision: 0.8721, recall: 0.8751\n",
      "2018-12-29T17:22:19.901960, step: 1100, loss: 0.2155047506093979, acc: 0.9141, auc: 0.9844, precision: 0.9166, recall: 0.9141\n",
      "\n",
      "Evaluation:\n",
      "2018-12-29T17:22:23.649873, step: 1100, loss: 0.3980553482587521, acc: 0.8169076923076923, auc: 0.9166384615384617, precision: 0.8260153846153846, recall: 0.8175051282051282\n",
      "2018-12-29T17:22:23.738366, step: 1101, loss: 0.2655230760574341, acc: 0.875, auc: 0.9483, precision: 0.875, recall: 0.8499\n",
      "2018-12-29T17:22:23.834174, step: 1102, loss: 0.1903107464313507, acc: 0.9219, auc: 0.9822, precision: 0.9306, recall: 0.9242\n",
      "2018-12-29T17:22:23.925651, step: 1103, loss: 0.28177130222320557, acc: 0.8906, auc: 0.9523, precision: 0.8912, recall: 0.8897\n",
      "2018-12-29T17:22:24.030973, step: 1104, loss: 0.20820273458957672, acc: 0.9219, auc: 0.9766, precision: 0.9219, recall: 0.9219\n",
      "2018-12-29T17:22:24.134001, step: 1105, loss: 0.15398722887039185, acc: 0.9297, auc: 0.9901, precision: 0.9347, recall: 0.9245\n",
      "2018-12-29T17:22:24.221810, step: 1106, loss: 0.19245687127113342, acc: 0.9141, auc: 0.98, precision: 0.917, recall: 0.9134\n",
      "2018-12-29T17:22:24.314933, step: 1107, loss: 0.2650431990623474, acc: 0.8672, auc: 0.9629, precision: 0.8681, recall: 0.8645\n",
      "2018-12-29T17:22:24.417589, step: 1108, loss: 0.28586480021476746, acc: 0.8516, auc: 0.9725, precision: 0.856, recall: 0.8641\n",
      "2018-12-29T17:22:24.501539, step: 1109, loss: 0.5559845566749573, acc: 0.8047, auc: 0.9723, precision: 0.8333, recall: 0.8397\n",
      "2018-12-29T17:22:24.591216, step: 1110, loss: 0.5125584006309509, acc: 0.6406, auc: 0.9596, precision: 0.785, recall: 0.6567\n",
      "2018-12-29T17:22:24.683769, step: 1111, loss: 0.40136390924453735, acc: 0.8359, auc: 0.934, precision: 0.8346, recall: 0.8467\n",
      "2018-12-29T17:22:24.776238, step: 1112, loss: 0.22560487687587738, acc: 0.9141, auc: 0.9915, precision: 0.9216, recall: 0.9151\n",
      "2018-12-29T17:22:24.862675, step: 1113, loss: 0.21913790702819824, acc: 0.9219, auc: 0.9755, precision: 0.9244, recall: 0.9202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-29T17:22:24.965761, step: 1114, loss: 0.23529410362243652, acc: 0.8906, auc: 0.9675, precision: 0.8962, recall: 0.8916\n",
      "2018-12-29T17:22:25.077391, step: 1115, loss: 0.23699253797531128, acc: 0.9141, auc: 0.9778, precision: 0.9113, recall: 0.9179\n",
      "2018-12-29T17:22:25.188027, step: 1116, loss: 0.2588772177696228, acc: 0.8516, auc: 0.9664, precision: 0.8707, recall: 0.8385\n",
      "2018-12-29T17:22:25.282662, step: 1117, loss: 0.27388396859169006, acc: 0.8984, auc: 0.9624, precision: 0.9054, recall: 0.8935\n",
      "2018-12-29T17:22:25.374943, step: 1118, loss: 0.28979283571243286, acc: 0.8281, auc: 0.9624, precision: 0.8542, recall: 0.8324\n",
      "2018-12-29T17:22:25.473591, step: 1119, loss: 0.3445179760456085, acc: 0.875, auc: 0.9518, precision: 0.8862, recall: 0.8711\n",
      "2018-12-29T17:22:25.560229, step: 1120, loss: 0.3699663281440735, acc: 0.7734, auc: 0.9616, precision: 0.8284, recall: 0.7828\n",
      "2018-12-29T17:22:25.649179, step: 1121, loss: 0.3583486080169678, acc: 0.8906, auc: 0.9395, precision: 0.9083, recall: 0.8843\n",
      "2018-12-29T17:22:25.752967, step: 1122, loss: 0.22984695434570312, acc: 0.9297, auc: 0.973, precision: 0.9342, recall: 0.9262\n",
      "2018-12-29T17:22:25.860597, step: 1123, loss: 0.3312849700450897, acc: 0.8594, auc: 0.9434, precision: 0.8651, recall: 0.8594\n",
      "2018-12-29T17:22:25.957700, step: 1124, loss: 0.2535504102706909, acc: 0.875, auc: 0.9627, precision: 0.8784, recall: 0.8784\n",
      "2018-12-29T17:22:26.051614, step: 1125, loss: 0.18394285440444946, acc: 0.9141, auc: 0.9865, precision: 0.9141, recall: 0.9166\n",
      "2018-12-29T17:22:26.159462, step: 1126, loss: 0.2795225977897644, acc: 0.8359, auc: 0.966, precision: 0.8718, recall: 0.8286\n",
      "2018-12-29T17:22:26.258642, step: 1127, loss: 0.4861116409301758, acc: 0.7734, auc: 0.9292, precision: 0.8011, recall: 0.7632\n",
      "2018-12-29T17:22:26.364722, step: 1128, loss: 0.2711784839630127, acc: 0.8906, auc: 0.9643, precision: 0.9131, recall: 0.8707\n",
      "2018-12-29T17:22:26.469355, step: 1129, loss: 0.22777880728244781, acc: 0.875, auc: 0.9751, precision: 0.8831, recall: 0.868\n",
      "2018-12-29T17:22:26.574172, step: 1130, loss: 0.18957793712615967, acc: 0.9375, auc: 0.9841, precision: 0.9388, recall: 0.938\n",
      "2018-12-29T17:22:26.679365, step: 1131, loss: 0.22934862971305847, acc: 0.9062, auc: 0.9676, precision: 0.9051, recall: 0.9051\n",
      "2018-12-29T17:22:26.772978, step: 1132, loss: 0.23825660347938538, acc: 0.8672, auc: 0.9695, precision: 0.883, recall: 0.8672\n",
      "2018-12-29T17:22:26.867526, step: 1133, loss: 0.2388727366924286, acc: 0.9297, auc: 0.9883, precision: 0.9308, recall: 0.9375\n",
      "2018-12-29T17:22:26.963329, step: 1134, loss: 0.6264066100120544, acc: 0.6953, auc: 0.967, precision: 0.799, recall: 0.7214\n",
      "2018-12-29T17:22:27.049753, step: 1135, loss: 0.6309283971786499, acc: 0.7188, auc: 0.9284, precision: 0.8092, recall: 0.701\n",
      "2018-12-29T17:22:27.140549, step: 1136, loss: 0.31600168347358704, acc: 0.8984, auc: 0.971, precision: 0.8984, recall: 0.9009\n",
      "2018-12-29T17:22:27.241833, step: 1137, loss: 0.29003143310546875, acc: 0.9141, auc: 0.957, precision: 0.917, recall: 0.9134\n",
      "2018-12-29T17:22:27.345431, step: 1138, loss: 0.2931923568248749, acc: 0.8906, auc: 0.953, precision: 0.8912, recall: 0.8897\n",
      "2018-12-29T17:22:27.440544, step: 1139, loss: 0.227927103638649, acc: 0.9141, auc: 0.9841, precision: 0.9179, recall: 0.9236\n",
      "2018-12-29T17:22:27.537961, step: 1140, loss: 0.1764456182718277, acc: 0.9609, auc: 0.9888, precision: 0.962, recall: 0.9609\n",
      "2018-12-29T17:22:27.625793, step: 1141, loss: 0.2683553695678711, acc: 0.875, auc: 0.9636, precision: 0.8923, recall: 0.8669\n",
      "2018-12-29T17:22:27.724261, step: 1142, loss: 0.2494177520275116, acc: 0.8984, auc: 0.9596, precision: 0.8998, recall: 0.8966\n",
      "2018-12-29T17:22:27.818208, step: 1143, loss: 0.18624141812324524, acc: 0.9219, auc: 0.9846, precision: 0.9219, recall: 0.9286\n",
      "2018-12-29T17:22:27.908406, step: 1144, loss: 0.2951507270336151, acc: 0.8984, auc: 0.9794, precision: 0.9138, recall: 0.8926\n",
      "2018-12-29T17:22:28.008881, step: 1145, loss: 0.3680739998817444, acc: 0.7891, auc: 0.9695, precision: 0.85, recall: 0.7923\n",
      "2018-12-29T17:22:28.103574, step: 1146, loss: 0.23341608047485352, acc: 0.9062, auc: 0.9788, precision: 0.9068, recall: 0.906\n",
      "2018-12-29T17:22:28.221485, step: 1147, loss: 0.21676474809646606, acc: 0.8828, auc: 0.9801, precision: 0.8909, recall: 0.8877\n",
      "2018-12-29T17:22:28.317822, step: 1148, loss: 0.15968911349773407, acc: 0.9453, auc: 0.9904, precision: 0.9446, recall: 0.9456\n",
      "2018-12-29T17:22:28.425720, step: 1149, loss: 0.30408239364624023, acc: 0.8516, auc: 0.9742, precision: 0.8733, recall: 0.8681\n",
      "2018-12-29T17:22:28.519086, step: 1150, loss: 0.6392738223075867, acc: 0.7734, auc: 0.9625, precision: 0.8269, recall: 0.7858\n",
      "2018-12-29T17:22:28.613544, step: 1151, loss: 0.3387053608894348, acc: 0.7812, auc: 0.9673, precision: 0.8368, recall: 0.7812\n",
      "2018-12-29T17:22:28.727369, step: 1152, loss: 0.2884582281112671, acc: 0.8516, auc: 0.9599, precision: 0.8641, recall: 0.856\n",
      "2018-12-29T17:22:28.822402, step: 1153, loss: 0.19487819075584412, acc: 0.9375, auc: 0.9895, precision: 0.9374, recall: 0.9374\n",
      "2018-12-29T17:22:28.925622, step: 1154, loss: 0.2384418398141861, acc: 0.9219, auc: 0.9653, precision: 0.927, recall: 0.9195\n",
      "2018-12-29T17:22:29.021123, step: 1155, loss: 0.263950377702713, acc: 0.8828, auc: 0.9574, precision: 0.8886, recall: 0.88\n",
      "2018-12-29T17:22:29.118176, step: 1156, loss: 0.2074836939573288, acc: 0.9062, auc: 0.9766, precision: 0.9049, recall: 0.9069\n",
      "2018-12-29T17:22:29.229397, step: 1157, loss: 0.2143063098192215, acc: 0.8828, auc: 0.979, precision: 0.8905, recall: 0.8828\n",
      "2018-12-29T17:22:29.341652, step: 1158, loss: 0.1582695096731186, acc: 0.9531, auc: 0.9973, precision: 0.9531, recall: 0.9549\n",
      "2018-12-29T17:22:29.436195, step: 1159, loss: 0.23587512969970703, acc: 0.8672, auc: 0.9827, precision: 0.8885, recall: 0.8672\n",
      "2018-12-29T17:22:29.527420, step: 1160, loss: 0.3367898464202881, acc: 0.8906, auc: 0.9721, precision: 0.8948, recall: 0.9019\n",
      "2018-12-29T17:22:29.616928, step: 1161, loss: 0.31338682770729065, acc: 0.8438, auc: 0.9681, precision: 0.875, recall: 0.8529\n",
      "2018-12-29T17:22:29.716526, step: 1162, loss: 0.31384941935539246, acc: 0.875, auc: 0.9474, precision: 0.8755, recall: 0.8762\n",
      "2018-12-29T17:22:29.823617, step: 1163, loss: 0.2582676112651825, acc: 0.8906, auc: 0.9582, precision: 0.8901, recall: 0.8924\n",
      "2018-12-29T17:22:29.927657, step: 1164, loss: 0.30154287815093994, acc: 0.8906, auc: 0.9432, precision: 0.8867, recall: 0.8929\n",
      "2018-12-29T17:22:30.039119, step: 1165, loss: 0.29303741455078125, acc: 0.8516, auc: 0.9498, precision: 0.8521, recall: 0.8534\n",
      "2018-12-29T17:22:30.133153, step: 1166, loss: 0.2113606333732605, acc: 0.9141, auc: 0.9733, precision: 0.9177, recall: 0.912\n",
      "2018-12-29T17:22:30.240488, step: 1167, loss: 0.23077347874641418, acc: 0.8828, auc: 0.9673, precision: 0.8828, recall: 0.8852\n",
      "2018-12-29T17:22:30.350660, step: 1168, loss: 0.2645284831523895, acc: 0.8984, auc: 0.9588, precision: 0.899, recall: 0.9005\n",
      "2018-12-29T17:22:30.455840, step: 1169, loss: 0.2565697133541107, acc: 0.8828, auc: 0.9619, precision: 0.8828, recall: 0.8829\n",
      "2018-12-29T17:22:30.548919, step: 1170, loss: 0.4739067554473877, acc: 0.7266, auc: 0.9553, precision: 0.8059, recall: 0.7344\n",
      "2018-12-29T17:22:30.639235, step: 1171, loss: 0.652796745300293, acc: 0.7734, auc: 0.9787, precision: 0.8474, recall: 0.7661\n",
      "2018-12-29T17:22:30.728621, step: 1172, loss: 0.41188353300094604, acc: 0.7812, auc: 0.9277, precision: 0.8283, recall: 0.7783\n",
      "2018-12-29T17:22:30.831491, step: 1173, loss: 0.3201165497303009, acc: 0.8438, auc: 0.9466, precision: 0.8538, recall: 0.8382\n",
      "2018-12-29T17:22:30.928676, step: 1174, loss: 0.24379290640354156, acc: 0.9219, auc: 0.9743, precision: 0.9202, recall: 0.9244\n",
      "2018-12-29T17:22:31.039747, step: 1175, loss: 0.23843148350715637, acc: 0.9375, auc: 0.9719, precision: 0.9382, recall: 0.937\n",
      "2018-12-29T17:22:31.131255, step: 1176, loss: 0.28204551339149475, acc: 0.8516, auc: 0.9474, precision: 0.8569, recall: 0.8545\n",
      "2018-12-29T17:22:31.223822, step: 1177, loss: 0.2277514636516571, acc: 0.8984, auc: 0.9738, precision: 0.8984, recall: 0.8993\n",
      "2018-12-29T17:22:31.315404, step: 1178, loss: 0.2209300398826599, acc: 0.9062, auc: 0.9777, precision: 0.9047, recall: 0.9107\n",
      "2018-12-29T17:22:31.418102, step: 1179, loss: 0.35268232226371765, acc: 0.8125, auc: 0.9593, precision: 0.836, recall: 0.8294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-29T17:22:31.511080, step: 1180, loss: 0.5617605447769165, acc: 0.7891, auc: 0.9543, precision: 0.8411, recall: 0.7891\n",
      "2018-12-29T17:22:31.617977, step: 1181, loss: 0.427609920501709, acc: 0.7578, auc: 0.9379, precision: 0.8062, recall: 0.7756\n",
      "2018-12-29T17:22:31.707753, step: 1182, loss: 0.3244341015815735, acc: 0.9062, auc: 0.9451, precision: 0.9159, recall: 0.8995\n",
      "2018-12-29T17:22:31.803096, step: 1183, loss: 0.27423757314682007, acc: 0.9141, auc: 0.9722, precision: 0.9226, recall: 0.9058\n",
      "2018-12-29T17:22:31.899904, step: 1184, loss: 0.256469190120697, acc: 0.8672, auc: 0.9667, precision: 0.8695, recall: 0.8701\n",
      "2018-12-29T17:22:31.994630, step: 1185, loss: 0.2539270520210266, acc: 0.8672, auc: 0.969, precision: 0.8811, recall: 0.8702\n",
      "2018-12-29T17:22:32.090619, step: 1186, loss: 0.2861782908439636, acc: 0.875, auc: 0.9555, precision: 0.875, recall: 0.8754\n",
      "2018-12-29T17:22:32.193216, step: 1187, loss: 0.20202475786209106, acc: 0.9141, auc: 0.9756, precision: 0.9157, recall: 0.9152\n",
      "2018-12-29T17:22:32.302074, step: 1188, loss: 0.18819834291934967, acc: 0.9609, auc: 0.9853, precision: 0.9606, recall: 0.9623\n",
      "2018-12-29T17:22:32.390836, step: 1189, loss: 0.2642150819301605, acc: 0.875, auc: 0.9569, precision: 0.8755, recall: 0.874\n",
      "2018-12-29T17:22:32.497208, step: 1190, loss: 0.31713518500328064, acc: 0.8516, auc: 0.9463, precision: 0.8581, recall: 0.8526\n",
      "2018-12-29T17:22:32.599703, step: 1191, loss: 0.27030810713768005, acc: 0.8672, auc: 0.9602, precision: 0.868, recall: 0.8672\n",
      "2018-12-29T17:22:32.693135, step: 1192, loss: 0.18407556414604187, acc: 0.8984, auc: 0.98, precision: 0.916, recall: 0.8877\n",
      "2018-12-29T17:22:32.794646, step: 1193, loss: 0.25530534982681274, acc: 0.9062, auc: 0.9677, precision: 0.9049, recall: 0.9069\n",
      "2018-12-29T17:22:32.890984, step: 1194, loss: 0.5664388537406921, acc: 0.7109, auc: 0.9504, precision: 0.7761, recall: 0.7421\n",
      "2018-12-29T17:22:32.980608, step: 1195, loss: 0.8559309840202332, acc: 0.6875, auc: 0.9152, precision: 0.7844, recall: 0.7049\n",
      "2018-12-29T17:22:33.069831, step: 1196, loss: 0.3781498074531555, acc: 0.8359, auc: 0.935, precision: 0.8438, recall: 0.8262\n",
      "2018-12-29T17:22:33.159521, step: 1197, loss: 0.3724232316017151, acc: 0.7734, auc: 0.93, precision: 0.7979, recall: 0.7799\n",
      "2018-12-29T17:22:33.252051, step: 1198, loss: 0.32325756549835205, acc: 0.875, auc: 0.942, precision: 0.8757, recall: 0.8779\n",
      "2018-12-29T17:22:33.351263, step: 1199, loss: 0.3133755326271057, acc: 0.8672, auc: 0.9424, precision: 0.868, recall: 0.8712\n",
      "2018-12-29T17:22:33.454808, step: 1200, loss: 0.35006651282310486, acc: 0.8438, auc: 0.9234, precision: 0.8453, recall: 0.8419\n",
      "\n",
      "Evaluation:\n",
      "2018-12-29T17:22:37.202089, step: 1200, loss: 0.36785741188587284, acc: 0.838148717948718, auc: 0.9201384615384615, precision: 0.8376769230769232, recall: 0.8393769230769229\n",
      "2018-12-29T17:22:37.289572, step: 1201, loss: 0.23781192302703857, acc: 0.9297, auc: 0.9717, precision: 0.9299, recall: 0.9295\n",
      "2018-12-29T17:22:37.381169, step: 1202, loss: 0.2995128929615021, acc: 0.9375, auc: 0.9609, precision: 0.9371, recall: 0.9371\n",
      "2018-12-29T17:22:37.479663, step: 1203, loss: 0.31152933835983276, acc: 0.8438, auc: 0.9437, precision: 0.8584, recall: 0.8485\n",
      "2018-12-29T17:22:37.569408, step: 1204, loss: 0.24713972210884094, acc: 0.9297, auc: 0.9648, precision: 0.9392, recall: 0.9286\n",
      "2018-12-29T17:22:37.669548, step: 1205, loss: 0.30643439292907715, acc: 0.8828, auc: 0.9603, precision: 0.8827, recall: 0.8819\n",
      "2018-12-29T17:22:37.760384, step: 1206, loss: 0.19934725761413574, acc: 0.8359, auc: 0.9839, precision: 0.8582, recall: 0.8418\n",
      "2018-12-29T17:22:37.853298, step: 1207, loss: 0.4071207046508789, acc: 0.8516, auc: 0.9574, precision: 0.8516, recall: 0.872\n",
      "2018-12-29T17:22:37.940551, step: 1208, loss: 0.40256792306900024, acc: 0.7812, auc: 0.9545, precision: 0.8478, recall: 0.7386\n",
      "2018-12-29T17:22:38.028986, step: 1209, loss: 0.32078230381011963, acc: 0.875, auc: 0.9639, precision: 0.8878, recall: 0.8764\n",
      "2018-12-29T17:22:38.118702, step: 1210, loss: 0.22074154019355774, acc: 0.9219, auc: 0.9752, precision: 0.9287, recall: 0.9147\n",
      "2018-12-29T17:22:38.217057, step: 1211, loss: 0.24559946358203888, acc: 0.875, auc: 0.9617, precision: 0.8844, recall: 0.875\n",
      "2018-12-29T17:22:38.308456, step: 1212, loss: 0.16879987716674805, acc: 0.9453, auc: 0.9871, precision: 0.9485, recall: 0.9447\n",
      "2018-12-29T17:22:38.414960, step: 1213, loss: 0.18628960847854614, acc: 0.9453, auc: 0.9756, precision: 0.9453, recall: 0.9454\n",
      "2018-12-29T17:22:38.527838, step: 1214, loss: 0.19735287129878998, acc: 0.9141, auc: 0.9843, precision: 0.92, recall: 0.9172\n",
      "2018-12-29T17:22:38.635304, step: 1215, loss: 0.24227941036224365, acc: 0.8906, auc: 0.9848, precision: 0.8988, recall: 0.893\n",
      "2018-12-29T17:22:38.727948, step: 1216, loss: 0.4397812485694885, acc: 0.7734, auc: 0.9567, precision: 0.8592, recall: 0.7315\n",
      "2018-12-29T17:22:38.815860, step: 1217, loss: 0.38047558069229126, acc: 0.8438, auc: 0.9692, precision: 0.8667, recall: 0.8438\n",
      "2018-12-29T17:22:38.918770, step: 1218, loss: 0.2569373846054077, acc: 0.875, auc: 0.9645, precision: 0.8826, recall: 0.8706\n",
      "2018-12-29T17:22:39.014351, step: 1219, loss: 0.2172846794128418, acc: 0.9219, auc: 0.9736, precision: 0.9228, recall: 0.9228\n",
      "2018-12-29T17:22:39.102351, step: 1220, loss: 0.20307637751102448, acc: 0.9688, auc: 0.9775, precision: 0.9714, recall: 0.9677\n",
      "2018-12-29T17:22:39.196174, step: 1221, loss: 0.26359719038009644, acc: 0.8984, auc: 0.9571, precision: 0.8978, recall: 0.9012\n",
      "2018-12-29T17:22:39.291566, step: 1222, loss: 0.13643205165863037, acc: 0.9609, auc: 0.9927, precision: 0.9611, recall: 0.9609\n",
      "2018-12-29T17:22:39.384394, step: 1223, loss: 0.28630194067955017, acc: 0.8906, auc: 0.9509, precision: 0.9002, recall: 0.8795\n",
      "2018-12-29T17:22:39.478668, step: 1224, loss: 0.14720861613750458, acc: 0.9531, auc: 0.9866, precision: 0.9541, recall: 0.9541\n",
      "2018-12-29T17:22:39.577033, step: 1225, loss: 0.2180563509464264, acc: 0.9375, auc: 0.9725, precision: 0.937, recall: 0.9382\n",
      "2018-12-29T17:22:39.668251, step: 1226, loss: 0.22858326137065887, acc: 0.9141, auc: 0.9675, precision: 0.9149, recall: 0.9185\n",
      "2018-12-29T17:22:39.763513, step: 1227, loss: 0.22383150458335876, acc: 0.9141, auc: 0.9779, precision: 0.9182, recall: 0.9105\n",
      "2018-12-29T17:22:39.854182, step: 1228, loss: 0.42478159070014954, acc: 0.7109, auc: 0.9765, precision: 0.8221, recall: 0.6967\n",
      "2018-12-29T17:22:39.961227, step: 1229, loss: 0.44368472695350647, acc: 0.8047, auc: 0.9628, precision: 0.8641, recall: 0.7951\n",
      "2018-12-29T17:22:40.067562, step: 1230, loss: 0.36890116333961487, acc: 0.8438, auc: 0.9218, precision: 0.8498, recall: 0.848\n",
      "2018-12-29T17:22:40.177369, step: 1231, loss: 0.1834903061389923, acc: 0.9609, auc: 0.9917, precision: 0.9572, recall: 0.9606\n",
      "2018-12-29T17:22:40.265641, step: 1232, loss: 0.21568503975868225, acc: 0.9062, auc: 0.9735, precision: 0.9221, recall: 0.8948\n",
      "2018-12-29T17:22:40.371380, step: 1233, loss: 0.238503098487854, acc: 0.9219, auc: 0.969, precision: 0.9224, recall: 0.9187\n",
      "2018-12-29T17:22:40.480994, step: 1234, loss: 0.21019181609153748, acc: 0.8906, auc: 0.9733, precision: 0.8959, recall: 0.8817\n",
      "2018-12-29T17:22:40.587501, step: 1235, loss: 0.2383732795715332, acc: 0.8594, auc: 0.9703, precision: 0.8473, recall: 0.8763\n",
      "2018-12-29T17:22:40.688356, step: 1236, loss: 0.30248069763183594, acc: 0.8828, auc: 0.9813, precision: 0.9085, recall: 0.8588\n",
      "2018-12-29T17:22:40.794479, step: 1237, loss: 0.4588928520679474, acc: 0.75, auc: 0.9422, precision: 0.8261, recall: 0.7647\n",
      "2018-12-29T17:22:40.896418, step: 1238, loss: 0.3909916877746582, acc: 0.8516, auc: 0.9386, precision: 0.8584, recall: 0.8574\n",
      "2018-12-29T17:22:40.997267, step: 1239, loss: 0.24463966488838196, acc: 0.8516, auc: 0.9784, precision: 0.8666, recall: 0.8599\n",
      "2018-12-29T17:22:41.097626, step: 1240, loss: 0.22547294199466705, acc: 0.9219, auc: 0.9734, precision: 0.9281, recall: 0.9167\n",
      "2018-12-29T17:22:41.190781, step: 1241, loss: 0.22957776486873627, acc: 0.9297, auc: 0.967, precision: 0.9262, recall: 0.9317\n",
      "2018-12-29T17:22:41.291332, step: 1242, loss: 0.2693808376789093, acc: 0.8984, auc: 0.9547, precision: 0.898, recall: 0.8995\n",
      "2018-12-29T17:22:41.392174, step: 1243, loss: 0.2659487724304199, acc: 0.9219, auc: 0.9592, precision: 0.9214, recall: 0.9214\n",
      "2018-12-29T17:22:41.486426, step: 1244, loss: 0.32076629996299744, acc: 0.8438, auc: 0.956, precision: 0.8709, recall: 0.833\n",
      "2018-12-29T17:22:41.574734, step: 1245, loss: 0.33398714661598206, acc: 0.8516, auc: 0.9699, precision: 0.8601, recall: 0.8553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-29T17:22:41.677110, step: 1246, loss: 0.40080562233924866, acc: 0.7969, auc: 0.9448, precision: 0.8298, recall: 0.7944\n",
      "2018-12-29T17:22:41.780573, step: 1247, loss: 0.2741144895553589, acc: 0.9062, auc: 0.9704, precision: 0.9067, recall: 0.9075\n",
      "2018-12-29T17:22:41.870390, step: 1248, loss: 0.21627333760261536, acc: 0.8828, auc: 0.9773, precision: 0.896, recall: 0.88\n",
      "start training model\n",
      "2018-12-29T17:22:42.125732, step: 1249, loss: 0.2784898281097412, acc: 0.8906, auc: 0.9515, precision: 0.8911, recall: 0.8892\n",
      "2018-12-29T17:22:42.217230, step: 1250, loss: 0.1974686235189438, acc: 0.9062, auc: 0.9765, precision: 0.9107, recall: 0.9047\n",
      "2018-12-29T17:22:42.310156, step: 1251, loss: 0.1769554615020752, acc: 0.9219, auc: 0.9834, precision: 0.9223, recall: 0.9232\n",
      "2018-12-29T17:22:42.410429, step: 1252, loss: 0.20097583532333374, acc: 0.9219, auc: 0.9771, precision: 0.9195, recall: 0.9195\n",
      "2018-12-29T17:22:42.497746, step: 1253, loss: 0.12877146899700165, acc: 0.9531, auc: 0.9939, precision: 0.9526, recall: 0.9539\n",
      "2018-12-29T17:22:42.592454, step: 1254, loss: 0.1108708381652832, acc: 0.9375, auc: 0.9965, precision: 0.9447, recall: 0.9316\n",
      "2018-12-29T17:22:42.686200, step: 1255, loss: 0.1428530514240265, acc: 0.9219, auc: 0.9897, precision: 0.9232, recall: 0.9223\n",
      "2018-12-29T17:22:42.793757, step: 1256, loss: 0.2168230414390564, acc: 0.8828, auc: 0.9829, precision: 0.9021, recall: 0.8778\n",
      "2018-12-29T17:22:42.881770, step: 1257, loss: 0.4480300545692444, acc: 0.8203, auc: 0.9707, precision: 0.8502, recall: 0.8226\n",
      "2018-12-29T17:22:42.970232, step: 1258, loss: 0.37951940298080444, acc: 0.7109, auc: 0.9713, precision: 0.7954, recall: 0.727\n",
      "2018-12-29T17:22:43.072986, step: 1259, loss: 0.32192495465278625, acc: 0.8516, auc: 0.9371, precision: 0.8522, recall: 0.8488\n",
      "2018-12-29T17:22:43.174784, step: 1260, loss: 0.20647162199020386, acc: 0.9297, auc: 0.9806, precision: 0.9279, recall: 0.925\n",
      "2018-12-29T17:22:43.270613, step: 1261, loss: 0.13197682797908783, acc: 0.9375, auc: 0.9972, precision: 0.9344, recall: 0.9467\n",
      "2018-12-29T17:22:43.376362, step: 1262, loss: 0.2836604714393616, acc: 0.8984, auc: 0.9746, precision: 0.9042, recall: 0.8966\n",
      "2018-12-29T17:22:43.472071, step: 1263, loss: 0.39899933338165283, acc: 0.8203, auc: 0.9748, precision: 0.8562, recall: 0.838\n",
      "2018-12-29T17:22:43.566277, step: 1264, loss: 0.5978255867958069, acc: 0.7188, auc: 0.9332, precision: 0.7862, recall: 0.7367\n",
      "2018-12-29T17:22:43.659616, step: 1265, loss: 0.26540082693099976, acc: 0.8516, auc: 0.9745, precision: 0.8666, recall: 0.8599\n",
      "2018-12-29T17:22:43.754732, step: 1266, loss: 0.23634271323680878, acc: 0.8984, auc: 0.9707, precision: 0.8986, recall: 0.8983\n",
      "2018-12-29T17:22:43.867307, step: 1267, loss: 0.20801511406898499, acc: 0.9219, auc: 0.9776, precision: 0.9249, recall: 0.9182\n",
      "2018-12-29T17:22:43.959472, step: 1268, loss: 0.1455422043800354, acc: 0.9297, auc: 0.9905, precision: 0.9319, recall: 0.9303\n",
      "2018-12-29T17:22:44.071378, step: 1269, loss: 0.1553497463464737, acc: 0.9219, auc: 0.9897, precision: 0.9218, recall: 0.9218\n",
      "2018-12-29T17:22:44.181959, step: 1270, loss: 0.13549259305000305, acc: 0.9688, auc: 0.989, precision: 0.9687, recall: 0.9687\n",
      "2018-12-29T17:22:44.286374, step: 1271, loss: 0.24619871377944946, acc: 0.9062, auc: 0.9761, precision: 0.9231, recall: 0.9032\n",
      "2018-12-29T17:22:44.379490, step: 1272, loss: 0.3725200891494751, acc: 0.875, auc: 0.9821, precision: 0.8968, recall: 0.8696\n",
      "2018-12-29T17:22:44.480855, step: 1273, loss: 0.3160986006259918, acc: 0.7969, auc: 0.9919, precision: 0.8617, recall: 0.7833\n",
      "2018-12-29T17:22:44.573836, step: 1274, loss: 0.3249121904373169, acc: 0.8594, auc: 0.9625, precision: 0.8594, recall: 0.8651\n",
      "2018-12-29T17:22:44.668615, step: 1275, loss: 0.1500231921672821, acc: 0.9453, auc: 0.989, precision: 0.948, recall: 0.9453\n",
      "2018-12-29T17:22:44.777691, step: 1276, loss: 0.1896832287311554, acc: 0.9297, auc: 0.9787, precision: 0.9274, recall: 0.9315\n",
      "2018-12-29T17:22:44.892110, step: 1277, loss: 0.18124496936798096, acc: 0.9141, auc: 0.9816, precision: 0.9152, recall: 0.9157\n",
      "2018-12-29T17:22:44.996368, step: 1278, loss: 0.12858742475509644, acc: 0.9453, auc: 0.9939, precision: 0.9472, recall: 0.9436\n",
      "2018-12-29T17:22:45.090350, step: 1279, loss: 0.24011963605880737, acc: 0.9062, auc: 0.969, precision: 0.9119, recall: 0.901\n",
      "2018-12-29T17:22:45.205614, step: 1280, loss: 0.20380450785160065, acc: 0.9062, auc: 0.9748, precision: 0.9021, recall: 0.9062\n",
      "2018-12-29T17:22:45.299940, step: 1281, loss: 0.10845175385475159, acc: 0.9609, auc: 0.9954, precision: 0.9625, recall: 0.9602\n",
      "2018-12-29T17:22:45.398881, step: 1282, loss: 0.17705225944519043, acc: 0.9531, auc: 0.9804, precision: 0.9526, recall: 0.9539\n",
      "2018-12-29T17:22:45.493626, step: 1283, loss: 0.1480715572834015, acc: 0.9297, auc: 0.9911, precision: 0.9444, recall: 0.9196\n",
      "2018-12-29T17:22:45.590339, step: 1284, loss: 0.4075402319431305, acc: 0.8359, auc: 0.9841, precision: 0.8632, recall: 0.8446\n",
      "2018-12-29T17:22:45.675987, step: 1285, loss: 0.930245041847229, acc: 0.6562, auc: 0.9479, precision: 0.812, recall: 0.6\n",
      "2018-12-29T17:22:45.769719, step: 1286, loss: 0.5657415986061096, acc: 0.7266, auc: 0.8532, precision: 0.7256, recall: 0.7292\n",
      "2018-12-29T17:22:45.858567, step: 1287, loss: 0.31030726432800293, acc: 0.9297, auc: 0.9817, precision: 0.9323, recall: 0.9297\n",
      "2018-12-29T17:22:45.962055, step: 1288, loss: 0.28025537729263306, acc: 0.9375, auc: 0.9721, precision: 0.9374, recall: 0.9374\n",
      "2018-12-29T17:22:46.049763, step: 1289, loss: 0.2210332453250885, acc: 0.9062, auc: 0.9785, precision: 0.9133, recall: 0.9053\n",
      "2018-12-29T17:22:46.143845, step: 1290, loss: 0.20961371064186096, acc: 0.875, auc: 0.9772, precision: 0.8813, recall: 0.8794\n",
      "2018-12-29T17:22:46.234711, step: 1291, loss: 0.16772113740444183, acc: 0.9375, auc: 0.9868, precision: 0.9414, recall: 0.9375\n",
      "2018-12-29T17:22:46.339184, step: 1292, loss: 0.2114681899547577, acc: 0.9141, auc: 0.973, precision: 0.9179, recall: 0.9113\n",
      "2018-12-29T17:22:46.429713, step: 1293, loss: 0.153790682554245, acc: 0.9375, auc: 0.991, precision: 0.9396, recall: 0.937\n",
      "2018-12-29T17:22:46.531162, step: 1294, loss: 0.21723157167434692, acc: 0.8984, auc: 0.9719, precision: 0.8985, recall: 0.8984\n",
      "2018-12-29T17:22:46.627167, step: 1295, loss: 0.18401899933815002, acc: 0.9062, auc: 0.9841, precision: 0.9172, recall: 0.905\n",
      "2018-12-29T17:22:46.720174, step: 1296, loss: 0.15818774700164795, acc: 0.9609, auc: 0.9919, precision: 0.9615, recall: 0.9632\n",
      "2018-12-29T17:22:46.814238, step: 1297, loss: 0.16441380977630615, acc: 0.9453, auc: 0.9859, precision: 0.9509, recall: 0.9403\n",
      "2018-12-29T17:22:46.910364, step: 1298, loss: 0.12903732061386108, acc: 0.9453, auc: 0.9895, precision: 0.948, recall: 0.9453\n",
      "2018-12-29T17:22:47.022161, step: 1299, loss: 0.16205231845378876, acc: 0.9531, auc: 0.9805, precision: 0.9538, recall: 0.9529\n",
      "2018-12-29T17:22:47.116299, step: 1300, loss: 0.21147865056991577, acc: 0.875, auc: 0.9761, precision: 0.8837, recall: 0.8762\n",
      "\n",
      "Evaluation:\n",
      "2018-12-29T17:22:50.981158, step: 1300, loss: 0.5127160258782215, acc: 0.8349487179487182, auc: 0.9196948717948717, precision: 0.8402205128205128, recall: 0.834294871794872\n",
      "2018-12-29T17:22:51.070199, step: 1301, loss: 0.27081772685050964, acc: 0.9141, auc: 0.978, precision: 0.9185, recall: 0.9149\n",
      "2018-12-29T17:22:51.162961, step: 1302, loss: 0.4727399945259094, acc: 0.7344, auc: 0.9882, precision: 0.8152, recall: 0.7571\n",
      "2018-12-29T17:22:51.252168, step: 1303, loss: 0.3781123459339142, acc: 0.8672, auc: 0.9665, precision: 0.8938, recall: 0.8692\n",
      "2018-12-29T17:22:51.340227, step: 1304, loss: 0.19324249029159546, acc: 0.9297, auc: 0.9946, precision: 0.9367, recall: 0.927\n",
      "2018-12-29T17:22:51.443362, step: 1305, loss: 0.16912530362606049, acc: 0.9375, auc: 0.9895, precision: 0.9408, recall: 0.9382\n",
      "2018-12-29T17:22:51.537330, step: 1306, loss: 0.2216910719871521, acc: 0.9375, auc: 0.9748, precision: 0.9359, recall: 0.9402\n",
      "2018-12-29T17:22:51.642901, step: 1307, loss: 0.2389979362487793, acc: 0.9062, auc: 0.9716, precision: 0.9348, recall: 0.875\n",
      "2018-12-29T17:22:51.738966, step: 1308, loss: 0.18075305223464966, acc: 0.9531, auc: 0.9812, precision: 0.9531, recall: 0.9531\n",
      "2018-12-29T17:22:51.844283, step: 1309, loss: 0.17916764318943024, acc: 0.9141, auc: 0.9858, precision: 0.9185, recall: 0.9149\n",
      "2018-12-29T17:22:51.950076, step: 1310, loss: 0.2636358439922333, acc: 0.8984, auc: 0.9829, precision: 0.9087, recall: 0.901\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-29T17:22:52.053770, step: 1311, loss: 0.35674506425857544, acc: 0.7969, auc: 0.9976, precision: 0.8539, recall: 0.8\n",
      "2018-12-29T17:22:52.146945, step: 1312, loss: 0.2676755487918854, acc: 0.8984, auc: 0.9634, precision: 0.8998, recall: 0.896\n",
      "2018-12-29T17:22:52.234851, step: 1313, loss: 0.20909716188907623, acc: 0.9297, auc: 0.9748, precision: 0.9317, recall: 0.9249\n",
      "2018-12-29T17:22:52.341577, step: 1314, loss: 0.20614613592624664, acc: 0.9062, auc: 0.9741, precision: 0.9081, recall: 0.9057\n",
      "2018-12-29T17:22:52.436052, step: 1315, loss: 0.18375937640666962, acc: 0.9453, auc: 0.9819, precision: 0.9431, recall: 0.9474\n",
      "2018-12-29T17:22:52.528485, step: 1316, loss: 0.1879454255104065, acc: 0.9141, auc: 0.9856, precision: 0.9286, recall: 0.9113\n",
      "2018-12-29T17:22:52.622997, step: 1317, loss: 0.13838538527488708, acc: 0.9453, auc: 0.9914, precision: 0.9446, recall: 0.9456\n",
      "2018-12-29T17:22:52.718237, step: 1318, loss: 0.23226581513881683, acc: 0.8906, auc: 0.9814, precision: 0.9058, recall: 0.8891\n",
      "2018-12-29T17:22:52.815810, step: 1319, loss: 0.3135887384414673, acc: 0.9141, auc: 0.9824, precision: 0.9276, recall: 0.9127\n",
      "2018-12-29T17:22:52.909943, step: 1320, loss: 0.43298259377479553, acc: 0.7266, auc: 0.9774, precision: 0.8077, recall: 0.7569\n",
      "2018-12-29T17:22:53.021735, step: 1321, loss: 0.2357197403907776, acc: 0.9453, auc: 0.9856, precision: 0.9454, recall: 0.9453\n",
      "2018-12-29T17:22:53.132185, step: 1322, loss: 0.16611137986183167, acc: 0.9375, auc: 0.9853, precision: 0.9334, recall: 0.9383\n",
      "2018-12-29T17:22:53.229049, step: 1323, loss: 0.15342575311660767, acc: 0.9375, auc: 0.9881, precision: 0.9365, recall: 0.9365\n",
      "2018-12-29T17:22:53.335758, step: 1324, loss: 0.26957398653030396, acc: 0.8672, auc: 0.9712, precision: 0.8896, recall: 0.8653\n",
      "2018-12-29T17:22:53.430448, step: 1325, loss: 0.2401711791753769, acc: 0.9141, auc: 0.9816, precision: 0.9216, recall: 0.9092\n",
      "2018-12-29T17:22:53.532252, step: 1326, loss: 0.17461277544498444, acc: 0.9219, auc: 0.9878, precision: 0.9306, recall: 0.9188\n",
      "2018-12-29T17:22:53.642171, step: 1327, loss: 0.1872413158416748, acc: 0.8984, auc: 0.9794, precision: 0.905, recall: 0.8946\n",
      "2018-12-29T17:22:53.750085, step: 1328, loss: 0.17780356109142303, acc: 0.9141, auc: 0.9844, precision: 0.9144, recall: 0.9147\n",
      "2018-12-29T17:22:53.847042, step: 1329, loss: 0.18969956040382385, acc: 0.9141, auc: 0.987, precision: 0.9214, recall: 0.9203\n",
      "2018-12-29T17:22:53.955554, step: 1330, loss: 0.418631374835968, acc: 0.875, auc: 0.9698, precision: 0.8985, recall: 0.8656\n",
      "2018-12-29T17:22:54.050098, step: 1331, loss: 0.4801822304725647, acc: 0.7188, auc: 0.9722, precision: 0.8364, recall: 0.6667\n",
      "2018-12-29T17:22:54.155480, step: 1332, loss: 0.3210213780403137, acc: 0.8984, auc: 0.9817, precision: 0.905, recall: 0.9005\n",
      "2018-12-29T17:22:54.245249, step: 1333, loss: 0.18839994072914124, acc: 0.8984, auc: 0.9906, precision: 0.8957, recall: 0.9123\n",
      "2018-12-29T17:22:54.348007, step: 1334, loss: 0.1573743224143982, acc: 0.9453, auc: 0.99, precision: 0.9435, recall: 0.9453\n",
      "2018-12-29T17:22:54.438062, step: 1335, loss: 0.163214311003685, acc: 0.9297, auc: 0.9909, precision: 0.9338, recall: 0.9348\n",
      "2018-12-29T17:22:54.540958, step: 1336, loss: 0.18649686872959137, acc: 0.9375, auc: 0.9808, precision: 0.9385, recall: 0.9359\n",
      "2018-12-29T17:22:54.639766, step: 1337, loss: 0.18043635785579681, acc: 0.9141, auc: 0.9868, precision: 0.9141, recall: 0.915\n",
      "2018-12-29T17:22:54.737523, step: 1338, loss: 0.3238467574119568, acc: 0.875, auc: 0.9604, precision: 0.8961, recall: 0.8806\n",
      "2018-12-29T17:22:54.841091, step: 1339, loss: 0.3886571228504181, acc: 0.8594, auc: 0.9817, precision: 0.8875, recall: 0.8636\n",
      "2018-12-29T17:22:54.952563, step: 1340, loss: 0.504884660243988, acc: 0.7031, auc: 0.9246, precision: 0.7868, recall: 0.7307\n",
      "2018-12-29T17:22:55.059066, step: 1341, loss: 0.3570154905319214, acc: 0.8672, auc: 0.9402, precision: 0.8728, recall: 0.8632\n",
      "2018-12-29T17:22:55.154641, step: 1342, loss: 0.1965949386358261, acc: 0.9297, auc: 0.9816, precision: 0.9367, recall: 0.927\n",
      "2018-12-29T17:22:55.247359, step: 1343, loss: 0.14303885400295258, acc: 0.9453, auc: 0.9958, precision: 0.9521, recall: 0.9435\n",
      "2018-12-29T17:22:55.340753, step: 1344, loss: 0.21994678676128387, acc: 0.8984, auc: 0.9689, precision: 0.9005, recall: 0.905\n",
      "2018-12-29T17:22:55.443936, step: 1345, loss: 0.24431896209716797, acc: 0.8906, auc: 0.9648, precision: 0.8882, recall: 0.8909\n",
      "2018-12-29T17:22:55.550251, step: 1346, loss: 0.16457854211330414, acc: 0.9297, auc: 0.9829, precision: 0.9239, recall: 0.9382\n",
      "2018-12-29T17:22:55.659667, step: 1347, loss: 0.21805500984191895, acc: 0.9297, auc: 0.9741, precision: 0.93, recall: 0.9304\n",
      "2018-12-29T17:22:55.752649, step: 1348, loss: 0.2931112051010132, acc: 0.8281, auc: 0.9655, precision: 0.8492, recall: 0.8399\n",
      "2018-12-29T17:22:55.847135, step: 1349, loss: 0.40574169158935547, acc: 0.8516, auc: 0.9708, precision: 0.872, recall: 0.8611\n",
      "2018-12-29T17:22:55.934633, step: 1350, loss: 0.34835898876190186, acc: 0.7969, auc: 0.9552, precision: 0.8602, recall: 0.7869\n",
      "2018-12-29T17:22:56.022889, step: 1351, loss: 0.22012993693351746, acc: 0.9062, auc: 0.977, precision: 0.9087, recall: 0.9039\n",
      "2018-12-29T17:22:56.131621, step: 1352, loss: 0.1970556080341339, acc: 0.9141, auc: 0.9848, precision: 0.9191, recall: 0.9181\n",
      "2018-12-29T17:22:56.224932, step: 1353, loss: 0.16187907755374908, acc: 0.9688, auc: 0.9919, precision: 0.9686, recall: 0.9686\n",
      "2018-12-29T17:22:56.331425, step: 1354, loss: 0.31923043727874756, acc: 0.8125, auc: 0.97, precision: 0.8531, recall: 0.8151\n",
      "2018-12-29T17:22:56.425791, step: 1355, loss: 0.590472936630249, acc: 0.7969, auc: 0.9531, precision: 0.848, recall: 0.7908\n",
      "2018-12-29T17:22:56.526512, step: 1356, loss: 0.3040769398212433, acc: 0.7969, auc: 0.9797, precision: 0.8571, recall: 0.7937\n",
      "2018-12-29T17:22:56.621379, step: 1357, loss: 0.21753187477588654, acc: 0.8906, auc: 0.9858, precision: 0.897, recall: 0.8951\n",
      "2018-12-29T17:22:56.721813, step: 1358, loss: 0.22845137119293213, acc: 0.9453, auc: 0.9699, precision: 0.947, recall: 0.9441\n",
      "2018-12-29T17:22:56.826079, step: 1359, loss: 0.1845502108335495, acc: 0.9453, auc: 0.9797, precision: 0.947, recall: 0.9441\n",
      "2018-12-29T17:22:56.923371, step: 1360, loss: 0.2000218629837036, acc: 0.9141, auc: 0.981, precision: 0.9141, recall: 0.9191\n",
      "2018-12-29T17:22:57.031846, step: 1361, loss: 0.21947190165519714, acc: 0.9297, auc: 0.9812, precision: 0.9298, recall: 0.9297\n",
      "2018-12-29T17:22:57.128656, step: 1362, loss: 0.16573210060596466, acc: 0.9141, auc: 0.9799, precision: 0.9157, recall: 0.9117\n",
      "2018-12-29T17:22:57.226056, step: 1363, loss: 0.19002267718315125, acc: 0.9375, auc: 0.9792, precision: 0.9375, recall: 0.9375\n",
      "2018-12-29T17:22:57.318425, step: 1364, loss: 0.1406816691160202, acc: 0.9375, auc: 0.9905, precision: 0.9467, recall: 0.9344\n",
      "2018-12-29T17:22:57.413369, step: 1365, loss: 0.23541361093521118, acc: 0.9375, auc: 0.9802, precision: 0.9381, recall: 0.9372\n",
      "2018-12-29T17:22:57.510276, step: 1366, loss: 0.23598548769950867, acc: 0.8672, auc: 0.9781, precision: 0.8874, recall: 0.8564\n",
      "2018-12-29T17:22:57.603971, step: 1367, loss: 0.2734300494194031, acc: 0.8984, auc: 0.9756, precision: 0.9097, recall: 0.8998\n",
      "2018-12-29T17:22:57.694279, step: 1368, loss: 0.25215497612953186, acc: 0.8438, auc: 0.9859, precision: 0.8624, recall: 0.8591\n",
      "2018-12-29T17:22:57.789314, step: 1369, loss: 0.34714430570602417, acc: 0.8906, auc: 0.9669, precision: 0.9008, recall: 0.8961\n",
      "2018-12-29T17:22:57.881010, step: 1370, loss: 0.3577862083911896, acc: 0.8516, auc: 0.9623, precision: 0.8812, recall: 0.8582\n",
      "2018-12-29T17:22:57.983596, step: 1371, loss: 0.19109505414962769, acc: 0.9297, auc: 0.9866, precision: 0.9297, recall: 0.9298\n",
      "2018-12-29T17:22:58.076773, step: 1372, loss: 0.192916601896286, acc: 0.9219, auc: 0.978, precision: 0.9224, recall: 0.9216\n",
      "2018-12-29T17:22:58.172537, step: 1373, loss: 0.23389770090579987, acc: 0.8984, auc: 0.9663, precision: 0.8964, recall: 0.9018\n",
      "2018-12-29T17:22:58.267995, step: 1374, loss: 0.2267199158668518, acc: 0.9297, auc: 0.9712, precision: 0.9295, recall: 0.9299\n",
      "2018-12-29T17:22:58.364764, step: 1375, loss: 0.2025955617427826, acc: 0.8984, auc: 0.9719, precision: 0.9023, recall: 0.8938\n",
      "2018-12-29T17:22:58.459511, step: 1376, loss: 0.13761907815933228, acc: 0.9297, auc: 0.9938, precision: 0.9305, recall: 0.9342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-29T17:22:58.564967, step: 1377, loss: 0.1896437108516693, acc: 0.9375, auc: 0.9772, precision: 0.9383, recall: 0.9366\n",
      "2018-12-29T17:22:58.659867, step: 1378, loss: 0.26517921686172485, acc: 0.9062, auc: 0.9617, precision: 0.9118, recall: 0.9118\n",
      "2018-12-29T17:22:58.756426, step: 1379, loss: 0.3498218059539795, acc: 0.9062, auc: 0.9709, precision: 0.918, recall: 0.9037\n",
      "2018-12-29T17:22:58.848515, step: 1380, loss: 0.48902446031570435, acc: 0.7031, auc: 0.9617, precision: 0.8155, recall: 0.6984\n",
      "2018-12-29T17:22:58.938750, step: 1381, loss: 0.34719228744506836, acc: 0.8984, auc: 0.9657, precision: 0.9042, recall: 0.9015\n",
      "2018-12-29T17:22:59.037344, step: 1382, loss: 0.20789608359336853, acc: 0.9062, auc: 0.9742, precision: 0.9089, recall: 0.9008\n",
      "2018-12-29T17:22:59.126120, step: 1383, loss: 0.1357952058315277, acc: 0.9688, auc: 0.995, precision: 0.968, recall: 0.968\n",
      "2018-12-29T17:22:59.228418, step: 1384, loss: 0.2738726735115051, acc: 0.8594, auc: 0.9597, precision: 0.8684, recall: 0.8594\n",
      "2018-12-29T17:22:59.323645, step: 1385, loss: 0.37498387694358826, acc: 0.8906, auc: 0.9532, precision: 0.8929, recall: 0.8882\n",
      "2018-12-29T17:22:59.414853, step: 1386, loss: 0.3339619040489197, acc: 0.8047, auc: 0.9665, precision: 0.8392, recall: 0.8096\n",
      "2018-12-29T17:22:59.521820, step: 1387, loss: 0.28490105271339417, acc: 0.9062, auc: 0.9703, precision: 0.9155, recall: 0.9008\n",
      "2018-12-29T17:22:59.626895, step: 1388, loss: 0.23436076939105988, acc: 0.8828, auc: 0.9707, precision: 0.8905, recall: 0.8828\n",
      "2018-12-29T17:22:59.731982, step: 1389, loss: 0.2095748484134674, acc: 0.9297, auc: 0.9743, precision: 0.9296, recall: 0.928\n",
      "2018-12-29T17:22:59.828094, step: 1390, loss: 0.22813545167446136, acc: 0.9062, auc: 0.9743, precision: 0.9072, recall: 0.912\n",
      "2018-12-29T17:22:59.924682, step: 1391, loss: 0.17317597568035126, acc: 0.9453, auc: 0.9893, precision: 0.9438, recall: 0.9454\n",
      "2018-12-29T17:23:00.041323, step: 1392, loss: 0.26217833161354065, acc: 0.875, auc: 0.9833, precision: 0.907, recall: 0.8621\n",
      "2018-12-29T17:23:00.141216, step: 1393, loss: 0.43262702226638794, acc: 0.8281, auc: 0.9748, precision: 0.875, recall: 0.8226\n",
      "2018-12-29T17:23:00.236231, step: 1394, loss: 0.37062519788742065, acc: 0.7656, auc: 0.9578, precision: 0.8227, recall: 0.7784\n",
      "2018-12-29T17:23:00.343650, step: 1395, loss: 0.28960973024368286, acc: 0.875, auc: 0.9641, precision: 0.875, recall: 0.8887\n",
      "2018-12-29T17:23:00.439265, step: 1396, loss: 0.2603129744529724, acc: 0.9141, auc: 0.9594, precision: 0.922, recall: 0.9081\n",
      "2018-12-29T17:23:00.531510, step: 1397, loss: 0.1535019874572754, acc: 0.9375, auc: 0.9905, precision: 0.9344, recall: 0.9344\n",
      "2018-12-29T17:23:00.621935, step: 1398, loss: 0.15121962130069733, acc: 0.9141, auc: 0.9882, precision: 0.9191, recall: 0.9181\n",
      "2018-12-29T17:23:00.717191, step: 1399, loss: 0.2654115557670593, acc: 0.8906, auc: 0.9717, precision: 0.8941, recall: 0.8906\n",
      "2018-12-29T17:23:00.822683, step: 1400, loss: 0.6000267267227173, acc: 0.7344, auc: 0.9408, precision: 0.8172, recall: 0.7536\n",
      "\n",
      "Evaluation:\n",
      "2018-12-29T17:23:04.724335, step: 1400, loss: 0.6059042895451571, acc: 0.7616153846153847, auc: 0.8914384615384614, precision: 0.8159307692307692, recall: 0.759746153846154\n",
      "2018-12-29T17:23:04.816009, step: 1401, loss: 0.4291246235370636, acc: 0.8438, auc: 0.9465, precision: 0.8755, recall: 0.8392\n",
      "2018-12-29T17:23:04.909986, step: 1402, loss: 0.26889508962631226, acc: 0.9219, auc: 0.9791, precision: 0.9189, recall: 0.9248\n",
      "2018-12-29T17:23:05.002361, step: 1403, loss: 0.23812232911586761, acc: 0.9141, auc: 0.9794, precision: 0.9141, recall: 0.915\n",
      "2018-12-29T17:23:05.102366, step: 1404, loss: 0.2962825894355774, acc: 0.8516, auc: 0.9519, precision: 0.8761, recall: 0.8414\n",
      "start training model\n",
      "2018-12-29T17:23:05.349799, step: 1405, loss: 0.17373915016651154, acc: 0.9453, auc: 0.9875, precision: 0.9456, recall: 0.9446\n",
      "2018-12-29T17:23:05.442960, step: 1406, loss: 0.17595089972019196, acc: 0.9219, auc: 0.9843, precision: 0.9274, recall: 0.9186\n",
      "2018-12-29T17:23:05.548750, step: 1407, loss: 0.1322135627269745, acc: 0.9453, auc: 0.9932, precision: 0.949, recall: 0.944\n",
      "2018-12-29T17:23:05.662957, step: 1408, loss: 0.09790374338626862, acc: 0.9766, auc: 0.9936, precision: 0.9763, recall: 0.9769\n",
      "2018-12-29T17:23:05.772565, step: 1409, loss: 0.12201808393001556, acc: 0.9531, auc: 0.9904, precision: 0.9567, recall: 0.9504\n",
      "2018-12-29T17:23:05.875840, step: 1410, loss: 0.23066535592079163, acc: 0.9141, auc: 0.9699, precision: 0.9144, recall: 0.9147\n",
      "2018-12-29T17:23:05.974709, step: 1411, loss: 0.12065982818603516, acc: 0.9531, auc: 0.9919, precision: 0.9541, recall: 0.9541\n",
      "2018-12-29T17:23:06.069906, step: 1412, loss: 0.10256187617778778, acc: 0.9688, auc: 0.9957, precision: 0.9678, recall: 0.9678\n",
      "2018-12-29T17:23:06.172529, step: 1413, loss: 0.08896570652723312, acc: 0.9531, auc: 0.9995, precision: 0.9571, recall: 0.9531\n",
      "2018-12-29T17:23:06.270340, step: 1414, loss: 0.17389798164367676, acc: 0.9453, auc: 0.9853, precision: 0.9455, recall: 0.9452\n",
      "2018-12-29T17:23:06.377823, step: 1415, loss: 0.3288688659667969, acc: 0.8125, auc: 0.9923, precision: 0.8737, recall: 0.7895\n",
      "2018-12-29T17:23:06.483218, step: 1416, loss: 0.4358641505241394, acc: 0.8359, auc: 0.9646, precision: 0.8939, recall: 0.79\n",
      "2018-12-29T17:23:06.576005, step: 1417, loss: 0.33112260699272156, acc: 0.8125, auc: 0.9561, precision: 0.8591, recall: 0.801\n",
      "2018-12-29T17:23:06.683952, step: 1418, loss: 0.21559607982635498, acc: 0.8984, auc: 0.9806, precision: 0.9057, recall: 0.9046\n",
      "2018-12-29T17:23:06.780070, step: 1419, loss: 0.1307821273803711, acc: 0.9453, auc: 0.9931, precision: 0.9453, recall: 0.9463\n",
      "2018-12-29T17:23:06.891248, step: 1420, loss: 0.16913199424743652, acc: 0.9453, auc: 0.9829, precision: 0.9455, recall: 0.9452\n",
      "2018-12-29T17:23:06.977713, step: 1421, loss: 0.08451096713542938, acc: 0.9844, auc: 0.9988, precision: 0.9843, recall: 0.9843\n",
      "2018-12-29T17:23:07.073444, step: 1422, loss: 0.17647865414619446, acc: 0.8984, auc: 0.9915, precision: 0.9044, recall: 0.911\n",
      "2018-12-29T17:23:07.186479, step: 1423, loss: 0.7944238781929016, acc: 0.7578, auc: 0.9584, precision: 0.8258, recall: 0.7786\n",
      "2018-12-29T17:23:07.289458, step: 1424, loss: 0.37485432624816895, acc: 0.7344, auc: 0.985, precision: 0.8333, recall: 0.7167\n",
      "2018-12-29T17:23:07.381400, step: 1425, loss: 0.31493479013442993, acc: 0.7812, auc: 0.975, precision: 0.8542, recall: 0.7667\n",
      "2018-12-29T17:23:07.477283, step: 1426, loss: 0.20517119765281677, acc: 0.9297, auc: 0.9888, precision: 0.9332, recall: 0.9284\n",
      "2018-12-29T17:23:07.570577, step: 1427, loss: 0.16441114246845245, acc: 0.9062, auc: 0.9912, precision: 0.9232, recall: 0.8914\n",
      "2018-12-29T17:23:07.677362, step: 1428, loss: 0.15513348579406738, acc: 0.9531, auc: 0.989, precision: 0.9563, recall: 0.951\n",
      "2018-12-29T17:23:07.776039, step: 1429, loss: 0.06941714882850647, acc: 0.9766, auc: 0.999, precision: 0.9769, recall: 0.9763\n",
      "2018-12-29T17:23:07.873738, step: 1430, loss: 0.1290130317211151, acc: 0.9609, auc: 0.9902, precision: 0.9613, recall: 0.9603\n",
      "2018-12-29T17:23:07.970220, step: 1431, loss: 0.13966301083564758, acc: 0.9531, auc: 0.9841, precision: 0.9522, recall: 0.9522\n",
      "2018-12-29T17:23:08.065255, step: 1432, loss: 0.12147659063339233, acc: 0.9609, auc: 0.9865, precision: 0.9562, recall: 0.9642\n",
      "2018-12-29T17:23:08.162811, step: 1433, loss: 0.1199616938829422, acc: 0.9609, auc: 0.9932, precision: 0.9612, recall: 0.9607\n",
      "2018-12-29T17:23:08.253351, step: 1434, loss: 0.10111770033836365, acc: 0.9531, auc: 0.9973, precision: 0.9595, recall: 0.95\n",
      "2018-12-29T17:23:08.346245, step: 1435, loss: 0.2860237658023834, acc: 0.875, auc: 0.9921, precision: 0.8919, recall: 0.8857\n",
      "2018-12-29T17:23:08.438018, step: 1436, loss: 0.6508394479751587, acc: 0.7031, auc: 0.9929, precision: 0.8081, recall: 0.7164\n",
      "2018-12-29T17:23:08.522345, step: 1437, loss: 0.6365241408348083, acc: 0.7109, auc: 0.928, precision: 0.8073, recall: 0.7319\n",
      "2018-12-29T17:23:08.609649, step: 1438, loss: 0.33636415004730225, acc: 0.875, auc: 0.9501, precision: 0.8733, recall: 0.8769\n",
      "2018-12-29T17:23:08.702811, step: 1439, loss: 0.205774188041687, acc: 0.9375, auc: 0.9787, precision: 0.9383, recall: 0.9334\n",
      "2018-12-29T17:23:08.809514, step: 1440, loss: 0.22121477127075195, acc: 0.8906, auc: 0.9697, precision: 0.8951, recall: 0.8882\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-29T17:23:08.902393, step: 1441, loss: 0.16603147983551025, acc: 0.9453, auc: 0.9889, precision: 0.9427, recall: 0.945\n",
      "2018-12-29T17:23:08.995018, step: 1442, loss: 0.17771635949611664, acc: 0.8906, auc: 0.9932, precision: 0.8971, recall: 0.9054\n",
      "2018-12-29T17:23:09.088320, step: 1443, loss: 0.16966921091079712, acc: 0.9453, auc: 0.9886, precision: 0.9436, recall: 0.9472\n",
      "2018-12-29T17:23:09.194234, step: 1444, loss: 0.17125730216503143, acc: 0.9141, auc: 0.9792, precision: 0.917, recall: 0.9134\n",
      "2018-12-29T17:23:09.291902, step: 1445, loss: 0.1268976926803589, acc: 0.9375, auc: 0.9914, precision: 0.937, recall: 0.9382\n",
      "2018-12-29T17:23:09.388774, step: 1446, loss: 0.10459314286708832, acc: 0.9453, auc: 0.9938, precision: 0.9476, recall: 0.9426\n",
      "2018-12-29T17:23:09.481009, step: 1447, loss: 0.09073600172996521, acc: 0.9609, auc: 0.9956, precision: 0.9637, recall: 0.9579\n",
      "2018-12-29T17:23:09.584081, step: 1448, loss: 0.09973230212926865, acc: 0.9453, auc: 0.9958, precision: 0.9453, recall: 0.9454\n",
      "2018-12-29T17:23:09.675798, step: 1449, loss: 0.15546250343322754, acc: 0.9219, auc: 0.9871, precision: 0.9112, recall: 0.9293\n",
      "2018-12-29T17:23:09.781809, step: 1450, loss: 0.16615888476371765, acc: 0.9375, auc: 0.9988, precision: 0.9437, recall: 0.9385\n",
      "2018-12-29T17:23:09.883361, step: 1451, loss: 0.30904823541641235, acc: 0.8438, auc: 0.9882, precision: 0.8734, recall: 0.8551\n",
      "2018-12-29T17:23:09.972188, step: 1452, loss: 0.27675870060920715, acc: 0.9141, auc: 0.9867, precision: 0.9203, recall: 0.9214\n",
      "2018-12-29T17:23:10.079621, step: 1453, loss: 0.16172870993614197, acc: 0.9141, auc: 0.9907, precision: 0.9253, recall: 0.9093\n",
      "2018-12-29T17:23:10.177712, step: 1454, loss: 0.136745885014534, acc: 0.9297, auc: 0.9916, precision: 0.9316, recall: 0.9268\n",
      "2018-12-29T17:23:10.270641, step: 1455, loss: 0.12904922664165497, acc: 0.9531, auc: 0.9966, precision: 0.9563, recall: 0.951\n",
      "2018-12-29T17:23:10.370029, step: 1456, loss: 0.35138821601867676, acc: 0.8047, auc: 0.9882, precision: 0.8529, recall: 0.8162\n",
      "2018-12-29T17:23:10.463345, step: 1457, loss: 0.4284732937812805, acc: 0.8359, auc: 0.9748, precision: 0.8729, recall: 0.826\n",
      "2018-12-29T17:23:10.560167, step: 1458, loss: 0.23633059859275818, acc: 0.8984, auc: 0.9748, precision: 0.909, recall: 0.8936\n",
      "2018-12-29T17:23:10.659817, step: 1459, loss: 0.12815134227275848, acc: 0.9297, auc: 0.9961, precision: 0.9345, recall: 0.9254\n",
      "2018-12-29T17:23:10.760106, step: 1460, loss: 0.10670773684978485, acc: 0.9688, auc: 0.9956, precision: 0.9686, recall: 0.9686\n",
      "2018-12-29T17:23:10.861664, step: 1461, loss: 0.08414653688669205, acc: 0.9844, auc: 0.9993, precision: 0.9853, recall: 0.9839\n",
      "2018-12-29T17:23:10.956458, step: 1462, loss: 0.08906740695238113, acc: 0.9609, auc: 0.9978, precision: 0.9632, recall: 0.9615\n",
      "2018-12-29T17:23:11.053157, step: 1463, loss: 0.122756227850914, acc: 0.9609, auc: 0.9907, precision: 0.9615, recall: 0.9632\n",
      "2018-12-29T17:23:11.149272, step: 1464, loss: 0.1250576376914978, acc: 0.9375, auc: 0.9922, precision: 0.9375, recall: 0.9379\n",
      "2018-12-29T17:23:11.249209, step: 1465, loss: 0.2014680802822113, acc: 0.9219, auc: 0.9772, precision: 0.9246, recall: 0.9196\n",
      "2018-12-29T17:23:11.345552, step: 1466, loss: 0.17200174927711487, acc: 0.9375, auc: 0.9845, precision: 0.9385, recall: 0.9355\n",
      "2018-12-29T17:23:11.445394, step: 1467, loss: 0.2846381664276123, acc: 0.8984, auc: 0.9822, precision: 0.9156, recall: 0.8984\n",
      "2018-12-29T17:23:11.541782, step: 1468, loss: 0.2571104168891907, acc: 0.9297, auc: 0.9892, precision: 0.926, recall: 0.9372\n",
      "2018-12-29T17:23:11.652199, step: 1469, loss: 0.1978117972612381, acc: 0.8984, auc: 0.9782, precision: 0.902, recall: 0.8956\n",
      "2018-12-29T17:23:11.743250, step: 1470, loss: 0.1345232129096985, acc: 0.9453, auc: 0.9916, precision: 0.9502, recall: 0.9419\n",
      "2018-12-29T17:23:11.837081, step: 1471, loss: 0.12371929734945297, acc: 0.9531, auc: 0.9892, precision: 0.952, recall: 0.952\n",
      "2018-12-29T17:23:11.936133, step: 1472, loss: 0.14353099465370178, acc: 0.9375, auc: 0.9855, precision: 0.9375, recall: 0.9392\n",
      "2018-12-29T17:23:12.041140, step: 1473, loss: 0.10363756865262985, acc: 0.9766, auc: 0.9956, precision: 0.9769, recall: 0.9763\n",
      "2018-12-29T17:23:12.150027, step: 1474, loss: 0.08755237609148026, acc: 0.9609, auc: 0.9971, precision: 0.9611, recall: 0.9609\n",
      "2018-12-29T17:23:12.261601, step: 1475, loss: 0.13671952486038208, acc: 0.9453, auc: 0.9918, precision: 0.9478, recall: 0.9415\n",
      "2018-12-29T17:23:12.366681, step: 1476, loss: 0.22077970206737518, acc: 0.9141, auc: 0.9751, precision: 0.9141, recall: 0.9142\n",
      "2018-12-29T17:23:12.465662, step: 1477, loss: 0.14309775829315186, acc: 0.9219, auc: 0.9919, precision: 0.9299, recall: 0.9198\n",
      "2018-12-29T17:23:12.580846, step: 1478, loss: 0.17584800720214844, acc: 0.9297, auc: 0.9897, precision: 0.9342, recall: 0.9305\n",
      "2018-12-29T17:23:12.680491, step: 1479, loss: 0.34926527738571167, acc: 0.8516, auc: 0.9818, precision: 0.8716, recall: 0.8699\n",
      "2018-12-29T17:23:12.781491, step: 1480, loss: 0.46831926703453064, acc: 0.8203, auc: 0.9539, precision: 0.8636, recall: 0.8093\n",
      "2018-12-29T17:23:12.879965, step: 1481, loss: 0.22114485502243042, acc: 0.8516, auc: 0.9895, precision: 0.8967, recall: 0.8273\n",
      "2018-12-29T17:23:12.977320, step: 1482, loss: 0.21458612382411957, acc: 0.8984, auc: 0.9813, precision: 0.9057, recall: 0.9046\n",
      "2018-12-29T17:23:13.075906, step: 1483, loss: 0.16583684086799622, acc: 0.9531, auc: 0.9924, precision: 0.9521, recall: 0.9557\n",
      "2018-12-29T17:23:13.179694, step: 1484, loss: 0.21482214331626892, acc: 0.9062, auc: 0.9821, precision: 0.9226, recall: 0.8932\n",
      "2018-12-29T17:23:13.286665, step: 1485, loss: 0.2672438621520996, acc: 0.9141, auc: 0.9618, precision: 0.9142, recall: 0.9044\n",
      "2018-12-29T17:23:13.379130, step: 1486, loss: 0.18947267532348633, acc: 0.9141, auc: 0.9851, precision: 0.9286, recall: 0.9113\n",
      "2018-12-29T17:23:13.475213, step: 1487, loss: 0.1949097365140915, acc: 0.9531, auc: 0.9753, precision: 0.9522, recall: 0.9522\n",
      "2018-12-29T17:23:13.583033, step: 1488, loss: 0.15214796364307404, acc: 0.9297, auc: 0.9887, precision: 0.9367, recall: 0.927\n",
      "2018-12-29T17:23:13.676288, step: 1489, loss: 0.2807304263114929, acc: 0.8984, auc: 0.9851, precision: 0.9188, recall: 0.8934\n",
      "2018-12-29T17:23:13.768643, step: 1490, loss: 0.4807625412940979, acc: 0.6797, auc: 0.99, precision: 0.799, recall: 0.694\n",
      "2018-12-29T17:23:13.870851, step: 1491, loss: 0.3434227705001831, acc: 0.8906, auc: 0.9635, precision: 0.8941, recall: 0.8941\n",
      "2018-12-29T17:23:13.982189, step: 1492, loss: 0.1735922396183014, acc: 0.9609, auc: 0.9895, precision: 0.9613, recall: 0.9603\n",
      "2018-12-29T17:23:14.086897, step: 1493, loss: 0.1677873283624649, acc: 0.9297, auc: 0.9851, precision: 0.9328, recall: 0.9291\n",
      "2018-12-29T17:23:14.180065, step: 1494, loss: 0.12015922367572784, acc: 0.9688, auc: 0.9953, precision: 0.9661, recall: 0.9726\n",
      "2018-12-29T17:23:14.285672, step: 1495, loss: 0.11007405072450638, acc: 0.9531, auc: 0.9944, precision: 0.9549, recall: 0.9531\n",
      "2018-12-29T17:23:14.395056, step: 1496, loss: 0.14097608625888824, acc: 0.9531, auc: 0.9973, precision: 0.9436, recall: 0.9586\n",
      "2018-12-29T17:23:14.506416, step: 1497, loss: 0.23523563146591187, acc: 0.8516, auc: 0.9941, precision: 0.8869, recall: 0.8492\n",
      "2018-12-29T17:23:14.613493, step: 1498, loss: 0.37481632828712463, acc: 0.8438, auc: 0.9848, precision: 0.8734, recall: 0.8551\n",
      "2018-12-29T17:23:14.710646, step: 1499, loss: 0.2643241882324219, acc: 0.8203, auc: 0.9784, precision: 0.8827, recall: 0.783\n",
      "2018-12-29T17:23:14.805256, step: 1500, loss: 0.22218242287635803, acc: 0.8984, auc: 0.9736, precision: 0.8961, recall: 0.9078\n",
      "\n",
      "Evaluation:\n",
      "2018-12-29T17:23:18.705421, step: 1500, loss: 0.4131571956169911, acc: 0.8379461538461539, auc: 0.9176256410256409, precision: 0.8383923076923074, recall: 0.8381820512820514\n",
      "2018-12-29T17:23:18.799117, step: 1501, loss: 0.18743616342544556, acc: 0.9453, auc: 0.9756, precision: 0.9454, recall: 0.9453\n",
      "2018-12-29T17:23:18.897488, step: 1502, loss: 0.16741183400154114, acc: 0.9453, auc: 0.9821, precision: 0.9413, recall: 0.9444\n",
      "2018-12-29T17:23:18.993363, step: 1503, loss: 0.11942531168460846, acc: 0.9688, auc: 0.9924, precision: 0.9696, recall: 0.9682\n",
      "2018-12-29T17:23:19.088965, step: 1504, loss: 0.23969405889511108, acc: 0.8828, auc: 0.9741, precision: 0.8922, recall: 0.8793\n",
      "2018-12-29T17:23:19.180015, step: 1505, loss: 0.2582651972770691, acc: 0.9141, auc: 0.9916, precision: 0.9329, recall: 0.9035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-29T17:23:19.282621, step: 1506, loss: 0.3198937773704529, acc: 0.8672, auc: 0.977, precision: 0.8896, recall: 0.875\n",
      "2018-12-29T17:23:19.380293, step: 1507, loss: 0.17050494253635406, acc: 0.9531, auc: 0.9955, precision: 0.9508, recall: 0.9589\n",
      "2018-12-29T17:23:19.479520, step: 1508, loss: 0.18570426106452942, acc: 0.9219, auc: 0.9846, precision: 0.9231, recall: 0.9315\n",
      "2018-12-29T17:23:19.576861, step: 1509, loss: 0.22667500376701355, acc: 0.9297, auc: 0.9823, precision: 0.9378, recall: 0.925\n",
      "2018-12-29T17:23:19.672448, step: 1510, loss: 0.28458935022354126, acc: 0.8125, auc: 0.9934, precision: 0.8667, recall: 0.8065\n",
      "2018-12-29T17:23:19.767398, step: 1511, loss: 0.2893804907798767, acc: 0.8828, auc: 0.9741, precision: 0.9062, recall: 0.881\n",
      "2018-12-29T17:23:19.869476, step: 1512, loss: 0.13394753634929657, acc: 0.9297, auc: 0.9938, precision: 0.928, recall: 0.9296\n",
      "2018-12-29T17:23:19.967374, step: 1513, loss: 0.1370251327753067, acc: 0.9297, auc: 0.991, precision: 0.94, recall: 0.9274\n",
      "2018-12-29T17:23:20.081311, step: 1514, loss: 0.11901052296161652, acc: 0.9531, auc: 0.9941, precision: 0.9541, recall: 0.9523\n",
      "2018-12-29T17:23:20.190543, step: 1515, loss: 0.2946600615978241, acc: 0.9219, auc: 0.975, precision: 0.9228, recall: 0.9278\n",
      "2018-12-29T17:23:20.287830, step: 1516, loss: 0.33207017183303833, acc: 0.8672, auc: 0.9755, precision: 0.8808, recall: 0.8613\n",
      "2018-12-29T17:23:20.382419, step: 1517, loss: 0.27233201265335083, acc: 0.8047, auc: 0.9895, precision: 0.8611, recall: 0.8016\n",
      "2018-12-29T17:23:20.478980, step: 1518, loss: 0.2303730845451355, acc: 0.9141, auc: 0.9721, precision: 0.9141, recall: 0.9132\n",
      "2018-12-29T17:23:20.573827, step: 1519, loss: 0.18714311718940735, acc: 0.9453, auc: 0.9804, precision: 0.9448, recall: 0.9456\n",
      "2018-12-29T17:23:20.681816, step: 1520, loss: 0.19229581952095032, acc: 0.9453, auc: 0.9814, precision: 0.9453, recall: 0.9454\n",
      "2018-12-29T17:23:20.799591, step: 1521, loss: 0.18828937411308289, acc: 0.9219, auc: 0.9794, precision: 0.9318, recall: 0.9165\n",
      "2018-12-29T17:23:20.896295, step: 1522, loss: 0.15264981985092163, acc: 0.9219, auc: 0.9909, precision: 0.9227, recall: 0.9206\n",
      "2018-12-29T17:23:21.010508, step: 1523, loss: 0.14315618574619293, acc: 0.9297, auc: 0.9916, precision: 0.9387, recall: 0.9228\n",
      "2018-12-29T17:23:21.119583, step: 1524, loss: 0.11659775674343109, acc: 0.9609, auc: 0.9927, precision: 0.9648, recall: 0.9597\n",
      "2018-12-29T17:23:21.218929, step: 1525, loss: 0.18818511068820953, acc: 0.9219, auc: 0.9781, precision: 0.9214, recall: 0.9214\n",
      "2018-12-29T17:23:21.315448, step: 1526, loss: 0.2182394564151764, acc: 0.9219, auc: 0.9805, precision: 0.9127, recall: 0.9333\n",
      "2018-12-29T17:23:21.411349, step: 1527, loss: 0.4167649745941162, acc: 0.8516, auc: 0.9629, precision: 0.8667, recall: 0.8516\n",
      "2018-12-29T17:23:21.515216, step: 1528, loss: 0.4036533236503601, acc: 0.7656, auc: 0.9407, precision: 0.8238, recall: 0.7361\n",
      "2018-12-29T17:23:21.610911, step: 1529, loss: 0.24715149402618408, acc: 0.9531, auc: 0.9782, precision: 0.952, recall: 0.952\n",
      "2018-12-29T17:23:21.697794, step: 1530, loss: 0.22195926308631897, acc: 0.8828, auc: 0.9844, precision: 0.8901, recall: 0.8938\n",
      "2018-12-29T17:23:21.792923, step: 1531, loss: 0.25092312693595886, acc: 0.9219, auc: 0.9731, precision: 0.929, recall: 0.9136\n",
      "2018-12-29T17:23:21.890419, step: 1532, loss: 0.18986675143241882, acc: 0.8984, auc: 0.9836, precision: 0.9084, recall: 0.8949\n",
      "2018-12-29T17:23:21.986057, step: 1533, loss: 0.10762281715869904, acc: 0.9609, auc: 0.9953, precision: 0.9635, recall: 0.9584\n",
      "2018-12-29T17:23:22.081865, step: 1534, loss: 0.13197727501392365, acc: 0.9609, auc: 0.9912, precision: 0.9613, recall: 0.9601\n",
      "2018-12-29T17:23:22.184911, step: 1535, loss: 0.08363761007785797, acc: 0.9609, auc: 0.9983, precision: 0.9662, recall: 0.9576\n",
      "2018-12-29T17:23:22.285433, step: 1536, loss: 0.15093134343624115, acc: 0.9375, auc: 0.9878, precision: 0.9379, recall: 0.9375\n",
      "2018-12-29T17:23:22.383955, step: 1537, loss: 0.3058916926383972, acc: 0.8828, auc: 0.9751, precision: 0.9046, recall: 0.8722\n",
      "2018-12-29T17:23:22.484709, step: 1538, loss: 0.28646421432495117, acc: 0.875, auc: 0.9909, precision: 0.9036, recall: 0.8689\n",
      "2018-12-29T17:23:22.583860, step: 1539, loss: 0.18763968348503113, acc: 0.8906, auc: 0.989, precision: 0.8979, recall: 0.8941\n",
      "2018-12-29T17:23:22.693507, step: 1540, loss: 0.17756399512290955, acc: 0.9453, auc: 0.9812, precision: 0.9421, recall: 0.9477\n",
      "2018-12-29T17:23:22.791119, step: 1541, loss: 0.15522748231887817, acc: 0.9609, auc: 0.9924, precision: 0.9621, recall: 0.9627\n",
      "2018-12-29T17:23:22.888329, step: 1542, loss: 0.1644582748413086, acc: 0.9219, auc: 0.99, precision: 0.9405, recall: 0.9074\n",
      "2018-12-29T17:23:22.984726, step: 1543, loss: 0.20877721905708313, acc: 0.9453, auc: 0.98, precision: 0.9441, recall: 0.9455\n",
      "2018-12-29T17:23:23.083728, step: 1544, loss: 0.15085534751415253, acc: 0.9219, auc: 0.9936, precision: 0.9242, recall: 0.9306\n",
      "2018-12-29T17:23:23.179241, step: 1545, loss: 0.2995207905769348, acc: 0.9141, auc: 0.9894, precision: 0.9321, recall: 0.9052\n",
      "2018-12-29T17:23:23.281638, step: 1546, loss: 0.39125290513038635, acc: 0.8125, auc: 0.9675, precision: 0.8681, recall: 0.8033\n",
      "2018-12-29T17:23:23.375245, step: 1547, loss: 0.2676776945590973, acc: 0.8516, auc: 0.9726, precision: 0.8659, recall: 0.8531\n",
      "2018-12-29T17:23:23.471189, step: 1548, loss: 0.23636840283870697, acc: 0.9297, auc: 0.9646, precision: 0.9328, recall: 0.9291\n",
      "2018-12-29T17:23:23.582182, step: 1549, loss: 0.179835706949234, acc: 0.9219, auc: 0.9778, precision: 0.9223, recall: 0.9219\n",
      "2018-12-29T17:23:23.680735, step: 1550, loss: 0.1529216468334198, acc: 0.9609, auc: 0.9858, precision: 0.9609, recall: 0.9611\n",
      "2018-12-29T17:23:23.784730, step: 1551, loss: 0.15043994784355164, acc: 0.9297, auc: 0.993, precision: 0.9211, recall: 0.9438\n",
      "2018-12-29T17:23:23.885635, step: 1552, loss: 0.3264263868331909, acc: 0.8906, auc: 0.9822, precision: 0.9029, recall: 0.8935\n",
      "2018-12-29T17:23:23.980908, step: 1553, loss: 0.5262570977210999, acc: 0.6953, auc: 0.9819, precision: 0.805, recall: 0.709\n",
      "2018-12-29T17:23:24.087392, step: 1554, loss: 0.4400089681148529, acc: 0.8438, auc: 0.9387, precision: 0.8733, recall: 0.8438\n",
      "2018-12-29T17:23:24.187600, step: 1555, loss: 0.23145362734794617, acc: 0.9297, auc: 0.9733, precision: 0.9314, recall: 0.9279\n",
      "2018-12-29T17:23:24.285566, step: 1556, loss: 0.1231633871793747, acc: 0.9688, auc: 0.9946, precision: 0.969, recall: 0.969\n",
      "2018-12-29T17:23:24.382406, step: 1557, loss: 0.10187134146690369, acc: 0.9688, auc: 0.9937, precision: 0.9708, recall: 0.9646\n",
      "2018-12-29T17:23:24.496963, step: 1558, loss: 0.13328376412391663, acc: 0.9453, auc: 0.9887, precision: 0.9498, recall: 0.9426\n",
      "2018-12-29T17:23:24.592205, step: 1559, loss: 0.1446021944284439, acc: 0.9531, auc: 0.9962, precision: 0.9483, recall: 0.9605\n",
      "2018-12-29T17:23:24.685596, step: 1560, loss: 0.24342891573905945, acc: 0.8516, auc: 0.9919, precision: 0.8827, recall: 0.8561\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "\n",
    "# 生成训练集和验证集\n",
    "trainReviews = data.trainReviews\n",
    "trainLabels = data.trainLabels\n",
    "evalReviews = data.evalReviews\n",
    "evalLabels = data.evalLabels\n",
    "\n",
    "charEmbedding = data.charEmbedding\n",
    "\n",
    "# 定义计算图\n",
    "with tf.Graph().as_default():\n",
    "\n",
    "    session_conf = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
    "    session_conf.gpu_options.allow_growth=True\n",
    "    session_conf.gpu_options.per_process_gpu_memory_fraction = 0.9  # 配置gpu占用率  \n",
    "\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    \n",
    "    # 定义会话\n",
    "    with sess.as_default():\n",
    "        \n",
    "        cnn = CharCNN(config, charEmbedding)\n",
    "        globalStep = tf.Variable(0, name=\"globalStep\", trainable=False)\n",
    "        # 定义优化函数，传入学习速率参数\n",
    "        optimizer = tf.train.RMSPropOptimizer(config.training.learningRate)\n",
    "        # 计算梯度,得到梯度和变量\n",
    "        gradsAndVars = optimizer.compute_gradients(cnn.loss)\n",
    "        # 将梯度应用到变量下，生成训练器\n",
    "        trainOp = optimizer.apply_gradients(gradsAndVars, global_step=globalStep)\n",
    "        \n",
    "        # 用summary绘制tensorBoard\n",
    "        gradSummaries = []\n",
    "        for g, v in gradsAndVars:\n",
    "            if g is not None:\n",
    "                tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "        \n",
    "        outDir = os.path.abspath(os.path.join(os.path.curdir, \"summarys\"))\n",
    "        print(\"Writing to {}\\n\".format(outDir))\n",
    "        \n",
    "        lossSummary = tf.summary.scalar(\"trainLoss\", cnn.loss)\n",
    "        \n",
    "        summaryOp = tf.summary.merge_all()\n",
    "        \n",
    "        trainSummaryDir = os.path.join(outDir, \"train\")\n",
    "        trainSummaryWriter = tf.summary.FileWriter(trainSummaryDir, sess.graph)\n",
    "        \n",
    "        evalSummaryDir = os.path.join(outDir, \"eval\")\n",
    "        evalSummaryWriter = tf.summary.FileWriter(evalSummaryDir, sess.graph)\n",
    "        \n",
    "        \n",
    "        # 初始化所有变量\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=5)\n",
    "        \n",
    "        # 保存模型的一种方式，保存为pb文件\n",
    "#         builder = tf.saved_model.builder.SavedModelBuilder(\"../model/charCNN/savedModel\")\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        def trainStep(batchX, batchY):\n",
    "            \"\"\"\n",
    "            训练函数\n",
    "            \"\"\"   \n",
    "            feed_dict = {\n",
    "              cnn.inputX: batchX,\n",
    "              cnn.inputY: batchY,\n",
    "              cnn.dropoutKeepProb: config.model.dropoutKeepProb,\n",
    "              cnn.isTraining: True\n",
    "            }\n",
    "            _, summary, step, loss, predictions, binaryPreds = sess.run(\n",
    "                [trainOp, summaryOp, globalStep, cnn.loss, cnn.predictions, cnn.binaryPreds],\n",
    "                feed_dict)\n",
    "            timeStr = datetime.datetime.now().isoformat()\n",
    "            acc, auc, precision, recall = genMetrics(batchY, predictions, binaryPreds)\n",
    "            print(\"{}, step: {}, loss: {}, acc: {}, auc: {}, precision: {}, recall: {}\".format(timeStr, step, loss, acc, auc, precision, recall))\n",
    "            trainSummaryWriter.add_summary(summary, step)\n",
    "\n",
    "        def devStep(batchX, batchY):\n",
    "            \"\"\"\n",
    "            验证函数\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.inputX: batchX,\n",
    "              cnn.inputY: batchY,\n",
    "              cnn.dropoutKeepProb: 1.0,\n",
    "              cnn.isTraining: False\n",
    "            }\n",
    "            summary, step, loss, predictions, binaryPreds = sess.run(\n",
    "                [summaryOp, globalStep, cnn.loss, cnn.predictions, cnn.binaryPreds],\n",
    "                feed_dict)\n",
    "            \n",
    "            acc, auc, precision, recall = genMetrics(batchY, predictions, binaryPreds)\n",
    "            \n",
    "            evalSummaryWriter.add_summary(summary, step)\n",
    "            \n",
    "            return loss, acc, auc, precision, recall\n",
    "        \n",
    "        for i in range(config.training.epoches):\n",
    "            # 训练模型\n",
    "            print(\"start training model\")\n",
    "            for batchTrain in nextBatch(trainReviews, trainLabels, config.batchSize):\n",
    "                trainStep(batchTrain[0], batchTrain[1])\n",
    "\n",
    "                currentStep = tf.train.global_step(sess, globalStep) \n",
    "                if currentStep % config.training.evaluateEvery == 0:\n",
    "                    print(\"\\nEvaluation:\")\n",
    "                    \n",
    "                    losses = []\n",
    "                    accs = []\n",
    "                    aucs = []\n",
    "                    precisions = []\n",
    "                    recalls = []\n",
    "                    \n",
    "                    for batchEval in nextBatch(evalReviews, evalLabels, config.batchSize):\n",
    "                        loss, acc, auc, precision, recall = devStep(batchEval[0], batchEval[1])\n",
    "                        losses.append(loss)\n",
    "                        accs.append(acc)\n",
    "                        aucs.append(auc)\n",
    "                        precisions.append(precision)\n",
    "                        recalls.append(recall)\n",
    "                        \n",
    "                    time_str = datetime.datetime.now().isoformat()\n",
    "                    print(\"{}, step: {}, loss: {}, acc: {}, auc: {}, precision: {}, recall: {}\".format(time_str, currentStep, mean(losses), \n",
    "                                                                                                       mean(accs), mean(aucs), mean(precisions),\n",
    "                                                                                                       mean(recalls)))\n",
    "                    \n",
    "#                 if currentStep % config.training.checkpointEvery == 0:\n",
    "#                     # 保存模型的另一种方法，保存checkpoint文件\n",
    "#                     path = saver.save(sess, \"../model/charCNN/model/my-model\", global_step=currentStep)\n",
    "#                     print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "                    \n",
    "#         inputs = {\"inputX\": tf.saved_model.utils.build_tensor_info(cnn.inputX),\n",
    "#                   \"keepProb\": tf.saved_model.utils.build_tensor_info(cnn.dropoutKeepProb)}\n",
    "\n",
    "#         outputs = {\"binaryPreds\": tf.saved_model.utils.build_tensor_info(cnn.binaryPreds)}\n",
    "\n",
    "#         prediction_signature = tf.saved_model.signature_def_utils.build_signature_def(inputs=inputs, outputs=outputs,\n",
    "#                                                                                       method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME)\n",
    "#         legacy_init_op = tf.group(tf.tables_initializer(), name=\"legacy_init_op\")\n",
    "#         builder.add_meta_graph_and_variables(sess, [tf.saved_model.tag_constants.SERVING],\n",
    "#                                             signature_def_map={\"predict\": prediction_signature}, legacy_init_op=legacy_init_op)\n",
    "\n",
    "#         builder.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
