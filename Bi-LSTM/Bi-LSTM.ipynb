{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import json\n",
    "\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from math import sqrt\n",
    "\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置参数\n",
    "\n",
    "class TrainingConfig(object):\n",
    "    epoches = 10\n",
    "    evaluateEvery = 100\n",
    "    checkpointEvery = 100\n",
    "    learningRate = 0.001\n",
    "    \n",
    "class ModelConfig(object):\n",
    "    embeddingSize = 200\n",
    "    \n",
    "    hiddenSizes = [256, 256]  # 单层LSTM结构的神经元个数\n",
    "    \n",
    "    dropoutKeepProb = 0.5\n",
    "    l2RegLambda = 0.0\n",
    "    \n",
    "class Config(object):\n",
    "    sequenceLength = 200  # 取了所有序列长度的均值\n",
    "    batchSize = 128\n",
    "    \n",
    "    dataSource = \"../data/preProcess/labeledTrain.csv\"\n",
    "    \n",
    "    stopWordSource = \"../data/english\"\n",
    "    \n",
    "    numClasses = 2\n",
    "    \n",
    "    rate = 0.8  # 训练集的比例\n",
    "    \n",
    "    training = TrainingConfig()\n",
    "    \n",
    "    model = ModelConfig()\n",
    "\n",
    "    \n",
    "# 实例化配置参数对象\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据预处理的类，生成训练集和测试集\n",
    "\n",
    "class Dataset(object):\n",
    "    def __init__(self, config):\n",
    "        self._dataSource = config.dataSource\n",
    "        self._stopWordSource = config.stopWordSource  \n",
    "        \n",
    "        self._sequenceLength = config.sequenceLength  # 每条输入的序列处理为定长\n",
    "        self._embeddingSize = config.model.embeddingSize\n",
    "        self._batchSize = config.batchSize\n",
    "        self._rate = config.rate\n",
    "        \n",
    "        self._stopWordDict = {}\n",
    "        \n",
    "        self.trainReviews = []\n",
    "        self.trainLabels = []\n",
    "        \n",
    "        self.evalReviews = []\n",
    "        self.evalLabels = []\n",
    "        \n",
    "        self.wordEmbedding =None\n",
    "        \n",
    "        self._wordToIndex = {}\n",
    "        self._indexToWord = {}\n",
    "        \n",
    "    def _readData(self, filePath):\n",
    "        \"\"\"\n",
    "        从csv文件中读取数据集\n",
    "        \"\"\"\n",
    "        \n",
    "        df = pd.read_csv(filePath)\n",
    "        labels = df[\"sentiment\"].tolist()\n",
    "        review = df[\"review\"].tolist()\n",
    "        reviews = [line.strip().split() for line in review]\n",
    "\n",
    "        return reviews, labels\n",
    "\n",
    "    def _reviewProcess(self, review, sequenceLength, wordToIndex):\n",
    "        \"\"\"\n",
    "        将数据集中的每条评论用index表示\n",
    "        wordToIndex中“pad”对应的index为0\n",
    "        \"\"\"\n",
    "        \n",
    "        reviewVec = np.zeros((sequenceLength))\n",
    "        sequenceLen = sequenceLength\n",
    "        \n",
    "        # 判断当前的序列是否小于定义的固定序列长度\n",
    "        if len(review) < sequenceLength:\n",
    "            sequenceLen = len(review)\n",
    "            \n",
    "        for i in range(sequenceLen):\n",
    "            if review[i] in wordToIndex:\n",
    "                reviewVec[i] = wordToIndex[review[i]]\n",
    "            else:\n",
    "                reviewVec[i] = wordToIndex[\"UNK\"]\n",
    "\n",
    "        return reviewVec\n",
    "\n",
    "    def _genTrainEvalData(self, x, y, rate):\n",
    "        \"\"\"\n",
    "        生成训练集和验证集\n",
    "        \"\"\"\n",
    "        \n",
    "        reviews = []\n",
    "        labels = []\n",
    "        \n",
    "        # 遍历所有的文本，将文本中的词转换成index表示\n",
    "        for i in range(len(x)):\n",
    "            reviewVec = self._reviewProcess(x[i], self._sequenceLength, self._wordToIndex)\n",
    "            reviews.append(reviewVec)\n",
    "            \n",
    "            labels.append([y[i]])\n",
    "            \n",
    "        trainIndex = int(len(x) * rate)\n",
    "        \n",
    "        trainReviews = np.asarray(reviews[:trainIndex], dtype=\"int64\")\n",
    "        trainLabels = np.array(labels[:trainIndex], dtype=\"float32\")\n",
    "        \n",
    "        evalReviews = np.asarray(reviews[trainIndex:], dtype=\"int64\")\n",
    "        evalLabels = np.array(labels[trainIndex:], dtype=\"float32\")\n",
    "\n",
    "        return trainReviews, trainLabels, evalReviews, evalLabels\n",
    "        \n",
    "    def _genVocabulary(self, reviews):\n",
    "        \"\"\"\n",
    "        生成词向量和词汇-索引映射字典，可以用全数据集\n",
    "        \"\"\"\n",
    "        \n",
    "        allWords = [word for review in reviews for word in review]\n",
    "        \n",
    "        # 去掉停用词\n",
    "        subWords = [word for word in allWords if word not in self.stopWordDict]\n",
    "        \n",
    "        wordCount = Counter(subWords)  # 统计词频\n",
    "        sortWordCount = sorted(wordCount.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # 去除低频词\n",
    "        words = [item[0] for item in sortWordCount if item[1] >= 5]\n",
    "        \n",
    "        vocab, wordEmbedding = self._getWordEmbedding(words)\n",
    "        self.wordEmbedding = wordEmbedding\n",
    "        \n",
    "        self._wordToIndex = dict(zip(vocab, list(range(len(vocab)))))\n",
    "        self._indexToWord = dict(zip(list(range(len(vocab))), vocab))\n",
    "        \n",
    "        # 将词汇-索引映射表保存为json数据，之后做inference时直接加载来处理数据\n",
    "        with open(\"../data/wordJson/wordToIndex.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self._wordToIndex, f)\n",
    "        \n",
    "        with open(\"../data/wordJson/indexToWord.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self._indexToWord, f)\n",
    "            \n",
    "    def _getWordEmbedding(self, words):\n",
    "        \"\"\"\n",
    "        按照我们的数据集中的单词取出预训练好的word2vec中的词向量\n",
    "        \"\"\"\n",
    "        \n",
    "        wordVec = gensim.models.KeyedVectors.load_word2vec_format(\"../word2vec/word2Vec.bin\", binary=True)\n",
    "        vocab = []\n",
    "        wordEmbedding = []\n",
    "        \n",
    "        # 添加 \"pad\" 和 \"UNK\", \n",
    "        vocab.append(\"pad\")\n",
    "        vocab.append(\"UNK\")\n",
    "        wordEmbedding.append(np.zeros(self._embeddingSize))\n",
    "        wordEmbedding.append(np.random.randn(self._embeddingSize))\n",
    "        \n",
    "        for word in words:\n",
    "            try:\n",
    "                vector = wordVec.wv[word]\n",
    "                vocab.append(word)\n",
    "                wordEmbedding.append(vector)\n",
    "            except:\n",
    "                print(word + \"不存在于词向量中\")\n",
    "                \n",
    "        return vocab, np.array(wordEmbedding)\n",
    "    \n",
    "    def _readStopWord(self, stopWordPath):\n",
    "        \"\"\"\n",
    "        读取停用词\n",
    "        \"\"\"\n",
    "        \n",
    "        with open(stopWordPath, \"r\") as f:\n",
    "            stopWords = f.read()\n",
    "            stopWordList = stopWords.splitlines()\n",
    "            # 将停用词用列表的形式生成，之后查找停用词时会比较快\n",
    "            self.stopWordDict = dict(zip(stopWordList, list(range(len(stopWordList)))))\n",
    "            \n",
    "    def dataGen(self):\n",
    "        \"\"\"\n",
    "        初始化训练集和验证集\n",
    "        \"\"\"\n",
    "        \n",
    "        # 初始化停用词\n",
    "        self._readStopWord(self._stopWordSource)\n",
    "        \n",
    "        # 初始化数据集\n",
    "        reviews, labels = self._readData(self._dataSource)\n",
    "        \n",
    "        # 初始化词汇-索引映射表和词向量矩阵\n",
    "        self._genVocabulary(reviews)\n",
    "        \n",
    "        # 初始化训练集和测试集\n",
    "        trainReviews, trainLabels, evalReviews, evalLabels = self._genTrainEvalData(reviews, labels, self._rate)\n",
    "        self.trainReviews = trainReviews\n",
    "        self.trainLabels = trainLabels\n",
    "        \n",
    "        self.evalReviews = evalReviews\n",
    "        self.evalLabels = evalLabels\n",
    "        \n",
    "        \n",
    "data = Dataset(config)\n",
    "data.dataGen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape: (20000, 200)\n",
      "train label shape: (20000, 1)\n",
      "eval data shape: (5000, 200)\n"
     ]
    }
   ],
   "source": [
    "print(\"train data shape: {}\".format(data.trainReviews.shape))\n",
    "print(\"train label shape: {}\".format(data.trainLabels.shape))\n",
    "print(\"eval data shape: {}\".format(data.evalReviews.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出batch数据集\n",
    "\n",
    "def nextBatch(x, y, batchSize):\n",
    "        \"\"\"\n",
    "        生成batch数据集，用生成器的方式输出\n",
    "        \"\"\"\n",
    "    \n",
    "        perm = np.arange(len(x))\n",
    "        np.random.shuffle(perm)\n",
    "        x = x[perm]\n",
    "        y = y[perm]\n",
    "        \n",
    "        numBatches = len(x) // batchSize\n",
    "\n",
    "        for i in range(numBatches):\n",
    "            start = i * batchSize\n",
    "            end = start + batchSize\n",
    "            batchX = np.array(x[start: end], dtype=\"int64\")\n",
    "            batchY = np.array(y[start: end], dtype=\"float32\")\n",
    "            \n",
    "            yield batchX, batchY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建模型\n",
    "class BiLSTM(object):\n",
    "    \"\"\"\n",
    "    Bi-LSTM 用于文本分类\n",
    "    \"\"\"\n",
    "    def __init__(self, config, wordEmbedding):\n",
    "\n",
    "        # 定义模型的输入\n",
    "        self.inputX = tf.placeholder(tf.int32, [None, config.sequenceLength], name=\"inputX\")\n",
    "        self.inputY = tf.placeholder(tf.float32, [None, 1], name=\"inputY\")\n",
    "        \n",
    "        self.dropoutKeepProb = tf.placeholder(tf.float32, name=\"dropoutKeepProb\")\n",
    "        \n",
    "        # 定义l2损失\n",
    "        l2Loss = tf.constant(0.0)\n",
    "        \n",
    "        # 词嵌入层\n",
    "        with tf.name_scope(\"embedding\"):\n",
    "\n",
    "            # 利用预训练的词向量初始化词嵌入矩阵\n",
    "            self.W = tf.Variable(tf.cast(wordEmbedding, dtype=tf.float32, name=\"word2vec\") ,name=\"W\")\n",
    "            # 利用词嵌入矩阵将输入的数据中的词转换成词向量，维度[batch_size, sequence_length, embedding_size]\n",
    "            self.embeddedWords = tf.nn.embedding_lookup(self.W, self.inputX)\n",
    "            \n",
    "        # 定义两层双向LSTM的模型结构\n",
    "        with tf.name_scope(\"Bi-LSTM\"):\n",
    "\n",
    "            for idx, hiddenSize in enumerate(config.model.hiddenSizes):\n",
    "                with tf.name_scope(\"Bi-LSTM\" + str(idx)):\n",
    "                    # 定义前向LSTM结构\n",
    "                    lstmFwCell = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(num_units=hiddenSize, state_is_tuple=True),\n",
    "                                                                 output_keep_prob=self.dropoutKeepProb)\n",
    "                    # 定义反向LSTM结构\n",
    "                    lstmBwCell = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(num_units=hiddenSize, state_is_tuple=True),\n",
    "                                                                 output_keep_prob=self.dropoutKeepProb)\n",
    "\n",
    "\n",
    "                    # 采用动态rnn，可以动态的输入序列的长度，若没有输入，则取序列的全长\n",
    "                    # outputs是一个元祖(output_fw, output_bw)，其中两个元素的维度都是[batch_size, max_time, hidden_size],fw和bw的hidden_size一样\n",
    "                    # self.current_state 是最终的状态，二元组(state_fw, state_bw)，state_fw=[batch_size, s]，s是一个元祖(h, c)\n",
    "                    outputs, self.current_state = tf.nn.bidirectional_dynamic_rnn(lstmFwCell, lstmBwCell, \n",
    "                                                                                  self.embeddedWords, dtype=tf.float32,\n",
    "                                                                                  scope=\"bi-lstm\" + str(idx))\n",
    "        \n",
    "                    # 对outputs中的fw和bw的结果拼接 [batch_size, time_step, hidden_size * 2]\n",
    "                    self.embeddedWords = tf.concat(outputs, 2)\n",
    "        \n",
    "        # 去除最后时间步的输出作为全连接的输入\n",
    "        finalOutput = self.embeddedWords[:, -1, :]\n",
    "        \n",
    "        outputSize = config.model.hiddenSizes[-1] * 2  # 因为是双向LSTM，最终的输出值是fw和bw的拼接，因此要乘以2\n",
    "        output = tf.reshape(finalOutput, [-1, outputSize])  # reshape成全连接层的输入维度\n",
    "        \n",
    "        # 全连接层的输出\n",
    "        with tf.name_scope(\"output\"):\n",
    "            outputW = tf.get_variable(\n",
    "                \"outputW\",\n",
    "                shape=[outputSize, 1],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "            outputB= tf.Variable(tf.constant(0.1, shape=[1]), name=\"outputB\")\n",
    "            l2Loss += tf.nn.l2_loss(outputW)\n",
    "            l2Loss += tf.nn.l2_loss(outputB)\n",
    "            self.predictions = tf.nn.xw_plus_b(output, outputW, outputB, name=\"predictions\")\n",
    "            self.binaryPreds = tf.cast(tf.greater_equal(self.predictions, 0.5), tf.float32, name=\"binaryPreds\")\n",
    "        \n",
    "        # 计算二元交叉熵损失\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            \n",
    "            losses = tf.nn.sigmoid_cross_entropy_with_logits(logits=self.predictions, labels=self.inputY)\n",
    "            self.loss = tf.reduce_mean(losses) + config.model.l2RegLambda * l2Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义性能指标函数\n",
    "\n",
    "def mean(item):\n",
    "    return sum(item) / len(item)\n",
    "\n",
    "\n",
    "def genMetrics(trueY, predY, binaryPredY):\n",
    "    \"\"\"\n",
    "    生成acc和auc值\n",
    "    \"\"\"\n",
    "    auc = roc_auc_score(trueY, predY)\n",
    "    accuracy = accuracy_score(trueY, binaryPredY)\n",
    "    precision = precision_score(trueY, binaryPredY)\n",
    "    recall = recall_score(trueY, binaryPredY)\n",
    "    \n",
    "    return round(accuracy, 4), round(auc, 4), round(precision, 4), round(recall, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name bi-lstm0/fw/lstm_cell/kernel:0/grad/hist is illegal; using bi-lstm0/fw/lstm_cell/kernel_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name bi-lstm0/fw/lstm_cell/kernel:0/grad/sparsity is illegal; using bi-lstm0/fw/lstm_cell/kernel_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name bi-lstm0/fw/lstm_cell/bias:0/grad/hist is illegal; using bi-lstm0/fw/lstm_cell/bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name bi-lstm0/fw/lstm_cell/bias:0/grad/sparsity is illegal; using bi-lstm0/fw/lstm_cell/bias_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name bi-lstm0/bw/lstm_cell/kernel:0/grad/hist is illegal; using bi-lstm0/bw/lstm_cell/kernel_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name bi-lstm0/bw/lstm_cell/kernel:0/grad/sparsity is illegal; using bi-lstm0/bw/lstm_cell/kernel_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name bi-lstm0/bw/lstm_cell/bias:0/grad/hist is illegal; using bi-lstm0/bw/lstm_cell/bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name bi-lstm0/bw/lstm_cell/bias:0/grad/sparsity is illegal; using bi-lstm0/bw/lstm_cell/bias_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name bi-lstm1/fw/lstm_cell/kernel:0/grad/hist is illegal; using bi-lstm1/fw/lstm_cell/kernel_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name bi-lstm1/fw/lstm_cell/kernel:0/grad/sparsity is illegal; using bi-lstm1/fw/lstm_cell/kernel_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name bi-lstm1/fw/lstm_cell/bias:0/grad/hist is illegal; using bi-lstm1/fw/lstm_cell/bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name bi-lstm1/fw/lstm_cell/bias:0/grad/sparsity is illegal; using bi-lstm1/fw/lstm_cell/bias_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name bi-lstm1/bw/lstm_cell/kernel:0/grad/hist is illegal; using bi-lstm1/bw/lstm_cell/kernel_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name bi-lstm1/bw/lstm_cell/kernel:0/grad/sparsity is illegal; using bi-lstm1/bw/lstm_cell/kernel_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name bi-lstm1/bw/lstm_cell/bias:0/grad/hist is illegal; using bi-lstm1/bw/lstm_cell/bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name bi-lstm1/bw/lstm_cell/bias:0/grad/sparsity is illegal; using bi-lstm1/bw/lstm_cell/bias_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name outputW:0/grad/hist is illegal; using outputW_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name outputW:0/grad/sparsity is illegal; using outputW_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/outputB:0/grad/hist is illegal; using output/outputB_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/outputB:0/grad/sparsity is illegal; using output/outputB_0/grad/sparsity instead.\n",
      "Writing to /data4T/share/jiangxinyang848/textClassifier/Bi-LSTM/summarys\n",
      "\n",
      "start training model\n",
      "2019-01-17T20:44:08.653069, step: 1, loss: 0.701030433177948, acc: 0.5391, auc: 0.4473, precision: 0.0, recall: 0.0\n",
      "2019-01-17T20:44:09.273920, step: 2, loss: 0.728533148765564, acc: 0.4688, auc: 0.462, precision: 0.4545, recall: 0.2308\n",
      "2019-01-17T20:44:09.967795, step: 3, loss: 0.733249306678772, acc: 0.5469, auc: 0.5116, precision: 0.0, recall: 0.0\n",
      "2019-01-17T20:44:10.661569, step: 4, loss: 0.7494784593582153, acc: 0.5, auc: 0.4602, precision: 0.0, recall: 0.0\n",
      "2019-01-17T20:44:11.369722, step: 5, loss: 0.6867613196372986, acc: 0.5234, auc: 0.5575, precision: 0.6667, recall: 0.0635\n",
      "2019-01-17T20:44:12.100652, step: 6, loss: 0.7203174829483032, acc: 0.5391, auc: 0.5681, precision: 0.5238, recall: 0.3607\n",
      "2019-01-17T20:44:12.816990, step: 7, loss: 0.7349355816841125, acc: 0.4766, auc: 0.4772, precision: 0.4783, recall: 0.3385\n",
      "2019-01-17T20:44:13.476202, step: 8, loss: 0.6955399513244629, acc: 0.5234, auc: 0.5174, precision: 0.5294, recall: 0.1452\n",
      "2019-01-17T20:44:14.189432, step: 9, loss: 0.6847569346427917, acc: 0.5156, auc: 0.5398, precision: 0.0, recall: 0.0\n",
      "2019-01-17T20:44:14.838538, step: 10, loss: 0.7007661461830139, acc: 0.4922, auc: 0.5011, precision: 0.0, recall: 0.0\n",
      "2019-01-17T20:44:15.541199, step: 11, loss: 0.7030569314956665, acc: 0.4922, auc: 0.5292, precision: 0.0, recall: 0.0\n",
      "2019-01-17T20:44:16.153854, step: 12, loss: 0.6893635988235474, acc: 0.5547, auc: 0.5204, precision: 0.0, recall: 0.0\n",
      "2019-01-17T20:44:16.800359, step: 13, loss: 0.7028353214263916, acc: 0.5078, auc: 0.4606, precision: 0.0, recall: 0.0\n",
      "2019-01-17T20:44:17.480799, step: 14, loss: 0.6874308586120605, acc: 0.5156, auc: 0.5354, precision: 0.0, recall: 0.0\n",
      "2019-01-17T20:44:18.171918, step: 15, loss: 0.6905365586280823, acc: 0.5391, auc: 0.5635, precision: 0.5, recall: 0.0169\n",
      "2019-01-17T20:44:18.860417, step: 16, loss: 0.715039849281311, acc: 0.5156, auc: 0.4517, precision: 0.2, recall: 0.0169\n",
      "2019-01-17T20:44:19.487677, step: 17, loss: 0.6972547173500061, acc: 0.4922, auc: 0.528, precision: 0.5, recall: 0.0308\n",
      "2019-01-17T20:44:20.179356, step: 18, loss: 0.6940984725952148, acc: 0.4453, auc: 0.4942, precision: 0.5, recall: 0.0141\n",
      "2019-01-17T20:44:20.905617, step: 19, loss: 0.6959854364395142, acc: 0.4844, auc: 0.5152, precision: 0.0, recall: 0.0\n",
      "2019-01-17T20:44:21.623910, step: 20, loss: 0.7106198668479919, acc: 0.4844, auc: 0.4124, precision: 0.0, recall: 0.0\n",
      "2019-01-17T20:44:22.349234, step: 21, loss: 0.7125754356384277, acc: 0.4922, auc: 0.4471, precision: 0.0, recall: 0.0\n",
      "2019-01-17T20:44:23.068630, step: 22, loss: 0.6921521425247192, acc: 0.5547, auc: 0.5345, precision: 1.0, recall: 0.0172\n",
      "2019-01-17T20:44:23.765882, step: 23, loss: 0.7023146152496338, acc: 0.5156, auc: 0.4712, precision: 0.5, recall: 0.0161\n",
      "2019-01-17T20:44:24.466887, step: 24, loss: 0.6939670443534851, acc: 0.4844, auc: 0.5383, precision: 1.0, recall: 0.0149\n",
      "2019-01-17T20:44:25.096946, step: 25, loss: 0.7077000141143799, acc: 0.4609, auc: 0.4517, precision: 0.0, recall: 0.0\n",
      "2019-01-17T20:44:25.791733, step: 26, loss: 0.6990852355957031, acc: 0.5391, auc: 0.4783, precision: 0.625, recall: 0.082\n",
      "2019-01-17T20:44:26.499118, step: 27, loss: 0.7144541144371033, acc: 0.4219, auc: 0.4556, precision: 0.0, recall: 0.0\n",
      "2019-01-17T20:44:27.118328, step: 28, loss: 0.699429988861084, acc: 0.5234, auc: 0.4946, precision: 0.0, recall: 0.0\n",
      "2019-01-17T20:44:27.819621, step: 29, loss: 0.6847914457321167, acc: 0.6094, auc: 0.5684, precision: 0.6, recall: 0.0588\n",
      "2019-01-17T20:44:28.496947, step: 30, loss: 0.6973572969436646, acc: 0.5391, auc: 0.5306, precision: 0.5, recall: 0.0169\n",
      "2019-01-17T20:44:29.166357, step: 31, loss: 0.7028972506523132, acc: 0.4688, auc: 0.5265, precision: 0.0, recall: 0.0\n",
      "2019-01-17T20:44:29.819270, step: 32, loss: 0.7169322967529297, acc: 0.4531, auc: 0.4456, precision: 0.0, recall: 0.0\n",
      "2019-01-17T20:44:30.543466, step: 33, loss: 0.6927285194396973, acc: 0.5312, auc: 0.525, precision: 0.0, recall: 0.0\n",
      "2019-01-17T20:44:31.221139, step: 34, loss: 0.6817715167999268, acc: 0.5859, auc: 0.5718, precision: 1.0, recall: 0.0185\n",
      "2019-01-17T20:44:31.886708, step: 35, loss: 0.7058179974555969, acc: 0.4297, auc: 0.4822, precision: 0.0, recall: 0.0\n",
      "2019-01-17T20:44:32.599072, step: 36, loss: 0.7193569540977478, acc: 0.4141, auc: 0.4621, precision: 0.0, recall: 0.0\n",
      "2019-01-17T20:44:33.308842, step: 37, loss: 0.6909937858581543, acc: 0.4844, auc: 0.5543, precision: 0.0, recall: 0.0\n",
      "2019-01-17T20:44:33.998544, step: 38, loss: 0.6864718198776245, acc: 0.5938, auc: 0.5654, precision: 1.0, recall: 0.0545\n",
      "2019-01-17T20:44:34.608530, step: 39, loss: 0.7142938375473022, acc: 0.5078, auc: 0.452, precision: 0.1667, recall: 0.0169\n",
      "2019-01-17T20:44:35.276200, step: 40, loss: 0.7029794454574585, acc: 0.4844, auc: 0.4457, precision: 0.3333, recall: 0.0154\n",
      "2019-01-17T20:44:35.925856, step: 41, loss: 0.6838145852088928, acc: 0.4062, auc: 0.566, precision: 0.8, recall: 0.0506\n",
      "2019-01-17T20:44:36.604774, step: 42, loss: 0.7078930139541626, acc: 0.5234, auc: 0.4487, precision: 0.6, recall: 0.0484\n",
      "2019-01-17T20:44:37.300336, step: 43, loss: 0.7051559686660767, acc: 0.4766, auc: 0.4089, precision: 0.5, recall: 0.0149\n",
      "2019-01-17T20:44:37.943065, step: 44, loss: 0.7006469964981079, acc: 0.6094, auc: 0.5343, precision: 0.6667, recall: 0.0392\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-17T20:44:38.594572, step: 45, loss: 0.6854075789451599, acc: 0.4688, auc: 0.5507, precision: 0.0, recall: 0.0\n",
      "2019-01-17T20:44:39.245404, step: 46, loss: 0.7037606239318848, acc: 0.5391, auc: 0.4776, precision: 0.4, recall: 0.0345\n",
      "2019-01-17T20:44:39.934466, step: 47, loss: 0.7124852538108826, acc: 0.4609, auc: 0.4049, precision: 0.25, recall: 0.0149\n",
      "2019-01-17T20:44:40.598915, step: 48, loss: 0.6867026090621948, acc: 0.5391, auc: 0.5348, precision: 0.6667, recall: 0.0333\n",
      "2019-01-17T20:44:41.329120, step: 49, loss: 0.7004528045654297, acc: 0.5078, auc: 0.4537, precision: 0.0, recall: 0.0\n",
      "2019-01-17T20:44:42.058718, step: 50, loss: 0.6933881640434265, acc: 0.5078, auc: 0.5278, precision: 1.0, recall: 0.0156\n",
      "2019-01-17T20:44:42.769413, step: 51, loss: 0.6953430771827698, acc: 0.4688, auc: 0.4995, precision: 0.0, recall: 0.0\n",
      "2019-01-17T20:44:43.400523, step: 52, loss: 0.7019579410552979, acc: 0.5156, auc: 0.4475, precision: 0.0, recall: 0.0\n",
      "2019-01-17T20:44:44.052988, step: 53, loss: 0.7086390256881714, acc: 0.5, auc: 0.4261, precision: 0.0, recall: 0.0\n",
      "2019-01-17T20:44:44.743341, step: 54, loss: 0.6863147020339966, acc: 0.6016, auc: 0.5742, precision: 0.0, recall: 0.0\n",
      "2019-01-17T20:44:45.474085, step: 55, loss: 0.6889355778694153, acc: 0.5391, auc: 0.5497, precision: 0.0, recall: 0.0\n",
      "2019-01-17T20:44:46.161466, step: 56, loss: 0.6961438655853271, acc: 0.5078, auc: 0.475, precision: 0.0, recall: 0.0\n",
      "2019-01-17T20:44:46.817223, step: 57, loss: 0.6942418813705444, acc: 0.4688, auc: 0.5385, precision: 0.0, recall: 0.0\n",
      "2019-01-17T20:44:47.549750, step: 58, loss: 0.6977701783180237, acc: 0.4766, auc: 0.5094, precision: 0.0, recall: 0.0\n",
      "2019-01-17T20:44:48.254646, step: 59, loss: 0.682586133480072, acc: 0.4766, auc: 0.625, precision: 1.0, recall: 0.0147\n",
      "2019-01-17T20:44:48.885794, step: 60, loss: 0.6983138918876648, acc: 0.4922, auc: 0.4778, precision: 0.3333, recall: 0.0156\n",
      "2019-01-17T20:44:49.549484, step: 61, loss: 0.6928908228874207, acc: 0.4766, auc: 0.5645, precision: 1.0, recall: 0.0147\n",
      "2019-01-17T20:44:50.191055, step: 62, loss: 0.7085496187210083, acc: 0.5938, auc: 0.4907, precision: 0.4286, recall: 0.0588\n",
      "2019-01-17T20:44:50.906521, step: 63, loss: 0.6934565901756287, acc: 0.5156, auc: 0.5359, precision: 0.75, recall: 0.0469\n",
      "2019-01-17T20:44:51.634194, step: 64, loss: 0.7049585580825806, acc: 0.4844, auc: 0.4841, precision: 0.25, recall: 0.0156\n",
      "2019-01-17T20:44:52.350580, step: 65, loss: 0.6898785829544067, acc: 0.5078, auc: 0.564, precision: 0.4, recall: 0.0656\n",
      "2019-01-17T20:44:53.070698, step: 66, loss: 0.7115219831466675, acc: 0.5391, auc: 0.4657, precision: 0.6667, recall: 0.0333\n",
      "2019-01-17T20:44:53.792220, step: 67, loss: 0.7070903778076172, acc: 0.5938, auc: 0.4034, precision: 1.0, recall: 0.037\n",
      "2019-01-17T20:44:54.512942, step: 68, loss: 0.709470272064209, acc: 0.4688, auc: 0.4949, precision: 0.0, recall: 0.0\n",
      "2019-01-17T20:44:55.212059, step: 69, loss: 0.69859778881073, acc: 0.5312, auc: 0.4689, precision: 0.0, recall: 0.0\n",
      "2019-01-17T20:44:55.827295, step: 70, loss: 0.708771288394928, acc: 0.3984, auc: 0.5432, precision: 0.0, recall: 0.0\n",
      "2019-01-17T20:44:56.524244, step: 71, loss: 0.6944323778152466, acc: 0.5078, auc: 0.4938, precision: 0.0, recall: 0.0\n",
      "2019-01-17T20:44:57.216075, step: 72, loss: 0.6849503517150879, acc: 0.5, auc: 0.5735, precision: 0.0, recall: 0.0\n",
      "2019-01-17T20:44:57.872941, step: 73, loss: 0.7044396996498108, acc: 0.4922, auc: 0.5043, precision: 0.0, recall: 0.0\n",
      "2019-01-17T20:44:58.559463, step: 74, loss: 0.7005980014801025, acc: 0.4766, auc: 0.5016, precision: 0.0, recall: 0.0\n",
      "2019-01-17T20:44:59.281468, step: 75, loss: 0.6811831593513489, acc: 0.5391, auc: 0.5974, precision: 0.0, recall: 0.0\n",
      "2019-01-17T20:45:00.023458, step: 76, loss: 0.7027770280838013, acc: 0.5078, auc: 0.4664, precision: 0.0, recall: 0.0\n",
      "2019-01-17T20:45:00.754127, step: 77, loss: 0.7059992551803589, acc: 0.5234, auc: 0.4722, precision: 0.0, recall: 0.0\n",
      "2019-01-17T20:45:01.477705, step: 78, loss: 0.701582133769989, acc: 0.5703, auc: 0.5144, precision: 0.6667, recall: 0.0357\n",
      "2019-01-17T20:45:02.206100, step: 79, loss: 0.689034640789032, acc: 0.5469, auc: 0.5522, precision: 1.0, recall: 0.0333\n",
      "2019-01-17T20:45:02.931586, step: 80, loss: 0.6847664713859558, acc: 0.5078, auc: 0.5678, precision: 1.0, recall: 0.0308\n",
      "2019-01-17T20:45:03.618912, step: 81, loss: 0.6887091994285583, acc: 0.5234, auc: 0.556, precision: 1.0, recall: 0.0161\n",
      "2019-01-17T20:45:04.294076, step: 82, loss: 0.700592577457428, acc: 0.5078, auc: 0.4573, precision: 0.25, recall: 0.0164\n",
      "2019-01-17T20:45:05.013628, step: 83, loss: 0.69693922996521, acc: 0.5, auc: 0.4639, precision: 0.0, recall: 0.0\n",
      "2019-01-17T20:45:05.747567, step: 84, loss: 0.6947060823440552, acc: 0.5625, auc: 0.5032, precision: 0.0, recall: 0.0\n",
      "2019-01-17T20:45:06.409706, step: 85, loss: 0.6847583055496216, acc: 0.4844, auc: 0.5868, precision: 0.0, recall: 0.0\n",
      "2019-01-17T20:45:07.051972, step: 86, loss: 0.689336895942688, acc: 0.4766, auc: 0.5872, precision: 0.0, recall: 0.0\n",
      "2019-01-17T20:45:07.775264, step: 87, loss: 0.704121470451355, acc: 0.4375, auc: 0.5017, precision: 0.0, recall: 0.0\n",
      "2019-01-17T20:45:08.485095, step: 88, loss: 0.6949156522750854, acc: 0.5469, auc: 0.4947, precision: 1.0, recall: 0.0169\n",
      "2019-01-17T20:45:09.184344, step: 89, loss: 0.7068965435028076, acc: 0.4141, auc: 0.4224, precision: 0.0, recall: 0.0\n",
      "2019-01-17T20:45:09.845650, step: 90, loss: 0.6941574215888977, acc: 0.4531, auc: 0.4833, precision: 1.0, recall: 0.0141\n",
      "2019-01-17T20:45:10.550822, step: 91, loss: 0.7075076103210449, acc: 0.5234, auc: 0.447, precision: 0.5714, recall: 0.0645\n",
      "2019-01-17T20:45:11.215344, step: 92, loss: 0.6934489607810974, acc: 0.4688, auc: 0.538, precision: 0.5385, recall: 0.1014\n",
      "2019-01-17T20:45:11.916813, step: 93, loss: 0.7144484519958496, acc: 0.5469, auc: 0.5236, precision: 0.4737, recall: 0.1579\n",
      "2019-01-17T20:45:12.634773, step: 94, loss: 0.6856948137283325, acc: 0.4766, auc: 0.5547, precision: 0.5714, recall: 0.1159\n",
      "2019-01-17T20:45:13.312772, step: 95, loss: 0.7134784460067749, acc: 0.4375, auc: 0.4674, precision: 0.2667, recall: 0.0615\n",
      "2019-01-17T20:45:14.020959, step: 96, loss: 0.6900774836540222, acc: 0.4922, auc: 0.5029, precision: 1.0, recall: 0.0441\n",
      "2019-01-17T20:45:14.666192, step: 97, loss: 0.6842155456542969, acc: 0.5547, auc: 0.6062, precision: 1.0, recall: 0.0339\n",
      "2019-01-17T20:45:15.363619, step: 98, loss: 0.6882057785987854, acc: 0.4609, auc: 0.5547, precision: 0.0, recall: 0.0\n",
      "2019-01-17T20:45:16.052611, step: 99, loss: 0.6781882047653198, acc: 0.4766, auc: 0.5995, precision: 0.0, recall: 0.0\n",
      "2019-01-17T20:45:16.732338, step: 100, loss: 0.6914594173431396, acc: 0.4453, auc: 0.4766, precision: 0.0, recall: 0.0\n",
      "\n",
      "Evaluation:\n",
      "2019-01-17T20:45:44.780843, step: 100, loss: 0.695315726292439, acc: 0.4947923076923078, auc: 0.5175923076923076, precision: 0.0, recall: 0.0\n",
      "Saved model checkpoint to ../model/Bi-LSTM/model/my-model-100\n",
      "\n",
      "2019-01-17T20:45:46.040785, step: 101, loss: 0.6771367788314819, acc: 0.4922, auc: 0.6037, precision: 0.0, recall: 0.0\n",
      "2019-01-17T20:45:46.759537, step: 102, loss: 0.7009493112564087, acc: 0.4922, auc: 0.495, precision: 0.0, recall: 0.0\n",
      "2019-01-17T20:45:47.391943, step: 103, loss: 0.7032355070114136, acc: 0.5469, auc: 0.467, precision: 0.0, recall: 0.0\n",
      "2019-01-17T20:45:48.040699, step: 104, loss: 0.6741193532943726, acc: 0.4297, auc: 0.6309, precision: 0.0, recall: 0.0\n",
      "2019-01-17T20:45:48.716883, step: 105, loss: 0.6813973188400269, acc: 0.5312, auc: 0.6225, precision: 1.0, recall: 0.0164\n",
      "2019-01-17T20:45:49.433739, step: 106, loss: 0.7013305425643921, acc: 0.5312, auc: 0.4765, precision: 1.0, recall: 0.0323\n",
      "2019-01-17T20:45:50.151063, step: 107, loss: 0.6889165639877319, acc: 0.5078, auc: 0.5402, precision: 0.6667, recall: 0.0615\n",
      "2019-01-17T20:45:50.822176, step: 108, loss: 0.6865108609199524, acc: 0.5156, auc: 0.5445, precision: 0.0, recall: 0.0\n",
      "2019-01-17T20:45:51.508338, step: 109, loss: 0.6827027201652527, acc: 0.4766, auc: 0.5608, precision: 0.0, recall: 0.0\n",
      "2019-01-17T20:45:52.221796, step: 110, loss: 0.674293577671051, acc: 0.4766, auc: 0.6565, precision: 0.0, recall: 0.0\n",
      "2019-01-17T20:45:52.906282, step: 111, loss: 0.6947557926177979, acc: 0.5781, auc: 0.4992, precision: 0.0, recall: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-17T20:45:53.608265, step: 112, loss: 0.6500290036201477, acc: 0.5469, auc: 0.6883, precision: 1.0, recall: 0.0169\n",
      "2019-01-17T20:45:54.345237, step: 113, loss: 0.6062589287757874, acc: 0.5859, auc: 0.7633, precision: 0.75, recall: 0.1053\n",
      "2019-01-17T20:45:55.063181, step: 114, loss: 0.6920719146728516, acc: 0.5156, auc: 0.5779, precision: 0.5417, recall: 0.5735\n",
      "2019-01-17T20:45:55.803638, step: 115, loss: 0.7255005836486816, acc: 0.5703, auc: 0.5718, precision: 0.4737, recall: 0.1667\n",
      "2019-01-17T20:45:56.541664, step: 116, loss: 0.6941976547241211, acc: 0.5469, auc: 0.5643, precision: 0.7273, recall: 0.127\n",
      "2019-01-17T20:45:57.278594, step: 117, loss: 0.6923136711120605, acc: 0.4531, auc: 0.4847, precision: 0.6667, recall: 0.0822\n",
      "2019-01-17T20:45:58.013159, step: 118, loss: 0.6886915564537048, acc: 0.4922, auc: 0.4947, precision: 0.75, recall: 0.0448\n",
      "2019-01-17T20:45:58.653627, step: 119, loss: 0.7095251679420471, acc: 0.5156, auc: 0.4384, precision: 0.5, recall: 0.0323\n",
      "2019-01-17T20:45:59.277276, step: 120, loss: 0.690301239490509, acc: 0.5, auc: 0.5665, precision: 1.0, recall: 0.0154\n",
      "2019-01-17T20:45:59.941714, step: 121, loss: 0.6949846744537354, acc: 0.4219, auc: 0.5913, precision: 0.0, recall: 0.0\n",
      "2019-01-17T20:46:00.608175, step: 122, loss: 0.6957769393920898, acc: 0.5078, auc: 0.5553, precision: 0.0, recall: 0.0\n",
      "2019-01-17T20:46:01.332417, step: 123, loss: 0.7145256996154785, acc: 0.3906, auc: 0.4821, precision: 0.0, recall: 0.0\n",
      "2019-01-17T20:46:02.032816, step: 124, loss: 0.686648964881897, acc: 0.5, auc: 0.5831, precision: 1.0, recall: 0.0303\n",
      "2019-01-17T20:46:02.724486, step: 125, loss: 0.6928406953811646, acc: 0.5156, auc: 0.5579, precision: 0.4, recall: 0.0328\n",
      "2019-01-17T20:46:03.389791, step: 126, loss: 0.696466326713562, acc: 0.6094, auc: 0.5783, precision: 0.6923, recall: 0.1636\n",
      "2019-01-17T20:46:04.105113, step: 127, loss: 0.7185017466545105, acc: 0.5078, auc: 0.4406, precision: 0.4815, recall: 0.2097\n",
      "2019-01-17T20:46:04.753120, step: 128, loss: 0.7002905607223511, acc: 0.4766, auc: 0.5241, precision: 0.4583, recall: 0.1692\n",
      "2019-01-17T20:46:05.360676, step: 129, loss: 0.6982316374778748, acc: 0.4844, auc: 0.5209, precision: 0.4545, recall: 0.0769\n",
      "2019-01-17T20:46:06.049177, step: 130, loss: 0.6810251474380493, acc: 0.5156, auc: 0.5802, precision: 0.8333, recall: 0.0758\n",
      "2019-01-17T20:46:06.758651, step: 131, loss: 0.6928268671035767, acc: 0.5469, auc: 0.5375, precision: 1.0, recall: 0.0169\n",
      "2019-01-17T20:46:07.427411, step: 132, loss: 0.6966695785522461, acc: 0.5469, auc: 0.525, precision: 0.0, recall: 0.0\n",
      "2019-01-17T20:46:08.181242, step: 133, loss: 0.6887335181236267, acc: 0.4844, auc: 0.5398, precision: 0.0, recall: 0.0\n",
      "2019-01-17T20:46:08.852533, step: 134, loss: 0.683275580406189, acc: 0.4453, auc: 0.5992, precision: 0.0, recall: 0.0\n",
      "2019-01-17T20:46:09.529169, step: 135, loss: 0.692513644695282, acc: 0.4922, auc: 0.5133, precision: 0.0, recall: 0.0\n",
      "2019-01-17T20:46:10.307907, step: 136, loss: 0.6890442371368408, acc: 0.5234, auc: 0.5373, precision: 0.0, recall: 0.0\n",
      "2019-01-17T20:46:11.000914, step: 137, loss: 0.6922526955604553, acc: 0.4375, auc: 0.5457, precision: 1.0, recall: 0.0137\n",
      "2019-01-17T20:46:11.721279, step: 138, loss: 0.6877024173736572, acc: 0.5, auc: 0.5404, precision: 1.0, recall: 0.0154\n",
      "2019-01-17T20:46:12.431479, step: 139, loss: 0.6712964177131653, acc: 0.4922, auc: 0.6697, precision: 0.8571, recall: 0.0857\n",
      "2019-01-17T20:46:13.155260, step: 140, loss: 0.6942249536514282, acc: 0.5547, auc: 0.5758, precision: 0.6667, recall: 0.0678\n",
      "2019-01-17T20:46:13.860526, step: 141, loss: 0.6831305027008057, acc: 0.625, auc: 0.5836, precision: 0.7333, recall: 0.2\n",
      "2019-01-17T20:46:14.549732, step: 142, loss: 0.6989390850067139, acc: 0.5312, auc: 0.5158, precision: 0.5652, recall: 0.2063\n",
      "2019-01-17T20:46:15.239469, step: 143, loss: 0.6533114910125732, acc: 0.5078, auc: 0.6593, precision: 0.8125, recall: 0.1781\n",
      "2019-01-17T20:46:15.955167, step: 144, loss: 0.6779338121414185, acc: 0.5078, auc: 0.5851, precision: 0.75, recall: 0.1304\n",
      "2019-01-17T20:46:16.685549, step: 145, loss: 0.6942968368530273, acc: 0.5391, auc: 0.523, precision: 0.6, recall: 0.05\n",
      "2019-01-17T20:46:17.393500, step: 146, loss: 0.6757228374481201, acc: 0.5312, auc: 0.5913, precision: 0.8333, recall: 0.0781\n",
      "2019-01-17T20:46:18.092586, step: 147, loss: 0.6880203485488892, acc: 0.4531, auc: 0.5887, precision: 0.0, recall: 0.0\n",
      "2019-01-17T20:46:18.915600, step: 148, loss: 0.6882683038711548, acc: 0.4844, auc: 0.512, precision: 0.75, recall: 0.0441\n",
      "2019-01-17T20:46:19.613130, step: 149, loss: 0.6849767565727234, acc: 0.5312, auc: 0.5943, precision: 1.0, recall: 0.0323\n",
      "2019-01-17T20:46:20.301615, step: 150, loss: 0.6710388660430908, acc: 0.4688, auc: 0.6337, precision: 1.0, recall: 0.0556\n",
      "2019-01-17T20:46:20.996355, step: 151, loss: 0.6786226630210876, acc: 0.5, auc: 0.6031, precision: 0.75, recall: 0.0455\n",
      "2019-01-17T20:46:21.650559, step: 152, loss: 0.6898757219314575, acc: 0.5156, auc: 0.5692, precision: 0.5, recall: 0.0161\n",
      "2019-01-17T20:46:22.393582, step: 153, loss: 0.6508995294570923, acc: 0.5547, auc: 0.7079, precision: 0.8, recall: 0.127\n",
      "2019-01-17T20:46:23.126642, step: 154, loss: 0.6633409261703491, acc: 0.5156, auc: 0.6115, precision: 0.8, recall: 0.1176\n",
      "2019-01-17T20:46:23.822474, step: 155, loss: 0.6634804010391235, acc: 0.5547, auc: 0.6029, precision: 0.6667, recall: 0.1311\n",
      "2019-01-17T20:46:24.465608, step: 156, loss: 0.6254830956459045, acc: 0.5234, auc: 0.715, precision: 0.7333, recall: 0.1618\n",
      "start training model\n",
      "2019-01-17T20:46:25.194412, step: 157, loss: 0.6427773237228394, acc: 0.6406, auc: 0.7176, precision: 0.9412, recall: 0.2623\n",
      "2019-01-17T20:46:25.861581, step: 158, loss: 0.6245385408401489, acc: 0.6094, auc: 0.737, precision: 0.8571, recall: 0.2769\n",
      "2019-01-17T20:46:26.641817, step: 159, loss: 0.5949275493621826, acc: 0.625, auc: 0.7348, precision: 0.8095, recall: 0.2787\n",
      "2019-01-17T20:46:27.323235, step: 160, loss: 0.6695108413696289, acc: 0.5938, auc: 0.6814, precision: 0.6333, recall: 0.3167\n",
      "2019-01-17T20:46:28.007945, step: 161, loss: 0.5490555167198181, acc: 0.6406, auc: 0.829, precision: 0.8788, recall: 0.4085\n",
      "2019-01-17T20:46:28.663705, step: 162, loss: 0.5492273569107056, acc: 0.7109, auc: 0.8283, precision: 0.825, recall: 0.5238\n",
      "2019-01-17T20:46:29.341161, step: 163, loss: 0.7908700108528137, acc: 0.5234, auc: 0.5232, precision: 0.5085, recall: 0.4839\n",
      "2019-01-17T20:46:30.013608, step: 164, loss: 0.6279914975166321, acc: 0.5781, auc: 0.833, precision: 0.8696, recall: 0.2817\n",
      "2019-01-17T20:46:30.782475, step: 165, loss: 0.4660700857639313, acc: 0.7891, auc: 0.8638, precision: 0.8039, recall: 0.7069\n",
      "2019-01-17T20:46:31.488338, step: 166, loss: 0.7358698844909668, acc: 0.6953, auc: 0.7006, precision: 0.8293, recall: 0.5152\n",
      "2019-01-17T20:46:32.121884, step: 167, loss: 0.550853967666626, acc: 0.7188, auc: 0.8248, precision: 0.8378, recall: 0.5082\n",
      "2019-01-17T20:46:32.773712, step: 168, loss: 0.5887027978897095, acc: 0.75, auc: 0.8447, precision: 0.6234, recall: 0.9412\n",
      "2019-01-17T20:46:33.454901, step: 169, loss: 0.5424346923828125, acc: 0.7734, auc: 0.795, precision: 0.7436, recall: 0.8657\n",
      "2019-01-17T20:46:34.154774, step: 170, loss: 0.4888213276863098, acc: 0.8047, auc: 0.8588, precision: 0.8548, recall: 0.7681\n",
      "2019-01-17T20:46:34.894714, step: 171, loss: 0.5209356546401978, acc: 0.7969, auc: 0.8159, precision: 0.7826, recall: 0.8308\n",
      "2019-01-17T20:46:35.557826, step: 172, loss: 0.5398147106170654, acc: 0.7656, auc: 0.8207, precision: 0.7121, recall: 0.8103\n",
      "2019-01-17T20:46:36.227869, step: 173, loss: 0.5752072930335999, acc: 0.7266, auc: 0.7446, precision: 0.6905, recall: 0.8657\n",
      "2019-01-17T20:46:36.912959, step: 174, loss: 0.5207488536834717, acc: 0.7578, auc: 0.8269, precision: 0.7286, recall: 0.8095\n",
      "2019-01-17T20:46:37.664204, step: 175, loss: 0.5248954892158508, acc: 0.7578, auc: 0.8187, precision: 0.6818, recall: 0.8182\n",
      "2019-01-17T20:46:38.419190, step: 176, loss: 0.40477249026298523, acc: 0.8438, auc: 0.9157, precision: 0.9375, recall: 0.7258\n",
      "2019-01-17T20:46:39.103528, step: 177, loss: 0.7035812139511108, acc: 0.6953, auc: 0.6908, precision: 0.8889, recall: 0.3019\n",
      "2019-01-17T20:46:39.752706, step: 178, loss: 1.0391857624053955, acc: 0.5234, auc: 0.586, precision: 1.0, recall: 0.0758\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-17T20:46:40.468309, step: 179, loss: 0.7888889312744141, acc: 0.5625, auc: 0.6489, precision: 1.0, recall: 0.082\n",
      "2019-01-17T20:46:41.205041, step: 180, loss: 0.6397580504417419, acc: 0.5938, auc: 0.7627, precision: 1.0, recall: 0.1475\n",
      "2019-01-17T20:46:41.861815, step: 181, loss: 0.6206620335578918, acc: 0.625, auc: 0.6904, precision: 1.0, recall: 0.1724\n",
      "2019-01-17T20:46:42.506069, step: 182, loss: 0.6419763565063477, acc: 0.6172, auc: 0.661, precision: 0.75, recall: 0.4348\n",
      "2019-01-17T20:46:43.175642, step: 183, loss: 0.6942650675773621, acc: 0.6094, auc: 0.6188, precision: 0.6, recall: 0.619\n",
      "2019-01-17T20:46:43.873139, step: 184, loss: 0.6516944169998169, acc: 0.6719, auc: 0.7235, precision: 0.6711, recall: 0.75\n",
      "2019-01-17T20:46:44.535513, step: 185, loss: 0.627532958984375, acc: 0.6562, auc: 0.7415, precision: 0.6709, recall: 0.7465\n",
      "2019-01-17T20:46:45.239200, step: 186, loss: 0.6456259489059448, acc: 0.6953, auc: 0.7483, precision: 0.7021, recall: 0.569\n",
      "2019-01-17T20:46:45.987234, step: 187, loss: 0.6275180578231812, acc: 0.6562, auc: 0.6636, precision: 0.8333, recall: 0.3906\n",
      "2019-01-17T20:46:46.693997, step: 188, loss: 0.6347578763961792, acc: 0.625, auc: 0.6654, precision: 0.7429, recall: 0.4\n",
      "2019-01-17T20:46:47.360225, step: 189, loss: 0.6081603765487671, acc: 0.6484, auc: 0.7027, precision: 0.8095, recall: 0.2931\n",
      "2019-01-17T20:46:48.089260, step: 190, loss: 0.628946840763092, acc: 0.6484, auc: 0.6456, precision: 0.8667, recall: 0.2321\n",
      "2019-01-17T20:46:48.808407, step: 191, loss: 0.5835334062576294, acc: 0.6484, auc: 0.7527, precision: 0.9048, recall: 0.3065\n",
      "2019-01-17T20:46:49.555678, step: 192, loss: 0.5426685810089111, acc: 0.6953, auc: 0.7478, precision: 0.9565, recall: 0.3667\n",
      "2019-01-17T20:46:50.273280, step: 193, loss: 0.564251184463501, acc: 0.7031, auc: 0.7428, precision: 0.9394, recall: 0.4627\n",
      "2019-01-17T20:46:51.018636, step: 194, loss: 0.6156003475189209, acc: 0.6953, auc: 0.707, precision: 0.7857, recall: 0.5238\n",
      "2019-01-17T20:46:51.710431, step: 195, loss: 0.5021859407424927, acc: 0.7969, auc: 0.771, precision: 0.9118, recall: 0.5741\n",
      "2019-01-17T20:46:52.381467, step: 196, loss: 0.5488609671592712, acc: 0.7266, auc: 0.7669, precision: 0.925, recall: 0.5362\n",
      "2019-01-17T20:46:53.109332, step: 197, loss: 0.5006239414215088, acc: 0.7578, auc: 0.8454, precision: 0.925, recall: 0.5692\n",
      "2019-01-17T20:46:53.850513, step: 198, loss: 0.49967026710510254, acc: 0.8359, auc: 0.8381, precision: 0.8621, recall: 0.7937\n",
      "2019-01-17T20:46:54.570667, step: 199, loss: 0.6854937076568604, acc: 0.7891, auc: 0.8458, precision: 0.7324, recall: 0.8667\n",
      "2019-01-17T20:46:55.246504, step: 200, loss: 0.63802170753479, acc: 0.7656, auc: 0.81, precision: 0.7215, recall: 0.8769\n",
      "\n",
      "Evaluation:\n",
      "2019-01-17T20:47:22.546043, step: 200, loss: 0.5283068319161733, acc: 0.7878641025641027, auc: 0.8116153846153845, precision: 0.760176923076923, recall: 0.8486076923076925\n",
      "Saved model checkpoint to ../model/Bi-LSTM/model/my-model-200\n",
      "\n",
      "2019-01-17T20:47:23.804007, step: 201, loss: 0.5798890590667725, acc: 0.7734, auc: 0.7195, precision: 0.7875, recall: 0.84\n",
      "2019-01-17T20:47:24.417205, step: 202, loss: 0.5307494401931763, acc: 0.7031, auc: 0.8206, precision: 0.72, recall: 0.6\n",
      "2019-01-17T20:47:25.103520, step: 203, loss: 0.4882346987724304, acc: 0.7031, auc: 0.8806, precision: 0.8947, recall: 0.5\n",
      "2019-01-17T20:47:25.796691, step: 204, loss: 0.5132101774215698, acc: 0.7578, auc: 0.8319, precision: 0.8889, recall: 0.6061\n",
      "2019-01-17T20:47:26.515448, step: 205, loss: 0.5413744449615479, acc: 0.7266, auc: 0.8155, precision: 0.9444, recall: 0.5075\n",
      "2019-01-17T20:47:27.271795, step: 206, loss: 0.6456952691078186, acc: 0.6641, auc: 0.7296, precision: 0.8947, recall: 0.2931\n",
      "2019-01-17T20:47:27.999840, step: 207, loss: 0.8234497904777527, acc: 0.5625, auc: 0.6206, precision: 1.0, recall: 0.1765\n",
      "2019-01-17T20:47:28.643360, step: 208, loss: 0.8323314785957336, acc: 0.5156, auc: 0.6071, precision: 1.0, recall: 0.1143\n",
      "2019-01-17T20:47:29.354166, step: 209, loss: 0.8129873275756836, acc: 0.5312, auc: 0.5528, precision: 1.0, recall: 0.0323\n",
      "2019-01-17T20:47:30.093409, step: 210, loss: 0.7533226013183594, acc: 0.5391, auc: 0.5328, precision: 1.0, recall: 0.0167\n",
      "2019-01-17T20:47:30.807870, step: 211, loss: 0.7422910928726196, acc: 0.5, auc: 0.6109, precision: 1.0, recall: 0.0303\n",
      "2019-01-17T20:47:31.582351, step: 212, loss: 0.7197431325912476, acc: 0.4922, auc: 0.5436, precision: 1.0, recall: 0.058\n",
      "2019-01-17T20:47:32.292164, step: 213, loss: 0.7092775106430054, acc: 0.5078, auc: 0.4967, precision: 1.0, recall: 0.0308\n",
      "2019-01-17T20:47:32.957593, step: 214, loss: 0.6875149607658386, acc: 0.4766, auc: 0.5691, precision: 1.0, recall: 0.0147\n",
      "2019-01-17T20:47:33.684859, step: 215, loss: 0.678402841091156, acc: 0.4922, auc: 0.584, precision: 1.0, recall: 0.0299\n",
      "2019-01-17T20:47:34.342308, step: 216, loss: 0.6998541355133057, acc: 0.5312, auc: 0.4735, precision: 0.6667, recall: 0.0952\n",
      "2019-01-17T20:47:35.036179, step: 217, loss: 0.7268569469451904, acc: 0.5078, auc: 0.464, precision: 0.4, recall: 0.1356\n",
      "2019-01-17T20:47:35.755766, step: 218, loss: 0.7251240611076355, acc: 0.5234, auc: 0.4861, precision: 0.4643, recall: 0.2203\n",
      "2019-01-17T20:47:36.508235, step: 219, loss: 0.7128593921661377, acc: 0.5078, auc: 0.5099, precision: 0.5, recall: 0.3016\n",
      "2019-01-17T20:47:37.208353, step: 220, loss: 0.7066915035247803, acc: 0.4688, auc: 0.4443, precision: 0.5312, recall: 0.2429\n",
      "2019-01-17T20:47:37.921894, step: 221, loss: 0.7152722477912903, acc: 0.5547, auc: 0.5407, precision: 0.5789, recall: 0.1833\n",
      "2019-01-17T20:47:38.624917, step: 222, loss: 0.6951322555541992, acc: 0.5156, auc: 0.5738, precision: 0.6, recall: 0.1818\n",
      "2019-01-17T20:47:39.269843, step: 223, loss: 0.700239896774292, acc: 0.5, auc: 0.5248, precision: 0.5455, recall: 0.0923\n",
      "2019-01-17T20:47:39.924604, step: 224, loss: 0.694603681564331, acc: 0.5078, auc: 0.5534, precision: 1.0, recall: 0.0308\n",
      "2019-01-17T20:47:40.600736, step: 225, loss: 0.7098262310028076, acc: 0.5078, auc: 0.4986, precision: 0.0, recall: 0.0\n",
      "2019-01-17T20:47:41.315663, step: 226, loss: 0.68128901720047, acc: 0.4531, auc: 0.606, precision: 0.0, recall: 0.0\n",
      "2019-01-17T20:47:42.020411, step: 227, loss: 0.7016661167144775, acc: 0.5703, auc: 0.5253, precision: 0.0, recall: 0.0\n",
      "2019-01-17T20:47:42.790356, step: 228, loss: 0.6891276240348816, acc: 0.4531, auc: 0.5446, precision: 1.0, recall: 0.0141\n",
      "2019-01-17T20:47:43.504209, step: 229, loss: 0.684619128704071, acc: 0.5625, auc: 0.5755, precision: 1.0, recall: 0.0175\n",
      "2019-01-17T20:47:44.195901, step: 230, loss: 0.6928102970123291, acc: 0.5, auc: 0.5039, precision: 0.0, recall: 0.0\n",
      "2019-01-17T20:47:44.917607, step: 231, loss: 0.6852777004241943, acc: 0.5469, auc: 0.5537, precision: 0.0, recall: 0.0\n",
      "2019-01-17T20:47:45.650829, step: 232, loss: 0.6938521862030029, acc: 0.5156, auc: 0.513, precision: 0.0, recall: 0.0\n",
      "2019-01-17T20:47:46.356380, step: 233, loss: 0.6824314594268799, acc: 0.5312, auc: 0.5924, precision: 0.0, recall: 0.0\n",
      "2019-01-17T20:47:47.059054, step: 234, loss: 0.6802746653556824, acc: 0.6016, auc: 0.5261, precision: 0.0, recall: 0.0\n",
      "2019-01-17T20:47:47.710605, step: 235, loss: 0.6938294172286987, acc: 0.4922, auc: 0.5165, precision: 0.0, recall: 0.0\n",
      "2019-01-17T20:47:48.385393, step: 236, loss: 0.6883245706558228, acc: 0.5, auc: 0.5668, precision: 1.0, recall: 0.0154\n",
      "2019-01-17T20:47:49.092674, step: 237, loss: 0.6723737716674805, acc: 0.4844, auc: 0.6676, precision: 1.0, recall: 0.0435\n",
      "2019-01-17T20:47:49.848337, step: 238, loss: 0.688402533531189, acc: 0.4531, auc: 0.6401, precision: 0.0, recall: 0.0\n",
      "2019-01-17T20:47:50.479128, step: 239, loss: 0.6971632838249207, acc: 0.4297, auc: 0.5998, precision: 0.0, recall: 0.0\n",
      "2019-01-17T20:47:51.138241, step: 240, loss: 0.6874208450317383, acc: 0.5, auc: 0.5684, precision: 0.0, recall: 0.0\n",
      "2019-01-17T20:47:51.804637, step: 241, loss: 0.697012186050415, acc: 0.4688, auc: 0.5216, precision: 0.0, recall: 0.0\n",
      "2019-01-17T20:47:52.525658, step: 242, loss: 0.6899176836013794, acc: 0.5078, auc: 0.5514, precision: 0.0, recall: 0.0\n",
      "2019-01-17T20:47:53.260308, step: 243, loss: 0.6804730296134949, acc: 0.4766, auc: 0.6205, precision: 0.0, recall: 0.0\n",
      "2019-01-17T20:47:54.012183, step: 244, loss: 0.6898841857910156, acc: 0.4609, auc: 0.5738, precision: 0.0, recall: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-17T20:47:54.756564, step: 245, loss: 0.6842005848884583, acc: 0.4609, auc: 0.5851, precision: 0.0, recall: 0.0\n",
      "2019-01-17T20:47:55.415266, step: 246, loss: 0.692381739616394, acc: 0.4844, auc: 0.526, precision: 0.0, recall: 0.0\n",
      "2019-01-17T20:47:56.102974, step: 247, loss: 0.6576493978500366, acc: 0.6016, auc: 0.75, precision: 1.0, recall: 0.0893\n",
      "2019-01-17T20:47:56.763933, step: 248, loss: 0.6651912331581116, acc: 0.5391, auc: 0.7328, precision: 1.0, recall: 0.0328\n",
      "2019-01-17T20:47:57.469703, step: 249, loss: 0.6721345782279968, acc: 0.5703, auc: 0.5994, precision: 1.0, recall: 0.0678\n",
      "2019-01-17T20:47:58.181584, step: 250, loss: 0.6672579050064087, acc: 0.5391, auc: 0.665, precision: 1.0, recall: 0.0328\n",
      "2019-01-17T20:47:58.966400, step: 251, loss: 0.6653781533241272, acc: 0.5859, auc: 0.6518, precision: 1.0, recall: 0.0364\n",
      "2019-01-17T20:47:59.643636, step: 252, loss: 0.6817029714584351, acc: 0.5078, auc: 0.5562, precision: 1.0, recall: 0.0156\n",
      "2019-01-17T20:48:00.359417, step: 253, loss: 0.6603942513465881, acc: 0.4844, auc: 0.6521, precision: 1.0, recall: 0.0959\n",
      "2019-01-17T20:48:01.089175, step: 254, loss: 0.6619454026222229, acc: 0.5547, auc: 0.6845, precision: 1.0, recall: 0.0806\n",
      "2019-01-17T20:48:01.784812, step: 255, loss: 0.6643669605255127, acc: 0.6094, auc: 0.7097, precision: 0.8, recall: 0.0755\n",
      "2019-01-17T20:48:02.457155, step: 256, loss: 0.6711363196372986, acc: 0.5078, auc: 0.6012, precision: 0.7143, recall: 0.0758\n",
      "2019-01-17T20:48:03.192499, step: 257, loss: 0.6554300785064697, acc: 0.5703, auc: 0.6772, precision: 0.8889, recall: 0.129\n",
      "2019-01-17T20:48:03.921143, step: 258, loss: 0.6769484877586365, acc: 0.5, auc: 0.5708, precision: 0.7059, recall: 0.169\n",
      "2019-01-17T20:48:04.666261, step: 259, loss: 0.6449591517448425, acc: 0.5469, auc: 0.7561, precision: 1.0, recall: 0.0938\n",
      "2019-01-17T20:48:05.385872, step: 260, loss: 0.6405107975006104, acc: 0.6328, auc: 0.7951, precision: 0.9091, recall: 0.1786\n",
      "2019-01-17T20:48:06.075358, step: 261, loss: 0.6414847373962402, acc: 0.5625, auc: 0.7608, precision: 1.0, recall: 0.0667\n",
      "2019-01-17T20:48:06.759584, step: 262, loss: 0.6487024426460266, acc: 0.5703, auc: 0.7108, precision: 0.8571, recall: 0.1\n",
      "2019-01-17T20:48:07.443300, step: 263, loss: 0.6403902769088745, acc: 0.5625, auc: 0.7275, precision: 0.9, recall: 0.1406\n",
      "2019-01-17T20:48:08.137728, step: 264, loss: 0.6114001870155334, acc: 0.5547, auc: 0.839, precision: 0.7778, recall: 0.1129\n",
      "2019-01-17T20:48:08.817527, step: 265, loss: 0.6624013185501099, acc: 0.5625, auc: 0.676, precision: 0.6364, recall: 0.1186\n",
      "2019-01-17T20:48:09.436755, step: 266, loss: 0.663518488407135, acc: 0.5156, auc: 0.6483, precision: 0.5, recall: 0.0968\n",
      "2019-01-17T20:48:10.108841, step: 267, loss: 0.6223616600036621, acc: 0.6094, auc: 0.7942, precision: 0.8182, recall: 0.1579\n",
      "2019-01-17T20:48:10.779140, step: 268, loss: 0.6560021638870239, acc: 0.5391, auc: 0.7272, precision: 0.6875, recall: 0.1692\n",
      "2019-01-17T20:48:11.512991, step: 269, loss: 0.6452035307884216, acc: 0.4922, auc: 0.701, precision: 0.6667, recall: 0.1429\n",
      "2019-01-17T20:48:12.261977, step: 270, loss: 0.6034618020057678, acc: 0.6328, auc: 0.7666, precision: 0.8333, recall: 0.1818\n",
      "2019-01-17T20:48:12.984578, step: 271, loss: 0.5930346846580505, acc: 0.5938, auc: 0.7949, precision: 1.0, recall: 0.2\n",
      "2019-01-17T20:48:13.655858, step: 272, loss: 0.5495615005493164, acc: 0.6094, auc: 0.8986, precision: 1.0, recall: 0.1935\n",
      "2019-01-17T20:48:14.361189, step: 273, loss: 0.6348251700401306, acc: 0.5312, auc: 0.7504, precision: 0.9091, recall: 0.1449\n",
      "2019-01-17T20:48:14.999283, step: 274, loss: 0.5677170157432556, acc: 0.5859, auc: 0.8451, precision: 0.9091, recall: 0.2817\n",
      "2019-01-17T20:48:15.724119, step: 275, loss: 0.6050306558609009, acc: 0.5781, auc: 0.757, precision: 0.6552, recall: 0.3016\n",
      "2019-01-17T20:48:16.442806, step: 276, loss: 0.7121620774269104, acc: 0.6172, auc: 0.6362, precision: 0.5593, recall: 0.5893\n",
      "2019-01-17T20:48:17.167893, step: 277, loss: 0.7104089856147766, acc: 0.5156, auc: 0.5407, precision: 0.5405, recall: 0.5882\n",
      "2019-01-17T20:48:17.923715, step: 278, loss: 0.6811291575431824, acc: 0.5938, auc: 0.6291, precision: 0.6154, recall: 0.597\n",
      "2019-01-17T20:48:18.644737, step: 279, loss: 0.7834725975990295, acc: 0.5078, auc: 0.5452, precision: 0.4324, recall: 0.6038\n",
      "2019-01-17T20:48:19.340420, step: 280, loss: 0.6823222637176514, acc: 0.6172, auc: 0.6283, precision: 0.6538, recall: 0.5231\n",
      "2019-01-17T20:48:20.029675, step: 281, loss: 0.6575378179550171, acc: 0.5625, auc: 0.6357, precision: 0.6458, recall: 0.4429\n",
      "2019-01-17T20:48:20.718688, step: 282, loss: 0.6848893165588379, acc: 0.6016, auc: 0.6251, precision: 0.6296, recall: 0.2931\n",
      "2019-01-17T20:48:21.419397, step: 283, loss: 0.6544623374938965, acc: 0.5625, auc: 0.6484, precision: 0.6842, recall: 0.2063\n",
      "2019-01-17T20:48:22.144156, step: 284, loss: 0.636607825756073, acc: 0.6094, auc: 0.6879, precision: 1.0, recall: 0.1379\n",
      "2019-01-17T20:48:22.789013, step: 285, loss: 0.665042519569397, acc: 0.5547, auc: 0.5793, precision: 0.8182, recall: 0.1406\n",
      "2019-01-17T20:48:23.540072, step: 286, loss: 0.6671801805496216, acc: 0.5078, auc: 0.6156, precision: 0.8, recall: 0.0606\n",
      "2019-01-17T20:48:24.233029, step: 287, loss: 0.6639437079429626, acc: 0.5469, auc: 0.5733, precision: 1.0, recall: 0.1343\n",
      "2019-01-17T20:48:24.905492, step: 288, loss: 0.6657787561416626, acc: 0.5156, auc: 0.6439, precision: 1.0, recall: 0.1268\n",
      "2019-01-17T20:48:25.551098, step: 289, loss: 0.5814327001571655, acc: 0.6406, auc: 0.7614, precision: 1.0, recall: 0.2459\n",
      "2019-01-17T20:48:26.208409, step: 290, loss: 0.6662481427192688, acc: 0.625, auc: 0.5889, precision: 0.8125, recall: 0.2241\n",
      "2019-01-17T20:48:26.866358, step: 291, loss: 0.630043089389801, acc: 0.5625, auc: 0.6561, precision: 0.7826, recall: 0.2609\n",
      "2019-01-17T20:48:27.668163, step: 292, loss: 0.5817441940307617, acc: 0.6953, auc: 0.7773, precision: 0.9286, recall: 0.4127\n",
      "2019-01-17T20:48:28.332223, step: 293, loss: 0.5954059362411499, acc: 0.6484, auc: 0.7216, precision: 0.7632, recall: 0.4462\n",
      "2019-01-17T20:48:28.989728, step: 294, loss: 0.6395289897918701, acc: 0.5938, auc: 0.6333, precision: 0.7931, recall: 0.3333\n",
      "2019-01-17T20:48:29.627626, step: 295, loss: 0.6871556043624878, acc: 0.5781, auc: 0.6181, precision: 0.6591, recall: 0.4265\n",
      "2019-01-17T20:48:30.359895, step: 296, loss: 0.6349383592605591, acc: 0.6172, auc: 0.6466, precision: 0.7429, recall: 0.3939\n",
      "2019-01-17T20:48:31.039431, step: 297, loss: 0.5697710514068604, acc: 0.6484, auc: 0.7674, precision: 0.7931, recall: 0.371\n",
      "2019-01-17T20:48:31.680986, step: 298, loss: 0.5777229070663452, acc: 0.6016, auc: 0.7691, precision: 0.9, recall: 0.36\n",
      "2019-01-17T20:48:32.380397, step: 299, loss: 0.6135008335113525, acc: 0.5938, auc: 0.7333, precision: 0.9, recall: 0.2647\n",
      "2019-01-17T20:48:33.092416, step: 300, loss: 0.6611713171005249, acc: 0.5078, auc: 0.6987, precision: 0.8235, recall: 0.1892\n",
      "\n",
      "Evaluation:\n",
      "2019-01-17T20:49:00.188718, step: 300, loss: 0.5968129482024755, acc: 0.6243948717948719, auc: 0.8019179487179487, precision: 0.8808564102564105, recall: 0.2985974358974359\n",
      "Saved model checkpoint to ../model/Bi-LSTM/model/my-model-300\n",
      "\n",
      "2019-01-17T20:49:01.433915, step: 301, loss: 0.5359102487564087, acc: 0.6641, auc: 0.7958, precision: 1.0, recall: 0.3385\n",
      "2019-01-17T20:49:02.081031, step: 302, loss: 0.5796693563461304, acc: 0.5859, auc: 0.8363, precision: 0.8889, recall: 0.3243\n",
      "2019-01-17T20:49:02.745426, step: 303, loss: 0.5539869070053101, acc: 0.6328, auc: 0.829, precision: 0.9286, recall: 0.3662\n",
      "2019-01-17T20:49:03.463249, step: 304, loss: 0.546066164970398, acc: 0.6562, auc: 0.8, precision: 0.8065, recall: 0.3968\n",
      "2019-01-17T20:49:04.127767, step: 305, loss: 0.5441184639930725, acc: 0.6562, auc: 0.8233, precision: 0.9167, recall: 0.4459\n",
      "2019-01-17T20:49:04.858278, step: 306, loss: 0.4817497730255127, acc: 0.6875, auc: 0.9, precision: 0.9375, recall: 0.4412\n",
      "2019-01-17T20:49:05.516430, step: 307, loss: 0.5801993608474731, acc: 0.7656, auc: 0.8158, precision: 0.7857, recall: 0.6111\n",
      "2019-01-17T20:49:06.212358, step: 308, loss: 0.5207374691963196, acc: 0.6719, auc: 0.8351, precision: 0.7907, recall: 0.5075\n",
      "2019-01-17T20:49:06.930369, step: 309, loss: 0.5442332029342651, acc: 0.6875, auc: 0.8284, precision: 0.7619, recall: 0.5161\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-17T20:49:07.691325, step: 310, loss: 0.4944465160369873, acc: 0.7344, auc: 0.8552, precision: 0.8372, recall: 0.5714\n",
      "2019-01-17T20:49:08.425955, step: 311, loss: 0.5511128902435303, acc: 0.7344, auc: 0.786, precision: 0.8889, recall: 0.5797\n",
      "2019-01-17T20:49:09.169826, step: 312, loss: 0.5485207438468933, acc: 0.7734, auc: 0.8099, precision: 0.8125, recall: 0.7536\n",
      "start training model\n",
      "2019-01-17T20:49:09.926288, step: 313, loss: 0.48245149850845337, acc: 0.8438, auc: 0.8611, precision: 0.8065, recall: 0.8621\n",
      "2019-01-17T20:49:10.637894, step: 314, loss: 0.47662705183029175, acc: 0.8438, auc: 0.8446, precision: 0.8052, recall: 0.9254\n",
      "2019-01-17T20:49:11.311624, step: 315, loss: 0.5596975088119507, acc: 0.7656, auc: 0.7927, precision: 0.7253, recall: 0.9296\n",
      "2019-01-17T20:49:11.996185, step: 316, loss: 0.6051561236381531, acc: 0.7031, auc: 0.7476, precision: 0.6806, recall: 0.7656\n",
      "2019-01-17T20:49:12.626177, step: 317, loss: 0.5793973207473755, acc: 0.625, auc: 0.7438, precision: 0.6923, recall: 0.4286\n",
      "2019-01-17T20:49:13.379765, step: 318, loss: 0.6739082932472229, acc: 0.5625, auc: 0.6271, precision: 0.5652, recall: 0.2203\n",
      "2019-01-17T20:49:14.068048, step: 319, loss: 0.6014643907546997, acc: 0.6172, auc: 0.7338, precision: 0.9231, recall: 0.2\n",
      "2019-01-17T20:49:14.792490, step: 320, loss: 0.6356296539306641, acc: 0.5391, auc: 0.7028, precision: 0.8125, recall: 0.1884\n",
      "2019-01-17T20:49:15.517306, step: 321, loss: 0.6476199626922607, acc: 0.5703, auc: 0.6291, precision: 0.6071, recall: 0.2787\n",
      "2019-01-17T20:49:16.228763, step: 322, loss: 0.6325269937515259, acc: 0.5859, auc: 0.6838, precision: 0.6136, recall: 0.4286\n",
      "2019-01-17T20:49:16.915611, step: 323, loss: 0.6114428043365479, acc: 0.625, auc: 0.7055, precision: 0.6842, recall: 0.4194\n",
      "2019-01-17T20:49:17.547938, step: 324, loss: 0.6212446689605713, acc: 0.6719, auc: 0.6845, precision: 0.6774, recall: 0.3962\n",
      "2019-01-17T20:49:18.233697, step: 325, loss: 0.571520209312439, acc: 0.6875, auc: 0.7436, precision: 0.7667, recall: 0.4107\n",
      "2019-01-17T20:49:18.954832, step: 326, loss: 0.5934388041496277, acc: 0.6562, auc: 0.7265, precision: 0.9444, recall: 0.2833\n",
      "2019-01-17T20:49:19.682930, step: 327, loss: 0.6510966420173645, acc: 0.5547, auc: 0.7352, precision: 0.9444, recall: 0.2329\n",
      "2019-01-17T20:49:20.410722, step: 328, loss: 0.6375313401222229, acc: 0.6172, auc: 0.7033, precision: 0.85, recall: 0.2698\n",
      "2019-01-17T20:49:21.090465, step: 329, loss: 0.6651655435562134, acc: 0.5938, auc: 0.6426, precision: 0.6579, recall: 0.3906\n",
      "2019-01-17T20:49:21.753860, step: 330, loss: 0.6546941995620728, acc: 0.6562, auc: 0.7322, precision: 0.6452, recall: 0.6452\n",
      "2019-01-17T20:49:22.409458, step: 331, loss: 0.5590007901191711, acc: 0.6875, auc: 0.78, precision: 0.7358, recall: 0.6\n",
      "2019-01-17T20:49:23.151358, step: 332, loss: 0.5801092386245728, acc: 0.6172, auc: 0.7363, precision: 0.7742, recall: 0.3636\n",
      "2019-01-17T20:49:23.860350, step: 333, loss: 0.5688340067863464, acc: 0.6797, auc: 0.7554, precision: 0.8966, recall: 0.4062\n",
      "2019-01-17T20:49:24.561608, step: 334, loss: 0.5668615102767944, acc: 0.6016, auc: 0.786, precision: 0.95, recall: 0.2754\n",
      "2019-01-17T20:49:25.253178, step: 335, loss: 0.5484137535095215, acc: 0.6719, auc: 0.7931, precision: 0.8684, recall: 0.4714\n",
      "2019-01-17T20:49:25.879747, step: 336, loss: 0.555869460105896, acc: 0.6094, auc: 0.7868, precision: 0.7143, recall: 0.493\n",
      "2019-01-17T20:49:26.584138, step: 337, loss: 0.5383281707763672, acc: 0.6953, auc: 0.8261, precision: 0.7436, recall: 0.5\n",
      "2019-01-17T20:49:27.299178, step: 338, loss: 0.5673582553863525, acc: 0.6484, auc: 0.7882, precision: 0.6829, recall: 0.4667\n",
      "2019-01-17T20:49:28.081748, step: 339, loss: 0.4362708330154419, acc: 0.7891, auc: 0.8857, precision: 0.8413, recall: 0.7571\n",
      "2019-01-17T20:49:28.763873, step: 340, loss: 0.48306456208229065, acc: 0.7812, auc: 0.8429, precision: 0.75, recall: 0.8226\n",
      "2019-01-17T20:49:29.504539, step: 341, loss: 0.39613422751426697, acc: 0.8281, auc: 0.8889, precision: 0.8475, recall: 0.7937\n",
      "2019-01-17T20:49:30.247248, step: 342, loss: 0.42656952142715454, acc: 0.8125, auc: 0.9134, precision: 0.9615, recall: 0.6944\n",
      "2019-01-17T20:49:30.903770, step: 343, loss: 0.4195758104324341, acc: 0.8281, auc: 0.9043, precision: 0.9245, recall: 0.7313\n",
      "2019-01-17T20:49:31.631189, step: 344, loss: 0.44161123037338257, acc: 0.8359, auc: 0.8737, precision: 0.8793, recall: 0.7846\n",
      "2019-01-17T20:49:32.332243, step: 345, loss: 0.3756489157676697, acc: 0.875, auc: 0.9014, precision: 0.8923, recall: 0.8657\n",
      "2019-01-17T20:49:33.059535, step: 346, loss: 0.39296942949295044, acc: 0.8594, auc: 0.9023, precision: 0.8108, recall: 0.9375\n",
      "2019-01-17T20:49:33.771031, step: 347, loss: 0.4081553816795349, acc: 0.8203, auc: 0.9018, precision: 0.7778, recall: 0.8889\n",
      "2019-01-17T20:49:34.484366, step: 348, loss: 0.6555950045585632, acc: 0.7031, auc: 0.8113, precision: 0.6341, recall: 0.8667\n",
      "2019-01-17T20:49:35.240622, step: 349, loss: 0.44234174489974976, acc: 0.7969, auc: 0.8896, precision: 0.76, recall: 0.8769\n",
      "2019-01-17T20:49:35.943066, step: 350, loss: 0.5935104489326477, acc: 0.7656, auc: 0.8138, precision: 0.875, recall: 0.6364\n",
      "2019-01-17T20:49:36.687414, step: 351, loss: 0.48802655935287476, acc: 0.8047, auc: 0.8569, precision: 0.9333, recall: 0.6562\n",
      "2019-01-17T20:49:37.372996, step: 352, loss: 0.7230441570281982, acc: 0.6875, auc: 0.7492, precision: 0.9655, recall: 0.4179\n",
      "2019-01-17T20:49:38.025327, step: 353, loss: 0.7584875822067261, acc: 0.6172, auc: 0.6542, precision: 0.7619, recall: 0.2667\n",
      "2019-01-17T20:49:38.692711, step: 354, loss: 0.6268509030342102, acc: 0.6797, auc: 0.7542, precision: 0.9524, recall: 0.3333\n",
      "2019-01-17T20:49:39.365918, step: 355, loss: 0.6843355298042297, acc: 0.6172, auc: 0.6747, precision: 0.7742, recall: 0.3636\n",
      "2019-01-17T20:49:40.054112, step: 356, loss: 0.7769143581390381, acc: 0.5391, auc: 0.5491, precision: 0.5278, recall: 0.3115\n",
      "2019-01-17T20:49:40.773164, step: 357, loss: 0.6904469728469849, acc: 0.6016, auc: 0.6469, precision: 0.6087, recall: 0.459\n",
      "2019-01-17T20:49:41.443281, step: 358, loss: 0.624452531337738, acc: 0.6484, auc: 0.7033, precision: 0.7561, recall: 0.4697\n",
      "2019-01-17T20:49:42.095928, step: 359, loss: 0.6141485571861267, acc: 0.6797, auc: 0.7235, precision: 0.7143, recall: 0.4464\n",
      "2019-01-17T20:49:42.818153, step: 360, loss: 0.6020017862319946, acc: 0.5859, auc: 0.6987, precision: 0.6216, recall: 0.371\n",
      "2019-01-17T20:49:43.527613, step: 361, loss: 0.5780701637268066, acc: 0.6562, auc: 0.7756, precision: 0.7949, recall: 0.4627\n",
      "2019-01-17T20:49:44.267046, step: 362, loss: 0.5811221599578857, acc: 0.6484, auc: 0.7507, precision: 0.7778, recall: 0.5\n",
      "2019-01-17T20:49:44.997778, step: 363, loss: 0.579797089099884, acc: 0.7109, auc: 0.7767, precision: 0.7018, recall: 0.6667\n",
      "2019-01-17T20:49:45.646841, step: 364, loss: 0.6514114141464233, acc: 0.6406, auc: 0.7569, precision: 0.5641, recall: 0.7857\n",
      "2019-01-17T20:49:46.375899, step: 365, loss: 0.6913147568702698, acc: 0.6016, auc: 0.63, precision: 0.5636, recall: 0.5345\n",
      "2019-01-17T20:49:47.100738, step: 366, loss: 0.6191427707672119, acc: 0.6172, auc: 0.7018, precision: 0.7188, recall: 0.3651\n",
      "2019-01-17T20:49:47.810810, step: 367, loss: 0.5818924307823181, acc: 0.6328, auc: 0.7315, precision: 0.75, recall: 0.3051\n",
      "2019-01-17T20:49:48.471537, step: 368, loss: 0.5567254424095154, acc: 0.5781, auc: 0.8261, precision: 0.6667, recall: 0.1379\n",
      "2019-01-17T20:49:49.109966, step: 369, loss: 0.5966852903366089, acc: 0.5781, auc: 0.8038, precision: 0.9286, recall: 0.197\n",
      "2019-01-17T20:49:49.792102, step: 370, loss: 0.5640537142753601, acc: 0.625, auc: 0.8193, precision: 0.9, recall: 0.2812\n",
      "2019-01-17T20:49:50.447906, step: 371, loss: 0.49428218603134155, acc: 0.6797, auc: 0.8956, precision: 1.0, recall: 0.3971\n",
      "2019-01-17T20:49:51.094572, step: 372, loss: 0.5674087405204773, acc: 0.6797, auc: 0.7781, precision: 0.7451, recall: 0.5758\n",
      "2019-01-17T20:49:51.807147, step: 373, loss: 0.5126674175262451, acc: 0.8047, auc: 0.8611, precision: 0.803, recall: 0.8154\n",
      "2019-01-17T20:49:52.508179, step: 374, loss: 0.4815976023674011, acc: 0.7422, auc: 0.8693, precision: 0.8103, recall: 0.6812\n",
      "2019-01-17T20:49:53.218853, step: 375, loss: 0.5489921569824219, acc: 0.6641, auc: 0.7821, precision: 0.75, recall: 0.5915\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-17T20:49:53.903896, step: 376, loss: 0.46088549494743347, acc: 0.7891, auc: 0.8871, precision: 0.7037, recall: 0.7755\n",
      "2019-01-17T20:49:54.612633, step: 377, loss: 0.4878399968147278, acc: 0.7188, auc: 0.8471, precision: 0.7667, recall: 0.6765\n",
      "2019-01-17T20:49:55.274219, step: 378, loss: 0.4330485463142395, acc: 0.8047, auc: 0.8879, precision: 0.8103, recall: 0.7705\n",
      "2019-01-17T20:49:55.921451, step: 379, loss: 0.46420031785964966, acc: 0.8125, auc: 0.8623, precision: 0.8333, recall: 0.8088\n",
      "2019-01-17T20:49:56.625943, step: 380, loss: 0.4754188060760498, acc: 0.7969, auc: 0.8558, precision: 0.7571, recall: 0.8548\n",
      "2019-01-17T20:49:57.302976, step: 381, loss: 0.40651968121528625, acc: 0.8203, auc: 0.885, precision: 0.8182, recall: 0.8308\n",
      "2019-01-17T20:49:57.963947, step: 382, loss: 0.5333477258682251, acc: 0.7578, auc: 0.8374, precision: 0.7324, recall: 0.8125\n",
      "2019-01-17T20:49:58.747731, step: 383, loss: 0.4632139205932617, acc: 0.7969, auc: 0.8503, precision: 0.7879, recall: 0.8125\n",
      "2019-01-17T20:49:59.437884, step: 384, loss: 0.6856799125671387, acc: 0.6719, auc: 0.8118, precision: 0.6, recall: 0.9344\n",
      "2019-01-17T20:50:00.122260, step: 385, loss: 0.7366660833358765, acc: 0.6484, auc: 0.8353, precision: 0.5755, recall: 1.0\n",
      "2019-01-17T20:50:00.855152, step: 386, loss: 0.7207475304603577, acc: 0.6172, auc: 0.6896, precision: 0.5876, recall: 0.8636\n",
      "2019-01-17T20:50:01.527181, step: 387, loss: 0.642776608467102, acc: 0.6484, auc: 0.7483, precision: 0.5977, recall: 0.8387\n",
      "2019-01-17T20:50:02.193162, step: 388, loss: 0.6772900223731995, acc: 0.6328, auc: 0.697, precision: 0.537, recall: 0.5686\n",
      "2019-01-17T20:50:02.855665, step: 389, loss: 0.5935571789741516, acc: 0.6719, auc: 0.7037, precision: 0.6765, recall: 0.4259\n",
      "2019-01-17T20:50:03.560704, step: 390, loss: 0.5899763107299805, acc: 0.7188, auc: 0.7633, precision: 0.7812, recall: 0.463\n",
      "2019-01-17T20:50:04.252164, step: 391, loss: 0.5072923898696899, acc: 0.7344, auc: 0.8155, precision: 0.8857, recall: 0.5082\n",
      "2019-01-17T20:50:04.985815, step: 392, loss: 0.5650444030761719, acc: 0.7266, auc: 0.745, precision: 0.8387, recall: 0.4643\n",
      "2019-01-17T20:50:05.720647, step: 393, loss: 0.6824861764907837, acc: 0.6016, auc: 0.66, precision: 0.9091, recall: 0.2899\n",
      "2019-01-17T20:50:06.424795, step: 394, loss: 0.6959996223449707, acc: 0.5781, auc: 0.646, precision: 0.7826, recall: 0.2687\n",
      "2019-01-17T20:50:07.079734, step: 395, loss: 0.7172973155975342, acc: 0.5703, auc: 0.6149, precision: 0.8214, recall: 0.3151\n",
      "2019-01-17T20:50:07.746243, step: 396, loss: 0.6002534031867981, acc: 0.5781, auc: 0.7677, precision: 0.8696, recall: 0.2817\n",
      "2019-01-17T20:50:08.356090, step: 397, loss: 0.7371786236763, acc: 0.5781, auc: 0.54, precision: 0.7083, recall: 0.2656\n",
      "2019-01-17T20:50:09.029077, step: 398, loss: 0.6121585369110107, acc: 0.6406, auc: 0.6943, precision: 0.875, recall: 0.4\n",
      "2019-01-17T20:50:09.761479, step: 399, loss: 0.6136305332183838, acc: 0.6016, auc: 0.7298, precision: 0.7111, recall: 0.4571\n",
      "2019-01-17T20:50:10.471911, step: 400, loss: 0.6275914311408997, acc: 0.6797, auc: 0.7271, precision: 0.6889, recall: 0.5345\n",
      "\n",
      "Evaluation:\n",
      "2019-01-17T20:50:37.580205, step: 400, loss: 0.6477406957210639, acc: 0.5971615384615385, auc: 0.7233333333333334, precision: 0.6675666666666665, recall: 0.4025589743589744\n",
      "Saved model checkpoint to ../model/Bi-LSTM/model/my-model-400\n",
      "\n",
      "2019-01-17T20:50:38.781820, step: 401, loss: 0.6440038084983826, acc: 0.6016, auc: 0.6352, precision: 0.7111, recall: 0.4571\n",
      "2019-01-17T20:50:39.461385, step: 402, loss: 0.6280263662338257, acc: 0.5859, auc: 0.6654, precision: 0.7143, recall: 0.3676\n",
      "2019-01-17T20:50:40.263233, step: 403, loss: 0.6122138500213623, acc: 0.6719, auc: 0.7307, precision: 0.7188, recall: 0.4107\n",
      "2019-01-17T20:50:40.961786, step: 404, loss: 0.6324637532234192, acc: 0.6016, auc: 0.6857, precision: 0.6522, recall: 0.2586\n",
      "2019-01-17T20:50:41.634709, step: 405, loss: 0.5897126793861389, acc: 0.6484, auc: 0.7684, precision: 0.8, recall: 0.3333\n",
      "2019-01-17T20:50:42.344911, step: 406, loss: 0.626075804233551, acc: 0.5781, auc: 0.7551, precision: 0.7857, recall: 0.1774\n",
      "2019-01-17T20:50:43.048002, step: 407, loss: 0.5628771185874939, acc: 0.6406, auc: 0.8396, precision: 1.0, recall: 0.2923\n",
      "2019-01-17T20:50:43.744916, step: 408, loss: 0.5692678093910217, acc: 0.6406, auc: 0.826, precision: 1.0, recall: 0.3235\n",
      "2019-01-17T20:50:44.447626, step: 409, loss: 0.5360381603240967, acc: 0.6328, auc: 0.8443, precision: 1.0, recall: 0.2419\n",
      "2019-01-17T20:50:45.129658, step: 410, loss: 0.5539181232452393, acc: 0.6797, auc: 0.8443, precision: 1.0, recall: 0.2807\n",
      "2019-01-17T20:50:45.829688, step: 411, loss: 0.5278910994529724, acc: 0.6641, auc: 0.8401, precision: 1.0, recall: 0.2182\n",
      "2019-01-17T20:50:46.468882, step: 412, loss: 0.5841904878616333, acc: 0.6328, auc: 0.8101, precision: 0.8947, recall: 0.2742\n",
      "2019-01-17T20:50:47.107090, step: 413, loss: 0.5768719911575317, acc: 0.5938, auc: 0.8461, precision: 0.9565, recall: 0.3014\n",
      "2019-01-17T20:50:47.762664, step: 414, loss: 0.4858913719654083, acc: 0.6094, auc: 0.8987, precision: 0.8696, recall: 0.2985\n",
      "2019-01-17T20:50:48.463573, step: 415, loss: 0.5309784412384033, acc: 0.6641, auc: 0.8273, precision: 0.8158, recall: 0.4627\n",
      "2019-01-17T20:50:49.235744, step: 416, loss: 0.554924488067627, acc: 0.7031, auc: 0.7947, precision: 0.7593, recall: 0.6212\n",
      "2019-01-17T20:50:49.945298, step: 417, loss: 0.5284953117370605, acc: 0.6953, auc: 0.8309, precision: 0.7556, recall: 0.5484\n",
      "2019-01-17T20:50:50.644420, step: 418, loss: 0.4857223629951477, acc: 0.6953, auc: 0.8375, precision: 0.7544, recall: 0.6324\n",
      "2019-01-17T20:50:51.279234, step: 419, loss: 0.5266324281692505, acc: 0.7188, auc: 0.8229, precision: 0.8545, recall: 0.6267\n",
      "2019-01-17T20:50:52.002590, step: 420, loss: 0.4027903974056244, acc: 0.7656, auc: 0.9268, precision: 0.9348, recall: 0.6143\n",
      "2019-01-17T20:50:52.720808, step: 421, loss: 0.410440057516098, acc: 0.7031, auc: 0.9164, precision: 0.8947, recall: 0.5\n",
      "2019-01-17T20:50:53.420076, step: 422, loss: 0.4723376929759979, acc: 0.75, auc: 0.8502, precision: 0.7885, recall: 0.6613\n",
      "2019-01-17T20:50:54.075341, step: 423, loss: 0.5264173746109009, acc: 0.7578, auc: 0.844, precision: 0.913, recall: 0.6087\n",
      "2019-01-17T20:50:54.830332, step: 424, loss: 0.44284677505493164, acc: 0.8438, auc: 0.866, precision: 0.8667, recall: 0.8125\n",
      "2019-01-17T20:50:55.611288, step: 425, loss: 0.4074947237968445, acc: 0.8516, auc: 0.8865, precision: 0.84, recall: 0.7925\n",
      "2019-01-17T20:50:56.313826, step: 426, loss: 0.43446823954582214, acc: 0.8672, auc: 0.8852, precision: 0.8286, recall: 0.9206\n",
      "2019-01-17T20:50:56.943696, step: 427, loss: 0.41707807779312134, acc: 0.8438, auc: 0.8951, precision: 0.7826, recall: 0.9153\n",
      "2019-01-17T20:50:57.612331, step: 428, loss: 0.41854244470596313, acc: 0.8438, auc: 0.8693, precision: 0.8254, recall: 0.8525\n",
      "2019-01-17T20:50:58.324697, step: 429, loss: 0.4887247085571289, acc: 0.7969, auc: 0.8454, precision: 0.8033, recall: 0.7778\n",
      "2019-01-17T20:50:59.014384, step: 430, loss: 0.4632110595703125, acc: 0.8047, auc: 0.8807, precision: 0.9107, recall: 0.7183\n",
      "2019-01-17T20:50:59.739598, step: 431, loss: 0.39128541946411133, acc: 0.875, auc: 0.8755, precision: 0.8889, recall: 0.8615\n",
      "2019-01-17T20:51:00.468839, step: 432, loss: 0.5259254574775696, acc: 0.7891, auc: 0.8202, precision: 0.7458, recall: 0.7857\n",
      "2019-01-17T20:51:01.162689, step: 433, loss: 0.3965343236923218, acc: 0.8516, auc: 0.896, precision: 0.8939, recall: 0.831\n",
      "2019-01-17T20:51:01.854664, step: 434, loss: 0.4501320719718933, acc: 0.8125, auc: 0.8659, precision: 0.8235, recall: 0.8235\n",
      "2019-01-17T20:51:02.484227, step: 435, loss: 0.47044050693511963, acc: 0.8203, auc: 0.8603, precision: 0.7681, recall: 0.8833\n",
      "2019-01-17T20:51:03.218914, step: 436, loss: 0.4358704090118408, acc: 0.8281, auc: 0.8929, precision: 0.7656, recall: 0.875\n",
      "2019-01-17T20:51:03.931983, step: 437, loss: 0.40963491797447205, acc: 0.8516, auc: 0.8966, precision: 0.806, recall: 0.9\n",
      "2019-01-17T20:51:04.641335, step: 438, loss: 0.3490739166736603, acc: 0.8672, auc: 0.926, precision: 0.8636, recall: 0.8769\n",
      "2019-01-17T20:51:05.386340, step: 439, loss: 0.41950151324272156, acc: 0.8438, auc: 0.8735, precision: 0.8667, recall: 0.8125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-17T20:51:06.061776, step: 440, loss: 0.40520501136779785, acc: 0.8281, auc: 0.8972, precision: 0.8889, recall: 0.75\n",
      "2019-01-17T20:51:06.808511, step: 441, loss: 0.4219701886177063, acc: 0.8203, auc: 0.876, precision: 0.8361, recall: 0.7969\n",
      "2019-01-17T20:51:07.522725, step: 442, loss: 0.28823190927505493, acc: 0.9062, auc: 0.955, precision: 0.9123, recall: 0.8814\n",
      "2019-01-17T20:51:08.186152, step: 443, loss: 0.40120232105255127, acc: 0.8125, auc: 0.8916, precision: 0.8036, recall: 0.7759\n",
      "2019-01-17T20:51:08.914663, step: 444, loss: 0.37189313769340515, acc: 0.8672, auc: 0.9076, precision: 0.8806, recall: 0.8676\n",
      "2019-01-17T20:51:09.613224, step: 445, loss: 0.4928399622440338, acc: 0.7891, auc: 0.8328, precision: 0.7541, recall: 0.7931\n",
      "2019-01-17T20:51:10.228142, step: 446, loss: 0.360656201839447, acc: 0.875, auc: 0.9158, precision: 0.8571, recall: 0.8852\n",
      "2019-01-17T20:51:10.872231, step: 447, loss: 0.4764504134654999, acc: 0.8047, auc: 0.8549, precision: 0.8571, recall: 0.7714\n",
      "2019-01-17T20:51:11.567219, step: 448, loss: 0.37250882387161255, acc: 0.8672, auc: 0.9118, precision: 0.8772, recall: 0.8333\n",
      "2019-01-17T20:51:12.255826, step: 449, loss: 0.40771979093551636, acc: 0.8672, auc: 0.8949, precision: 0.9306, recall: 0.8481\n",
      "2019-01-17T20:51:12.924901, step: 450, loss: 0.350052148103714, acc: 0.8672, auc: 0.9159, precision: 0.9014, recall: 0.8649\n",
      "2019-01-17T20:51:13.664127, step: 451, loss: 0.3471132516860962, acc: 0.8594, auc: 0.9308, precision: 0.8235, recall: 0.9032\n",
      "2019-01-17T20:51:14.388599, step: 452, loss: 0.39002013206481934, acc: 0.8359, auc: 0.9035, precision: 0.8281, recall: 0.8413\n",
      "2019-01-17T20:51:15.106459, step: 453, loss: 0.2992652952671051, acc: 0.9062, auc: 0.9448, precision: 0.9483, recall: 0.8594\n",
      "2019-01-17T20:51:15.864528, step: 454, loss: 0.4492695927619934, acc: 0.8203, auc: 0.875, precision: 0.7857, recall: 0.873\n",
      "2019-01-17T20:51:16.654535, step: 455, loss: 0.472625732421875, acc: 0.7734, auc: 0.8722, precision: 0.9583, recall: 0.6301\n",
      "2019-01-17T20:51:17.296632, step: 456, loss: 0.5242822170257568, acc: 0.7656, auc: 0.8485, precision: 0.9318, recall: 0.6029\n",
      "2019-01-17T20:51:17.950560, step: 457, loss: 0.48588329553604126, acc: 0.7969, auc: 0.8752, precision: 0.9722, recall: 0.5833\n",
      "2019-01-17T20:51:18.658269, step: 458, loss: 0.4887998104095459, acc: 0.7578, auc: 0.8532, precision: 1.0, recall: 0.4918\n",
      "2019-01-17T20:51:19.363448, step: 459, loss: 0.6346163749694824, acc: 0.6641, auc: 0.8008, precision: 1.0, recall: 0.4028\n",
      "2019-01-17T20:51:20.098248, step: 460, loss: 0.4348861575126648, acc: 0.8125, auc: 0.8715, precision: 0.9459, recall: 0.614\n",
      "2019-01-17T20:51:20.769714, step: 461, loss: 0.4091262221336365, acc: 0.8047, auc: 0.8899, precision: 0.9787, recall: 0.6571\n",
      "2019-01-17T20:51:21.444285, step: 462, loss: 0.4352404475212097, acc: 0.7969, auc: 0.8723, precision: 0.8936, recall: 0.6667\n",
      "2019-01-17T20:51:22.134780, step: 463, loss: 0.48607540130615234, acc: 0.8359, auc: 0.8475, precision: 0.8475, recall: 0.8065\n",
      "2019-01-17T20:51:22.809466, step: 464, loss: 0.4471016526222229, acc: 0.8359, auc: 0.9195, precision: 0.7949, recall: 0.9254\n",
      "2019-01-17T20:51:23.530833, step: 465, loss: 0.479138046503067, acc: 0.7734, auc: 0.8909, precision: 0.7349, recall: 0.8971\n",
      "2019-01-17T20:51:24.281435, step: 466, loss: 0.5753251314163208, acc: 0.7891, auc: 0.8583, precision: 0.7349, recall: 0.9242\n",
      "2019-01-17T20:51:25.082781, step: 467, loss: 0.6112711429595947, acc: 0.7812, auc: 0.8435, precision: 0.7333, recall: 0.873\n",
      "2019-01-17T20:51:25.798057, step: 468, loss: 0.4891945719718933, acc: 0.8203, auc: 0.9172, precision: 0.76, recall: 0.9194\n",
      "start training model\n",
      "2019-01-17T20:51:26.477151, step: 469, loss: 0.4991627633571625, acc: 0.8516, auc: 0.8887, precision: 0.7778, recall: 0.9844\n",
      "2019-01-17T20:51:27.239079, step: 470, loss: 0.5750204920768738, acc: 0.7812, auc: 0.8496, precision: 0.6957, recall: 0.8727\n",
      "2019-01-17T20:51:27.965605, step: 471, loss: 0.41119951009750366, acc: 0.8203, auc: 0.9039, precision: 0.84, recall: 0.7368\n",
      "2019-01-17T20:51:28.644892, step: 472, loss: 0.48369231820106506, acc: 0.7656, auc: 0.8439, precision: 0.8039, recall: 0.6721\n",
      "2019-01-17T20:51:29.280301, step: 473, loss: 0.46069565415382385, acc: 0.7656, auc: 0.8715, precision: 0.8438, recall: 0.5192\n",
      "2019-01-17T20:51:29.961747, step: 474, loss: 0.4993666410446167, acc: 0.6797, auc: 0.8994, precision: 0.9688, recall: 0.4366\n",
      "2019-01-17T20:51:30.667302, step: 475, loss: 0.6058298349380493, acc: 0.6562, auc: 0.7552, precision: 0.9231, recall: 0.2182\n",
      "2019-01-17T20:51:31.383281, step: 476, loss: 0.5254310369491577, acc: 0.7422, auc: 0.8414, precision: 0.963, recall: 0.4483\n",
      "2019-01-17T20:51:32.155580, step: 477, loss: 0.5955565571784973, acc: 0.6406, auc: 0.851, precision: 1.0, recall: 0.3947\n",
      "2019-01-17T20:51:32.854630, step: 478, loss: 0.5398659706115723, acc: 0.7109, auc: 0.8304, precision: 0.913, recall: 0.375\n",
      "2019-01-17T20:51:33.496650, step: 479, loss: 0.5058410167694092, acc: 0.7188, auc: 0.8381, precision: 0.88, recall: 0.4\n",
      "2019-01-17T20:51:34.162035, step: 480, loss: 0.5083324909210205, acc: 0.7734, auc: 0.7939, precision: 0.925, recall: 0.5873\n",
      "2019-01-17T20:51:34.880669, step: 481, loss: 0.4393664002418518, acc: 0.7969, auc: 0.877, precision: 0.913, recall: 0.6562\n",
      "2019-01-17T20:51:35.540382, step: 482, loss: 0.4519420862197876, acc: 0.8125, auc: 0.8686, precision: 0.9286, recall: 0.7222\n",
      "2019-01-17T20:51:36.206777, step: 483, loss: 0.4336432218551636, acc: 0.8359, auc: 0.8372, precision: 0.9355, recall: 0.6042\n",
      "2019-01-17T20:51:36.852317, step: 484, loss: 0.39556410908699036, acc: 0.8281, auc: 0.8998, precision: 0.8958, recall: 0.7167\n",
      "2019-01-17T20:51:37.551609, step: 485, loss: 0.47957858443260193, acc: 0.7891, auc: 0.8535, precision: 0.8462, recall: 0.6984\n",
      "2019-01-17T20:51:38.264675, step: 486, loss: 0.38568899035453796, acc: 0.8359, auc: 0.9087, precision: 0.96, recall: 0.7164\n",
      "2019-01-17T20:51:38.978731, step: 487, loss: 0.43430620431900024, acc: 0.8203, auc: 0.8777, precision: 0.9107, recall: 0.7391\n",
      "2019-01-17T20:51:39.706696, step: 488, loss: 0.40690815448760986, acc: 0.8516, auc: 0.895, precision: 0.8947, recall: 0.7969\n",
      "2019-01-17T20:51:40.415863, step: 489, loss: 0.3480440378189087, acc: 0.8516, auc: 0.9319, precision: 0.9107, recall: 0.7846\n",
      "2019-01-17T20:51:41.158058, step: 490, loss: 0.4329865574836731, acc: 0.8594, auc: 0.9152, precision: 0.8103, recall: 0.8704\n",
      "2019-01-17T20:51:41.833789, step: 491, loss: 0.4272107481956482, acc: 0.8203, auc: 0.9007, precision: 0.8627, recall: 0.7333\n",
      "2019-01-17T20:51:42.539724, step: 492, loss: 0.3979199230670929, acc: 0.8125, auc: 0.9062, precision: 0.8966, recall: 0.7429\n",
      "2019-01-17T20:51:43.215174, step: 493, loss: 0.36535027623176575, acc: 0.8438, auc: 0.9091, precision: 0.9649, recall: 0.7534\n",
      "2019-01-17T20:51:43.898247, step: 494, loss: 0.4618372917175293, acc: 0.8203, auc: 0.8717, precision: 0.8525, recall: 0.7879\n",
      "2019-01-17T20:51:44.606856, step: 495, loss: 0.3317406177520752, acc: 0.8672, auc: 0.9124, precision: 0.9796, recall: 0.75\n",
      "2019-01-17T20:51:45.258270, step: 496, loss: 0.36328762769699097, acc: 0.8594, auc: 0.9089, precision: 0.9242, recall: 0.8243\n",
      "2019-01-17T20:51:45.948413, step: 497, loss: 0.364496111869812, acc: 0.8594, auc: 0.9047, precision: 0.9322, recall: 0.7971\n",
      "2019-01-17T20:51:46.609130, step: 498, loss: 0.41677728295326233, acc: 0.7891, auc: 0.9133, precision: 0.9107, recall: 0.6986\n",
      "2019-01-17T20:51:47.356363, step: 499, loss: 0.36092403531074524, acc: 0.8594, auc: 0.9091, precision: 0.9333, recall: 0.8\n",
      "2019-01-17T20:51:48.029108, step: 500, loss: 0.29303935170173645, acc: 0.8984, auc: 0.9543, precision: 0.9153, recall: 0.871\n",
      "\n",
      "Evaluation:\n",
      "2019-01-17T20:52:15.121011, step: 500, loss: 0.4422072898118924, acc: 0.8317358974358974, auc: 0.8790051282051282, precision: 0.8668179487179487, recall: 0.7896769230769232\n",
      "Saved model checkpoint to ../model/Bi-LSTM/model/my-model-500\n",
      "\n",
      "2019-01-17T20:52:16.349058, step: 501, loss: 0.32734018564224243, acc: 0.8828, auc: 0.9299, precision: 0.918, recall: 0.8485\n",
      "2019-01-17T20:52:17.012736, step: 502, loss: 0.39973437786102295, acc: 0.8672, auc: 0.9185, precision: 0.807, recall: 0.8846\n",
      "2019-01-17T20:52:17.723373, step: 503, loss: 0.3605298399925232, acc: 0.8984, auc: 0.9026, precision: 0.8871, recall: 0.9016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-17T20:52:18.439281, step: 504, loss: 0.3956588804721832, acc: 0.8828, auc: 0.9107, precision: 0.8361, recall: 0.9107\n",
      "2019-01-17T20:52:19.125055, step: 505, loss: 0.30549904704093933, acc: 0.8984, auc: 0.9492, precision: 0.8966, recall: 0.8814\n",
      "2019-01-17T20:52:19.839520, step: 506, loss: 0.3173522651195526, acc: 0.8672, auc: 0.9369, precision: 0.9385, recall: 0.8243\n",
      "2019-01-17T20:52:20.529037, step: 507, loss: 0.32439908385276794, acc: 0.8828, auc: 0.9469, precision: 0.8596, recall: 0.875\n",
      "2019-01-17T20:52:21.190160, step: 508, loss: 0.3417744040489197, acc: 0.8984, auc: 0.9262, precision: 0.8475, recall: 0.9259\n",
      "2019-01-17T20:52:21.855801, step: 509, loss: 0.2808383107185364, acc: 0.9141, auc: 0.9533, precision: 0.8871, recall: 0.9322\n",
      "2019-01-17T20:52:22.527466, step: 510, loss: 0.45557913184165955, acc: 0.8594, auc: 0.8556, precision: 0.8254, recall: 0.8814\n",
      "2019-01-17T20:52:23.140320, step: 511, loss: 0.467955619096756, acc: 0.8281, auc: 0.8624, precision: 0.8438, recall: 0.8182\n",
      "2019-01-17T20:52:23.922739, step: 512, loss: 0.2971312999725342, acc: 0.9062, auc: 0.9248, precision: 0.9254, recall: 0.8986\n",
      "2019-01-17T20:52:24.644512, step: 513, loss: 0.3435079753398895, acc: 0.875, auc: 0.9226, precision: 0.8841, recall: 0.8841\n",
      "2019-01-17T20:52:25.419233, step: 514, loss: 0.3986767828464508, acc: 0.8594, auc: 0.9019, precision: 0.8769, recall: 0.8507\n",
      "2019-01-17T20:52:26.097592, step: 515, loss: 0.3922044038772583, acc: 0.8516, auc: 0.9002, precision: 0.8356, recall: 0.8971\n",
      "2019-01-17T20:52:26.828312, step: 516, loss: 0.3434612452983856, acc: 0.875, auc: 0.9255, precision: 0.8939, recall: 0.8676\n",
      "2019-01-17T20:52:27.559422, step: 517, loss: 0.3125695586204529, acc: 0.8828, auc: 0.935, precision: 0.9254, recall: 0.8611\n",
      "2019-01-17T20:52:28.315292, step: 518, loss: 0.3943338096141815, acc: 0.8516, auc: 0.8653, precision: 0.8649, recall: 0.8767\n",
      "2019-01-17T20:52:29.049827, step: 519, loss: 0.30245065689086914, acc: 0.9062, auc: 0.9279, precision: 0.8889, recall: 0.9412\n",
      "2019-01-17T20:52:29.755739, step: 520, loss: 0.35319897532463074, acc: 0.8672, auc: 0.9174, precision: 0.7761, recall: 0.963\n",
      "2019-01-17T20:52:30.480686, step: 521, loss: 0.2903434634208679, acc: 0.9219, auc: 0.9294, precision: 0.9265, recall: 0.9265\n",
      "2019-01-17T20:52:31.211508, step: 522, loss: 0.3458592891693115, acc: 0.8594, auc: 0.9339, precision: 0.8769, recall: 0.8507\n",
      "2019-01-17T20:52:31.963387, step: 523, loss: 0.3135398328304291, acc: 0.8984, auc: 0.928, precision: 0.8714, recall: 0.9385\n",
      "2019-01-17T20:52:32.652612, step: 524, loss: 0.31062719225883484, acc: 0.9062, auc: 0.933, precision: 0.9016, recall: 0.9016\n",
      "2019-01-17T20:52:33.447043, step: 525, loss: 0.32703539729118347, acc: 0.8828, auc: 0.9276, precision: 0.8594, recall: 0.9016\n",
      "2019-01-17T20:52:34.136937, step: 526, loss: 0.40113288164138794, acc: 0.8438, auc: 0.9101, precision: 0.9091, recall: 0.7692\n",
      "2019-01-17T20:52:34.814299, step: 527, loss: 0.41875022649765015, acc: 0.8281, auc: 0.9153, precision: 0.9057, recall: 0.7385\n",
      "2019-01-17T20:52:35.524609, step: 528, loss: 0.4019028842449188, acc: 0.8594, auc: 0.8872, precision: 0.9194, recall: 0.8143\n",
      "2019-01-17T20:52:36.292806, step: 529, loss: 0.3446322977542877, acc: 0.8438, auc: 0.932, precision: 0.9123, recall: 0.7761\n",
      "2019-01-17T20:52:36.903554, step: 530, loss: 0.27757567167282104, acc: 0.9141, auc: 0.9519, precision: 0.9333, recall: 0.8889\n",
      "2019-01-17T20:52:37.590299, step: 531, loss: 0.2742643654346466, acc: 0.9062, auc: 0.9647, precision: 0.9516, recall: 0.8676\n",
      "2019-01-17T20:52:38.258102, step: 532, loss: 0.34749844670295715, acc: 0.8672, auc: 0.9157, precision: 0.8986, recall: 0.8611\n",
      "2019-01-17T20:52:38.969363, step: 533, loss: 0.3433030843734741, acc: 0.875, auc: 0.9355, precision: 0.8788, recall: 0.8788\n",
      "2019-01-17T20:52:39.629366, step: 534, loss: 0.4139518737792969, acc: 0.8516, auc: 0.8848, precision: 0.8333, recall: 0.8475\n",
      "2019-01-17T20:52:40.359622, step: 535, loss: 0.3429456353187561, acc: 0.8594, auc: 0.9293, precision: 0.9104, recall: 0.8356\n",
      "2019-01-17T20:52:41.070083, step: 536, loss: 0.31316518783569336, acc: 0.9062, auc: 0.9382, precision: 0.8971, recall: 0.9242\n",
      "2019-01-17T20:52:41.759291, step: 537, loss: 0.28528332710266113, acc: 0.8984, auc: 0.9596, precision: 0.9444, recall: 0.8361\n",
      "2019-01-17T20:52:42.420912, step: 538, loss: 0.3398454785346985, acc: 0.8516, auc: 0.9355, precision: 0.9474, recall: 0.7714\n",
      "2019-01-17T20:52:43.060418, step: 539, loss: 0.3486776053905487, acc: 0.8594, auc: 0.932, precision: 0.9474, recall: 0.7826\n",
      "2019-01-17T20:52:43.762200, step: 540, loss: 0.34138667583465576, acc: 0.8672, auc: 0.9263, precision: 0.875, recall: 0.8305\n",
      "2019-01-17T20:52:44.491680, step: 541, loss: 0.21430814266204834, acc: 0.9297, auc: 0.9702, precision: 0.9508, recall: 0.9062\n",
      "2019-01-17T20:52:45.194685, step: 542, loss: 0.32875722646713257, acc: 0.8906, auc: 0.9277, precision: 0.9091, recall: 0.8824\n",
      "2019-01-17T20:52:45.907318, step: 543, loss: 0.27938729524612427, acc: 0.8984, auc: 0.9587, precision: 0.931, recall: 0.8571\n",
      "2019-01-17T20:52:46.577493, step: 544, loss: 0.31030598282814026, acc: 0.8906, auc: 0.9387, precision: 0.9016, recall: 0.873\n",
      "2019-01-17T20:52:47.263071, step: 545, loss: 0.4587920606136322, acc: 0.8047, auc: 0.8983, precision: 0.8983, recall: 0.7361\n",
      "2019-01-17T20:52:47.939234, step: 546, loss: 0.39264780282974243, acc: 0.8203, auc: 0.9293, precision: 0.8, recall: 0.8136\n",
      "2019-01-17T20:52:48.566949, step: 547, loss: 0.3845744729042053, acc: 0.8516, auc: 0.9167, precision: 0.8028, recall: 0.9194\n",
      "2019-01-17T20:52:49.251805, step: 548, loss: 0.2583830654621124, acc: 0.8984, auc: 0.9626, precision: 0.8679, recall: 0.8846\n",
      "2019-01-17T20:52:49.984100, step: 549, loss: 0.40627384185791016, acc: 0.8438, auc: 0.893, precision: 0.8644, recall: 0.8095\n",
      "2019-01-17T20:52:50.663745, step: 550, loss: 0.31871843338012695, acc: 0.8828, auc: 0.9402, precision: 0.9038, recall: 0.8246\n",
      "2019-01-17T20:52:51.308751, step: 551, loss: 0.4500119984149933, acc: 0.8047, auc: 0.8936, precision: 0.9375, recall: 0.6716\n",
      "2019-01-17T20:52:51.974625, step: 552, loss: 0.37987685203552246, acc: 0.8281, auc: 0.9374, precision: 0.9615, recall: 0.7143\n",
      "2019-01-17T20:52:52.688496, step: 553, loss: 0.3503011167049408, acc: 0.8438, auc: 0.9336, precision: 0.9365, recall: 0.7867\n",
      "2019-01-17T20:52:53.341579, step: 554, loss: 0.33396774530410767, acc: 0.8203, auc: 0.943, precision: 0.9524, recall: 0.6557\n",
      "2019-01-17T20:52:54.105279, step: 555, loss: 0.38410598039627075, acc: 0.8281, auc: 0.9139, precision: 0.9592, recall: 0.7015\n",
      "2019-01-17T20:52:54.911719, step: 556, loss: 0.3684960603713989, acc: 0.8438, auc: 0.9198, precision: 0.8478, recall: 0.75\n",
      "2019-01-17T20:52:55.596668, step: 557, loss: 0.31325140595436096, acc: 0.8594, auc: 0.9466, precision: 0.9649, recall: 0.7746\n",
      "2019-01-17T20:52:56.300031, step: 558, loss: 0.2972606420516968, acc: 0.8984, auc: 0.9526, precision: 0.9818, recall: 0.8182\n",
      "2019-01-17T20:52:57.029429, step: 559, loss: 0.4621812403202057, acc: 0.8125, auc: 0.8725, precision: 0.8429, recall: 0.8194\n",
      "2019-01-17T20:52:57.784528, step: 560, loss: 0.3639979660511017, acc: 0.8516, auc: 0.9155, precision: 0.9259, recall: 0.7692\n",
      "2019-01-17T20:52:58.542373, step: 561, loss: 0.3426869511604309, acc: 0.8438, auc: 0.9323, precision: 0.92, recall: 0.7419\n",
      "2019-01-17T20:52:59.253295, step: 562, loss: 0.35624223947525024, acc: 0.8438, auc: 0.921, precision: 0.9608, recall: 0.7313\n",
      "2019-01-17T20:52:59.936942, step: 563, loss: 0.3424433469772339, acc: 0.7812, auc: 0.9586, precision: 0.9459, recall: 0.5738\n",
      "2019-01-17T20:53:00.640645, step: 564, loss: 0.3454979956150055, acc: 0.8438, auc: 0.9282, precision: 0.9583, recall: 0.7188\n",
      "2019-01-17T20:53:01.365145, step: 565, loss: 0.4850371778011322, acc: 0.7109, auc: 0.8773, precision: 0.9167, recall: 0.5714\n",
      "2019-01-17T20:53:02.106476, step: 566, loss: 0.3589501976966858, acc: 0.8125, auc: 0.9257, precision: 0.9737, recall: 0.6167\n",
      "2019-01-17T20:53:02.762526, step: 567, loss: 0.3397834599018097, acc: 0.8359, auc: 0.9324, precision: 0.9783, recall: 0.6923\n",
      "2019-01-17T20:53:03.461612, step: 568, loss: 0.33882442116737366, acc: 0.7969, auc: 0.9495, precision: 0.9756, recall: 0.6154\n",
      "2019-01-17T20:53:04.119964, step: 569, loss: 0.40294742584228516, acc: 0.7891, auc: 0.8989, precision: 0.9524, recall: 0.6154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-17T20:53:04.837969, step: 570, loss: 0.3781702518463135, acc: 0.7969, auc: 0.9182, precision: 0.913, recall: 0.6562\n",
      "2019-01-17T20:53:05.530487, step: 571, loss: 0.30683743953704834, acc: 0.8359, auc: 0.9548, precision: 0.9375, recall: 0.7143\n",
      "2019-01-17T20:53:06.292121, step: 572, loss: 0.3357540965080261, acc: 0.8594, auc: 0.943, precision: 0.902, recall: 0.7797\n",
      "2019-01-17T20:53:07.005240, step: 573, loss: 0.2908068895339966, acc: 0.9062, auc: 0.9629, precision: 0.9412, recall: 0.8421\n",
      "2019-01-17T20:53:07.659142, step: 574, loss: 0.2933141589164734, acc: 0.8594, auc: 0.9592, precision: 1.0, recall: 0.7391\n",
      "2019-01-17T20:53:08.369871, step: 575, loss: 0.3008109927177429, acc: 0.8516, auc: 0.9469, precision: 0.963, recall: 0.7536\n",
      "2019-01-17T20:53:09.030844, step: 576, loss: 0.3288508653640747, acc: 0.8359, auc: 0.952, precision: 0.9455, recall: 0.7429\n",
      "2019-01-17T20:53:09.693041, step: 577, loss: 0.30973589420318604, acc: 0.8594, auc: 0.9462, precision: 0.902, recall: 0.7797\n",
      "2019-01-17T20:53:10.418528, step: 578, loss: 0.28379037976264954, acc: 0.875, auc: 0.9453, precision: 0.9615, recall: 0.7812\n",
      "2019-01-17T20:53:11.147938, step: 579, loss: 0.23897971212863922, acc: 0.8984, auc: 0.962, precision: 0.9556, recall: 0.7963\n",
      "2019-01-17T20:53:11.869096, step: 580, loss: 0.29556944966316223, acc: 0.8672, auc: 0.9324, precision: 0.875, recall: 0.8305\n",
      "2019-01-17T20:53:12.527980, step: 581, loss: 0.3596341013908386, acc: 0.8594, auc: 0.9286, precision: 0.8197, recall: 0.8772\n",
      "2019-01-17T20:53:13.212436, step: 582, loss: 0.32126420736312866, acc: 0.8672, auc: 0.9324, precision: 0.8966, recall: 0.8254\n",
      "2019-01-17T20:53:13.937294, step: 583, loss: 0.3553866744041443, acc: 0.8594, auc: 0.9241, precision: 0.8696, recall: 0.8696\n",
      "2019-01-17T20:53:14.707190, step: 584, loss: 0.40934306383132935, acc: 0.8516, auc: 0.9224, precision: 0.803, recall: 0.8983\n",
      "2019-01-17T20:53:15.398092, step: 585, loss: 0.32042771577835083, acc: 0.875, auc: 0.9365, precision: 0.8421, recall: 0.8727\n",
      "2019-01-17T20:53:16.117398, step: 586, loss: 0.4342007339000702, acc: 0.8125, auc: 0.9099, precision: 0.7826, recall: 0.8571\n",
      "2019-01-17T20:53:16.808181, step: 587, loss: 0.4333697557449341, acc: 0.8438, auc: 0.8897, precision: 0.875, recall: 0.8235\n",
      "2019-01-17T20:53:17.537491, step: 588, loss: 0.3425869941711426, acc: 0.8516, auc: 0.936, precision: 0.8852, recall: 0.8182\n",
      "2019-01-17T20:53:18.327875, step: 589, loss: 0.33176764845848083, acc: 0.8672, auc: 0.9391, precision: 0.8906, recall: 0.8507\n",
      "2019-01-17T20:53:19.030902, step: 590, loss: 0.17915531992912292, acc: 0.9062, auc: 0.9851, precision: 0.9811, recall: 0.8254\n",
      "2019-01-17T20:53:19.757232, step: 591, loss: 0.42753785848617554, acc: 0.8516, auc: 0.8965, precision: 0.9259, recall: 0.7692\n",
      "2019-01-17T20:53:20.511803, step: 592, loss: 0.33956778049468994, acc: 0.8594, auc: 0.9227, precision: 0.9623, recall: 0.7612\n",
      "2019-01-17T20:53:21.204226, step: 593, loss: 0.3273712396621704, acc: 0.8906, auc: 0.9218, precision: 0.9184, recall: 0.8182\n",
      "2019-01-17T20:53:21.878445, step: 594, loss: 0.45936477184295654, acc: 0.8203, auc: 0.8805, precision: 0.8305, recall: 0.7903\n",
      "2019-01-17T20:53:22.562742, step: 595, loss: 0.27022016048431396, acc: 0.8984, auc: 0.9531, precision: 0.9516, recall: 0.8551\n",
      "2019-01-17T20:53:23.381675, step: 596, loss: 0.2292817234992981, acc: 0.9141, auc: 0.9618, precision: 0.9178, recall: 0.9306\n",
      "2019-01-17T20:53:24.083988, step: 597, loss: 0.3019907474517822, acc: 0.9141, auc: 0.9399, precision: 0.9091, recall: 0.9231\n",
      "2019-01-17T20:53:24.763824, step: 598, loss: 0.38056057691574097, acc: 0.875, auc: 0.9158, precision: 0.8904, recall: 0.8904\n",
      "2019-01-17T20:53:25.434061, step: 599, loss: 0.31956514716148376, acc: 0.8984, auc: 0.9409, precision: 0.9028, recall: 0.9155\n",
      "2019-01-17T20:53:26.154913, step: 600, loss: 0.29084837436676025, acc: 0.8828, auc: 0.9556, precision: 0.875, recall: 0.8889\n",
      "\n",
      "Evaluation:\n",
      "2019-01-17T20:53:54.155655, step: 600, loss: 0.397359237456933, acc: 0.8429461538461539, auc: 0.9123615384615386, precision: 0.8392871794871792, recall: 0.8532102564102565\n",
      "Saved model checkpoint to ../model/Bi-LSTM/model/my-model-600\n",
      "\n",
      "2019-01-17T20:53:55.362478, step: 601, loss: 0.25132718682289124, acc: 0.9062, auc: 0.9728, precision: 0.9286, recall: 0.8667\n",
      "2019-01-17T20:53:56.042304, step: 602, loss: 0.41615816950798035, acc: 0.7969, auc: 0.8839, precision: 0.8378, recall: 0.8158\n",
      "2019-01-17T20:53:56.712609, step: 603, loss: 0.3136748671531677, acc: 0.8906, auc: 0.9388, precision: 0.8615, recall: 0.918\n",
      "2019-01-17T20:53:57.439964, step: 604, loss: 0.2691596448421478, acc: 0.8906, auc: 0.9593, precision: 0.9355, recall: 0.8529\n",
      "2019-01-17T20:53:58.158913, step: 605, loss: 0.24570158123970032, acc: 0.9219, auc: 0.9605, precision: 0.9231, recall: 0.8889\n",
      "2019-01-17T20:53:58.964069, step: 606, loss: 0.2331685721874237, acc: 0.9141, auc: 0.9624, precision: 0.95, recall: 0.8769\n",
      "2019-01-17T20:53:59.680353, step: 607, loss: 0.24732083082199097, acc: 0.9141, auc: 0.9674, precision: 0.9423, recall: 0.8596\n",
      "2019-01-17T20:54:00.393247, step: 608, loss: 0.393748015165329, acc: 0.8516, auc: 0.9118, precision: 0.8939, recall: 0.831\n",
      "2019-01-17T20:54:01.044021, step: 609, loss: 0.2984880805015564, acc: 0.8984, auc: 0.9414, precision: 0.9434, recall: 0.8333\n",
      "2019-01-17T20:54:01.702702, step: 610, loss: 0.286487340927124, acc: 0.875, auc: 0.9502, precision: 0.9038, recall: 0.8103\n",
      "2019-01-17T20:54:02.389541, step: 611, loss: 0.24706117808818817, acc: 0.9141, auc: 0.9591, precision: 0.9123, recall: 0.8966\n",
      "2019-01-17T20:54:03.103572, step: 612, loss: 0.3368101119995117, acc: 0.8672, auc: 0.9212, precision: 0.902, recall: 0.7931\n",
      "2019-01-17T20:54:03.772448, step: 613, loss: 0.334572970867157, acc: 0.875, auc: 0.9294, precision: 0.9107, recall: 0.8226\n",
      "2019-01-17T20:54:04.496251, step: 614, loss: 0.2484191656112671, acc: 0.9141, auc: 0.9463, precision: 0.9348, recall: 0.8431\n",
      "2019-01-17T20:54:05.242559, step: 615, loss: 0.3385642170906067, acc: 0.8984, auc: 0.9202, precision: 0.9, recall: 0.8852\n",
      "2019-01-17T20:54:05.992535, step: 616, loss: 0.318317174911499, acc: 0.8906, auc: 0.9253, precision: 0.8923, recall: 0.8923\n",
      "2019-01-17T20:54:06.729312, step: 617, loss: 0.3333469331264496, acc: 0.8906, auc: 0.9221, precision: 0.9167, recall: 0.8594\n",
      "2019-01-17T20:54:07.464111, step: 618, loss: 0.32788336277008057, acc: 0.8516, auc: 0.9271, precision: 0.8824, recall: 0.7759\n",
      "2019-01-17T20:54:08.100765, step: 619, loss: 0.2697150707244873, acc: 0.9062, auc: 0.956, precision: 0.9365, recall: 0.8806\n",
      "2019-01-17T20:54:08.745062, step: 620, loss: 0.2889941334724426, acc: 0.9062, auc: 0.9462, precision: 0.9032, recall: 0.9032\n",
      "2019-01-17T20:54:09.441136, step: 621, loss: 0.3335352838039398, acc: 0.875, auc: 0.9263, precision: 0.8529, recall: 0.9062\n",
      "2019-01-17T20:54:10.252615, step: 622, loss: 0.42849209904670715, acc: 0.7891, auc: 0.8987, precision: 0.8409, recall: 0.6491\n",
      "2019-01-17T20:54:10.948886, step: 623, loss: 0.2576550841331482, acc: 0.8984, auc: 0.9628, precision: 0.9385, recall: 0.8714\n",
      "2019-01-17T20:54:11.690697, step: 624, loss: 0.26599428057670593, acc: 0.9062, auc: 0.9477, precision: 0.9636, recall: 0.8413\n",
      "start training model\n",
      "2019-01-17T20:54:12.365097, step: 625, loss: 0.3409729599952698, acc: 0.8359, auc: 0.9289, precision: 0.9362, recall: 0.7097\n",
      "2019-01-17T20:54:13.004414, step: 626, loss: 0.1925545483827591, acc: 0.9453, auc: 0.98, precision: 1.0, recall: 0.9\n",
      "2019-01-17T20:54:13.624153, step: 627, loss: 0.262054443359375, acc: 0.8906, auc: 0.9596, precision: 0.8793, recall: 0.8793\n",
      "2019-01-17T20:54:14.280003, step: 628, loss: 0.24472704529762268, acc: 0.8906, auc: 0.9712, precision: 0.9062, recall: 0.8788\n",
      "2019-01-17T20:54:14.972100, step: 629, loss: 0.2218126803636551, acc: 0.9297, auc: 0.9739, precision: 0.9667, recall: 0.8923\n",
      "2019-01-17T20:54:15.633958, step: 630, loss: 0.21089832484722137, acc: 0.9141, auc: 0.9718, precision: 0.9672, recall: 0.8676\n",
      "2019-01-17T20:54:16.309436, step: 631, loss: 0.22499649226665497, acc: 0.8906, auc: 0.9765, precision: 0.9492, recall: 0.8358\n",
      "2019-01-17T20:54:17.024017, step: 632, loss: 0.26473233103752136, acc: 0.8984, auc: 0.9492, precision: 0.9649, recall: 0.8333\n",
      "2019-01-17T20:54:17.738493, step: 633, loss: 0.2913355529308319, acc: 0.8906, auc: 0.9474, precision: 0.9077, recall: 0.8806\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-17T20:54:18.453291, step: 634, loss: 0.29718077182769775, acc: 0.8906, auc: 0.9455, precision: 0.9194, recall: 0.8636\n",
      "2019-01-17T20:54:19.123155, step: 635, loss: 0.2007565200328827, acc: 0.9219, auc: 0.9804, precision: 0.9672, recall: 0.8806\n",
      "2019-01-17T20:54:19.770999, step: 636, loss: 0.28110525012016296, acc: 0.8906, auc: 0.9504, precision: 0.9333, recall: 0.8485\n",
      "2019-01-17T20:54:20.493803, step: 637, loss: 0.245966374874115, acc: 0.8828, auc: 0.9624, precision: 0.9265, recall: 0.863\n",
      "2019-01-17T20:54:21.243852, step: 638, loss: 0.26521188020706177, acc: 0.8984, auc: 0.9613, precision: 0.9138, recall: 0.8689\n",
      "2019-01-17T20:54:21.917438, step: 639, loss: 0.20950017869472504, acc: 0.9219, auc: 0.9759, precision: 0.9455, recall: 0.8814\n",
      "2019-01-17T20:54:22.568143, step: 640, loss: 0.18448716402053833, acc: 0.9219, auc: 0.9853, precision: 0.9661, recall: 0.8769\n",
      "2019-01-17T20:54:23.253809, step: 641, loss: 0.31424790620803833, acc: 0.8828, auc: 0.9385, precision: 0.9444, recall: 0.8095\n",
      "2019-01-17T20:54:23.991092, step: 642, loss: 0.38270965218544006, acc: 0.8672, auc: 0.9097, precision: 0.873, recall: 0.8594\n",
      "2019-01-17T20:54:24.752252, step: 643, loss: 0.14960891008377075, acc: 0.9531, auc: 0.9929, precision: 0.9697, recall: 0.9412\n",
      "2019-01-17T20:54:25.456716, step: 644, loss: 0.1924373358488083, acc: 0.9219, auc: 0.9766, precision: 0.9592, recall: 0.8545\n",
      "2019-01-17T20:54:26.216365, step: 645, loss: 0.20489682257175446, acc: 0.9375, auc: 0.9747, precision: 0.918, recall: 0.9492\n",
      "2019-01-17T20:54:26.958624, step: 646, loss: 0.2581115663051605, acc: 0.8984, auc: 0.9594, precision: 0.9016, recall: 0.8871\n",
      "2019-01-17T20:54:27.610987, step: 647, loss: 0.17785274982452393, acc: 0.9062, auc: 0.9858, precision: 0.9167, recall: 0.8871\n",
      "2019-01-17T20:54:28.248808, step: 648, loss: 0.25757110118865967, acc: 0.9062, auc: 0.9571, precision: 0.9259, recall: 0.8621\n",
      "2019-01-17T20:54:28.923289, step: 649, loss: 0.2430417537689209, acc: 0.9141, auc: 0.9583, precision: 0.9552, recall: 0.8889\n",
      "2019-01-17T20:54:29.620950, step: 650, loss: 0.26330721378326416, acc: 0.8906, auc: 0.9526, precision: 0.918, recall: 0.8615\n",
      "2019-01-17T20:54:30.298962, step: 651, loss: 0.2818884253501892, acc: 0.8594, auc: 0.974, precision: 1.0, recall: 0.7\n",
      "2019-01-17T20:54:30.991377, step: 652, loss: 0.32820236682891846, acc: 0.8203, auc: 0.9717, precision: 1.0, recall: 0.6349\n",
      "2019-01-17T20:54:31.649181, step: 653, loss: 0.41093432903289795, acc: 0.8125, auc: 0.9242, precision: 0.9524, recall: 0.6452\n",
      "2019-01-17T20:54:32.334185, step: 654, loss: 0.47087374329566956, acc: 0.7109, auc: 0.9323, precision: 0.9677, recall: 0.4545\n",
      "2019-01-17T20:54:33.064640, step: 655, loss: 0.31115955114364624, acc: 0.7891, auc: 0.96, precision: 1.0, recall: 0.5781\n",
      "2019-01-17T20:54:33.742645, step: 656, loss: 0.35114195942878723, acc: 0.7812, auc: 0.9338, precision: 0.9394, recall: 0.5439\n",
      "2019-01-17T20:54:34.411389, step: 657, loss: 0.3377561867237091, acc: 0.8047, auc: 0.9482, precision: 0.9362, recall: 0.6667\n",
      "2019-01-17T20:54:35.113477, step: 658, loss: 0.3322484791278839, acc: 0.875, auc: 0.9576, precision: 0.9333, recall: 0.8235\n",
      "2019-01-17T20:54:35.767463, step: 659, loss: 0.3956083059310913, acc: 0.875, auc: 0.9302, precision: 0.8852, recall: 0.8571\n",
      "2019-01-17T20:54:36.495232, step: 660, loss: 0.31324639916419983, acc: 0.9297, auc: 0.9637, precision: 0.918, recall: 0.9333\n",
      "2019-01-17T20:54:37.169710, step: 661, loss: 0.3107631206512451, acc: 0.8594, auc: 0.9404, precision: 0.8448, recall: 0.8448\n",
      "2019-01-17T20:54:37.872162, step: 662, loss: 0.20640623569488525, acc: 0.9375, auc: 0.9822, precision: 0.9265, recall: 0.9545\n",
      "2019-01-17T20:54:38.573714, step: 663, loss: 0.2787244915962219, acc: 0.8828, auc: 0.9535, precision: 0.92, recall: 0.807\n",
      "2019-01-17T20:54:39.318837, step: 664, loss: 0.2516325116157532, acc: 0.9141, auc: 0.9516, precision: 0.9412, recall: 0.8571\n",
      "2019-01-17T20:54:40.061013, step: 665, loss: 0.2541850805282593, acc: 0.9219, auc: 0.9413, precision: 0.9375, recall: 0.8654\n",
      "2019-01-17T20:54:40.811440, step: 666, loss: 0.2751317024230957, acc: 0.8906, auc: 0.963, precision: 0.9552, recall: 0.8533\n",
      "2019-01-17T20:54:41.521186, step: 667, loss: 0.24291613698005676, acc: 0.9141, auc: 0.9631, precision: 0.931, recall: 0.8852\n",
      "2019-01-17T20:54:42.241526, step: 668, loss: 0.24176707863807678, acc: 0.9062, auc: 0.9646, precision: 0.95, recall: 0.8636\n",
      "2019-01-17T20:54:42.965316, step: 669, loss: 0.2723682224750519, acc: 0.9141, auc: 0.9498, precision: 0.9516, recall: 0.8806\n",
      "2019-01-17T20:54:43.646716, step: 670, loss: 0.35730546712875366, acc: 0.8906, auc: 0.9173, precision: 0.8475, recall: 0.9091\n",
      "2019-01-17T20:54:44.336238, step: 671, loss: 0.24030692875385284, acc: 0.9141, auc: 0.9548, precision: 0.9322, recall: 0.8871\n",
      "2019-01-17T20:54:45.094054, step: 672, loss: 0.24784953892230988, acc: 0.9141, auc: 0.9633, precision: 0.9, recall: 0.9403\n",
      "2019-01-17T20:54:45.814561, step: 673, loss: 0.33858248591423035, acc: 0.875, auc: 0.9358, precision: 0.8714, recall: 0.8971\n",
      "2019-01-17T20:54:46.517840, step: 674, loss: 0.2458849549293518, acc: 0.9062, auc: 0.9724, precision: 0.931, recall: 0.871\n",
      "2019-01-17T20:54:47.249259, step: 675, loss: 0.18588072061538696, acc: 0.9375, auc: 0.9767, precision: 0.9636, recall: 0.8983\n",
      "2019-01-17T20:54:47.987360, step: 676, loss: 0.31979790329933167, acc: 0.875, auc: 0.936, precision: 0.9242, recall: 0.8472\n",
      "2019-01-17T20:54:48.731388, step: 677, loss: 0.219183087348938, acc: 0.9297, auc: 0.9609, precision: 0.9649, recall: 0.8871\n",
      "2019-01-17T20:54:49.449746, step: 678, loss: 0.3091853857040405, acc: 0.8594, auc: 0.946, precision: 0.9231, recall: 0.8219\n",
      "2019-01-17T20:54:50.140167, step: 679, loss: 0.1746736615896225, acc: 0.9531, auc: 0.9731, precision: 0.9661, recall: 0.9344\n",
      "2019-01-17T20:54:50.816468, step: 680, loss: 0.13004711270332336, acc: 0.9609, auc: 0.9829, precision: 1.0, recall: 0.9231\n",
      "2019-01-17T20:54:51.489900, step: 681, loss: 0.23351413011550903, acc: 0.9219, auc: 0.9611, precision: 0.9322, recall: 0.9016\n",
      "2019-01-17T20:54:52.221762, step: 682, loss: 0.1731208860874176, acc: 0.9375, auc: 0.9792, precision: 0.9355, recall: 0.9355\n",
      "2019-01-17T20:54:52.891604, step: 683, loss: 0.3539235591888428, acc: 0.8672, auc: 0.9496, precision: 0.8333, recall: 0.9016\n",
      "2019-01-17T20:54:53.579975, step: 684, loss: 0.2724722623825073, acc: 0.8906, auc: 0.9706, precision: 0.8732, recall: 0.9254\n",
      "2019-01-17T20:54:54.318274, step: 685, loss: 0.24946120381355286, acc: 0.9375, auc: 0.9578, precision: 0.9254, recall: 0.9538\n",
      "2019-01-17T20:54:55.043719, step: 686, loss: 0.2313147783279419, acc: 0.9297, auc: 0.9608, precision: 0.9437, recall: 0.9306\n",
      "2019-01-17T20:54:55.776824, step: 687, loss: 0.19206546247005463, acc: 0.9062, auc: 0.9851, precision: 0.9194, recall: 0.8906\n",
      "2019-01-17T20:54:56.480133, step: 688, loss: 0.29810789227485657, acc: 0.8906, auc: 0.9569, precision: 0.8533, recall: 0.9552\n",
      "2019-01-17T20:54:57.211319, step: 689, loss: 0.19431564211845398, acc: 0.9375, auc: 0.9789, precision: 0.9483, recall: 0.9167\n",
      "2019-01-17T20:54:57.965615, step: 690, loss: 0.2139996886253357, acc: 0.9297, auc: 0.9766, precision: 0.9355, recall: 0.9206\n",
      "2019-01-17T20:54:58.720151, step: 691, loss: 0.19799768924713135, acc: 0.9219, auc: 0.9786, precision: 0.9839, recall: 0.8714\n",
      "2019-01-17T20:54:59.446502, step: 692, loss: 0.3136022090911865, acc: 0.875, auc: 0.9442, precision: 0.9245, recall: 0.8033\n",
      "2019-01-17T20:55:00.254306, step: 693, loss: 0.24419674277305603, acc: 0.8906, auc: 0.9589, precision: 0.9577, recall: 0.8608\n",
      "2019-01-17T20:55:00.945287, step: 694, loss: 0.20545268058776855, acc: 0.9219, auc: 0.9674, precision: 0.931, recall: 0.9\n",
      "2019-01-17T20:55:01.602716, step: 695, loss: 0.18257412314414978, acc: 0.9297, auc: 0.9786, precision: 0.9455, recall: 0.8966\n",
      "2019-01-17T20:55:02.278657, step: 696, loss: 0.2814589738845825, acc: 0.9297, auc: 0.9608, precision: 0.873, recall: 0.9821\n",
      "2019-01-17T20:55:02.964235, step: 697, loss: 0.2096027135848999, acc: 0.9453, auc: 0.9648, precision: 0.9375, recall: 0.9524\n",
      "2019-01-17T20:55:03.659513, step: 698, loss: 0.23998935520648956, acc: 0.9141, auc: 0.9526, precision: 0.9552, recall: 0.8889\n",
      "2019-01-17T20:55:04.425700, step: 699, loss: 0.23599687218666077, acc: 0.9141, auc: 0.9674, precision: 0.8889, recall: 0.9333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-17T20:55:05.131755, step: 700, loss: 0.25209516286849976, acc: 0.9141, auc: 0.969, precision: 0.8824, recall: 0.9524\n",
      "\n",
      "Evaluation:\n",
      "2019-01-17T20:55:32.805456, step: 700, loss: 0.4021969078442989, acc: 0.8497564102564101, auc: 0.9103769230769233, precision: 0.8517743589743588, recall: 0.8522000000000001\n",
      "Saved model checkpoint to ../model/Bi-LSTM/model/my-model-700\n",
      "\n",
      "2019-01-17T20:55:34.215735, step: 701, loss: 0.19632109999656677, acc: 0.9375, auc: 0.9795, precision: 0.9153, recall: 0.9474\n",
      "2019-01-17T20:55:34.907890, step: 702, loss: 0.3070557713508606, acc: 0.8906, auc: 0.9377, precision: 0.9048, recall: 0.8769\n",
      "2019-01-17T20:55:35.582039, step: 703, loss: 0.13140656054019928, acc: 0.9609, auc: 0.9801, precision: 0.9808, recall: 0.9273\n",
      "2019-01-17T20:55:36.317874, step: 704, loss: 0.26929986476898193, acc: 0.9062, auc: 0.9554, precision: 0.94, recall: 0.8393\n",
      "2019-01-17T20:55:37.049681, step: 705, loss: 0.19361257553100586, acc: 0.9297, auc: 0.9597, precision: 0.9818, recall: 0.871\n",
      "2019-01-17T20:55:37.780135, step: 706, loss: 0.2337489277124405, acc: 0.9141, auc: 0.9653, precision: 0.9508, recall: 0.8788\n",
      "2019-01-17T20:55:38.471415, step: 707, loss: 0.3109931945800781, acc: 0.8906, auc: 0.9461, precision: 0.9655, recall: 0.8235\n",
      "2019-01-17T20:55:39.194455, step: 708, loss: 0.18664595484733582, acc: 0.9219, auc: 0.9839, precision: 0.9333, recall: 0.9032\n",
      "2019-01-17T20:55:39.910729, step: 709, loss: 0.3833824694156647, acc: 0.8516, auc: 0.9265, precision: 0.9219, recall: 0.8082\n",
      "2019-01-17T20:55:40.625683, step: 710, loss: 0.2873876690864563, acc: 0.8984, auc: 0.9348, precision: 0.931, recall: 0.8571\n",
      "2019-01-17T20:55:41.335565, step: 711, loss: 0.11260976642370224, acc: 0.9688, auc: 0.9958, precision: 1.0, recall: 0.9394\n",
      "2019-01-17T20:55:42.057613, step: 712, loss: 0.20459383726119995, acc: 0.9297, auc: 0.9639, precision: 0.9661, recall: 0.8906\n",
      "2019-01-17T20:55:42.728929, step: 713, loss: 0.2088557779788971, acc: 0.9062, auc: 0.9714, precision: 0.9231, recall: 0.8955\n",
      "2019-01-17T20:55:43.419973, step: 714, loss: 0.2521216869354248, acc: 0.9297, auc: 0.9601, precision: 0.9571, recall: 0.9178\n",
      "2019-01-17T20:55:44.116005, step: 715, loss: 0.20938672125339508, acc: 0.9375, auc: 0.9721, precision: 0.9167, recall: 0.9706\n",
      "2019-01-17T20:55:44.806102, step: 716, loss: 0.20494410395622253, acc: 0.9453, auc: 0.9807, precision: 0.9016, recall: 0.9821\n",
      "2019-01-17T20:55:45.486270, step: 717, loss: 0.18520589172840118, acc: 0.9297, auc: 0.9882, precision: 0.8923, recall: 0.9667\n",
      "2019-01-17T20:55:46.161402, step: 718, loss: 0.2561328411102295, acc: 0.9062, auc: 0.9653, precision: 0.9062, recall: 0.9062\n",
      "2019-01-17T20:55:46.844064, step: 719, loss: 0.2011670619249344, acc: 0.9375, auc: 0.9691, precision: 0.9273, recall: 0.9273\n",
      "2019-01-17T20:55:47.589088, step: 720, loss: 0.18910714983940125, acc: 0.9453, auc: 0.965, precision: 0.9623, recall: 0.9107\n",
      "2019-01-17T20:55:48.356467, step: 721, loss: 0.3264838755130768, acc: 0.9062, auc: 0.9427, precision: 0.9455, recall: 0.8525\n",
      "2019-01-17T20:55:49.027111, step: 722, loss: 0.3168727159500122, acc: 0.8828, auc: 0.9659, precision: 0.9655, recall: 0.8116\n",
      "2019-01-17T20:55:49.696180, step: 723, loss: 0.3698495626449585, acc: 0.875, auc: 0.9358, precision: 0.9783, recall: 0.75\n",
      "2019-01-17T20:55:50.361728, step: 724, loss: 0.2690098285675049, acc: 0.9062, auc: 0.9527, precision: 0.9423, recall: 0.8448\n",
      "2019-01-17T20:55:51.099242, step: 725, loss: 0.4207865595817566, acc: 0.8047, auc: 0.9121, precision: 0.8714, recall: 0.7922\n",
      "2019-01-17T20:55:51.862520, step: 726, loss: 0.19943657517433167, acc: 0.9219, auc: 0.9732, precision: 0.9688, recall: 0.8857\n",
      "2019-01-17T20:55:52.579231, step: 727, loss: 0.2399040162563324, acc: 0.9219, auc: 0.957, precision: 0.9324, recall: 0.9324\n",
      "2019-01-17T20:55:53.351622, step: 728, loss: 0.37048646807670593, acc: 0.8906, auc: 0.9302, precision: 0.8481, recall: 0.971\n",
      "2019-01-17T20:55:54.025823, step: 729, loss: 0.26143956184387207, acc: 0.9141, auc: 0.9663, precision: 0.9048, recall: 0.9194\n",
      "2019-01-17T20:55:54.719585, step: 730, loss: 0.4043862819671631, acc: 0.875, auc: 0.9394, precision: 0.8088, recall: 0.9483\n",
      "2019-01-17T20:55:55.420784, step: 731, loss: 0.39017254114151, acc: 0.8438, auc: 0.9532, precision: 0.7857, recall: 0.9706\n",
      "2019-01-17T20:55:56.148414, step: 732, loss: 0.3563796281814575, acc: 0.8984, auc: 0.9563, precision: 0.8354, recall: 1.0\n",
      "2019-01-17T20:55:56.874081, step: 733, loss: 0.3637525737285614, acc: 0.8516, auc: 0.9753, precision: 0.7714, recall: 0.9474\n",
      "2019-01-17T20:55:57.688553, step: 734, loss: 0.23265856504440308, acc: 0.9062, auc: 0.9781, precision: 0.8816, recall: 0.9571\n",
      "2019-01-17T20:55:58.361255, step: 735, loss: 0.26084810495376587, acc: 0.9062, auc: 0.9643, precision: 0.9028, recall: 0.9286\n",
      "2019-01-17T20:55:59.018584, step: 736, loss: 0.2609667181968689, acc: 0.9297, auc: 0.9471, precision: 0.9194, recall: 0.9344\n",
      "2019-01-17T20:55:59.671503, step: 737, loss: 0.2589406371116638, acc: 0.9062, auc: 0.9656, precision: 0.8983, recall: 0.8983\n",
      "2019-01-17T20:56:00.417429, step: 738, loss: 0.16918590664863586, acc: 0.9531, auc: 0.9904, precision: 1.0, recall: 0.9118\n",
      "2019-01-17T20:56:01.074655, step: 739, loss: 0.17443257570266724, acc: 0.9219, auc: 0.994, precision: 1.0, recall: 0.863\n",
      "2019-01-17T20:56:01.768446, step: 740, loss: 0.3303315043449402, acc: 0.8359, auc: 0.9473, precision: 0.9623, recall: 0.7286\n",
      "2019-01-17T20:56:02.439284, step: 741, loss: 0.17747405171394348, acc: 0.9219, auc: 0.9872, precision: 0.9783, recall: 0.8333\n",
      "2019-01-17T20:56:03.075259, step: 742, loss: 0.17680524289608002, acc: 0.9062, auc: 0.9857, precision: 0.9592, recall: 0.8246\n",
      "2019-01-17T20:56:03.832787, step: 743, loss: 0.27451056241989136, acc: 0.8828, auc: 0.9704, precision: 1.0, recall: 0.7581\n",
      "2019-01-17T20:56:04.549827, step: 744, loss: 0.19713455438613892, acc: 0.9219, auc: 0.9821, precision: 0.963, recall: 0.8667\n",
      "2019-01-17T20:56:05.236853, step: 745, loss: 0.2616254389286041, acc: 0.8984, auc: 0.9523, precision: 0.9623, recall: 0.8226\n",
      "2019-01-17T20:56:05.902587, step: 746, loss: 0.18627144396305084, acc: 0.9297, auc: 0.9812, precision: 0.9608, recall: 0.875\n",
      "2019-01-17T20:56:06.557738, step: 747, loss: 0.210523322224617, acc: 0.9453, auc: 0.9585, precision: 0.9538, recall: 0.9394\n",
      "2019-01-17T20:56:07.278552, step: 748, loss: 0.2247088998556137, acc: 0.9219, auc: 0.9632, precision: 0.8889, recall: 0.9492\n",
      "2019-01-17T20:56:07.956464, step: 749, loss: 0.22785404324531555, acc: 0.9219, auc: 0.9717, precision: 0.9103, recall: 0.9595\n",
      "2019-01-17T20:56:08.594266, step: 750, loss: 0.18524077534675598, acc: 0.9219, auc: 0.9836, precision: 0.9375, recall: 0.9091\n",
      "2019-01-17T20:56:09.269906, step: 751, loss: 0.31555312871932983, acc: 0.8984, auc: 0.9431, precision: 0.8448, recall: 0.9245\n",
      "2019-01-17T20:56:09.996156, step: 752, loss: 0.2384846806526184, acc: 0.9219, auc: 0.9596, precision: 0.9524, recall: 0.8955\n",
      "2019-01-17T20:56:10.732771, step: 753, loss: 0.3043227195739746, acc: 0.9062, auc: 0.9289, precision: 0.9211, recall: 0.9211\n",
      "2019-01-17T20:56:11.443989, step: 754, loss: 0.20431604981422424, acc: 0.9297, auc: 0.9706, precision: 0.918, recall: 0.9333\n",
      "2019-01-17T20:56:12.102957, step: 755, loss: 0.15037763118743896, acc: 0.9531, auc: 0.9886, precision: 0.9577, recall: 0.9577\n",
      "2019-01-17T20:56:12.810236, step: 756, loss: 0.2807163596153259, acc: 0.9141, auc: 0.9437, precision: 0.8772, recall: 0.9259\n",
      "2019-01-17T20:56:13.480232, step: 757, loss: 0.2821674048900604, acc: 0.9062, auc: 0.9556, precision: 0.8667, recall: 0.9286\n",
      "2019-01-17T20:56:14.195280, step: 758, loss: 0.22704032063484192, acc: 0.9375, auc: 0.9561, precision: 0.9839, recall: 0.8971\n",
      "2019-01-17T20:56:14.857471, step: 759, loss: 0.238137349486351, acc: 0.9219, auc: 0.9594, precision: 0.9054, recall: 0.9571\n",
      "2019-01-17T20:56:15.598177, step: 760, loss: 0.12889796495437622, acc: 0.9375, auc: 0.9901, precision: 0.9455, recall: 0.9123\n",
      "2019-01-17T20:56:16.350936, step: 761, loss: 0.37651869654655457, acc: 0.8672, auc: 0.9223, precision: 0.8933, recall: 0.8816\n",
      "2019-01-17T20:56:17.087037, step: 762, loss: 0.2765592634677887, acc: 0.9062, auc: 0.945, precision: 0.9091, recall: 0.9091\n",
      "2019-01-17T20:56:17.817577, step: 763, loss: 0.1690768003463745, acc: 0.9453, auc: 0.9783, precision: 0.9394, recall: 0.9538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-17T20:56:18.497394, step: 764, loss: 0.22246429324150085, acc: 0.9141, auc: 0.9653, precision: 0.9219, recall: 0.9077\n",
      "2019-01-17T20:56:19.131026, step: 765, loss: 0.25175368785858154, acc: 0.9219, auc: 0.9636, precision: 0.9365, recall: 0.9077\n",
      "2019-01-17T20:56:19.790312, step: 766, loss: 0.3127237856388092, acc: 0.8672, auc: 0.9446, precision: 0.9286, recall: 0.8\n",
      "2019-01-17T20:56:20.518600, step: 767, loss: 0.34583261609077454, acc: 0.875, auc: 0.9385, precision: 0.9623, recall: 0.7846\n",
      "2019-01-17T20:56:21.200548, step: 768, loss: 0.26411736011505127, acc: 0.8594, auc: 0.96, precision: 0.8333, recall: 0.8333\n",
      "2019-01-17T20:56:21.860932, step: 769, loss: 0.298117458820343, acc: 0.8438, auc: 0.9559, precision: 0.95, recall: 0.6786\n",
      "2019-01-17T20:56:22.603531, step: 770, loss: 0.250280499458313, acc: 0.9219, auc: 0.9536, precision: 0.9492, recall: 0.8889\n",
      "2019-01-17T20:56:23.311053, step: 771, loss: 0.2784368693828583, acc: 0.875, auc: 0.9594, precision: 0.8387, recall: 0.8966\n",
      "2019-01-17T20:56:24.005498, step: 772, loss: 0.2933769226074219, acc: 0.8828, auc: 0.9483, precision: 0.8986, recall: 0.8857\n",
      "2019-01-17T20:56:24.724868, step: 773, loss: 0.31375229358673096, acc: 0.8828, auc: 0.9471, precision: 0.8824, recall: 0.8955\n",
      "2019-01-17T20:56:25.439993, step: 774, loss: 0.2562916874885559, acc: 0.9219, auc: 0.9593, precision: 0.9167, recall: 0.9167\n",
      "2019-01-17T20:56:26.155756, step: 775, loss: 0.24087825417518616, acc: 0.8828, auc: 0.9732, precision: 0.8868, recall: 0.8393\n",
      "2019-01-17T20:56:26.826909, step: 776, loss: 0.2985602617263794, acc: 0.8672, auc: 0.9461, precision: 0.8923, recall: 0.8529\n",
      "2019-01-17T20:56:27.596377, step: 777, loss: 0.23597052693367004, acc: 0.9219, auc: 0.9649, precision: 0.9153, recall: 0.9153\n",
      "2019-01-17T20:56:28.318975, step: 778, loss: 0.2966151535511017, acc: 0.8516, auc: 0.9481, precision: 0.9286, recall: 0.7761\n",
      "2019-01-17T20:56:28.995734, step: 779, loss: 0.14173275232315063, acc: 0.9609, auc: 0.9954, precision: 0.9839, recall: 0.9385\n",
      "2019-01-17T20:56:29.756731, step: 780, loss: 0.3182990849018097, acc: 0.8438, auc: 0.9407, precision: 0.8696, recall: 0.8451\n",
      "start training model\n",
      "2019-01-17T20:56:30.416955, step: 781, loss: 0.17411410808563232, acc: 0.9141, auc: 0.9848, precision: 1.0, recall: 0.8493\n",
      "2019-01-17T20:56:31.085260, step: 782, loss: 0.24510298669338226, acc: 0.8984, auc: 0.9666, precision: 0.96, recall: 0.8136\n",
      "2019-01-17T20:56:31.807564, step: 783, loss: 0.2651960849761963, acc: 0.8672, auc: 0.9687, precision: 1.0, recall: 0.7258\n",
      "2019-01-17T20:56:32.607490, step: 784, loss: 0.2578698992729187, acc: 0.8672, auc: 0.9899, precision: 1.0, recall: 0.7763\n",
      "2019-01-17T20:56:33.264186, step: 785, loss: 0.3651137948036194, acc: 0.8125, auc: 0.9555, precision: 0.9636, recall: 0.7067\n",
      "2019-01-17T20:56:33.955253, step: 786, loss: 0.20800445973873138, acc: 0.9141, auc: 0.9924, precision: 1.0, recall: 0.8333\n",
      "2019-01-17T20:56:34.697612, step: 787, loss: 0.2409026324748993, acc: 0.9141, auc: 0.9635, precision: 0.9118, recall: 0.9254\n",
      "2019-01-17T20:56:35.448715, step: 788, loss: 0.13419300317764282, acc: 0.9688, auc: 0.9978, precision: 0.9661, recall: 0.9661\n",
      "2019-01-17T20:56:36.148057, step: 789, loss: 0.2455664575099945, acc: 0.9219, auc: 0.9669, precision: 0.9692, recall: 0.8873\n",
      "2019-01-17T20:56:36.883638, step: 790, loss: 0.26340457797050476, acc: 0.9062, auc: 0.9548, precision: 0.8701, recall: 0.971\n",
      "2019-01-17T20:56:37.606409, step: 791, loss: 0.27250733971595764, acc: 0.8906, auc: 0.957, precision: 0.8696, recall: 0.9231\n",
      "2019-01-17T20:56:38.353856, step: 792, loss: 0.23860067129135132, acc: 0.9141, auc: 0.9768, precision: 0.873, recall: 0.9483\n",
      "2019-01-17T20:56:39.117975, step: 793, loss: 0.21725167334079742, acc: 0.9531, auc: 0.956, precision: 0.9692, recall: 0.9403\n",
      "2019-01-17T20:56:39.829541, step: 794, loss: 0.1873856633901596, acc: 0.9531, auc: 0.9707, precision: 0.9254, recall: 0.9841\n",
      "2019-01-17T20:56:40.486906, step: 795, loss: 0.17392820119857788, acc: 0.9453, auc: 0.9878, precision: 0.9655, recall: 0.918\n",
      "2019-01-17T20:56:41.202199, step: 796, loss: 0.2020794302225113, acc: 0.9453, auc: 0.9661, precision: 1.0, recall: 0.8772\n",
      "2019-01-17T20:56:41.932030, step: 797, loss: 0.25794482231140137, acc: 0.9219, auc: 0.9659, precision: 1.0, recall: 0.863\n",
      "2019-01-17T20:56:42.639144, step: 798, loss: 0.21158486604690552, acc: 0.9141, auc: 0.9714, precision: 0.9403, recall: 0.9\n",
      "2019-01-17T20:56:43.410190, step: 799, loss: 0.2194703370332718, acc: 0.9297, auc: 0.9685, precision: 0.9592, recall: 0.8704\n",
      "2019-01-17T20:56:44.078536, step: 800, loss: 0.14030490815639496, acc: 0.9375, auc: 0.9887, precision: 0.9636, recall: 0.8983\n",
      "\n",
      "Evaluation:\n",
      "2019-01-17T20:57:11.537803, step: 800, loss: 0.4270093716107882, acc: 0.8451589743589745, auc: 0.9064102564102564, precision: 0.8975564102564101, recall: 0.7829794871794874\n",
      "Saved model checkpoint to ../model/Bi-LSTM/model/my-model-800\n",
      "\n",
      "2019-01-17T20:57:12.860614, step: 801, loss: 0.1861238181591034, acc: 0.9453, auc: 0.964, precision: 0.9688, recall: 0.9254\n",
      "2019-01-17T20:57:13.560815, step: 802, loss: 0.22976990044116974, acc: 0.9219, auc: 0.966, precision: 0.9333, recall: 0.9032\n",
      "2019-01-17T20:57:14.243466, step: 803, loss: 0.18931587040424347, acc: 0.9297, auc: 0.9758, precision: 0.9355, recall: 0.9206\n",
      "2019-01-17T20:57:14.920994, step: 804, loss: 0.22354906797409058, acc: 0.9297, auc: 0.9713, precision: 0.8871, recall: 0.9649\n",
      "2019-01-17T20:57:15.570226, step: 805, loss: 0.15430578589439392, acc: 0.9609, auc: 0.9834, precision: 0.9672, recall: 0.9516\n",
      "2019-01-17T20:57:16.334862, step: 806, loss: 0.10701432824134827, acc: 0.9766, auc: 0.9931, precision: 0.9722, recall: 0.9859\n",
      "2019-01-17T20:57:17.012078, step: 807, loss: 0.2618681788444519, acc: 0.9219, auc: 0.9609, precision: 0.9091, recall: 0.9375\n",
      "2019-01-17T20:57:17.696628, step: 808, loss: 0.18344418704509735, acc: 0.9453, auc: 0.9678, precision: 0.9595, recall: 0.9467\n",
      "2019-01-17T20:57:18.428755, step: 809, loss: 0.18907898664474487, acc: 0.9453, auc: 0.9733, precision: 0.9344, recall: 0.95\n",
      "2019-01-17T20:57:19.097871, step: 810, loss: 0.11635930836200714, acc: 0.9766, auc: 0.9937, precision: 0.9844, recall: 0.9692\n",
      "2019-01-17T20:57:19.798812, step: 811, loss: 0.21520540118217468, acc: 0.9219, auc: 0.9751, precision: 0.9365, recall: 0.9077\n",
      "2019-01-17T20:57:20.498961, step: 812, loss: 0.16837772727012634, acc: 0.9297, auc: 0.9801, precision: 0.9474, recall: 0.9\n",
      "2019-01-17T20:57:21.222872, step: 813, loss: 0.24244225025177002, acc: 0.9141, auc: 0.966, precision: 1.0, recall: 0.8333\n",
      "2019-01-17T20:57:21.853342, step: 814, loss: 0.2074015885591507, acc: 0.9062, auc: 0.9855, precision: 0.9833, recall: 0.8429\n",
      "2019-01-17T20:57:22.527860, step: 815, loss: 0.20145565271377563, acc: 0.9141, auc: 0.9731, precision: 0.95, recall: 0.8769\n",
      "2019-01-17T20:57:23.199054, step: 816, loss: 0.11845823377370834, acc: 0.9688, auc: 0.9959, precision: 0.9434, recall: 0.9804\n",
      "2019-01-17T20:57:23.955050, step: 817, loss: 0.1977182775735855, acc: 0.9219, auc: 0.9677, precision: 0.918, recall: 0.918\n",
      "2019-01-17T20:57:24.622624, step: 818, loss: 0.16409368813037872, acc: 0.9531, auc: 0.9804, precision: 0.9844, recall: 0.9265\n",
      "2019-01-17T20:57:25.263928, step: 819, loss: 0.222921684384346, acc: 0.9297, auc: 0.9719, precision: 0.9143, recall: 0.9552\n",
      "2019-01-17T20:57:26.061117, step: 820, loss: 0.16553525626659393, acc: 0.9609, auc: 0.9828, precision: 0.9167, recall: 1.0\n",
      "2019-01-17T20:57:26.767004, step: 821, loss: 0.14866980910301208, acc: 0.9531, auc: 0.9875, precision: 1.0, recall: 0.9189\n",
      "2019-01-17T20:57:27.440794, step: 822, loss: 0.16718937456607819, acc: 0.9375, auc: 0.9841, precision: 0.9492, recall: 0.918\n",
      "2019-01-17T20:57:28.063454, step: 823, loss: 0.29659849405288696, acc: 0.8984, auc: 0.9443, precision: 0.8947, recall: 0.8793\n",
      "2019-01-17T20:57:28.699766, step: 824, loss: 0.1539551317691803, acc: 0.9609, auc: 0.9788, precision: 0.9531, recall: 0.9683\n",
      "2019-01-17T20:57:29.398431, step: 825, loss: 0.16601310670375824, acc: 0.9375, auc: 0.9827, precision: 0.9701, recall: 0.9155\n",
      "2019-01-17T20:57:30.107034, step: 826, loss: 0.17025470733642578, acc: 0.9531, auc: 0.9685, precision: 0.9688, recall: 0.9394\n",
      "2019-01-17T20:57:30.855408, step: 827, loss: 0.21080732345581055, acc: 0.9219, auc: 0.9714, precision: 0.9492, recall: 0.8889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-17T20:57:31.667373, step: 828, loss: 0.23583680391311646, acc: 0.9141, auc: 0.969, precision: 0.8772, recall: 0.9259\n",
      "2019-01-17T20:57:32.365172, step: 829, loss: 0.12794309854507446, acc: 0.9531, auc: 0.9892, precision: 0.9688, recall: 0.9394\n",
      "2019-01-17T20:57:33.169616, step: 830, loss: 0.28193625807762146, acc: 0.9062, auc: 0.9389, precision: 0.931, recall: 0.871\n",
      "2019-01-17T20:57:33.870559, step: 831, loss: 0.09028791636228561, acc: 0.9844, auc: 0.9908, precision: 0.9863, recall: 0.9863\n",
      "2019-01-17T20:57:34.575366, step: 832, loss: 0.21294032037258148, acc: 0.9375, auc: 0.9604, precision: 0.9492, recall: 0.918\n",
      "2019-01-17T20:57:35.312094, step: 833, loss: 0.19665855169296265, acc: 0.9375, auc: 0.9817, precision: 0.9254, recall: 0.9538\n",
      "2019-01-17T20:57:36.060038, step: 834, loss: 0.20019574463367462, acc: 0.9375, auc: 0.9676, precision: 0.9577, recall: 0.9315\n",
      "2019-01-17T20:57:36.788910, step: 835, loss: 0.20649504661560059, acc: 0.9453, auc: 0.9699, precision: 0.9231, recall: 0.9677\n",
      "2019-01-17T20:57:37.508670, step: 836, loss: 0.11793061345815659, acc: 0.9688, auc: 0.9949, precision: 0.9516, recall: 0.9833\n",
      "2019-01-17T20:57:38.200400, step: 837, loss: 0.1985650360584259, acc: 0.9297, auc: 0.981, precision: 0.9219, recall: 0.9365\n",
      "2019-01-17T20:57:38.906316, step: 838, loss: 0.2136717140674591, acc: 0.9219, auc: 0.9662, precision: 0.9385, recall: 0.9104\n",
      "2019-01-17T20:57:39.624284, step: 839, loss: 0.11586765944957733, acc: 0.9688, auc: 0.9884, precision: 0.9459, recall: 1.0\n",
      "2019-01-17T20:57:40.411384, step: 840, loss: 0.11557675898075104, acc: 0.9688, auc: 0.9907, precision: 0.9697, recall: 0.9697\n",
      "2019-01-17T20:57:41.137818, step: 841, loss: 0.1797778606414795, acc: 0.9219, auc: 0.9726, precision: 0.9667, recall: 0.8788\n",
      "2019-01-17T20:57:41.832243, step: 842, loss: 0.27249959111213684, acc: 0.8828, auc: 0.9569, precision: 0.9412, recall: 0.8\n",
      "2019-01-17T20:57:42.509377, step: 843, loss: 0.15032319724559784, acc: 0.9453, auc: 0.9868, precision: 0.9365, recall: 0.9516\n",
      "2019-01-17T20:57:43.267851, step: 844, loss: 0.17932769656181335, acc: 0.9375, auc: 0.9778, precision: 0.95, recall: 0.9194\n",
      "2019-01-17T20:57:43.956130, step: 845, loss: 0.127033531665802, acc: 0.9609, auc: 0.99, precision: 1.0, recall: 0.9206\n",
      "2019-01-17T20:57:44.653918, step: 846, loss: 0.12826912105083466, acc: 0.9531, auc: 0.9861, precision: 0.9825, recall: 0.918\n",
      "2019-01-17T20:57:45.286504, step: 847, loss: 0.31875211000442505, acc: 0.875, auc: 0.9402, precision: 0.9167, recall: 0.7857\n",
      "2019-01-17T20:57:46.001046, step: 848, loss: 0.1796526312828064, acc: 0.9062, auc: 0.9834, precision: 0.9153, recall: 0.8852\n",
      "2019-01-17T20:57:46.686607, step: 849, loss: 0.10917516052722931, acc: 0.9609, auc: 0.997, precision: 0.9273, recall: 0.9808\n",
      "2019-01-17T20:57:47.455598, step: 850, loss: 0.2700696587562561, acc: 0.9141, auc: 0.9565, precision: 0.8806, recall: 0.9516\n",
      "2019-01-17T20:57:48.125020, step: 851, loss: 0.11520669609308243, acc: 0.9531, auc: 0.9923, precision: 0.9853, recall: 0.9306\n",
      "2019-01-17T20:57:48.844433, step: 852, loss: 0.1575702726840973, acc: 0.9688, auc: 0.9795, precision: 0.9836, recall: 0.9524\n",
      "2019-01-17T20:57:49.595061, step: 853, loss: 0.19628235697746277, acc: 0.9141, auc: 0.9745, precision: 0.9592, recall: 0.8393\n",
      "2019-01-17T20:57:50.380781, step: 854, loss: 0.1917257308959961, acc: 0.9297, auc: 0.9778, precision: 0.9706, recall: 0.9041\n",
      "2019-01-17T20:57:51.078183, step: 855, loss: 0.23847192525863647, acc: 0.9219, auc: 0.9707, precision: 0.9412, recall: 0.9143\n",
      "2019-01-17T20:57:51.765855, step: 856, loss: 0.1042962372303009, acc: 0.9766, auc: 0.9929, precision: 0.9688, recall: 0.9841\n",
      "2019-01-17T20:57:52.526935, step: 857, loss: 0.23179981112480164, acc: 0.9062, auc: 0.9709, precision: 0.96, recall: 0.8276\n",
      "2019-01-17T20:57:53.161268, step: 858, loss: 0.24036821722984314, acc: 0.9219, auc: 0.957, precision: 0.9661, recall: 0.8769\n",
      "2019-01-17T20:57:53.808860, step: 859, loss: 0.21171802282333374, acc: 0.9531, auc: 0.9531, precision: 0.9577, recall: 0.9577\n",
      "2019-01-17T20:57:54.470536, step: 860, loss: 0.18548715114593506, acc: 0.9375, auc: 0.9731, precision: 0.9531, recall: 0.9242\n",
      "2019-01-17T20:57:55.125716, step: 861, loss: 0.144000843167305, acc: 0.9531, auc: 0.9873, precision: 0.9394, recall: 0.9688\n",
      "2019-01-17T20:57:55.794618, step: 862, loss: 0.1797083169221878, acc: 0.9297, auc: 0.9826, precision: 0.9524, recall: 0.9091\n",
      "2019-01-17T20:57:56.518179, step: 863, loss: 0.11837253719568253, acc: 0.9531, auc: 0.9861, precision: 0.9516, recall: 0.9516\n",
      "2019-01-17T20:57:57.213412, step: 864, loss: 0.21858087182044983, acc: 0.9297, auc: 0.9653, precision: 0.9219, recall: 0.9365\n",
      "2019-01-17T20:57:57.904625, step: 865, loss: 0.12329883128404617, acc: 0.9531, auc: 0.9941, precision: 0.9655, recall: 0.9333\n",
      "2019-01-17T20:57:58.623419, step: 866, loss: 0.104677215218544, acc: 0.9609, auc: 0.9946, precision: 0.9815, recall: 0.9298\n",
      "2019-01-17T20:57:59.368978, step: 867, loss: 0.15569519996643066, acc: 0.9297, auc: 0.9873, precision: 0.9815, recall: 0.8689\n",
      "2019-01-17T20:58:00.154193, step: 868, loss: 0.16326844692230225, acc: 0.9453, auc: 0.9791, precision: 0.9846, recall: 0.9143\n",
      "2019-01-17T20:58:00.877408, step: 869, loss: 0.1623496115207672, acc: 0.9141, auc: 0.9853, precision: 0.9333, recall: 0.8889\n",
      "2019-01-17T20:58:01.533022, step: 870, loss: 0.1644134670495987, acc: 0.9531, auc: 0.9714, precision: 0.9483, recall: 0.9483\n",
      "2019-01-17T20:58:02.236947, step: 871, loss: 0.15436144173145294, acc: 0.9531, auc: 0.9784, precision: 0.9492, recall: 0.9492\n",
      "2019-01-17T20:58:02.898329, step: 872, loss: 0.21304278075695038, acc: 0.9219, auc: 0.9686, precision: 0.9153, recall: 0.9153\n",
      "2019-01-17T20:58:03.554016, step: 873, loss: 0.14694488048553467, acc: 0.9297, auc: 0.9906, precision: 0.9692, recall: 0.9\n",
      "2019-01-17T20:58:04.218748, step: 874, loss: 0.2963714301586151, acc: 0.8828, auc: 0.9543, precision: 0.8451, recall: 0.9375\n",
      "2019-01-17T20:58:04.911828, step: 875, loss: 0.1490011215209961, acc: 0.9531, auc: 0.9912, precision: 0.9344, recall: 0.9661\n",
      "2019-01-17T20:58:05.569293, step: 876, loss: 0.13520488142967224, acc: 0.9766, auc: 0.9877, precision: 0.9831, recall: 0.9667\n",
      "2019-01-17T20:58:06.229653, step: 877, loss: 0.24262835085391998, acc: 0.9141, auc: 0.9679, precision: 0.9014, recall: 0.9412\n",
      "2019-01-17T20:58:06.947208, step: 878, loss: 0.10142260044813156, acc: 0.9766, auc: 0.9932, precision: 0.9841, recall: 0.9688\n",
      "2019-01-17T20:58:07.632738, step: 879, loss: 0.14544934034347534, acc: 0.9609, auc: 0.9807, precision: 0.9672, recall: 0.9516\n",
      "2019-01-17T20:58:08.315080, step: 880, loss: 0.1894763708114624, acc: 0.9453, auc: 0.977, precision: 0.9655, recall: 0.918\n",
      "2019-01-17T20:58:08.966731, step: 881, loss: 0.10040780156850815, acc: 0.9688, auc: 0.9934, precision: 1.0, recall: 0.942\n",
      "2019-01-17T20:58:09.604027, step: 882, loss: 0.16962724924087524, acc: 0.9375, auc: 0.9807, precision: 0.9298, recall: 0.9298\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-5e3f22166588>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"start training model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mbatchTrain\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnextBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainReviews\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainLabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatchSize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m                 \u001b[0mtrainStep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatchTrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatchTrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0mcurrentStep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobalStep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-5e3f22166588>\u001b[0m in \u001b[0;36mtrainStep\u001b[0;34m(batchX, batchY)\u001b[0m\n\u001b[1;32m     68\u001b[0m             _, summary, step, loss, predictions, binaryPreds = sess.run(\n\u001b[1;32m     69\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mtrainOp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummaryOp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobalStep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinaryPreds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m                 feed_dict)\n\u001b[0m\u001b[1;32m     71\u001b[0m             \u001b[0mtimeStr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misoformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenMetrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatchY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinaryPreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/jiang/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/jiang/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1100\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/jiang/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1272\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1273\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/jiang/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1276\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/jiang/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1261\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1263\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/jiang/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "\n",
    "# 生成训练集和验证集\n",
    "trainReviews = data.trainReviews\n",
    "trainLabels = data.trainLabels\n",
    "evalReviews = data.evalReviews\n",
    "evalLabels = data.evalLabels\n",
    "\n",
    "wordEmbedding = data.wordEmbedding\n",
    "\n",
    "# 定义计算图\n",
    "with tf.Graph().as_default():\n",
    "\n",
    "    session_conf = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
    "    session_conf.gpu_options.allow_growth=True\n",
    "    session_conf.gpu_options.per_process_gpu_memory_fraction = 0.9  # 配置gpu占用率  \n",
    "\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    \n",
    "    # 定义会话\n",
    "    with sess.as_default():\n",
    "        lstm = BiLSTM(config, wordEmbedding)\n",
    "        \n",
    "        globalStep = tf.Variable(0, name=\"globalStep\", trainable=False)\n",
    "        # 定义优化函数，传入学习速率参数\n",
    "        optimizer = tf.train.AdamOptimizer(config.training.learningRate)\n",
    "        # 计算梯度,得到梯度和变量\n",
    "        gradsAndVars = optimizer.compute_gradients(lstm.loss)\n",
    "        # 将梯度应用到变量下，生成训练器\n",
    "        trainOp = optimizer.apply_gradients(gradsAndVars, global_step=globalStep)\n",
    "        \n",
    "        # 用summary绘制tensorBoard\n",
    "        gradSummaries = []\n",
    "        for g, v in gradsAndVars:\n",
    "            if g is not None:\n",
    "                tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "        \n",
    "        outDir = os.path.abspath(os.path.join(os.path.curdir, \"summarys\"))\n",
    "        print(\"Writing to {}\\n\".format(outDir))\n",
    "        \n",
    "        lossSummary = tf.summary.scalar(\"loss\", lstm.loss)\n",
    "        summaryOp = tf.summary.merge_all()\n",
    "        \n",
    "        trainSummaryDir = os.path.join(outDir, \"train\")\n",
    "        trainSummaryWriter = tf.summary.FileWriter(trainSummaryDir, sess.graph)\n",
    "        \n",
    "        evalSummaryDir = os.path.join(outDir, \"eval\")\n",
    "        evalSummaryWriter = tf.summary.FileWriter(evalSummaryDir, sess.graph)\n",
    "        \n",
    "        \n",
    "        # 初始化所有变量\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=5)\n",
    "        \n",
    "        # 保存模型的一种方式，保存为pb文件\n",
    "        builder = tf.saved_model.builder.SavedModelBuilder(\"../model/Bi-LSTM/savedModel\")\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        def trainStep(batchX, batchY):\n",
    "            \"\"\"\n",
    "            训练函数\n",
    "            \"\"\"   \n",
    "            feed_dict = {\n",
    "              lstm.inputX: batchX,\n",
    "              lstm.inputY: batchY,\n",
    "              lstm.dropoutKeepProb: config.model.dropoutKeepProb\n",
    "            }\n",
    "            _, summary, step, loss, predictions, binaryPreds = sess.run(\n",
    "                [trainOp, summaryOp, globalStep, lstm.loss, lstm.predictions, lstm.binaryPreds],\n",
    "                feed_dict)\n",
    "            timeStr = datetime.datetime.now().isoformat()\n",
    "            acc, auc, precision, recall = genMetrics(batchY, predictions, binaryPreds)\n",
    "            print(\"{}, step: {}, loss: {}, acc: {}, auc: {}, precision: {}, recall: {}\".format(timeStr, step, loss, acc, auc, precision, recall))\n",
    "            trainSummaryWriter.add_summary(summary, step)\n",
    "\n",
    "        def devStep(batchX, batchY):\n",
    "            \"\"\"\n",
    "            验证函数\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              lstm.inputX: batchX,\n",
    "              lstm.inputY: batchY,\n",
    "              lstm.dropoutKeepProb: 1.0\n",
    "            }\n",
    "            summary, step, loss, predictions, binaryPreds = sess.run(\n",
    "                [summaryOp, globalStep, lstm.loss, lstm.predictions, lstm.binaryPreds],\n",
    "                feed_dict)\n",
    "            \n",
    "            acc, auc, precision, recall = genMetrics(batchY, predictions, binaryPreds)\n",
    "            \n",
    "            evalSummaryWriter.add_summary(summary, step)\n",
    "            \n",
    "            return loss, acc, auc, precision, recall\n",
    "        \n",
    "        for i in range(config.training.epoches):\n",
    "            # 训练模型\n",
    "            print(\"start training model\")\n",
    "            for batchTrain in nextBatch(trainReviews, trainLabels, config.batchSize):\n",
    "                trainStep(batchTrain[0], batchTrain[1])\n",
    "\n",
    "                currentStep = tf.train.global_step(sess, globalStep) \n",
    "                if currentStep % config.training.evaluateEvery == 0:\n",
    "                    print(\"\\nEvaluation:\")\n",
    "                    \n",
    "                    losses = []\n",
    "                    accs = []\n",
    "                    aucs = []\n",
    "                    precisions = []\n",
    "                    recalls = []\n",
    "                    \n",
    "                    for batchEval in nextBatch(evalReviews, evalLabels, config.batchSize):\n",
    "                        loss, acc, auc, precision, recall = devStep(batchEval[0], batchEval[1])\n",
    "                        losses.append(loss)\n",
    "                        accs.append(acc)\n",
    "                        aucs.append(auc)\n",
    "                        precisions.append(precision)\n",
    "                        recalls.append(recall)\n",
    "                        \n",
    "                    time_str = datetime.datetime.now().isoformat()\n",
    "                    print(\"{}, step: {}, loss: {}, acc: {}, auc: {}, precision: {}, recall: {}\".format(time_str, currentStep, mean(losses), \n",
    "                                                                                                       mean(accs), mean(aucs), mean(precisions),\n",
    "                                                                                                       mean(recalls)))\n",
    "                    \n",
    "                if currentStep % config.training.checkpointEvery == 0:\n",
    "                    # 保存模型的另一种方法，保存checkpoint文件\n",
    "                    path = saver.save(sess, \"../model/Bi-LSTM/model/my-model\", global_step=currentStep)\n",
    "                    print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "                    \n",
    "        inputs = {\"inputX\": tf.saved_model.utils.build_tensor_info(lstm.inputX),\n",
    "                  \"keepProb\": tf.saved_model.utils.build_tensor_info(lstm.dropoutKeepProb)}\n",
    "\n",
    "        outputs = {\"binaryPreds\": tf.saved_model.utils.build_tensor_info(lstm.binaryPreds)}\n",
    "\n",
    "        prediction_signature = tf.saved_model.signature_def_utils.build_signature_def(inputs=inputs, outputs=outputs,\n",
    "                                                                                      method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME)\n",
    "        legacy_init_op = tf.group(tf.tables_initializer(), name=\"legacy_init_op\")\n",
    "        builder.add_meta_graph_and_variables(sess, [tf.saved_model.tag_constants.SERVING],\n",
    "                                            signature_def_map={\"predict\": prediction_signature}, legacy_init_op=legacy_init_op)\n",
    "\n",
    "        builder.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
