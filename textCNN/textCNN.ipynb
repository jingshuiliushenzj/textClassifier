{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import json\n",
    "\n",
    "from collections import Counter\n",
    "from math import sqrt\n",
    "\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置参数\n",
    "\n",
    "class TrainingConfig(object):\n",
    "    epoches = 10\n",
    "    evaluateEvery = 100\n",
    "    checkpointEvery = 100\n",
    "    learningRate = 0.001\n",
    "    \n",
    "class ModelConfig(object):\n",
    "    embeddingSize = 200\n",
    "    numFilters = 128\n",
    "\n",
    "    filterSizes = [2, 3, 4, 5]\n",
    "    dropoutKeepProb = 0.5\n",
    "    l2RegLambda = 0.0\n",
    "    \n",
    "class Config(object):\n",
    "    sequenceLength = 200  # 取了所有序列长度的均值\n",
    "    batchSize = 128\n",
    "    \n",
    "    dataSource = \"../data/preProcess/labeledTrain.csv\"\n",
    "    \n",
    "    stopWordSource = \"../data/english\"\n",
    "    \n",
    "    numClasses = 2\n",
    "    \n",
    "    rate = 0.8  # 训练集的比例\n",
    "    \n",
    "    training = TrainingConfig()\n",
    "    \n",
    "    model = ModelConfig()\n",
    "\n",
    "    \n",
    "# 实例化配置参数对象\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiangxinyang848/anaconda3/envs/jiang/lib/python3.5/site-packages/ipykernel_launcher.py:130: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
     ]
    }
   ],
   "source": [
    "# 数据预处理的类，生成训练集和测试集\n",
    "\n",
    "class Dataset(object):\n",
    "    def __init__(self, config):\n",
    "        self._dataSource = config.dataSource\n",
    "        self._stopWordSource = config.stopWordSource  \n",
    "        \n",
    "        self._sequenceLength = config.sequenceLength  # 每条输入的序列处理为定长\n",
    "        self._embeddingSize = config.model.embeddingSize\n",
    "        self._batchSize = config.batchSize\n",
    "        self._rate = config.rate\n",
    "        \n",
    "        self._stopWordDict = {}\n",
    "        \n",
    "        self.trainReviews = []\n",
    "        self.trainLabels = []\n",
    "        \n",
    "        self.evalReviews = []\n",
    "        self.evalLabels = []\n",
    "        \n",
    "        self.wordEmbedding =None\n",
    "        \n",
    "        self._wordToIndex = {}\n",
    "        self._indexToWord = {}\n",
    "        \n",
    "    def _readData(self, filePath):\n",
    "        \"\"\"\n",
    "        从csv文件中读取数据集\n",
    "        \"\"\"\n",
    "        \n",
    "        df = pd.read_csv(filePath)\n",
    "        labels = df[\"sentiment\"].tolist()\n",
    "        review = df[\"review\"].tolist()\n",
    "        reviews = [line.strip().split() for line in review]\n",
    "\n",
    "        return reviews, labels\n",
    "\n",
    "    def _reviewProcess(self, review, sequenceLength, wordToIndex):\n",
    "        \"\"\"\n",
    "        将数据集中的每条评论用index表示\n",
    "        wordToIndex中“pad”对应的index为0\n",
    "        \"\"\"\n",
    "        \n",
    "        reviewVec = np.zeros((sequenceLength))\n",
    "        sequenceLen = sequenceLength\n",
    "        \n",
    "        # 判断当前的序列是否小于定义的固定序列长度\n",
    "        if len(review) < sequenceLength:\n",
    "            sequenceLen = len(review)\n",
    "            \n",
    "        for i in range(sequenceLen):\n",
    "            if review[i] in wordToIndex:\n",
    "                reviewVec[i] = wordToIndex[review[i]]\n",
    "            else:\n",
    "                reviewVec[i] = wordToIndex[\"UNK\"]\n",
    "\n",
    "        return reviewVec\n",
    "\n",
    "    def _genTrainEvalData(self, x, y, rate):\n",
    "        \"\"\"\n",
    "        生成训练集和验证集\n",
    "        \"\"\"\n",
    "        \n",
    "        reviews = []\n",
    "        labels = []\n",
    "        \n",
    "        # 遍历所有的文本，将文本中的词转换成index表示\n",
    "        for i in range(len(x)):\n",
    "            reviewVec = self._reviewProcess(x[i], self._sequenceLength, self._wordToIndex)\n",
    "            reviews.append(reviewVec)\n",
    "            \n",
    "            labels.append([y[i]])\n",
    "            \n",
    "        trainIndex = int(len(x) * rate)\n",
    "        \n",
    "        trainReviews = np.asarray(reviews[:trainIndex], dtype=\"int64\")\n",
    "        trainLabels = np.array(labels[:trainIndex], dtype=\"float32\")\n",
    "        \n",
    "        evalReviews = np.asarray(reviews[trainIndex:], dtype=\"int64\")\n",
    "        evalLabels = np.array(labels[trainIndex:], dtype=\"float32\")\n",
    "\n",
    "        return trainReviews, trainLabels, evalReviews, evalLabels\n",
    "        \n",
    "    def _genVocabulary(self, reviews):\n",
    "        \"\"\"\n",
    "        生成词向量和词汇-索引映射字典，可以用全数据集\n",
    "        \"\"\"\n",
    "        \n",
    "        allWords = [word for review in reviews for word in review]\n",
    "        \n",
    "        # 去掉停用词\n",
    "        subWords = [word for word in allWords if word not in self.stopWordDict]\n",
    "        \n",
    "        wordCount = Counter(subWords)  # 统计词频\n",
    "        sortWordCount = sorted(wordCount.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # 去除低频词\n",
    "        words = [item[0] for item in sortWordCount if item[1] >= 5]\n",
    "        \n",
    "        vocab, wordEmbedding = self._getWordEmbedding(words)\n",
    "        self.wordEmbedding = wordEmbedding\n",
    "        \n",
    "        self._wordToIndex = dict(zip(vocab, list(range(len(vocab)))))\n",
    "        self._indexToWord = dict(zip(list(range(len(vocab))), vocab))\n",
    "        \n",
    "        # 将词汇-索引映射表保存为json数据，之后做inference时直接加载来处理数据\n",
    "        with open(\"../data/wordJson/wordToIndex.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self._wordToIndex, f)\n",
    "        \n",
    "        with open(\"../data/wordJson/indexToWord.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self._indexToWord, f)\n",
    "            \n",
    "    def _getWordEmbedding(self, words):\n",
    "        \"\"\"\n",
    "        按照我们的数据集中的单词取出预训练好的word2vec中的词向量\n",
    "        \"\"\"\n",
    "        \n",
    "        wordVec = gensim.models.KeyedVectors.load_word2vec_format(\"../word2vec/word2Vec.bin\", binary=True)\n",
    "        vocab = []\n",
    "        wordEmbedding = []\n",
    "        \n",
    "        # 添加 \"pad\" 和 \"UNK\", \n",
    "        vocab.append(\"pad\")\n",
    "        vocab.append(\"UNK\")\n",
    "        wordEmbedding.append(np.zeros(self._embeddingSize))\n",
    "        wordEmbedding.append(np.random.randn(self._embeddingSize))\n",
    "        \n",
    "        for word in words:\n",
    "            try:\n",
    "                vector = wordVec.wv[word]\n",
    "                vocab.append(word)\n",
    "                wordEmbedding.append(vector)\n",
    "            except:\n",
    "                print(word + \"不存在于词向量中\")\n",
    "                \n",
    "        return vocab, np.array(wordEmbedding)\n",
    "    \n",
    "    def _readStopWord(self, stopWordPath):\n",
    "        \"\"\"\n",
    "        读取停用词\n",
    "        \"\"\"\n",
    "        \n",
    "        with open(stopWordPath, \"r\") as f:\n",
    "            stopWords = f.read()\n",
    "            stopWordList = stopWords.splitlines()\n",
    "            # 将停用词用列表的形式生成，之后查找停用词时会比较快\n",
    "            self.stopWordDict = dict(zip(stopWordList, list(range(len(stopWordList)))))\n",
    "            \n",
    "    def dataGen(self):\n",
    "        \"\"\"\n",
    "        初始化训练集和验证集\n",
    "        \"\"\"\n",
    "        \n",
    "        # 初始化停用词\n",
    "        self._readStopWord(self._stopWordSource)\n",
    "        \n",
    "        # 初始化数据集\n",
    "        reviews, labels = self._readData(self._dataSource)\n",
    "        \n",
    "        # 初始化词汇-索引映射表和词向量矩阵\n",
    "        self._genVocabulary(reviews)\n",
    "        \n",
    "        # 初始化训练集和测试集\n",
    "        trainReviews, trainLabels, evalReviews, evalLabels = self._genTrainEvalData(reviews, labels, self._rate)\n",
    "        self.trainReviews = trainReviews\n",
    "        self.trainLabels = trainLabels\n",
    "        \n",
    "        self.evalReviews = evalReviews\n",
    "        self.evalLabels = evalLabels\n",
    "        \n",
    "        \n",
    "data = Dataset(config)\n",
    "data.dataGen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape: (20000, 200)\n",
      "train label shape: (20000, 1)\n",
      "eval data shape: (5000, 200)\n"
     ]
    }
   ],
   "source": [
    "print(\"train data shape: {}\".format(data.trainReviews.shape))\n",
    "print(\"train label shape: {}\".format(data.trainLabels.shape))\n",
    "print(\"eval data shape: {}\".format(data.evalReviews.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出batch数据集\n",
    "\n",
    "def nextBatch(x, y, batchSize):\n",
    "        \"\"\"\n",
    "        生成batch数据集，用生成器的方式输出\n",
    "        \"\"\"\n",
    "    \n",
    "        perm = np.arange(len(x))\n",
    "        np.random.shuffle(perm)\n",
    "        x = x[perm]\n",
    "        y = y[perm]\n",
    "        \n",
    "        numBatches = len(x) // batchSize\n",
    "\n",
    "        for i in range(numBatches):\n",
    "            start = i * batchSize\n",
    "            end = start + batchSize\n",
    "            batchX = np.array(x[start: end], dtype=\"int64\")\n",
    "            batchY = np.array(y[start: end], dtype=\"float32\")\n",
    "            \n",
    "            yield batchX, batchY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建模型\n",
    "class TextCNN(object):\n",
    "    \"\"\"\n",
    "    Text CNN 用于文本分类\n",
    "    \"\"\"\n",
    "    def __init__(self, config, wordEmbedding):\n",
    "\n",
    "        # 定义模型的输入\n",
    "        self.inputX = tf.placeholder(tf.int32, [None, config.sequenceLength], name=\"inputX\")\n",
    "        self.inputY = tf.placeholder(tf.float32, [None, 1], name=\"inputY\")\n",
    "        \n",
    "        self.dropoutKeepProb = tf.placeholder(tf.float32, name=\"dropoutKeepProb\")\n",
    "        \n",
    "        # 定义l2损失\n",
    "        l2Loss = tf.constant(0.0)\n",
    "        \n",
    "        # 词嵌入层\n",
    "        with tf.name_scope(\"embedding\"):\n",
    "\n",
    "            # 利用预训练的词向量初始化词嵌入矩阵\n",
    "            self.W = tf.Variable(tf.cast(wordEmbedding, dtype=tf.float32, name=\"word2vec\") ,name=\"W\")\n",
    "            # 利用词嵌入矩阵将输入的数据中的词转换成词向量，维度[batch_size, sequence_length, embedding_size]\n",
    "            self.embeddedWords = tf.nn.embedding_lookup(self.W, self.inputX)\n",
    "            # 卷积的输入是思维[batch_size, width, height, channel]，因此需要增加维度，用tf.expand_dims来增大维度\n",
    "            self.embeddedWordsExpanded = tf.expand_dims(self.embeddedWords, -1)\n",
    "\n",
    "        # 创建卷积和池化层\n",
    "        pooledOutputs = []\n",
    "        # 有三种size的filter，3， 4， 5，textCNN是个多通道单层卷积的模型，可以看作三个单层的卷积模型的融合\n",
    "        for i, filterSize in enumerate(config.model.filterSizes):\n",
    "            with tf.name_scope(\"conv-maxpool-%s\" % filterSize):\n",
    "                # 卷积层，卷积核尺寸为filterSize * embeddingSize，卷积核的个数为numFilters\n",
    "                # 初始化权重矩阵和偏置\n",
    "                filterShape = [filterSize, config.model.embeddingSize, 1, config.model.numFilters]\n",
    "                W = tf.Variable(tf.truncated_normal(filterShape, stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[config.model.numFilters]), name=\"b\")\n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.embeddedWordsExpanded,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "                \n",
    "                # relu函数的非线性映射\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "                \n",
    "                # 池化层，最大池化，池化是对卷积后的序列取一个最大值\n",
    "                pooled = tf.nn.max_pool(\n",
    "                    h,\n",
    "                    ksize=[1, config.sequenceLength - filterSize + 1, 1, 1],  # ksize shape: [batch, height, width, channels]\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                pooledOutputs.append(pooled)  # 将三种size的filter的输出一起加入到列表中\n",
    "\n",
    "        # 得到CNN网络的输出长度\n",
    "        numFiltersTotal = config.model.numFilters * len(config.model.filterSizes)\n",
    "        \n",
    "        # 池化后的维度不变，按照最后的维度channel来concat\n",
    "        self.hPool = tf.concat(pooledOutputs, 3)\n",
    "        \n",
    "        # 摊平成二维的数据输入到全连接层\n",
    "        self.hPoolFlat = tf.reshape(self.hPool, [-1, numFiltersTotal])\n",
    "\n",
    "        # dropout\n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            self.hDrop = tf.nn.dropout(self.hPoolFlat, self.dropoutKeepProb)\n",
    "       \n",
    "        # 全连接层的输出\n",
    "        with tf.name_scope(\"output\"):\n",
    "            outputW = tf.get_variable(\n",
    "                \"outputW\",\n",
    "                shape=[numFiltersTotal, 1],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            outputB= tf.Variable(tf.constant(0.1, shape=[1]), name=\"outputB\")\n",
    "            l2Loss += tf.nn.l2_loss(outputW)\n",
    "            l2Loss += tf.nn.l2_loss(outputB)\n",
    "            self.predictions = tf.nn.xw_plus_b(self.hDrop, outputW, outputB, name=\"predictions\")\n",
    "            self.binaryPreds = tf.cast(tf.greater_equal(self.predictions, 0.5), tf.float32, name=\"binaryPreds\")\n",
    "        \n",
    "        # 计算二元交叉熵损失\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            \n",
    "            losses = tf.nn.sigmoid_cross_entropy_with_logits(logits=self.predictions, labels=self.inputY)\n",
    "            self.loss = tf.reduce_mean(losses) + config.model.l2RegLambda * l2Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义性能指标函数\n",
    "\n",
    "def mean(item):\n",
    "    return sum(item) / len(item)\n",
    "\n",
    "\n",
    "def genMetrics(trueY, predY, binaryPredY):\n",
    "    \"\"\"\n",
    "    生成acc和auc值\n",
    "    \"\"\"\n",
    "    auc = roc_auc_score(trueY, predY)\n",
    "    accuracy = accuracy_score(trueY, binaryPredY)\n",
    "    precision = precision_score(trueY, binaryPredY)\n",
    "    recall = recall_score(trueY, binaryPredY)\n",
    "    \n",
    "    return round(accuracy, 4), round(auc, 4), round(precision, 4), round(recall, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-2/W:0/grad/hist is illegal; using conv-maxpool-2/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-2/W:0/grad/sparsity is illegal; using conv-maxpool-2/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-2/b:0/grad/hist is illegal; using conv-maxpool-2/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-2/b:0/grad/sparsity is illegal; using conv-maxpool-2/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/hist is illegal; using conv-maxpool-3/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/sparsity is illegal; using conv-maxpool-3/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/hist is illegal; using conv-maxpool-3/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/sparsity is illegal; using conv-maxpool-3/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/hist is illegal; using conv-maxpool-4/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/sparsity is illegal; using conv-maxpool-4/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/hist is illegal; using conv-maxpool-4/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/sparsity is illegal; using conv-maxpool-4/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/hist is illegal; using conv-maxpool-5/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/sparsity is illegal; using conv-maxpool-5/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/hist is illegal; using conv-maxpool-5/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/sparsity is illegal; using conv-maxpool-5/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name outputW:0/grad/hist is illegal; using outputW_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name outputW:0/grad/sparsity is illegal; using outputW_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/outputB:0/grad/hist is illegal; using output/outputB_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/outputB:0/grad/sparsity is illegal; using output/outputB_0/grad/sparsity instead.\n",
      "Writing to /data4T/share/jiangxinyang848/textClassifier/textCNN/summarys\n",
      "\n",
      "start training model\n",
      "2018-12-26T18:21:59.233362, step: 1, loss: 2.6160378456115723, acc: 0.5234, auc: 0.5017, precision: 0.4946, recall: 0.7667\n",
      "2018-12-26T18:21:59.394803, step: 2, loss: 2.591568946838379, acc: 0.5547, auc: 0.5838, precision: 0.7778, recall: 0.209\n",
      "2018-12-26T18:21:59.527625, step: 3, loss: 2.6081743240356445, acc: 0.5312, auc: 0.4775, precision: 0.5, recall: 0.1167\n",
      "2018-12-26T18:21:59.708540, step: 4, loss: 1.7060577869415283, acc: 0.5156, auc: 0.5515, precision: 0.5833, recall: 0.4\n",
      "2018-12-26T18:21:59.865583, step: 5, loss: 1.9583171606063843, acc: 0.5625, auc: 0.6212, precision: 0.5474, recall: 0.8\n",
      "2018-12-26T18:22:00.016371, step: 6, loss: 2.6395812034606934, acc: 0.5312, auc: 0.5239, precision: 0.5446, recall: 0.8714\n",
      "2018-12-26T18:22:00.170749, step: 7, loss: 3.011916399002075, acc: 0.5156, auc: 0.4828, precision: 0.5047, recall: 0.8571\n",
      "2018-12-26T18:22:00.333591, step: 8, loss: 1.8879151344299316, acc: 0.4688, auc: 0.4436, precision: 0.5, recall: 0.5294\n",
      "2018-12-26T18:22:00.493481, step: 9, loss: 1.6873680353164673, acc: 0.5547, auc: 0.5692, precision: 0.6, recall: 0.3692\n",
      "2018-12-26T18:22:00.645792, step: 10, loss: 2.627941370010376, acc: 0.4297, auc: 0.4988, precision: 0.4118, recall: 0.1\n",
      "2018-12-26T18:22:00.805870, step: 11, loss: 2.209689140319824, acc: 0.4766, auc: 0.47, precision: 0.3448, recall: 0.1724\n",
      "2018-12-26T18:22:00.955258, step: 12, loss: 2.2322511672973633, acc: 0.4844, auc: 0.4978, precision: 0.5312, recall: 0.25\n",
      "2018-12-26T18:22:01.109684, step: 13, loss: 2.0893630981445312, acc: 0.4531, auc: 0.4232, precision: 0.3636, recall: 0.2759\n",
      "2018-12-26T18:22:01.261022, step: 14, loss: 1.564658522605896, acc: 0.5469, auc: 0.5652, precision: 0.5185, recall: 0.6885\n",
      "2018-12-26T18:22:01.408123, step: 15, loss: 1.7832331657409668, acc: 0.4922, auc: 0.4962, precision: 0.5116, recall: 0.6567\n",
      "2018-12-26T18:22:01.565086, step: 16, loss: 1.6426761150360107, acc: 0.5391, auc: 0.5254, precision: 0.5443, recall: 0.6515\n",
      "2018-12-26T18:22:01.724601, step: 17, loss: 1.475843906402588, acc: 0.5547, auc: 0.5906, precision: 0.5467, recall: 0.6406\n",
      "2018-12-26T18:22:01.884751, step: 18, loss: 1.4046857357025146, acc: 0.6016, auc: 0.6085, precision: 0.5833, recall: 0.5738\n",
      "2018-12-26T18:22:02.037123, step: 19, loss: 1.6348862648010254, acc: 0.4922, auc: 0.4972, precision: 0.413, recall: 0.3333\n",
      "2018-12-26T18:22:02.213684, step: 20, loss: 2.087120532989502, acc: 0.4766, auc: 0.4335, precision: 0.4333, recall: 0.2063\n",
      "2018-12-26T18:22:02.363745, step: 21, loss: 2.0792088508605957, acc: 0.4297, auc: 0.4978, precision: 0.4839, recall: 0.2083\n",
      "2018-12-26T18:22:02.514657, step: 22, loss: 1.6050132513046265, acc: 0.4922, auc: 0.4467, precision: 0.4, recall: 0.3214\n",
      "2018-12-26T18:22:02.665360, step: 23, loss: 1.4901841878890991, acc: 0.4844, auc: 0.4716, precision: 0.5192, recall: 0.3971\n",
      "2018-12-26T18:22:02.825851, step: 24, loss: 1.343853235244751, acc: 0.5625, auc: 0.5621, precision: 0.6167, recall: 0.5286\n",
      "2018-12-26T18:22:02.978404, step: 25, loss: 1.5521862506866455, acc: 0.5781, auc: 0.5818, precision: 0.5641, recall: 0.6875\n",
      "2018-12-26T18:22:03.131886, step: 26, loss: 1.5078835487365723, acc: 0.5625, auc: 0.6022, precision: 0.5366, recall: 0.7097\n",
      "2018-12-26T18:22:03.288125, step: 27, loss: 1.746259331703186, acc: 0.4609, auc: 0.4865, precision: 0.4494, recall: 0.6667\n",
      "2018-12-26T18:22:03.442494, step: 28, loss: 1.349118947982788, acc: 0.5469, auc: 0.5847, precision: 0.5417, recall: 0.6094\n",
      "2018-12-26T18:22:03.593659, step: 29, loss: 2.026606559753418, acc: 0.4922, auc: 0.4405, precision: 0.5, recall: 0.3385\n",
      "2018-12-26T18:22:03.744506, step: 30, loss: 1.536325454711914, acc: 0.5312, auc: 0.5795, precision: 0.5455, recall: 0.2857\n",
      "2018-12-26T18:22:03.895842, step: 31, loss: 1.5696897506713867, acc: 0.5547, auc: 0.5707, precision: 0.625, recall: 0.3077\n",
      "2018-12-26T18:22:04.050670, step: 32, loss: 1.4284452199935913, acc: 0.5469, auc: 0.5444, precision: 0.5385, recall: 0.3443\n",
      "2018-12-26T18:22:04.202848, step: 33, loss: 1.3278324604034424, acc: 0.5859, auc: 0.5716, precision: 0.5814, recall: 0.4167\n",
      "2018-12-26T18:22:04.350514, step: 34, loss: 1.167975902557373, acc: 0.5625, auc: 0.5893, precision: 0.6212, recall: 0.5694\n",
      "2018-12-26T18:22:04.500913, step: 35, loss: 1.187729001045227, acc: 0.5391, auc: 0.5234, precision: 0.5882, recall: 0.5634\n",
      "2018-12-26T18:22:04.650020, step: 36, loss: 1.6645596027374268, acc: 0.4922, auc: 0.532, precision: 0.4535, recall: 0.6842\n",
      "2018-12-26T18:22:04.801480, step: 37, loss: 1.548293113708496, acc: 0.5312, auc: 0.5517, precision: 0.5, recall: 0.7\n",
      "2018-12-26T18:22:04.951805, step: 38, loss: 1.3194352388381958, acc: 0.5391, auc: 0.5228, precision: 0.6129, recall: 0.5205\n",
      "2018-12-26T18:22:05.109955, step: 39, loss: 0.9884992241859436, acc: 0.6562, auc: 0.6736, precision: 0.6102, recall: 0.6316\n",
      "2018-12-26T18:22:05.261612, step: 40, loss: 1.2541773319244385, acc: 0.5781, auc: 0.5727, precision: 0.55, recall: 0.3793\n",
      "2018-12-26T18:22:05.411096, step: 41, loss: 1.4471731185913086, acc: 0.4531, auc: 0.5893, precision: 0.5185, recall: 0.1972\n",
      "2018-12-26T18:22:05.566011, step: 42, loss: 1.4715989828109741, acc: 0.4766, auc: 0.5028, precision: 0.4, recall: 0.1967\n",
      "2018-12-26T18:22:05.717278, step: 43, loss: 1.4499361515045166, acc: 0.4766, auc: 0.4468, precision: 0.4314, recall: 0.3667\n",
      "2018-12-26T18:22:05.875634, step: 44, loss: 0.967687726020813, acc: 0.5703, auc: 0.6282, precision: 0.5532, recall: 0.4333\n",
      "2018-12-26T18:22:06.040017, step: 45, loss: 1.1044925451278687, acc: 0.5625, auc: 0.581, precision: 0.5493, recall: 0.619\n",
      "2018-12-26T18:22:06.200399, step: 46, loss: 1.029276728630066, acc: 0.5703, auc: 0.6236, precision: 0.6818, recall: 0.5696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-26T18:22:06.351388, step: 47, loss: 1.1892750263214111, acc: 0.5469, auc: 0.5813, precision: 0.48, recall: 0.6545\n",
      "2018-12-26T18:22:06.505589, step: 48, loss: 0.9242684841156006, acc: 0.6484, auc: 0.6821, precision: 0.678, recall: 0.6061\n",
      "2018-12-26T18:22:06.653700, step: 49, loss: 1.0500402450561523, acc: 0.5859, auc: 0.6112, precision: 0.6207, recall: 0.5373\n",
      "2018-12-26T18:22:06.813964, step: 50, loss: 1.200317144393921, acc: 0.5156, auc: 0.5431, precision: 0.48, recall: 0.4\n",
      "2018-12-26T18:22:06.962367, step: 51, loss: 1.1802515983581543, acc: 0.5703, auc: 0.5512, precision: 0.5641, recall: 0.3667\n",
      "2018-12-26T18:22:07.111723, step: 52, loss: 1.2376327514648438, acc: 0.5547, auc: 0.549, precision: 0.5106, recall: 0.4138\n",
      "2018-12-26T18:22:07.260346, step: 53, loss: 1.5337551832199097, acc: 0.4297, auc: 0.4303, precision: 0.4091, recall: 0.2769\n",
      "2018-12-26T18:22:07.418023, step: 54, loss: 1.1761348247528076, acc: 0.5078, auc: 0.522, precision: 0.4884, recall: 0.3387\n",
      "2018-12-26T18:22:07.570281, step: 55, loss: 1.196786880493164, acc: 0.5703, auc: 0.5512, precision: 0.5439, recall: 0.5167\n",
      "2018-12-26T18:22:07.721110, step: 56, loss: 1.0143885612487793, acc: 0.5312, auc: 0.6034, precision: 0.5424, recall: 0.4923\n",
      "2018-12-26T18:22:07.873259, step: 57, loss: 1.0507972240447998, acc: 0.5469, auc: 0.6091, precision: 0.5862, recall: 0.5\n",
      "2018-12-26T18:22:08.031873, step: 58, loss: 0.9446589350700378, acc: 0.6328, auc: 0.6833, precision: 0.6418, recall: 0.6515\n",
      "2018-12-26T18:22:08.191963, step: 59, loss: 1.0447666645050049, acc: 0.5156, auc: 0.5724, precision: 0.569, recall: 0.4714\n",
      "2018-12-26T18:22:08.344566, step: 60, loss: 0.9213109016418457, acc: 0.5859, auc: 0.6455, precision: 0.6129, recall: 0.5672\n",
      "2018-12-26T18:22:08.496812, step: 61, loss: 1.0691914558410645, acc: 0.6172, auc: 0.6383, precision: 0.5588, recall: 0.6667\n",
      "2018-12-26T18:22:08.644798, step: 62, loss: 0.7912805676460266, acc: 0.6406, auc: 0.6928, precision: 0.6786, recall: 0.5758\n",
      "2018-12-26T18:22:08.795756, step: 63, loss: 0.8944541811943054, acc: 0.5859, auc: 0.623, precision: 0.6122, recall: 0.4688\n",
      "2018-12-26T18:22:08.952557, step: 64, loss: 1.1641523838043213, acc: 0.5312, auc: 0.5582, precision: 0.6122, recall: 0.4225\n",
      "2018-12-26T18:22:09.105452, step: 65, loss: 1.0370434522628784, acc: 0.5469, auc: 0.5756, precision: 0.5778, recall: 0.4\n",
      "2018-12-26T18:22:09.257138, step: 66, loss: 0.763890266418457, acc: 0.5938, auc: 0.7141, precision: 0.7209, recall: 0.4366\n",
      "2018-12-26T18:22:09.417684, step: 67, loss: 1.1241894960403442, acc: 0.5312, auc: 0.5448, precision: 0.4915, recall: 0.4915\n",
      "2018-12-26T18:22:09.592083, step: 68, loss: 0.9220280647277832, acc: 0.6328, auc: 0.6303, precision: 0.6716, recall: 0.6429\n",
      "2018-12-26T18:22:09.743890, step: 69, loss: 0.9579026699066162, acc: 0.6719, auc: 0.6647, precision: 0.7101, recall: 0.6901\n",
      "2018-12-26T18:22:09.900943, step: 70, loss: 1.0592055320739746, acc: 0.5938, auc: 0.5836, precision: 0.5455, recall: 0.7119\n",
      "2018-12-26T18:22:10.053519, step: 71, loss: 0.714353621006012, acc: 0.6719, auc: 0.7182, precision: 0.6613, recall: 0.6613\n",
      "2018-12-26T18:22:10.207690, step: 72, loss: 1.093826174736023, acc: 0.5312, auc: 0.5235, precision: 0.6, recall: 0.3913\n",
      "2018-12-26T18:22:10.356300, step: 73, loss: 0.8482780456542969, acc: 0.5781, auc: 0.667, precision: 0.6667, recall: 0.3881\n",
      "2018-12-26T18:22:10.506602, step: 74, loss: 0.8813896179199219, acc: 0.5938, auc: 0.6131, precision: 0.4762, recall: 0.4\n",
      "2018-12-26T18:22:10.656738, step: 75, loss: 0.8957400918006897, acc: 0.5859, auc: 0.6664, precision: 0.8056, recall: 0.3867\n",
      "2018-12-26T18:22:10.808345, step: 76, loss: 0.9205743670463562, acc: 0.5938, auc: 0.6336, precision: 0.5532, recall: 0.4561\n",
      "2018-12-26T18:22:10.964026, step: 77, loss: 0.7554564476013184, acc: 0.6562, auc: 0.7167, precision: 0.6129, recall: 0.6552\n",
      "2018-12-26T18:22:11.116560, step: 78, loss: 0.8716112375259399, acc: 0.6406, auc: 0.6718, precision: 0.6429, recall: 0.5806\n",
      "2018-12-26T18:22:11.267142, step: 79, loss: 0.7846865653991699, acc: 0.6094, auc: 0.6652, precision: 0.5882, recall: 0.5085\n",
      "2018-12-26T18:22:11.420196, step: 80, loss: 0.8666285276412964, acc: 0.5547, auc: 0.6276, precision: 0.5789, recall: 0.5\n",
      "2018-12-26T18:22:11.571869, step: 81, loss: 0.8017240166664124, acc: 0.625, auc: 0.6734, precision: 0.614, recall: 0.5738\n",
      "2018-12-26T18:22:11.725623, step: 82, loss: 0.7876102924346924, acc: 0.6016, auc: 0.6709, precision: 0.619, recall: 0.4262\n",
      "2018-12-26T18:22:11.883152, step: 83, loss: 0.8150615096092224, acc: 0.5625, auc: 0.6891, precision: 0.6389, recall: 0.3485\n",
      "2018-12-26T18:22:12.033271, step: 84, loss: 0.7591904401779175, acc: 0.6875, auc: 0.6872, precision: 0.6304, recall: 0.5577\n",
      "2018-12-26T18:22:12.181969, step: 85, loss: 0.7800408601760864, acc: 0.6406, auc: 0.6843, precision: 0.7368, recall: 0.4375\n",
      "2018-12-26T18:22:12.333921, step: 86, loss: 0.7982872724533081, acc: 0.5312, auc: 0.6596, precision: 0.5405, recall: 0.3175\n",
      "2018-12-26T18:22:12.485205, step: 87, loss: 0.7947526574134827, acc: 0.6641, auc: 0.6953, precision: 0.5781, recall: 0.6981\n",
      "2018-12-26T18:22:12.632245, step: 88, loss: 0.7197802066802979, acc: 0.6172, auc: 0.7086, precision: 0.6667, recall: 0.5373\n",
      "2018-12-26T18:22:12.795031, step: 89, loss: 0.6517975926399231, acc: 0.6562, auc: 0.7436, precision: 0.7241, recall: 0.6\n",
      "2018-12-26T18:22:12.943408, step: 90, loss: 0.6729879975318909, acc: 0.7031, auc: 0.7252, precision: 0.6905, recall: 0.537\n",
      "2018-12-26T18:22:13.093342, step: 91, loss: 0.5839070081710815, acc: 0.7109, auc: 0.7863, precision: 0.86, recall: 0.589\n",
      "2018-12-26T18:22:13.240258, step: 92, loss: 0.7230072021484375, acc: 0.5859, auc: 0.6726, precision: 0.6552, recall: 0.5352\n",
      "2018-12-26T18:22:13.389646, step: 93, loss: 0.7649747133255005, acc: 0.6328, auc: 0.6938, precision: 0.6078, recall: 0.5345\n",
      "2018-12-26T18:22:13.541590, step: 94, loss: 0.7403772473335266, acc: 0.6406, auc: 0.6867, precision: 0.6735, recall: 0.5238\n",
      "2018-12-26T18:22:13.696425, step: 95, loss: 0.7567771673202515, acc: 0.6094, auc: 0.689, precision: 0.6607, recall: 0.5441\n",
      "2018-12-26T18:22:13.852460, step: 96, loss: 0.6861812472343445, acc: 0.6406, auc: 0.7357, precision: 0.6909, recall: 0.5672\n",
      "2018-12-26T18:22:14.005943, step: 97, loss: 0.670313835144043, acc: 0.6484, auc: 0.7274, precision: 0.6122, recall: 0.5357\n",
      "2018-12-26T18:22:14.155882, step: 98, loss: 0.7322877645492554, acc: 0.625, auc: 0.6906, precision: 0.64, recall: 0.5161\n",
      "2018-12-26T18:22:14.313557, step: 99, loss: 0.5633875727653503, acc: 0.6875, auc: 0.7881, precision: 0.7826, recall: 0.5455\n",
      "2018-12-26T18:22:14.465097, step: 100, loss: 0.7278707027435303, acc: 0.6016, auc: 0.6948, precision: 0.6415, recall: 0.5152\n",
      "\n",
      "Evaluation:\n",
      "2018-12-26T18:22:20.160719, step: 100, loss: 0.4788484259655601, acc: 0.7261526315789474, auc: 0.8684289473684211, precision: 0.8754315789473686, recall: 0.5341921052631579\n",
      "Saved model checkpoint to ../model/textCNN/model/my-model-100\n",
      "\n",
      "2018-12-26T18:22:20.632335, step: 101, loss: 0.582827091217041, acc: 0.7109, auc: 0.7815, precision: 0.8, recall: 0.5625\n",
      "2018-12-26T18:22:20.782909, step: 102, loss: 0.7768416404724121, acc: 0.5625, auc: 0.6542, precision: 0.6364, recall: 0.4118\n",
      "2018-12-26T18:22:20.934405, step: 103, loss: 0.5127041339874268, acc: 0.7422, auc: 0.8307, precision: 0.8269, recall: 0.6418\n",
      "2018-12-26T18:22:21.085755, step: 104, loss: 0.610034704208374, acc: 0.7109, auc: 0.788, precision: 0.6491, recall: 0.6852\n",
      "2018-12-26T18:22:21.237723, step: 105, loss: 0.6705350875854492, acc: 0.6797, auc: 0.7499, precision: 0.7031, recall: 0.6716\n",
      "2018-12-26T18:22:21.385874, step: 106, loss: 0.6317442059516907, acc: 0.7266, auc: 0.7623, precision: 0.6885, recall: 0.7241\n",
      "2018-12-26T18:22:21.538312, step: 107, loss: 0.5670357942581177, acc: 0.6719, auc: 0.8005, precision: 0.76, recall: 0.5588\n",
      "2018-12-26T18:22:21.688158, step: 108, loss: 0.6832587718963623, acc: 0.6406, auc: 0.7477, precision: 0.8108, recall: 0.4348\n",
      "2018-12-26T18:22:21.840071, step: 109, loss: 0.6438934803009033, acc: 0.6094, auc: 0.7324, precision: 0.6667, recall: 0.3871\n",
      "2018-12-26T18:22:21.988729, step: 110, loss: 0.671436071395874, acc: 0.6562, auc: 0.7123, precision: 0.6591, recall: 0.5\n",
      "2018-12-26T18:22:22.139901, step: 111, loss: 0.4815436601638794, acc: 0.7812, auc: 0.8498, precision: 0.8033, recall: 0.7538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-26T18:22:22.291422, step: 112, loss: 0.6048027276992798, acc: 0.7031, auc: 0.7848, precision: 0.7586, recall: 0.6471\n",
      "2018-12-26T18:22:22.448718, step: 113, loss: 0.5197864174842834, acc: 0.7422, auc: 0.8292, precision: 0.7797, recall: 0.697\n",
      "2018-12-26T18:22:22.599481, step: 114, loss: 0.60899817943573, acc: 0.6641, auc: 0.7582, precision: 0.6613, recall: 0.6508\n",
      "2018-12-26T18:22:22.751757, step: 115, loss: 0.6150535345077515, acc: 0.6719, auc: 0.7664, precision: 0.7755, recall: 0.5507\n",
      "2018-12-26T18:22:22.897501, step: 116, loss: 0.6705420613288879, acc: 0.6094, auc: 0.7289, precision: 0.6833, recall: 0.5694\n",
      "2018-12-26T18:22:23.054478, step: 117, loss: 0.5960569977760315, acc: 0.7031, auc: 0.7768, precision: 0.7931, recall: 0.6389\n",
      "2018-12-26T18:22:23.211213, step: 118, loss: 0.5037750005722046, acc: 0.7266, auc: 0.8266, precision: 0.791, recall: 0.7162\n",
      "2018-12-26T18:22:23.361339, step: 119, loss: 0.6424188613891602, acc: 0.7109, auc: 0.7717, precision: 0.7049, recall: 0.6935\n",
      "2018-12-26T18:22:23.508830, step: 120, loss: 0.661569356918335, acc: 0.6562, auc: 0.7312, precision: 0.6774, recall: 0.6364\n",
      "2018-12-26T18:22:23.656049, step: 121, loss: 0.631096601486206, acc: 0.6406, auc: 0.7488, precision: 0.5926, recall: 0.5714\n",
      "2018-12-26T18:22:23.807456, step: 122, loss: 0.5835511088371277, acc: 0.6953, auc: 0.7944, precision: 0.7561, recall: 0.5167\n",
      "2018-12-26T18:22:23.953402, step: 123, loss: 0.5926763415336609, acc: 0.7031, auc: 0.7897, precision: 0.7234, recall: 0.5763\n",
      "2018-12-26T18:22:24.113091, step: 124, loss: 0.5624977946281433, acc: 0.6797, auc: 0.8139, precision: 0.8, recall: 0.4923\n",
      "2018-12-26T18:22:24.265152, step: 125, loss: 0.5182284116744995, acc: 0.7266, auc: 0.8316, precision: 0.8049, recall: 0.55\n",
      "2018-12-26T18:22:24.422233, step: 126, loss: 0.5022487044334412, acc: 0.7344, auc: 0.8363, precision: 0.8182, recall: 0.5806\n",
      "2018-12-26T18:22:24.574793, step: 127, loss: 0.5956705808639526, acc: 0.7031, auc: 0.7862, precision: 0.7255, recall: 0.6066\n",
      "2018-12-26T18:22:24.739457, step: 128, loss: 0.48694857954978943, acc: 0.75, auc: 0.8396, precision: 0.8136, recall: 0.6957\n",
      "2018-12-26T18:22:24.890446, step: 129, loss: 0.5109717845916748, acc: 0.7188, auc: 0.8352, precision: 0.7258, recall: 0.7031\n",
      "2018-12-26T18:22:25.042722, step: 130, loss: 0.5752269625663757, acc: 0.7266, auc: 0.8083, precision: 0.7049, recall: 0.7167\n",
      "2018-12-26T18:22:25.192793, step: 131, loss: 0.6604713201522827, acc: 0.6641, auc: 0.7414, precision: 0.7, recall: 0.5556\n",
      "2018-12-26T18:22:25.352023, step: 132, loss: 0.5543561577796936, acc: 0.7188, auc: 0.8013, precision: 0.7358, recall: 0.6393\n",
      "2018-12-26T18:22:25.507548, step: 133, loss: 0.5146654844284058, acc: 0.7266, auc: 0.8239, precision: 0.74, recall: 0.6271\n",
      "2018-12-26T18:22:25.658626, step: 134, loss: 0.564311146736145, acc: 0.7109, auc: 0.8022, precision: 0.7805, recall: 0.5333\n",
      "2018-12-26T18:22:25.808922, step: 135, loss: 0.5761366486549377, acc: 0.625, auc: 0.7985, precision: 0.7632, recall: 0.4265\n",
      "2018-12-26T18:22:25.962441, step: 136, loss: 0.5951666831970215, acc: 0.7031, auc: 0.7975, precision: 0.6604, recall: 0.6364\n",
      "2018-12-26T18:22:26.114009, step: 137, loss: 0.5201244950294495, acc: 0.7031, auc: 0.8241, precision: 0.7, recall: 0.6034\n",
      "2018-12-26T18:22:26.266061, step: 138, loss: 0.48635607957839966, acc: 0.7812, auc: 0.8483, precision: 0.8235, recall: 0.6885\n",
      "2018-12-26T18:22:26.417075, step: 139, loss: 0.673284113407135, acc: 0.625, auc: 0.7262, precision: 0.6939, recall: 0.5075\n",
      "2018-12-26T18:22:26.568317, step: 140, loss: 0.5953842997550964, acc: 0.6797, auc: 0.7853, precision: 0.7581, recall: 0.6438\n",
      "2018-12-26T18:22:26.720710, step: 141, loss: 0.5386850833892822, acc: 0.7344, auc: 0.806, precision: 0.7963, recall: 0.6515\n",
      "2018-12-26T18:22:26.870822, step: 142, loss: 0.5447194576263428, acc: 0.75, auc: 0.8231, precision: 0.7536, recall: 0.7761\n",
      "2018-12-26T18:22:27.017423, step: 143, loss: 0.4827665388584137, acc: 0.7578, auc: 0.8462, precision: 0.8113, recall: 0.6719\n",
      "2018-12-26T18:22:27.165850, step: 144, loss: 0.6200658082962036, acc: 0.6562, auc: 0.7635, precision: 0.6379, recall: 0.6167\n",
      "2018-12-26T18:22:27.318655, step: 145, loss: 0.5921354293823242, acc: 0.6797, auc: 0.7656, precision: 0.7358, recall: 0.5909\n",
      "2018-12-26T18:22:27.469539, step: 146, loss: 0.5829885005950928, acc: 0.7109, auc: 0.7869, precision: 0.7647, recall: 0.6094\n",
      "2018-12-26T18:22:27.622176, step: 147, loss: 0.44079646468162537, acc: 0.8125, auc: 0.8841, precision: 0.8723, recall: 0.6949\n",
      "2018-12-26T18:22:27.769017, step: 148, loss: 0.6280430555343628, acc: 0.6641, auc: 0.7727, precision: 0.7143, recall: 0.4918\n",
      "2018-12-26T18:22:27.920036, step: 149, loss: 0.606520414352417, acc: 0.6797, auc: 0.7752, precision: 0.814, recall: 0.5147\n",
      "2018-12-26T18:22:28.067035, step: 150, loss: 0.6200963854789734, acc: 0.6172, auc: 0.7547, precision: 0.6279, recall: 0.45\n",
      "2018-12-26T18:22:28.220084, step: 151, loss: 0.5517578125, acc: 0.7344, auc: 0.8139, precision: 0.7556, recall: 0.5965\n",
      "2018-12-26T18:22:28.374966, step: 152, loss: 0.6139994263648987, acc: 0.7266, auc: 0.7895, precision: 0.7778, recall: 0.6462\n",
      "2018-12-26T18:22:28.536578, step: 153, loss: 0.5473281145095825, acc: 0.7109, auc: 0.8037, precision: 0.7353, recall: 0.7246\n",
      "2018-12-26T18:22:28.691406, step: 154, loss: 0.5676872134208679, acc: 0.7031, auc: 0.8037, precision: 0.7193, recall: 0.6508\n",
      "2018-12-26T18:22:28.843622, step: 155, loss: 0.501109778881073, acc: 0.7031, auc: 0.8285, precision: 0.7705, recall: 0.662\n",
      "start training model\n",
      "2018-12-26T18:22:29.026442, step: 156, loss: 0.3619389832019806, acc: 0.8438, auc: 0.9201, precision: 0.8644, recall: 0.8095\n",
      "2018-12-26T18:22:29.175630, step: 157, loss: 0.5979956388473511, acc: 0.6797, auc: 0.7826, precision: 0.7414, recall: 0.6232\n",
      "2018-12-26T18:22:29.326385, step: 158, loss: 0.4300253987312317, acc: 0.7578, auc: 0.8784, precision: 0.8113, recall: 0.6719\n",
      "2018-12-26T18:22:29.484827, step: 159, loss: 0.4184848368167877, acc: 0.7969, auc: 0.8931, precision: 0.8182, recall: 0.7377\n",
      "2018-12-26T18:22:29.648556, step: 160, loss: 0.3991856575012207, acc: 0.8047, auc: 0.9046, precision: 0.8889, recall: 0.7164\n",
      "2018-12-26T18:22:29.810880, step: 161, loss: 0.3998994827270508, acc: 0.8047, auc: 0.9055, precision: 0.9, recall: 0.6923\n",
      "2018-12-26T18:22:29.958708, step: 162, loss: 0.4327216148376465, acc: 0.7734, auc: 0.8839, precision: 0.8136, recall: 0.7273\n",
      "2018-12-26T18:22:30.108798, step: 163, loss: 0.5472321510314941, acc: 0.7109, auc: 0.8096, precision: 0.6977, recall: 0.5556\n",
      "2018-12-26T18:22:30.263740, step: 164, loss: 0.436756432056427, acc: 0.8203, auc: 0.8872, precision: 0.8667, recall: 0.7761\n",
      "2018-12-26T18:22:30.420313, step: 165, loss: 0.424056738615036, acc: 0.7656, auc: 0.8897, precision: 0.8627, recall: 0.6567\n",
      "2018-12-26T18:22:30.584004, step: 166, loss: 0.4403284192085266, acc: 0.7891, auc: 0.8883, precision: 0.7869, recall: 0.7742\n",
      "2018-12-26T18:22:30.735209, step: 167, loss: 0.36721986532211304, acc: 0.8203, auc: 0.915, precision: 0.8727, recall: 0.75\n",
      "2018-12-26T18:22:30.888239, step: 168, loss: 0.47310343384742737, acc: 0.7812, auc: 0.8658, precision: 0.7636, recall: 0.7368\n",
      "2018-12-26T18:22:31.041641, step: 169, loss: 0.5287308692932129, acc: 0.7812, auc: 0.8297, precision: 0.7667, recall: 0.7667\n",
      "2018-12-26T18:22:31.193849, step: 170, loss: 0.4562837481498718, acc: 0.7812, auc: 0.8757, precision: 0.902, recall: 0.6667\n",
      "2018-12-26T18:22:31.340304, step: 171, loss: 0.5380362868309021, acc: 0.7031, auc: 0.8319, precision: 0.8372, recall: 0.5373\n",
      "2018-12-26T18:22:31.489515, step: 172, loss: 0.3462517559528351, acc: 0.8594, auc: 0.9354, precision: 0.902, recall: 0.7797\n",
      "2018-12-26T18:22:31.649330, step: 173, loss: 0.5081808567047119, acc: 0.7578, auc: 0.8302, precision: 0.8261, recall: 0.623\n",
      "2018-12-26T18:22:31.805342, step: 174, loss: 0.36025723814964294, acc: 0.8203, auc: 0.9234, precision: 0.8163, recall: 0.7407\n",
      "2018-12-26T18:22:31.962816, step: 175, loss: 0.43135619163513184, acc: 0.7812, auc: 0.8782, precision: 0.8478, recall: 0.65\n",
      "2018-12-26T18:22:32.112755, step: 176, loss: 0.5020265579223633, acc: 0.7656, auc: 0.841, precision: 0.7347, recall: 0.6792\n",
      "2018-12-26T18:22:32.261559, step: 177, loss: 0.4075919985771179, acc: 0.7891, auc: 0.9026, precision: 0.8936, recall: 0.6562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-26T18:22:32.419352, step: 178, loss: 0.47027653455734253, acc: 0.7656, auc: 0.874, precision: 0.925, recall: 0.5781\n",
      "2018-12-26T18:22:32.575397, step: 179, loss: 0.4400181174278259, acc: 0.7734, auc: 0.8949, precision: 0.898, recall: 0.6471\n",
      "2018-12-26T18:22:32.725227, step: 180, loss: 0.35575369000434875, acc: 0.8203, auc: 0.9239, precision: 0.9107, recall: 0.7391\n",
      "2018-12-26T18:22:32.883190, step: 181, loss: 0.5393904447555542, acc: 0.7656, auc: 0.8276, precision: 0.7708, recall: 0.6607\n",
      "2018-12-26T18:22:33.036049, step: 182, loss: 0.43009111285209656, acc: 0.75, auc: 0.8818, precision: 0.7778, recall: 0.7313\n",
      "2018-12-26T18:22:33.186172, step: 183, loss: 0.4205598831176758, acc: 0.8125, auc: 0.9154, precision: 0.7571, recall: 0.8833\n",
      "2018-12-26T18:22:33.344071, step: 184, loss: 0.3894326090812683, acc: 0.8281, auc: 0.8881, precision: 0.8734, recall: 0.8519\n",
      "2018-12-26T18:22:33.494771, step: 185, loss: 0.41712504625320435, acc: 0.7969, auc: 0.8877, precision: 0.8276, recall: 0.75\n",
      "2018-12-26T18:22:33.648927, step: 186, loss: 0.45398807525634766, acc: 0.7891, auc: 0.8743, precision: 0.7895, recall: 0.75\n",
      "2018-12-26T18:22:33.803867, step: 187, loss: 0.43162694573402405, acc: 0.7734, auc: 0.9007, precision: 0.9149, recall: 0.6324\n",
      "2018-12-26T18:22:33.952536, step: 188, loss: 0.44135409593582153, acc: 0.8125, auc: 0.881, precision: 0.8833, recall: 0.7571\n",
      "2018-12-26T18:22:34.101479, step: 189, loss: 0.43077999353408813, acc: 0.8047, auc: 0.8811, precision: 0.8519, recall: 0.7302\n",
      "2018-12-26T18:22:34.234721, step: 190, loss: 0.4082779884338379, acc: 0.8203, auc: 0.8963, precision: 0.9024, recall: 0.6607\n",
      "2018-12-26T18:22:34.383696, step: 191, loss: 0.4595766067504883, acc: 0.7344, auc: 0.8562, precision: 0.7818, recall: 0.6615\n",
      "2018-12-26T18:22:34.538790, step: 192, loss: 0.44514113664627075, acc: 0.7734, auc: 0.8799, precision: 0.7627, recall: 0.75\n",
      "2018-12-26T18:22:34.675633, step: 193, loss: 0.40379083156585693, acc: 0.7891, auc: 0.899, precision: 0.9057, recall: 0.6857\n",
      "2018-12-26T18:22:34.821516, step: 194, loss: 0.5030917525291443, acc: 0.7422, auc: 0.8431, precision: 0.7377, recall: 0.7258\n",
      "2018-12-26T18:22:34.953230, step: 195, loss: 0.5001229643821716, acc: 0.7422, auc: 0.8634, precision: 0.8, recall: 0.6957\n",
      "2018-12-26T18:22:35.107266, step: 196, loss: 0.4490705728530884, acc: 0.8047, auc: 0.8695, precision: 0.875, recall: 0.7671\n",
      "2018-12-26T18:22:35.258523, step: 197, loss: 0.36112383008003235, acc: 0.8438, auc: 0.9238, precision: 0.9091, recall: 0.7692\n",
      "2018-12-26T18:22:35.408353, step: 198, loss: 0.492576539516449, acc: 0.8125, auc: 0.8528, precision: 0.7818, recall: 0.7818\n",
      "2018-12-26T18:22:35.561652, step: 199, loss: 0.3527976870536804, acc: 0.8125, auc: 0.9253, precision: 0.9362, recall: 0.6769\n",
      "2018-12-26T18:22:35.714236, step: 200, loss: 0.4978225827217102, acc: 0.7656, auc: 0.8422, precision: 0.8491, recall: 0.6716\n",
      "\n",
      "Evaluation:\n",
      "2018-12-26T18:22:41.445866, step: 200, loss: 0.3825199211898603, acc: 0.8178500000000002, auc: 0.9123657894736845, precision: 0.8781578947368422, recall: 0.7423289473684211\n",
      "Saved model checkpoint to ../model/textCNN/model/my-model-200\n",
      "\n",
      "2018-12-26T18:22:41.863330, step: 201, loss: 0.37921398878097534, acc: 0.8125, auc: 0.9117, precision: 0.8909, recall: 0.7313\n",
      "2018-12-26T18:22:42.011904, step: 202, loss: 0.4138856530189514, acc: 0.8047, auc: 0.8936, precision: 0.8182, recall: 0.75\n",
      "2018-12-26T18:22:42.168948, step: 203, loss: 0.35177284479141235, acc: 0.8359, auc: 0.9227, precision: 0.86, recall: 0.7544\n",
      "2018-12-26T18:22:42.322589, step: 204, loss: 0.3768833875656128, acc: 0.8203, auc: 0.9097, precision: 0.8793, recall: 0.7612\n",
      "2018-12-26T18:22:42.474706, step: 205, loss: 0.4399535655975342, acc: 0.7891, auc: 0.8803, precision: 0.7966, recall: 0.7581\n",
      "2018-12-26T18:22:42.626140, step: 206, loss: 0.4359011948108673, acc: 0.7812, auc: 0.8816, precision: 0.8214, recall: 0.7188\n",
      "2018-12-26T18:22:42.824852, step: 207, loss: 0.39381760358810425, acc: 0.8203, auc: 0.9034, precision: 0.8788, recall: 0.7945\n",
      "2018-12-26T18:22:42.975447, step: 208, loss: 0.3345935344696045, acc: 0.8281, auc: 0.9326, precision: 0.9423, recall: 0.7206\n",
      "2018-12-26T18:22:43.122767, step: 209, loss: 0.344978928565979, acc: 0.8359, auc: 0.9323, precision: 0.8788, recall: 0.8169\n",
      "2018-12-26T18:22:43.278754, step: 210, loss: 0.5191364288330078, acc: 0.7812, auc: 0.8483, precision: 0.7778, recall: 0.7241\n",
      "2018-12-26T18:22:43.431429, step: 211, loss: 0.34842050075531006, acc: 0.8359, auc: 0.9268, precision: 0.9, recall: 0.7377\n",
      "2018-12-26T18:22:43.581151, step: 212, loss: 0.4366607367992401, acc: 0.8281, auc: 0.8912, precision: 0.8654, recall: 0.75\n",
      "2018-12-26T18:22:43.728034, step: 213, loss: 0.38964852690696716, acc: 0.8516, auc: 0.9044, precision: 0.873, recall: 0.8333\n",
      "2018-12-26T18:22:43.878824, step: 214, loss: 0.33065932989120483, acc: 0.8281, auc: 0.9385, precision: 0.902, recall: 0.7302\n",
      "2018-12-26T18:22:44.034642, step: 215, loss: 0.40657973289489746, acc: 0.7812, auc: 0.8971, precision: 0.8654, recall: 0.6818\n",
      "2018-12-26T18:22:44.195554, step: 216, loss: 0.4959276020526886, acc: 0.7891, auc: 0.8623, precision: 0.7895, recall: 0.75\n",
      "2018-12-26T18:22:44.347675, step: 217, loss: 0.37751996517181396, acc: 0.8203, auc: 0.9073, precision: 0.8548, recall: 0.791\n",
      "2018-12-26T18:22:44.509066, step: 218, loss: 0.428052693605423, acc: 0.7969, auc: 0.8825, precision: 0.8627, recall: 0.6984\n",
      "2018-12-26T18:22:44.670101, step: 219, loss: 0.4529339671134949, acc: 0.7578, auc: 0.8654, precision: 0.8167, recall: 0.7101\n",
      "2018-12-26T18:22:44.819583, step: 220, loss: 0.4328787922859192, acc: 0.8125, auc: 0.879, precision: 0.8529, recall: 0.8056\n",
      "2018-12-26T18:22:44.973827, step: 221, loss: 0.4207248091697693, acc: 0.8047, auc: 0.8989, precision: 0.7963, recall: 0.7544\n",
      "2018-12-26T18:22:45.127963, step: 222, loss: 0.4720774292945862, acc: 0.8281, auc: 0.8712, precision: 0.8333, recall: 0.8065\n",
      "2018-12-26T18:22:45.285561, step: 223, loss: 0.4728981554508209, acc: 0.75, auc: 0.8604, precision: 0.7907, recall: 0.5965\n",
      "2018-12-26T18:22:45.450777, step: 224, loss: 0.4555499851703644, acc: 0.8047, auc: 0.8782, precision: 0.898, recall: 0.6875\n",
      "2018-12-26T18:22:45.602189, step: 225, loss: 0.4024398624897003, acc: 0.8125, auc: 0.8984, precision: 0.8163, recall: 0.7273\n",
      "2018-12-26T18:22:45.759915, step: 226, loss: 0.39165568351745605, acc: 0.7891, auc: 0.9062, precision: 0.88, recall: 0.6769\n",
      "2018-12-26T18:22:45.912409, step: 227, loss: 0.4099920988082886, acc: 0.7656, auc: 0.9003, precision: 0.881, recall: 0.5968\n",
      "2018-12-26T18:22:46.076974, step: 228, loss: 0.36501985788345337, acc: 0.8516, auc: 0.9195, precision: 0.9, recall: 0.806\n",
      "2018-12-26T18:22:46.231530, step: 229, loss: 0.4147866368293762, acc: 0.7812, auc: 0.887, precision: 0.8361, recall: 0.7391\n",
      "2018-12-26T18:22:46.385684, step: 230, loss: 0.4551456868648529, acc: 0.7578, auc: 0.8694, precision: 0.7857, recall: 0.6984\n",
      "2018-12-26T18:22:46.539307, step: 231, loss: 0.40397703647613525, acc: 0.8594, auc: 0.9097, precision: 0.8431, recall: 0.8113\n",
      "2018-12-26T18:22:46.689958, step: 232, loss: 0.35787665843963623, acc: 0.8594, auc: 0.9203, precision: 0.8548, recall: 0.8548\n",
      "2018-12-26T18:22:46.842328, step: 233, loss: 0.31412169337272644, acc: 0.8281, auc: 0.9399, precision: 0.902, recall: 0.7302\n",
      "2018-12-26T18:22:46.996579, step: 234, loss: 0.3714679479598999, acc: 0.8203, auc: 0.9142, precision: 0.9216, recall: 0.7121\n",
      "2018-12-26T18:22:47.160048, step: 235, loss: 0.3850151300430298, acc: 0.8047, auc: 0.9119, precision: 0.898, recall: 0.6875\n",
      "2018-12-26T18:22:47.312723, step: 236, loss: 0.47969794273376465, acc: 0.7969, auc: 0.8865, precision: 0.9057, recall: 0.6957\n",
      "2018-12-26T18:22:47.465071, step: 237, loss: 0.3664751648902893, acc: 0.8047, auc: 0.9192, precision: 0.8929, recall: 0.7246\n",
      "2018-12-26T18:22:47.628851, step: 238, loss: 0.5225774049758911, acc: 0.7578, auc: 0.8547, precision: 0.7467, recall: 0.8235\n",
      "2018-12-26T18:22:47.792310, step: 239, loss: 0.4585811495780945, acc: 0.7188, auc: 0.8612, precision: 0.7463, recall: 0.7246\n",
      "2018-12-26T18:22:47.953783, step: 240, loss: 0.40070778131484985, acc: 0.8516, auc: 0.9059, precision: 0.8596, recall: 0.8167\n",
      "2018-12-26T18:22:48.106494, step: 241, loss: 0.4546220898628235, acc: 0.7891, auc: 0.8802, precision: 0.7636, recall: 0.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-26T18:22:48.258283, step: 242, loss: 0.36950287222862244, acc: 0.8047, auc: 0.9138, precision: 0.8276, recall: 0.7619\n",
      "2018-12-26T18:22:48.412907, step: 243, loss: 0.4091613292694092, acc: 0.8125, auc: 0.9035, precision: 0.9318, recall: 0.6613\n",
      "2018-12-26T18:22:48.568704, step: 244, loss: 0.43924424052238464, acc: 0.8047, auc: 0.8821, precision: 0.8545, recall: 0.7344\n",
      "2018-12-26T18:22:48.727596, step: 245, loss: 0.40640804171562195, acc: 0.7656, auc: 0.9062, precision: 0.8864, recall: 0.6094\n",
      "2018-12-26T18:22:48.886979, step: 246, loss: 0.4484177231788635, acc: 0.7812, auc: 0.8667, precision: 0.7647, recall: 0.7091\n",
      "2018-12-26T18:22:49.045430, step: 247, loss: 0.5184038877487183, acc: 0.7891, auc: 0.8608, precision: 0.8364, recall: 0.7188\n",
      "2018-12-26T18:22:49.196181, step: 248, loss: 0.3007338047027588, acc: 0.8203, auc: 0.9451, precision: 0.9123, recall: 0.7429\n",
      "2018-12-26T18:22:49.355311, step: 249, loss: 0.4071643650531769, acc: 0.8203, auc: 0.8988, precision: 0.8421, recall: 0.7742\n",
      "2018-12-26T18:22:49.509818, step: 250, loss: 0.42716604471206665, acc: 0.7891, auc: 0.8811, precision: 0.8361, recall: 0.75\n",
      "2018-12-26T18:22:49.662412, step: 251, loss: 0.3363823890686035, acc: 0.8438, auc: 0.9364, precision: 0.8545, recall: 0.7966\n",
      "2018-12-26T18:22:49.815367, step: 252, loss: 0.374996542930603, acc: 0.8438, auc: 0.9113, precision: 0.9, recall: 0.7941\n",
      "2018-12-26T18:22:49.966832, step: 253, loss: 0.4170970320701599, acc: 0.8125, auc: 0.8971, precision: 0.8636, recall: 0.6786\n",
      "2018-12-26T18:22:50.121040, step: 254, loss: 0.37350547313690186, acc: 0.8125, auc: 0.9201, precision: 0.9231, recall: 0.7059\n",
      "2018-12-26T18:22:50.283662, step: 255, loss: 0.29884108901023865, acc: 0.875, auc: 0.9501, precision: 0.9516, recall: 0.8194\n",
      "2018-12-26T18:22:50.437121, step: 256, loss: 0.33067238330841064, acc: 0.8281, auc: 0.938, precision: 0.9153, recall: 0.7606\n",
      "2018-12-26T18:22:50.594909, step: 257, loss: 0.35372284054756165, acc: 0.8125, auc: 0.9238, precision: 0.8727, recall: 0.7385\n",
      "2018-12-26T18:22:50.748166, step: 258, loss: 0.45163705945014954, acc: 0.7891, auc: 0.873, precision: 0.8125, recall: 0.7761\n",
      "2018-12-26T18:22:50.898792, step: 259, loss: 0.355319082736969, acc: 0.8281, auc: 0.9302, precision: 0.8116, recall: 0.8615\n",
      "2018-12-26T18:22:51.071621, step: 260, loss: 0.41601768136024475, acc: 0.8125, auc: 0.8886, precision: 0.8243, recall: 0.8472\n",
      "2018-12-26T18:22:51.226248, step: 261, loss: 0.3347359895706177, acc: 0.8828, auc: 0.9529, precision: 0.8448, recall: 0.8909\n",
      "2018-12-26T18:22:51.380599, step: 262, loss: 0.3868793547153473, acc: 0.8047, auc: 0.9101, precision: 0.8033, recall: 0.7903\n",
      "2018-12-26T18:22:51.533886, step: 263, loss: 0.40439143776893616, acc: 0.7891, auc: 0.8998, precision: 0.8621, recall: 0.7246\n",
      "2018-12-26T18:22:51.688749, step: 264, loss: 0.3237103223800659, acc: 0.875, auc: 0.9418, precision: 0.931, recall: 0.8182\n",
      "2018-12-26T18:22:51.842152, step: 265, loss: 0.30139338970184326, acc: 0.8281, auc: 0.9512, precision: 0.9184, recall: 0.7143\n",
      "2018-12-26T18:22:52.003065, step: 266, loss: 0.30043140053749084, acc: 0.8594, auc: 0.9519, precision: 0.9434, recall: 0.7692\n",
      "2018-12-26T18:22:52.154250, step: 267, loss: 0.39654871821403503, acc: 0.8125, auc: 0.8971, precision: 0.88, recall: 0.7097\n",
      "2018-12-26T18:22:52.310136, step: 268, loss: 0.3619720935821533, acc: 0.8281, auc: 0.919, precision: 0.898, recall: 0.7213\n",
      "2018-12-26T18:22:52.462107, step: 269, loss: 0.32540494203567505, acc: 0.8672, auc: 0.939, precision: 0.898, recall: 0.7857\n",
      "2018-12-26T18:22:52.614553, step: 270, loss: 0.3701673448085785, acc: 0.8203, auc: 0.9151, precision: 0.8793, recall: 0.7612\n",
      "2018-12-26T18:22:52.769236, step: 271, loss: 0.3836897611618042, acc: 0.8203, auc: 0.9086, precision: 0.8947, recall: 0.75\n",
      "2018-12-26T18:22:52.923393, step: 272, loss: 0.3152942657470703, acc: 0.8828, auc: 0.9401, precision: 0.9194, recall: 0.8507\n",
      "2018-12-26T18:22:53.077124, step: 273, loss: 0.318290650844574, acc: 0.8594, auc: 0.9415, precision: 0.9016, recall: 0.8209\n",
      "2018-12-26T18:22:53.229358, step: 274, loss: 0.37550461292266846, acc: 0.8672, auc: 0.9152, precision: 0.875, recall: 0.9091\n",
      "2018-12-26T18:22:53.382903, step: 275, loss: 0.35814955830574036, acc: 0.8672, auc: 0.9231, precision: 0.8714, recall: 0.8841\n",
      "2018-12-26T18:22:53.537411, step: 276, loss: 0.38575267791748047, acc: 0.8281, auc: 0.9107, precision: 0.9149, recall: 0.7049\n",
      "2018-12-26T18:22:53.730076, step: 277, loss: 0.2849607467651367, acc: 0.8516, auc: 0.9558, precision: 0.9107, recall: 0.7846\n",
      "2018-12-26T18:22:53.886818, step: 278, loss: 0.41149622201919556, acc: 0.8203, auc: 0.8927, precision: 0.8909, recall: 0.7424\n",
      "2018-12-26T18:22:54.025786, step: 279, loss: 0.3749540448188782, acc: 0.8281, auc: 0.9157, precision: 0.831, recall: 0.8551\n",
      "2018-12-26T18:22:54.187371, step: 280, loss: 0.4823109209537506, acc: 0.8203, auc: 0.8746, precision: 0.8, recall: 0.8387\n",
      "2018-12-26T18:22:54.341087, step: 281, loss: 0.2997714877128601, acc: 0.8594, auc: 0.9504, precision: 0.8923, recall: 0.8406\n",
      "2018-12-26T18:22:54.494362, step: 282, loss: 0.34476953744888306, acc: 0.7891, auc: 0.9268, precision: 0.878, recall: 0.6207\n",
      "2018-12-26T18:22:54.651243, step: 283, loss: 0.34880006313323975, acc: 0.7969, auc: 0.9283, precision: 0.8511, recall: 0.678\n",
      "2018-12-26T18:22:54.809120, step: 284, loss: 0.44066447019577026, acc: 0.7891, auc: 0.8877, precision: 0.875, recall: 0.6667\n",
      "2018-12-26T18:22:54.964370, step: 285, loss: 0.3422355055809021, acc: 0.8438, auc: 0.9305, precision: 0.88, recall: 0.7586\n",
      "2018-12-26T18:22:55.123078, step: 286, loss: 0.33803272247314453, acc: 0.8203, auc: 0.9304, precision: 0.9111, recall: 0.6833\n",
      "2018-12-26T18:22:55.267466, step: 287, loss: 0.4571295976638794, acc: 0.8047, auc: 0.8789, precision: 0.8519, recall: 0.7302\n",
      "2018-12-26T18:22:55.417154, step: 288, loss: 0.4207516312599182, acc: 0.8516, auc: 0.9003, precision: 0.8772, recall: 0.8065\n",
      "2018-12-26T18:22:55.570589, step: 289, loss: 0.44995084404945374, acc: 0.7891, auc: 0.8837, precision: 0.7544, recall: 0.7679\n",
      "2018-12-26T18:22:55.721575, step: 290, loss: 0.4707416296005249, acc: 0.7891, auc: 0.8619, precision: 0.7636, recall: 0.75\n",
      "2018-12-26T18:22:55.876831, step: 291, loss: 0.3124842643737793, acc: 0.8672, auc: 0.9437, precision: 0.8929, recall: 0.8197\n",
      "2018-12-26T18:22:56.032522, step: 292, loss: 0.35046645998954773, acc: 0.8203, auc: 0.9216, precision: 0.8704, recall: 0.746\n",
      "2018-12-26T18:22:56.183393, step: 293, loss: 0.303521066904068, acc: 0.8516, auc: 0.9481, precision: 0.9286, recall: 0.7761\n",
      "2018-12-26T18:22:56.336782, step: 294, loss: 0.35914546251296997, acc: 0.8359, auc: 0.9216, precision: 0.9149, recall: 0.7167\n",
      "2018-12-26T18:22:56.495431, step: 295, loss: 0.42954060435295105, acc: 0.8047, auc: 0.8953, precision: 0.8824, recall: 0.7031\n",
      "2018-12-26T18:22:56.654623, step: 296, loss: 0.44430047273635864, acc: 0.75, auc: 0.8708, precision: 0.7925, recall: 0.6667\n",
      "2018-12-26T18:22:56.807385, step: 297, loss: 0.3770681321620941, acc: 0.8281, auc: 0.9086, precision: 0.8462, recall: 0.7586\n",
      "2018-12-26T18:22:56.959095, step: 298, loss: 0.3539484739303589, acc: 0.8359, auc: 0.9213, precision: 0.8824, recall: 0.8219\n",
      "2018-12-26T18:22:57.154225, step: 299, loss: 0.306630939245224, acc: 0.8516, auc: 0.9437, precision: 0.8788, recall: 0.8406\n",
      "2018-12-26T18:22:57.307127, step: 300, loss: 0.34252023696899414, acc: 0.875, auc: 0.9283, precision: 0.8475, recall: 0.8772\n",
      "\n",
      "Evaluation:\n",
      "2018-12-26T18:23:03.067577, step: 300, loss: 0.3513072164435136, acc: 0.8468394736842106, auc: 0.926413157894737, precision: 0.8659210526315788, recall: 0.8277605263157897\n",
      "Saved model checkpoint to ../model/textCNN/model/my-model-300\n",
      "\n",
      "2018-12-26T18:23:03.488513, step: 301, loss: 0.3499484658241272, acc: 0.8672, auc: 0.9392, precision: 0.8361, recall: 0.8793\n",
      "2018-12-26T18:23:03.640798, step: 302, loss: 0.37792062759399414, acc: 0.7969, auc: 0.9137, precision: 0.8333, recall: 0.7576\n",
      "2018-12-26T18:23:03.793904, step: 303, loss: 0.39069440960884094, acc: 0.8203, auc: 0.9083, precision: 0.9286, recall: 0.7324\n",
      "2018-12-26T18:23:03.945512, step: 304, loss: 0.3408242464065552, acc: 0.8594, auc: 0.9294, precision: 0.9123, recall: 0.8\n",
      "2018-12-26T18:23:04.115320, step: 305, loss: 0.38062596321105957, acc: 0.8438, auc: 0.9108, precision: 0.8889, recall: 0.7742\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-26T18:23:04.277237, step: 306, loss: 0.42586904764175415, acc: 0.8203, auc: 0.8909, precision: 0.8431, recall: 0.7414\n",
      "2018-12-26T18:23:04.430814, step: 307, loss: 0.3595346212387085, acc: 0.8359, auc: 0.9172, precision: 0.875, recall: 0.8116\n",
      "2018-12-26T18:23:04.587672, step: 308, loss: 0.44622448086738586, acc: 0.7812, auc: 0.8804, precision: 0.8333, recall: 0.7353\n",
      "2018-12-26T18:23:04.747486, step: 309, loss: 0.3739108145236969, acc: 0.8516, auc: 0.9133, precision: 0.8571, recall: 0.8438\n",
      "2018-12-26T18:23:04.898995, step: 310, loss: 0.3691330552101135, acc: 0.8359, auc: 0.9163, precision: 0.8772, recall: 0.7812\n",
      "start training model\n",
      "2018-12-26T18:23:05.088841, step: 311, loss: 0.24287444353103638, acc: 0.8984, auc: 0.9692, precision: 0.942, recall: 0.8784\n",
      "2018-12-26T18:23:05.240224, step: 312, loss: 0.2909168004989624, acc: 0.9219, auc: 0.9523, precision: 0.9123, recall: 0.9123\n",
      "2018-12-26T18:23:05.394689, step: 313, loss: 0.32575175166130066, acc: 0.8516, auc: 0.937, precision: 0.8475, recall: 0.8333\n",
      "2018-12-26T18:23:05.551098, step: 314, loss: 0.3202711343765259, acc: 0.9141, auc: 0.9422, precision: 0.8889, recall: 0.9333\n",
      "2018-12-26T18:23:05.701706, step: 315, loss: 0.28984707593917847, acc: 0.8672, auc: 0.9614, precision: 0.9804, recall: 0.7576\n",
      "2018-12-26T18:23:05.864661, step: 316, loss: 0.3708697557449341, acc: 0.8125, auc: 0.9216, precision: 0.8679, recall: 0.7302\n",
      "2018-12-26T18:23:06.037636, step: 317, loss: 0.4149203598499298, acc: 0.8125, auc: 0.9143, precision: 0.9091, recall: 0.7246\n",
      "2018-12-26T18:23:06.199199, step: 318, loss: 0.3799692690372467, acc: 0.8594, auc: 0.9099, precision: 0.9184, recall: 0.7627\n",
      "2018-12-26T18:23:06.355238, step: 319, loss: 0.33493882417678833, acc: 0.8594, auc: 0.9344, precision: 0.8889, recall: 0.8358\n",
      "2018-12-26T18:23:06.509516, step: 320, loss: 0.34156084060668945, acc: 0.8438, auc: 0.9307, precision: 0.8235, recall: 0.875\n",
      "2018-12-26T18:23:06.659732, step: 321, loss: 0.3845810294151306, acc: 0.8828, auc: 0.9208, precision: 0.9322, recall: 0.8333\n",
      "2018-12-26T18:23:06.810977, step: 322, loss: 0.23586505651474, acc: 0.8906, auc: 0.9727, precision: 0.9091, recall: 0.8475\n",
      "2018-12-26T18:23:06.965125, step: 323, loss: 0.2403646856546402, acc: 0.8828, auc: 0.9739, precision: 0.9, recall: 0.8571\n",
      "2018-12-26T18:23:07.116903, step: 324, loss: 0.346088707447052, acc: 0.8516, auc: 0.92, precision: 0.8511, recall: 0.7692\n",
      "2018-12-26T18:23:07.271581, step: 325, loss: 0.3366764187812805, acc: 0.8203, auc: 0.9535, precision: 0.9636, recall: 0.7162\n",
      "2018-12-26T18:23:07.424785, step: 326, loss: 0.29947230219841003, acc: 0.8594, auc: 0.9424, precision: 0.86, recall: 0.7963\n",
      "2018-12-26T18:23:07.579705, step: 327, loss: 0.2702525556087494, acc: 0.9062, auc: 0.9594, precision: 0.9737, recall: 0.7708\n",
      "2018-12-26T18:23:07.744462, step: 328, loss: 0.2661706507205963, acc: 0.8672, auc: 0.9617, precision: 0.9483, recall: 0.7971\n",
      "2018-12-26T18:23:07.895578, step: 329, loss: 0.3394938111305237, acc: 0.8281, auc: 0.9317, precision: 0.8197, recall: 0.8197\n",
      "2018-12-26T18:23:08.049814, step: 330, loss: 0.2801496982574463, acc: 0.8906, auc: 0.9562, precision: 0.8889, recall: 0.9143\n",
      "2018-12-26T18:23:08.201353, step: 331, loss: 0.34006446599960327, acc: 0.8516, auc: 0.9286, precision: 0.8841, recall: 0.8472\n",
      "2018-12-26T18:23:08.352467, step: 332, loss: 0.2258593887090683, acc: 0.9219, auc: 0.9797, precision: 0.8966, recall: 0.9286\n",
      "2018-12-26T18:23:08.502960, step: 333, loss: 0.3471128046512604, acc: 0.8516, auc: 0.9249, precision: 0.8571, recall: 0.7778\n",
      "2018-12-26T18:23:08.653002, step: 334, loss: 0.22807659208774567, acc: 0.9219, auc: 0.9754, precision: 0.96, recall: 0.8571\n",
      "2018-12-26T18:23:08.821422, step: 335, loss: 0.2891260087490082, acc: 0.8594, auc: 0.9601, precision: 0.9804, recall: 0.7463\n",
      "2018-12-26T18:23:08.980505, step: 336, loss: 0.2574160695075989, acc: 0.8516, auc: 0.9795, precision: 0.9796, recall: 0.7273\n",
      "2018-12-26T18:23:09.160520, step: 337, loss: 0.3320346176624298, acc: 0.8672, auc: 0.9316, precision: 0.9091, recall: 0.8065\n",
      "2018-12-26T18:23:09.313337, step: 338, loss: 0.3266229033470154, acc: 0.8438, auc: 0.9387, precision: 0.8772, recall: 0.7937\n",
      "2018-12-26T18:23:09.470005, step: 339, loss: 0.2733088433742523, acc: 0.8594, auc: 0.9553, precision: 0.8806, recall: 0.8551\n",
      "2018-12-26T18:23:09.624009, step: 340, loss: 0.3222665786743164, acc: 0.8906, auc: 0.9315, precision: 0.9038, recall: 0.8393\n",
      "2018-12-26T18:23:09.778361, step: 341, loss: 0.27977558970451355, acc: 0.875, auc: 0.9586, precision: 0.8571, recall: 0.8852\n",
      "2018-12-26T18:23:09.931221, step: 342, loss: 0.26648062467575073, acc: 0.9141, auc: 0.9582, precision: 0.9643, recall: 0.8571\n",
      "2018-12-26T18:23:10.086480, step: 343, loss: 0.3040745258331299, acc: 0.8516, auc: 0.9451, precision: 0.8966, recall: 0.8\n",
      "2018-12-26T18:23:10.238200, step: 344, loss: 0.3673814833164215, acc: 0.8203, auc: 0.9087, precision: 0.8592, recall: 0.8243\n",
      "2018-12-26T18:23:10.394253, step: 345, loss: 0.388545960187912, acc: 0.8359, auc: 0.9123, precision: 0.8769, recall: 0.8143\n",
      "2018-12-26T18:23:10.552943, step: 346, loss: 0.2862994372844696, acc: 0.9141, auc: 0.9565, precision: 0.9636, recall: 0.8548\n",
      "2018-12-26T18:23:10.707532, step: 347, loss: 0.3109024167060852, acc: 0.8594, auc: 0.9388, precision: 0.9322, recall: 0.7971\n",
      "2018-12-26T18:23:10.860854, step: 348, loss: 0.30113545060157776, acc: 0.9062, auc: 0.9477, precision: 0.918, recall: 0.8889\n",
      "2018-12-26T18:23:11.013245, step: 349, loss: 0.3659352660179138, acc: 0.8047, auc: 0.9168, precision: 0.8214, recall: 0.7541\n",
      "2018-12-26T18:23:11.166670, step: 350, loss: 0.33588355779647827, acc: 0.8828, auc: 0.9264, precision: 0.9231, recall: 0.8571\n",
      "2018-12-26T18:23:11.319801, step: 351, loss: 0.30940863490104675, acc: 0.8516, auc: 0.9434, precision: 0.9091, recall: 0.7812\n",
      "2018-12-26T18:23:11.478162, step: 352, loss: 0.3337552547454834, acc: 0.8672, auc: 0.9323, precision: 0.9245, recall: 0.7903\n",
      "2018-12-26T18:23:11.642352, step: 353, loss: 0.27481794357299805, acc: 0.875, auc: 0.9609, precision: 0.963, recall: 0.7879\n",
      "2018-12-26T18:23:11.795126, step: 354, loss: 0.27926766872406006, acc: 0.9141, auc: 0.9536, precision: 0.9206, recall: 0.9062\n",
      "2018-12-26T18:23:11.946970, step: 355, loss: 0.3719101548194885, acc: 0.8203, auc: 0.922, precision: 0.8525, recall: 0.7879\n",
      "2018-12-26T18:23:12.098721, step: 356, loss: 0.3727484941482544, acc: 0.8281, auc: 0.9255, precision: 0.9038, recall: 0.7344\n",
      "2018-12-26T18:23:12.263834, step: 357, loss: 0.27745264768600464, acc: 0.875, auc: 0.9573, precision: 0.9206, recall: 0.8406\n",
      "2018-12-26T18:23:12.416002, step: 358, loss: 0.3137674033641815, acc: 0.8906, auc: 0.9448, precision: 0.9322, recall: 0.8462\n",
      "2018-12-26T18:23:12.576186, step: 359, loss: 0.27690574526786804, acc: 0.8984, auc: 0.9612, precision: 0.8676, recall: 0.9365\n",
      "2018-12-26T18:23:12.731508, step: 360, loss: 0.28138241171836853, acc: 0.9141, auc: 0.9546, precision: 0.9091, recall: 0.8929\n",
      "2018-12-26T18:23:12.891711, step: 361, loss: 0.2572373151779175, acc: 0.8594, auc: 0.9643, precision: 0.9153, recall: 0.806\n",
      "2018-12-26T18:23:13.047959, step: 362, loss: 0.3219299614429474, acc: 0.8359, auc: 0.9358, precision: 0.8852, recall: 0.7941\n",
      "2018-12-26T18:23:13.204836, step: 363, loss: 0.2725452780723572, acc: 0.8906, auc: 0.9584, precision: 0.9492, recall: 0.8358\n",
      "2018-12-26T18:23:13.358038, step: 364, loss: 0.32040515542030334, acc: 0.8516, auc: 0.9368, precision: 0.9245, recall: 0.7656\n",
      "2018-12-26T18:23:13.509822, step: 365, loss: 0.25518661737442017, acc: 0.8594, auc: 0.9706, precision: 0.9455, recall: 0.7761\n",
      "2018-12-26T18:23:13.660475, step: 366, loss: 0.24208176136016846, acc: 0.9062, auc: 0.9691, precision: 0.9828, recall: 0.8382\n",
      "2018-12-26T18:23:13.813742, step: 367, loss: 0.24395909905433655, acc: 0.9062, auc: 0.9733, precision: 0.9508, recall: 0.8657\n",
      "2018-12-26T18:23:13.963788, step: 368, loss: 0.3116942346096039, acc: 0.8828, auc: 0.9484, precision: 0.873, recall: 0.8871\n",
      "2018-12-26T18:23:14.113956, step: 369, loss: 0.3101364076137543, acc: 0.875, auc: 0.9442, precision: 0.8923, recall: 0.8657\n",
      "2018-12-26T18:23:14.264806, step: 370, loss: 0.2605641782283783, acc: 0.9219, auc: 0.9704, precision: 0.8939, recall: 0.9516\n",
      "2018-12-26T18:23:14.413785, step: 371, loss: 0.206904798746109, acc: 0.9375, auc: 0.9822, precision: 0.9355, recall: 0.9355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-26T18:23:14.570210, step: 372, loss: 0.2988471984863281, acc: 0.875, auc: 0.9539, precision: 0.9516, recall: 0.8194\n",
      "2018-12-26T18:23:14.708219, step: 373, loss: 0.21372056007385254, acc: 0.9219, auc: 0.9784, precision: 0.9833, recall: 0.8676\n",
      "2018-12-26T18:23:14.861411, step: 374, loss: 0.26980075240135193, acc: 0.8672, auc: 0.9637, precision: 0.9811, recall: 0.7647\n",
      "2018-12-26T18:23:14.998427, step: 375, loss: 0.21947380900382996, acc: 0.9141, auc: 0.9751, precision: 0.9474, recall: 0.871\n",
      "2018-12-26T18:23:15.134156, step: 376, loss: 0.328305721282959, acc: 0.8594, auc: 0.9339, precision: 0.8889, recall: 0.8358\n",
      "2018-12-26T18:23:15.291616, step: 377, loss: 0.2680906653404236, acc: 0.8984, auc: 0.9604, precision: 0.95, recall: 0.8507\n",
      "2018-12-26T18:23:15.451543, step: 378, loss: 0.28178542852401733, acc: 0.8828, auc: 0.9628, precision: 0.88, recall: 0.9167\n",
      "2018-12-26T18:23:15.593062, step: 379, loss: 0.28385478258132935, acc: 0.9219, auc: 0.9634, precision: 0.9219, recall: 0.9219\n",
      "2018-12-26T18:23:15.729563, step: 380, loss: 0.3654305934906006, acc: 0.8594, auc: 0.9343, precision: 0.8429, recall: 0.8939\n",
      "2018-12-26T18:23:15.886750, step: 381, loss: 0.3046565651893616, acc: 0.8594, auc: 0.9505, precision: 0.9483, recall: 0.7857\n",
      "2018-12-26T18:23:16.023982, step: 382, loss: 0.2093060463666916, acc: 0.9062, auc: 0.9802, precision: 0.9661, recall: 0.8507\n",
      "2018-12-26T18:23:16.165090, step: 383, loss: 0.31615149974823, acc: 0.8594, auc: 0.9383, precision: 0.902, recall: 0.7797\n",
      "2018-12-26T18:23:16.317800, step: 384, loss: 0.3468839228153229, acc: 0.875, auc: 0.9359, precision: 0.9492, recall: 0.8116\n",
      "2018-12-26T18:23:16.472658, step: 385, loss: 0.2936302423477173, acc: 0.8516, auc: 0.9455, precision: 0.9286, recall: 0.8228\n",
      "2018-12-26T18:23:16.631516, step: 386, loss: 0.34389716386795044, acc: 0.8359, auc: 0.9352, precision: 0.9259, recall: 0.7463\n",
      "2018-12-26T18:23:16.787164, step: 387, loss: 0.34483852982521057, acc: 0.8516, auc: 0.9275, precision: 0.8966, recall: 0.8\n",
      "2018-12-26T18:23:16.943946, step: 388, loss: 0.30276384949684143, acc: 0.8203, auc: 0.9455, precision: 0.8387, recall: 0.8\n",
      "2018-12-26T18:23:17.097988, step: 389, loss: 0.28576239943504333, acc: 0.8672, auc: 0.9494, precision: 0.8986, recall: 0.8611\n",
      "2018-12-26T18:23:17.252502, step: 390, loss: 0.3395201563835144, acc: 0.8438, auc: 0.9357, precision: 0.8276, recall: 0.8276\n",
      "2018-12-26T18:23:17.409704, step: 391, loss: 0.2871416211128235, acc: 0.8984, auc: 0.9526, precision: 0.9062, recall: 0.8923\n",
      "2018-12-26T18:23:17.582067, step: 392, loss: 0.35308241844177246, acc: 0.8281, auc: 0.9202, precision: 0.8462, recall: 0.7586\n",
      "2018-12-26T18:23:17.739549, step: 393, loss: 0.277999609708786, acc: 0.875, auc: 0.9564, precision: 0.9048, recall: 0.8507\n",
      "2018-12-26T18:23:17.893424, step: 394, loss: 0.3085809051990509, acc: 0.8594, auc: 0.9431, precision: 0.9074, recall: 0.7903\n",
      "2018-12-26T18:23:18.049408, step: 395, loss: 0.2904987633228302, acc: 0.8438, auc: 0.9525, precision: 0.8958, recall: 0.7414\n",
      "2018-12-26T18:23:18.205463, step: 396, loss: 0.2859590947628021, acc: 0.8984, auc: 0.9562, precision: 0.9615, recall: 0.8197\n",
      "2018-12-26T18:23:18.363434, step: 397, loss: 0.2959728240966797, acc: 0.8828, auc: 0.9506, precision: 0.9608, recall: 0.7903\n",
      "2018-12-26T18:23:18.525374, step: 398, loss: 0.30240389704704285, acc: 0.8828, auc: 0.9398, precision: 0.9333, recall: 0.8358\n",
      "2018-12-26T18:23:18.683427, step: 399, loss: 0.3338051736354828, acc: 0.8125, auc: 0.9316, precision: 0.8923, recall: 0.7733\n",
      "2018-12-26T18:23:18.842864, step: 400, loss: 0.2967885136604309, acc: 0.8594, auc: 0.9451, precision: 0.875, recall: 0.8167\n",
      "\n",
      "Evaluation:\n",
      "2018-12-26T18:23:24.726093, step: 400, loss: 0.336755528261787, acc: 0.8505394736842106, auc: 0.9302315789473685, precision: 0.8791342105263158, recall: 0.8180499999999999\n",
      "Saved model checkpoint to ../model/textCNN/model/my-model-400\n",
      "\n",
      "2018-12-26T18:23:25.232144, step: 401, loss: 0.28177499771118164, acc: 0.8672, auc: 0.9511, precision: 0.8704, recall: 0.8246\n",
      "2018-12-26T18:23:25.384001, step: 402, loss: 0.3082311749458313, acc: 0.8984, auc: 0.941, precision: 0.898, recall: 0.8462\n",
      "2018-12-26T18:23:25.535530, step: 403, loss: 0.30096378922462463, acc: 0.8906, auc: 0.9556, precision: 0.8923, recall: 0.8923\n",
      "2018-12-26T18:23:25.688117, step: 404, loss: 0.2674746811389923, acc: 0.8984, auc: 0.9633, precision: 0.95, recall: 0.8507\n",
      "2018-12-26T18:23:25.838451, step: 405, loss: 0.29573798179626465, acc: 0.8672, auc: 0.9467, precision: 0.8929, recall: 0.8197\n",
      "2018-12-26T18:23:25.991550, step: 406, loss: 0.3240763545036316, acc: 0.8906, auc: 0.9494, precision: 0.9333, recall: 0.8485\n",
      "2018-12-26T18:23:26.144414, step: 407, loss: 0.3540560007095337, acc: 0.8281, auc: 0.932, precision: 0.907, recall: 0.6842\n",
      "2018-12-26T18:23:26.296564, step: 408, loss: 0.30518150329589844, acc: 0.875, auc: 0.9473, precision: 0.9273, recall: 0.8095\n",
      "2018-12-26T18:23:26.454383, step: 409, loss: 0.262668639421463, acc: 0.8828, auc: 0.9629, precision: 0.8704, recall: 0.8545\n",
      "2018-12-26T18:23:26.606568, step: 410, loss: 0.263315886259079, acc: 0.8906, auc: 0.9606, precision: 0.9123, recall: 0.8525\n",
      "2018-12-26T18:23:26.771547, step: 411, loss: 0.2670423090457916, acc: 0.8594, auc: 0.9623, precision: 0.9574, recall: 0.7377\n",
      "2018-12-26T18:23:26.925543, step: 412, loss: 0.2874438464641571, acc: 0.8672, auc: 0.9492, precision: 0.9194, recall: 0.8261\n",
      "2018-12-26T18:23:27.100513, step: 413, loss: 0.24267128109931946, acc: 0.8906, auc: 0.9738, precision: 0.8772, recall: 0.8772\n",
      "2018-12-26T18:23:27.249443, step: 414, loss: 0.2708744406700134, acc: 0.875, auc: 0.9544, precision: 0.9038, recall: 0.8103\n",
      "2018-12-26T18:23:27.400724, step: 415, loss: 0.25112009048461914, acc: 0.9297, auc: 0.9687, precision: 0.9153, recall: 0.931\n",
      "2018-12-26T18:23:27.553926, step: 416, loss: 0.2780097723007202, acc: 0.8906, auc: 0.958, precision: 0.9104, recall: 0.8841\n",
      "2018-12-26T18:23:27.710830, step: 417, loss: 0.2767934501171112, acc: 0.8516, auc: 0.9543, precision: 0.8983, recall: 0.803\n",
      "2018-12-26T18:23:27.863695, step: 418, loss: 0.2684655487537384, acc: 0.875, auc: 0.9563, precision: 0.9, recall: 0.8036\n",
      "2018-12-26T18:23:28.010933, step: 419, loss: 0.25336402654647827, acc: 0.8906, auc: 0.9672, precision: 0.9583, recall: 0.7931\n",
      "2018-12-26T18:23:28.168939, step: 420, loss: 0.30456414818763733, acc: 0.8438, auc: 0.9632, precision: 0.9636, recall: 0.7465\n",
      "2018-12-26T18:23:28.329552, step: 421, loss: 0.23604777455329895, acc: 0.8984, auc: 0.9697, precision: 0.8636, recall: 0.9344\n",
      "2018-12-26T18:23:28.482740, step: 422, loss: 0.33763453364372253, acc: 0.8438, auc: 0.9355, precision: 0.8525, recall: 0.8254\n",
      "2018-12-26T18:23:28.634273, step: 423, loss: 0.22625169157981873, acc: 0.8828, auc: 0.9753, precision: 0.9273, recall: 0.8226\n",
      "2018-12-26T18:23:28.788516, step: 424, loss: 0.26009178161621094, acc: 0.9062, auc: 0.9603, precision: 0.9615, recall: 0.8333\n",
      "2018-12-26T18:23:28.942102, step: 425, loss: 0.3234480023384094, acc: 0.8281, auc: 0.9489, precision: 0.7797, recall: 0.8364\n",
      "2018-12-26T18:23:29.094543, step: 426, loss: 0.3102741837501526, acc: 0.8516, auc: 0.9427, precision: 0.9138, recall: 0.791\n",
      "2018-12-26T18:23:29.255890, step: 427, loss: 0.29414600133895874, acc: 0.8281, auc: 0.9515, precision: 0.9318, recall: 0.6833\n",
      "2018-12-26T18:23:29.409986, step: 428, loss: 0.30393680930137634, acc: 0.8438, auc: 0.9417, precision: 0.8871, recall: 0.8088\n",
      "2018-12-26T18:23:29.564794, step: 429, loss: 0.2784396708011627, acc: 0.8594, auc: 0.9543, precision: 0.94, recall: 0.7581\n",
      "2018-12-26T18:23:29.718481, step: 430, loss: 0.21711793541908264, acc: 0.9141, auc: 0.9746, precision: 0.9516, recall: 0.8806\n",
      "2018-12-26T18:23:29.879921, step: 431, loss: 0.2707587480545044, acc: 0.8984, auc: 0.9616, precision: 0.9524, recall: 0.8571\n",
      "2018-12-26T18:23:30.034146, step: 432, loss: 0.29919156432151794, acc: 0.8828, auc: 0.9526, precision: 0.8868, recall: 0.8393\n",
      "2018-12-26T18:23:30.185994, step: 433, loss: 0.275734007358551, acc: 0.8906, auc: 0.9574, precision: 0.9107, recall: 0.85\n",
      "2018-12-26T18:23:30.341325, step: 434, loss: 0.3080938458442688, acc: 0.8672, auc: 0.9455, precision: 0.8824, recall: 0.8696\n",
      "2018-12-26T18:23:30.498437, step: 435, loss: 0.3122374713420868, acc: 0.8516, auc: 0.9395, precision: 0.8947, recall: 0.7969\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-26T18:23:30.659745, step: 436, loss: 0.25484520196914673, acc: 0.9375, auc: 0.9708, precision: 0.9455, recall: 0.9123\n",
      "2018-12-26T18:23:30.808951, step: 437, loss: 0.2870171368122101, acc: 0.8125, auc: 0.9483, precision: 0.8929, recall: 0.7353\n",
      "2018-12-26T18:23:30.963308, step: 438, loss: 0.2808176279067993, acc: 0.8438, auc: 0.9565, precision: 0.9231, recall: 0.75\n",
      "2018-12-26T18:23:31.117448, step: 439, loss: 0.26804637908935547, acc: 0.875, auc: 0.9638, precision: 1.0, recall: 0.7419\n",
      "2018-12-26T18:23:31.280548, step: 440, loss: 0.27171164751052856, acc: 0.8906, auc: 0.9559, precision: 0.94, recall: 0.8103\n",
      "2018-12-26T18:23:31.443239, step: 441, loss: 0.20954084396362305, acc: 0.9375, auc: 0.9811, precision: 0.9815, recall: 0.8833\n",
      "2018-12-26T18:23:31.597585, step: 442, loss: 0.26353752613067627, acc: 0.875, auc: 0.9607, precision: 0.8551, recall: 0.9077\n",
      "2018-12-26T18:23:31.751317, step: 443, loss: 0.237324059009552, acc: 0.875, auc: 0.9637, precision: 0.8873, recall: 0.8873\n",
      "2018-12-26T18:23:31.907809, step: 444, loss: 0.3583775758743286, acc: 0.8672, auc: 0.9337, precision: 0.8182, recall: 0.8654\n",
      "2018-12-26T18:23:32.060066, step: 445, loss: 0.3313700556755066, acc: 0.8359, auc: 0.9363, precision: 0.8305, recall: 0.8167\n",
      "2018-12-26T18:23:32.213697, step: 446, loss: 0.3028026819229126, acc: 0.8594, auc: 0.9468, precision: 0.9483, recall: 0.7857\n",
      "2018-12-26T18:23:32.367587, step: 447, loss: 0.3181995153427124, acc: 0.8594, auc: 0.9438, precision: 0.873, recall: 0.8462\n",
      "2018-12-26T18:23:32.520439, step: 448, loss: 0.335532546043396, acc: 0.8672, auc: 0.9336, precision: 0.9273, recall: 0.7969\n",
      "2018-12-26T18:23:32.673506, step: 449, loss: 0.2482842355966568, acc: 0.8672, auc: 0.9645, precision: 0.9184, recall: 0.7759\n",
      "2018-12-26T18:23:32.825428, step: 450, loss: 0.181527242064476, acc: 0.9219, auc: 0.9883, precision: 0.8966, recall: 0.9286\n",
      "2018-12-26T18:23:32.979903, step: 451, loss: 0.3222423493862152, acc: 0.8438, auc: 0.9472, precision: 0.96, recall: 0.7273\n",
      "2018-12-26T18:23:33.134757, step: 452, loss: 0.32140040397644043, acc: 0.875, auc: 0.9335, precision: 0.9167, recall: 0.7857\n",
      "2018-12-26T18:23:33.289376, step: 453, loss: 0.37880364060401917, acc: 0.7969, auc: 0.911, precision: 0.8704, recall: 0.7121\n",
      "2018-12-26T18:23:33.442837, step: 454, loss: 0.3371414542198181, acc: 0.8359, auc: 0.9328, precision: 0.913, recall: 0.8077\n",
      "2018-12-26T18:23:33.601781, step: 455, loss: 0.28102532029151917, acc: 0.8984, auc: 0.9541, precision: 0.9155, recall: 0.9028\n",
      "2018-12-26T18:23:33.757152, step: 456, loss: 0.2849835157394409, acc: 0.8906, auc: 0.9593, precision: 0.878, recall: 0.9474\n",
      "2018-12-26T18:23:33.911156, step: 457, loss: 0.3772334158420563, acc: 0.8281, auc: 0.9228, precision: 0.8143, recall: 0.8636\n",
      "2018-12-26T18:23:34.066565, step: 458, loss: 0.4065212905406952, acc: 0.8281, auc: 0.9179, precision: 0.7941, recall: 0.871\n",
      "2018-12-26T18:23:34.217012, step: 459, loss: 0.3331425189971924, acc: 0.8516, auc: 0.9308, precision: 0.8772, recall: 0.8065\n",
      "2018-12-26T18:23:34.379973, step: 460, loss: 0.3106323182582855, acc: 0.8516, auc: 0.9441, precision: 0.8947, recall: 0.7969\n",
      "2018-12-26T18:23:34.534357, step: 461, loss: 0.2699449062347412, acc: 0.8984, auc: 0.9596, precision: 0.9388, recall: 0.8214\n",
      "2018-12-26T18:23:34.692090, step: 462, loss: 0.30788689851760864, acc: 0.8516, auc: 0.9535, precision: 0.9643, recall: 0.7606\n",
      "2018-12-26T18:23:34.848923, step: 463, loss: 0.20237644016742706, acc: 0.8984, auc: 0.9842, precision: 0.9677, recall: 0.8451\n",
      "2018-12-26T18:23:35.008487, step: 464, loss: 0.4036159813404083, acc: 0.8125, auc: 0.8988, precision: 0.85, recall: 0.7727\n",
      "2018-12-26T18:23:35.168712, step: 465, loss: 0.3026279807090759, acc: 0.8516, auc: 0.9455, precision: 0.8644, recall: 0.8226\n",
      "start training model\n",
      "2018-12-26T18:23:35.355494, step: 466, loss: 0.21551990509033203, acc: 0.9375, auc: 0.9823, precision: 1.0, recall: 0.8841\n",
      "2018-12-26T18:23:35.510806, step: 467, loss: 0.29897546768188477, acc: 0.875, auc: 0.9465, precision: 0.9016, recall: 0.8462\n",
      "2018-12-26T18:23:35.672353, step: 468, loss: 0.2567084729671478, acc: 0.8672, auc: 0.9659, precision: 0.9091, recall: 0.8451\n",
      "2018-12-26T18:23:35.825962, step: 469, loss: 0.2168268859386444, acc: 0.9297, auc: 0.9773, precision: 0.9492, recall: 0.9032\n",
      "2018-12-26T18:23:35.982151, step: 470, loss: 0.18512296676635742, acc: 0.9297, auc: 0.9821, precision: 0.9444, recall: 0.9315\n",
      "2018-12-26T18:23:36.136187, step: 471, loss: 0.20724759995937347, acc: 0.9375, auc: 0.9736, precision: 0.9565, recall: 0.9296\n",
      "2018-12-26T18:23:36.296947, step: 472, loss: 0.22717581689357758, acc: 0.9141, auc: 0.975, precision: 0.8704, recall: 0.9216\n",
      "2018-12-26T18:23:36.461740, step: 473, loss: 0.16373340785503387, acc: 0.9453, auc: 0.9908, precision: 0.9804, recall: 0.8929\n",
      "2018-12-26T18:23:36.616547, step: 474, loss: 0.28180307149887085, acc: 0.8828, auc: 0.9532, precision: 0.9104, recall: 0.8714\n",
      "2018-12-26T18:23:36.770635, step: 475, loss: 0.19082260131835938, acc: 0.9141, auc: 0.99, precision: 0.9811, recall: 0.8387\n",
      "2018-12-26T18:23:36.925412, step: 476, loss: 0.29197072982788086, acc: 0.8672, auc: 0.9575, precision: 0.9623, recall: 0.7727\n",
      "2018-12-26T18:23:37.084596, step: 477, loss: 0.2550082206726074, acc: 0.8828, auc: 0.9603, precision: 0.8958, recall: 0.8113\n",
      "2018-12-26T18:23:37.236016, step: 478, loss: 0.2571953237056732, acc: 0.8594, auc: 0.9687, precision: 0.9574, recall: 0.7377\n",
      "2018-12-26T18:23:37.394229, step: 479, loss: 0.24540600180625916, acc: 0.9062, auc: 0.967, precision: 0.9804, recall: 0.8197\n",
      "2018-12-26T18:23:37.547815, step: 480, loss: 0.19110897183418274, acc: 0.9219, auc: 0.9883, precision: 0.9831, recall: 0.8657\n",
      "2018-12-26T18:23:37.701245, step: 481, loss: 0.24150104820728302, acc: 0.9141, auc: 0.9788, precision: 0.8841, recall: 0.9531\n",
      "2018-12-26T18:23:37.851953, step: 482, loss: 0.20134632289409637, acc: 0.9219, auc: 0.986, precision: 0.9138, recall: 0.9138\n",
      "2018-12-26T18:23:38.012931, step: 483, loss: 0.22965696454048157, acc: 0.9141, auc: 0.9786, precision: 0.9028, recall: 0.942\n",
      "2018-12-26T18:23:38.165548, step: 484, loss: 0.22658193111419678, acc: 0.9141, auc: 0.9757, precision: 0.9452, recall: 0.9079\n",
      "2018-12-26T18:23:38.314316, step: 485, loss: 0.301575243473053, acc: 0.9141, auc: 0.9442, precision: 0.8909, recall: 0.9074\n",
      "2018-12-26T18:23:38.474958, step: 486, loss: 0.26758795976638794, acc: 0.8984, auc: 0.9632, precision: 0.9516, recall: 0.8551\n",
      "2018-12-26T18:23:38.632872, step: 487, loss: 0.23825591802597046, acc: 0.8984, auc: 0.9667, precision: 0.9123, recall: 0.8667\n",
      "2018-12-26T18:23:38.789882, step: 488, loss: 0.21556776762008667, acc: 0.9062, auc: 0.9892, precision: 0.9808, recall: 0.8226\n",
      "2018-12-26T18:23:38.948546, step: 489, loss: 0.24179361760616302, acc: 0.8516, auc: 0.9738, precision: 0.9556, recall: 0.7167\n",
      "2018-12-26T18:23:39.101698, step: 490, loss: 0.202992781996727, acc: 0.9062, auc: 0.9875, precision: 1.0, recall: 0.8154\n",
      "2018-12-26T18:23:39.262615, step: 491, loss: 0.20971111953258514, acc: 0.9375, auc: 0.9781, precision: 0.9697, recall: 0.9143\n",
      "2018-12-26T18:23:39.418944, step: 492, loss: 0.24199238419532776, acc: 0.8906, auc: 0.9667, precision: 0.9375, recall: 0.8571\n",
      "2018-12-26T18:23:39.574800, step: 493, loss: 0.22866511344909668, acc: 0.9141, auc: 0.9702, precision: 0.9333, recall: 0.8889\n",
      "2018-12-26T18:23:39.732524, step: 494, loss: 0.2197658121585846, acc: 0.9062, auc: 0.9727, precision: 0.9265, recall: 0.9\n",
      "2018-12-26T18:23:39.887033, step: 495, loss: 0.24734798073768616, acc: 0.9219, auc: 0.9787, precision: 0.8906, recall: 0.95\n",
      "2018-12-26T18:23:40.036730, step: 496, loss: 0.21281419694423676, acc: 0.9375, auc: 0.9783, precision: 0.9189, recall: 0.9714\n",
      "2018-12-26T18:23:40.189485, step: 497, loss: 0.19644898176193237, acc: 0.9141, auc: 0.9826, precision: 0.9552, recall: 0.8889\n",
      "2018-12-26T18:23:40.341722, step: 498, loss: 0.2454507052898407, acc: 0.875, auc: 0.9718, precision: 0.9661, recall: 0.8028\n",
      "2018-12-26T18:23:40.507157, step: 499, loss: 0.23536550998687744, acc: 0.8984, auc: 0.9765, precision: 0.9818, recall: 0.8182\n",
      "2018-12-26T18:23:40.664830, step: 500, loss: 0.19885563850402832, acc: 0.9219, auc: 0.9839, precision: 0.9355, recall: 0.9062\n",
      "\n",
      "Evaluation:\n",
      "2018-12-26T18:23:46.373264, step: 500, loss: 0.3264955031244378, acc: 0.8501342105263161, auc: 0.935184210526316, precision: 0.90155, recall: 0.791613157894737\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model checkpoint to ../model/textCNN/model/my-model-500\n",
      "\n",
      "2018-12-26T18:23:46.794262, step: 501, loss: 0.21416695415973663, acc: 0.9219, auc: 0.9727, precision: 0.9302, recall: 0.8511\n",
      "2018-12-26T18:23:46.951760, step: 502, loss: 0.2653137445449829, acc: 0.8984, auc: 0.9604, precision: 0.9545, recall: 0.863\n",
      "2018-12-26T18:23:47.107768, step: 503, loss: 0.24279430508613586, acc: 0.8828, auc: 0.9695, precision: 0.9455, recall: 0.8125\n",
      "2018-12-26T18:23:47.261398, step: 504, loss: 0.2631341218948364, acc: 0.8984, auc: 0.9567, precision: 0.9344, recall: 0.8636\n",
      "2018-12-26T18:23:47.414788, step: 505, loss: 0.2751542031764984, acc: 0.9141, auc: 0.9541, precision: 0.9, recall: 0.9153\n",
      "2018-12-26T18:23:47.564549, step: 506, loss: 0.19804707169532776, acc: 0.9141, auc: 0.977, precision: 0.9315, recall: 0.9189\n",
      "2018-12-26T18:23:47.717266, step: 507, loss: 0.28451672196388245, acc: 0.8906, auc: 0.9562, precision: 0.8958, recall: 0.8269\n",
      "2018-12-26T18:23:47.870435, step: 508, loss: 0.19641615450382233, acc: 0.9141, auc: 0.9841, precision: 0.9815, recall: 0.8413\n",
      "2018-12-26T18:23:48.020744, step: 509, loss: 0.20541301369667053, acc: 0.9141, auc: 0.9726, precision: 0.9242, recall: 0.9104\n",
      "2018-12-26T18:23:48.173484, step: 510, loss: 0.2824983596801758, acc: 0.8359, auc: 0.9641, precision: 0.9815, recall: 0.726\n",
      "2018-12-26T18:23:48.330331, step: 511, loss: 0.21143530309200287, acc: 0.8984, auc: 0.9771, precision: 0.9592, recall: 0.8103\n",
      "2018-12-26T18:23:48.489001, step: 512, loss: 0.26358067989349365, acc: 0.8828, auc: 0.9609, precision: 0.9423, recall: 0.8033\n",
      "2018-12-26T18:23:48.644476, step: 513, loss: 0.2167617678642273, acc: 0.9219, auc: 0.974, precision: 0.9464, recall: 0.8833\n",
      "2018-12-26T18:23:48.799314, step: 514, loss: 0.15286065638065338, acc: 0.9609, auc: 0.9961, precision: 0.9538, recall: 0.9688\n",
      "2018-12-26T18:23:48.953489, step: 515, loss: 0.28231343626976013, acc: 0.8828, auc: 0.9609, precision: 0.871, recall: 0.8852\n",
      "2018-12-26T18:23:49.107614, step: 516, loss: 0.2237025797367096, acc: 0.9141, auc: 0.9756, precision: 0.9104, recall: 0.9242\n",
      "2018-12-26T18:23:49.259185, step: 517, loss: 0.23973020911216736, acc: 0.8672, auc: 0.9699, precision: 0.9464, recall: 0.791\n",
      "2018-12-26T18:23:49.411266, step: 518, loss: 0.16639748215675354, acc: 0.9453, auc: 0.9907, precision: 0.9831, recall: 0.9062\n",
      "2018-12-26T18:23:49.564960, step: 519, loss: 0.2560791075229645, acc: 0.8984, auc: 0.9639, precision: 0.9483, recall: 0.8462\n",
      "2018-12-26T18:23:49.714917, step: 520, loss: 0.18768715858459473, acc: 0.9219, auc: 0.9812, precision: 0.9273, recall: 0.8947\n",
      "2018-12-26T18:23:49.869169, step: 521, loss: 0.24230223894119263, acc: 0.9219, auc: 0.9661, precision: 0.9038, recall: 0.9038\n",
      "2018-12-26T18:23:50.027664, step: 522, loss: 0.21544405817985535, acc: 0.9297, auc: 0.9777, precision: 0.9833, recall: 0.8806\n",
      "2018-12-26T18:23:50.179526, step: 523, loss: 0.2405746877193451, acc: 0.8672, auc: 0.9733, precision: 0.9821, recall: 0.7746\n",
      "2018-12-26T18:23:50.342740, step: 524, loss: 0.22784751653671265, acc: 0.8906, auc: 0.9676, precision: 0.9091, recall: 0.8824\n",
      "2018-12-26T18:23:50.495900, step: 525, loss: 0.2155674248933792, acc: 0.9297, auc: 0.9782, precision: 0.9333, recall: 0.9459\n",
      "2018-12-26T18:23:50.650205, step: 526, loss: 0.1676294505596161, acc: 0.9062, auc: 0.9912, precision: 0.9444, recall: 0.85\n",
      "2018-12-26T18:23:50.802913, step: 527, loss: 0.2363530397415161, acc: 0.9297, auc: 0.9692, precision: 0.9508, recall: 0.9062\n",
      "2018-12-26T18:23:50.958503, step: 528, loss: 0.16843152046203613, acc: 0.9297, auc: 0.9927, precision: 1.0, recall: 0.8615\n",
      "2018-12-26T18:23:51.114181, step: 529, loss: 0.1956188976764679, acc: 0.9375, auc: 0.9812, precision: 0.9583, recall: 0.9324\n",
      "2018-12-26T18:23:51.264601, step: 530, loss: 0.17137590050697327, acc: 0.9375, auc: 0.987, precision: 0.9265, recall: 0.9545\n",
      "2018-12-26T18:23:51.418658, step: 531, loss: 0.2640168368816376, acc: 0.8672, auc: 0.9592, precision: 0.8824, recall: 0.8696\n",
      "2018-12-26T18:23:51.566141, step: 532, loss: 0.2763911187648773, acc: 0.8828, auc: 0.9552, precision: 0.9394, recall: 0.8493\n",
      "2018-12-26T18:23:51.716215, step: 533, loss: 0.2614344656467438, acc: 0.9062, auc: 0.9613, precision: 0.9571, recall: 0.8816\n",
      "2018-12-26T18:23:51.869829, step: 534, loss: 0.260355144739151, acc: 0.9062, auc: 0.9604, precision: 0.9048, recall: 0.9048\n",
      "2018-12-26T18:23:52.017333, step: 535, loss: 0.18326784670352936, acc: 0.9219, auc: 0.9817, precision: 0.9206, recall: 0.9206\n",
      "2018-12-26T18:23:52.166214, step: 536, loss: 0.2201492041349411, acc: 0.9219, auc: 0.9737, precision: 0.9683, recall: 0.8841\n",
      "2018-12-26T18:23:52.299351, step: 537, loss: 0.1715669333934784, acc: 0.9531, auc: 0.9862, precision: 0.9474, recall: 0.9474\n",
      "2018-12-26T18:23:52.432581, step: 538, loss: 0.18018627166748047, acc: 0.9141, auc: 0.985, precision: 0.9672, recall: 0.8676\n",
      "2018-12-26T18:23:52.583483, step: 539, loss: 0.24978728592395782, acc: 0.9219, auc: 0.9644, precision: 0.9846, recall: 0.8767\n",
      "2018-12-26T18:23:52.733162, step: 540, loss: 0.2196829915046692, acc: 0.8828, auc: 0.9765, precision: 0.9516, recall: 0.831\n",
      "2018-12-26T18:23:52.886500, step: 541, loss: 0.2177932858467102, acc: 0.9141, auc: 0.9726, precision: 0.9815, recall: 0.8413\n",
      "2018-12-26T18:23:53.039499, step: 542, loss: 0.17219574749469757, acc: 0.9219, auc: 0.9854, precision: 0.9655, recall: 0.875\n",
      "2018-12-26T18:23:53.196932, step: 543, loss: 0.1716020107269287, acc: 0.9531, auc: 0.9895, precision: 0.9516, recall: 0.9516\n",
      "2018-12-26T18:23:53.349800, step: 544, loss: 0.25535887479782104, acc: 0.8828, auc: 0.9663, precision: 0.8788, recall: 0.8923\n",
      "2018-12-26T18:23:53.503581, step: 545, loss: 0.28145503997802734, acc: 0.8984, auc: 0.9604, precision: 0.8939, recall: 0.9077\n",
      "2018-12-26T18:23:53.661093, step: 546, loss: 0.20458322763442993, acc: 0.9219, auc: 0.9807, precision: 0.8983, recall: 0.9298\n",
      "2018-12-26T18:23:53.824290, step: 547, loss: 0.18956054747104645, acc: 0.9219, auc: 0.9822, precision: 0.95, recall: 0.8906\n",
      "2018-12-26T18:23:53.977029, step: 548, loss: 0.2910153269767761, acc: 0.8516, auc: 0.9588, precision: 0.9556, recall: 0.7167\n",
      "2018-12-26T18:23:54.139520, step: 549, loss: 0.22525560855865479, acc: 0.8438, auc: 0.9766, precision: 0.9464, recall: 0.7571\n",
      "2018-12-26T18:23:54.293081, step: 550, loss: 0.24900133907794952, acc: 0.8906, auc: 0.9624, precision: 0.9412, recall: 0.8136\n",
      "2018-12-26T18:23:54.446584, step: 551, loss: 0.1583365947008133, acc: 0.9375, auc: 0.991, precision: 0.9821, recall: 0.8871\n",
      "2018-12-26T18:23:54.601397, step: 552, loss: 0.1989041417837143, acc: 0.9219, auc: 0.9761, precision: 0.95, recall: 0.8906\n",
      "2018-12-26T18:23:54.755657, step: 553, loss: 0.17441897094249725, acc: 0.9453, auc: 0.9897, precision: 0.9429, recall: 0.9565\n",
      "2018-12-26T18:23:54.916518, step: 554, loss: 0.20560714602470398, acc: 0.9297, auc: 0.9824, precision: 0.9242, recall: 0.9385\n",
      "2018-12-26T18:23:55.071111, step: 555, loss: 0.2470637708902359, acc: 0.9062, auc: 0.9691, precision: 0.8772, recall: 0.9091\n",
      "2018-12-26T18:23:55.225373, step: 556, loss: 0.15748536586761475, acc: 0.9609, auc: 0.9914, precision: 0.9655, recall: 0.9492\n",
      "2018-12-26T18:23:55.377340, step: 557, loss: 0.22720478475093842, acc: 0.8906, auc: 0.9708, precision: 0.95, recall: 0.8382\n",
      "2018-12-26T18:23:55.531231, step: 558, loss: 0.21102803945541382, acc: 0.9062, auc: 0.9795, precision: 0.9836, recall: 0.8451\n",
      "2018-12-26T18:23:55.683499, step: 559, loss: 0.24041138589382172, acc: 0.875, auc: 0.9651, precision: 0.9216, recall: 0.7966\n",
      "2018-12-26T18:23:55.862981, step: 560, loss: 0.1793302297592163, acc: 0.9375, auc: 0.9841, precision: 0.9444, recall: 0.9107\n",
      "2018-12-26T18:23:56.016295, step: 561, loss: 0.26480910181999207, acc: 0.875, auc: 0.9582, precision: 0.9273, recall: 0.8095\n",
      "2018-12-26T18:23:56.173692, step: 562, loss: 0.21335111558437347, acc: 0.8984, auc: 0.9751, precision: 0.9559, recall: 0.8667\n",
      "2018-12-26T18:23:56.337105, step: 563, loss: 0.2068893313407898, acc: 0.9297, auc: 0.9773, precision: 0.9818, recall: 0.871\n",
      "2018-12-26T18:23:56.493429, step: 564, loss: 0.20947116613388062, acc: 0.9219, auc: 0.974, precision: 0.9403, recall: 0.913\n",
      "2018-12-26T18:23:56.656194, step: 565, loss: 0.21641552448272705, acc: 0.9297, auc: 0.9802, precision: 0.9062, recall: 0.9508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-26T18:23:56.806298, step: 566, loss: 0.18091866374015808, acc: 0.9531, auc: 0.9794, precision: 0.9464, recall: 0.9464\n",
      "2018-12-26T18:23:56.958862, step: 567, loss: 0.21263454854488373, acc: 0.8828, auc: 0.9744, precision: 0.9104, recall: 0.8714\n",
      "2018-12-26T18:23:57.110555, step: 568, loss: 0.16880197823047638, acc: 0.9062, auc: 0.9871, precision: 0.918, recall: 0.8889\n",
      "2018-12-26T18:23:57.259740, step: 569, loss: 0.20263884961605072, acc: 0.9375, auc: 0.9807, precision: 0.9649, recall: 0.9016\n",
      "2018-12-26T18:23:57.420071, step: 570, loss: 0.16144973039627075, acc: 0.9297, auc: 0.9909, precision: 0.9808, recall: 0.8644\n",
      "2018-12-26T18:23:57.572109, step: 571, loss: 0.16965779662132263, acc: 0.9375, auc: 0.9867, precision: 0.9455, recall: 0.9123\n",
      "2018-12-26T18:23:57.730029, step: 572, loss: 0.20503047108650208, acc: 0.9219, auc: 0.9765, precision: 0.9608, recall: 0.8596\n",
      "2018-12-26T18:23:57.883253, step: 573, loss: 0.1379808932542801, acc: 0.9531, auc: 0.9932, precision: 0.9839, recall: 0.9242\n",
      "2018-12-26T18:23:58.033832, step: 574, loss: 0.23948565125465393, acc: 0.8672, auc: 0.9699, precision: 0.9636, recall: 0.7794\n",
      "2018-12-26T18:23:58.185262, step: 575, loss: 0.20341338217258453, acc: 0.8828, auc: 0.9791, precision: 0.9167, recall: 0.8\n",
      "2018-12-26T18:23:58.340561, step: 576, loss: 0.18860068917274475, acc: 0.9297, auc: 0.9822, precision: 0.9508, recall: 0.9062\n",
      "2018-12-26T18:23:58.498887, step: 577, loss: 0.21123498678207397, acc: 0.9531, auc: 0.9703, precision: 0.9655, recall: 0.9333\n",
      "2018-12-26T18:23:58.656445, step: 578, loss: 0.1903514564037323, acc: 0.9219, auc: 0.9792, precision: 0.9667, recall: 0.8788\n",
      "2018-12-26T18:23:58.806097, step: 579, loss: 0.2503451704978943, acc: 0.9141, auc: 0.9613, precision: 0.9254, recall: 0.9118\n",
      "2018-12-26T18:23:58.972023, step: 580, loss: 0.2211746871471405, acc: 0.8906, auc: 0.9712, precision: 0.94, recall: 0.8103\n",
      "2018-12-26T18:23:59.122856, step: 581, loss: 0.21537472307682037, acc: 0.8828, auc: 0.9749, precision: 0.9583, recall: 0.7797\n",
      "2018-12-26T18:23:59.273841, step: 582, loss: 0.29622912406921387, acc: 0.8906, auc: 0.9469, precision: 0.9077, recall: 0.8806\n",
      "2018-12-26T18:23:59.429036, step: 583, loss: 0.21192489564418793, acc: 0.9375, auc: 0.98, precision: 0.9254, recall: 0.9538\n",
      "2018-12-26T18:23:59.584689, step: 584, loss: 0.2678792476654053, acc: 0.8594, auc: 0.9605, precision: 0.9344, recall: 0.8028\n",
      "2018-12-26T18:23:59.743854, step: 585, loss: 0.2229021191596985, acc: 0.9219, auc: 0.9725, precision: 0.9836, recall: 0.8696\n",
      "2018-12-26T18:23:59.894994, step: 586, loss: 0.23328855633735657, acc: 0.9141, auc: 0.9705, precision: 0.9492, recall: 0.875\n",
      "2018-12-26T18:24:00.052683, step: 587, loss: 0.22122015058994293, acc: 0.9062, auc: 0.977, precision: 0.9298, recall: 0.8689\n",
      "2018-12-26T18:24:00.206935, step: 588, loss: 0.19377955794334412, acc: 0.9375, auc: 0.9824, precision: 0.9545, recall: 0.9265\n",
      "2018-12-26T18:24:00.360237, step: 589, loss: 0.1666809320449829, acc: 0.9453, auc: 0.9862, precision: 0.9706, recall: 0.9296\n",
      "2018-12-26T18:24:00.510167, step: 590, loss: 0.22163306176662445, acc: 0.8984, auc: 0.9735, precision: 0.9118, recall: 0.8986\n",
      "2018-12-26T18:24:00.664662, step: 591, loss: 0.23314806818962097, acc: 0.8984, auc: 0.9706, precision: 0.9655, recall: 0.8358\n",
      "2018-12-26T18:24:00.820129, step: 592, loss: 0.2767731249332428, acc: 0.8672, auc: 0.9558, precision: 0.8571, recall: 0.871\n",
      "2018-12-26T18:24:00.982431, step: 593, loss: 0.2995789647102356, acc: 0.8516, auc: 0.9435, precision: 0.8772, recall: 0.8065\n",
      "2018-12-26T18:24:01.137650, step: 594, loss: 0.28426647186279297, acc: 0.8672, auc: 0.9503, precision: 0.9231, recall: 0.7869\n",
      "2018-12-26T18:24:01.288933, step: 595, loss: 0.17774255573749542, acc: 0.9453, auc: 0.9888, precision: 0.9677, recall: 0.9231\n",
      "2018-12-26T18:24:01.444127, step: 596, loss: 0.20901361107826233, acc: 0.8906, auc: 0.9764, precision: 0.9531, recall: 0.8472\n",
      "2018-12-26T18:24:01.599576, step: 597, loss: 0.2837323248386383, acc: 0.9141, auc: 0.95, precision: 0.9298, recall: 0.8833\n",
      "2018-12-26T18:24:01.751403, step: 598, loss: 0.2292378693819046, acc: 0.9297, auc: 0.9699, precision: 0.9483, recall: 0.9016\n",
      "2018-12-26T18:24:01.903236, step: 599, loss: 0.24087196588516235, acc: 0.8906, auc: 0.9763, precision: 0.8413, recall: 0.9298\n",
      "2018-12-26T18:24:02.052692, step: 600, loss: 0.2531642019748688, acc: 0.8984, auc: 0.9617, precision: 0.8906, recall: 0.9048\n",
      "\n",
      "Evaluation:\n",
      "2018-12-26T18:24:07.889389, step: 600, loss: 0.32889649664100845, acc: 0.8451947368421053, auc: 0.9378368421052633, precision: 0.9160710526315788, recall: 0.7633684210526316\n",
      "Saved model checkpoint to ../model/textCNN/model/my-model-600\n",
      "\n",
      "2018-12-26T18:24:08.400173, step: 601, loss: 0.15531273186206818, acc: 0.9453, auc: 0.9912, precision: 0.9565, recall: 0.898\n",
      "2018-12-26T18:24:08.553635, step: 602, loss: 0.1209821105003357, acc: 0.9609, auc: 0.9951, precision: 0.9831, recall: 0.9355\n",
      "2018-12-26T18:24:08.706220, step: 603, loss: 0.26912057399749756, acc: 0.8828, auc: 0.9599, precision: 0.9423, recall: 0.8033\n",
      "2018-12-26T18:24:08.864007, step: 604, loss: 0.2471739798784256, acc: 0.8438, auc: 0.9875, precision: 1.0, recall: 0.6825\n",
      "2018-12-26T18:24:09.030739, step: 605, loss: 0.18434184789657593, acc: 0.9141, auc: 0.9844, precision: 0.9773, recall: 0.8113\n",
      "2018-12-26T18:24:09.191607, step: 606, loss: 0.2408468872308731, acc: 0.9297, auc: 0.9634, precision: 0.9231, recall: 0.9375\n",
      "2018-12-26T18:24:09.360191, step: 607, loss: 0.2020224928855896, acc: 0.9297, auc: 0.9785, precision: 0.9014, recall: 0.9697\n",
      "2018-12-26T18:24:09.512924, step: 608, loss: 0.20632551610469818, acc: 0.9219, auc: 0.9846, precision: 0.9219, recall: 0.9219\n",
      "2018-12-26T18:24:09.664993, step: 609, loss: 0.23882795870304108, acc: 0.9375, auc: 0.965, precision: 0.9559, recall: 0.9286\n",
      "2018-12-26T18:24:09.802950, step: 610, loss: 0.2372361570596695, acc: 0.9297, auc: 0.9789, precision: 0.8889, recall: 0.9412\n",
      "2018-12-26T18:24:09.953213, step: 611, loss: 0.2161732316017151, acc: 0.9141, auc: 0.9735, precision: 0.9216, recall: 0.8704\n",
      "2018-12-26T18:24:10.110332, step: 612, loss: 0.2952086329460144, acc: 0.8594, auc: 0.9607, precision: 0.9583, recall: 0.7419\n",
      "2018-12-26T18:24:10.262234, step: 613, loss: 0.2732928991317749, acc: 0.8359, auc: 0.9646, precision: 0.94, recall: 0.7231\n",
      "2018-12-26T18:24:10.423109, step: 614, loss: 0.20441719889640808, acc: 0.9062, auc: 0.9872, precision: 1.0, recall: 0.831\n",
      "2018-12-26T18:24:10.576588, step: 615, loss: 0.2102643996477127, acc: 0.9062, auc: 0.977, precision: 0.9655, recall: 0.8485\n",
      "2018-12-26T18:24:10.730673, step: 616, loss: 0.2598714232444763, acc: 0.9062, auc: 0.9611, precision: 0.8966, recall: 0.8966\n",
      "2018-12-26T18:24:10.889205, step: 617, loss: 0.22340494394302368, acc: 0.9062, auc: 0.9776, precision: 0.8833, recall: 0.9138\n",
      "2018-12-26T18:24:11.047474, step: 618, loss: 0.2775842249393463, acc: 0.9062, auc: 0.9708, precision: 0.8475, recall: 0.9434\n",
      "2018-12-26T18:24:11.200236, step: 619, loss: 0.17254647612571716, acc: 0.9531, auc: 0.9861, precision: 0.9828, recall: 0.9194\n",
      "2018-12-26T18:24:11.350451, step: 620, loss: 0.19491146504878998, acc: 0.9375, auc: 0.9724, precision: 0.963, recall: 0.8966\n",
      "start training model\n",
      "2018-12-26T18:24:11.529026, step: 621, loss: 0.1509246975183487, acc: 0.9453, auc: 0.9941, precision: 0.9839, recall: 0.9104\n",
      "2018-12-26T18:24:11.679809, step: 622, loss: 0.14155618846416473, acc: 0.9531, auc: 0.9919, precision: 0.9828, recall: 0.9194\n",
      "2018-12-26T18:24:11.836515, step: 623, loss: 0.20577730238437653, acc: 0.8906, auc: 0.9853, precision: 1.0, recall: 0.7971\n",
      "2018-12-26T18:24:11.989975, step: 624, loss: 0.21872863173484802, acc: 0.8984, auc: 0.9816, precision: 0.9836, recall: 0.8333\n",
      "2018-12-26T18:24:12.143385, step: 625, loss: 0.12061606347560883, acc: 0.9609, auc: 0.9965, precision: 0.963, recall: 0.9455\n",
      "2018-12-26T18:24:12.293851, step: 626, loss: 0.1979224681854248, acc: 0.9297, auc: 0.9781, precision: 0.9688, recall: 0.8986\n",
      "2018-12-26T18:24:12.469323, step: 627, loss: 0.1775793731212616, acc: 0.9609, auc: 0.9878, precision: 0.9552, recall: 0.9697\n",
      "2018-12-26T18:24:12.620360, step: 628, loss: 0.2027391493320465, acc: 0.9297, auc: 0.9792, precision: 0.9483, recall: 0.9016\n",
      "2018-12-26T18:24:12.772281, step: 629, loss: 0.17516222596168518, acc: 0.9609, auc: 0.9868, precision: 0.9508, recall: 0.9667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-26T18:24:12.925830, step: 630, loss: 0.2207394242286682, acc: 0.9297, auc: 0.9802, precision: 0.9062, recall: 0.9508\n",
      "2018-12-26T18:24:13.078349, step: 631, loss: 0.1714913547039032, acc: 0.9453, auc: 0.9857, precision: 0.931, recall: 0.9474\n",
      "2018-12-26T18:24:13.234794, step: 632, loss: 0.1296079158782959, acc: 0.9609, auc: 0.9941, precision: 0.9672, recall: 0.9516\n",
      "2018-12-26T18:24:13.384459, step: 633, loss: 0.2048933506011963, acc: 0.9219, auc: 0.9829, precision: 1.0, recall: 0.8438\n",
      "2018-12-26T18:24:13.535899, step: 634, loss: 0.16653941571712494, acc: 0.9141, auc: 0.991, precision: 0.9821, recall: 0.8462\n",
      "2018-12-26T18:24:13.696301, step: 635, loss: 0.13229382038116455, acc: 0.9297, auc: 0.9993, precision: 1.0, recall: 0.8594\n",
      "2018-12-26T18:24:13.848974, step: 636, loss: 0.17380154132843018, acc: 0.9219, auc: 0.9922, precision: 1.0, recall: 0.8462\n",
      "2018-12-26T18:24:14.004738, step: 637, loss: 0.2461605668067932, acc: 0.8906, auc: 0.9673, precision: 0.9592, recall: 0.7966\n",
      "2018-12-26T18:24:14.164455, step: 638, loss: 0.19226527214050293, acc: 0.9297, auc: 0.9797, precision: 0.9683, recall: 0.8971\n",
      "2018-12-26T18:24:14.318009, step: 639, loss: 0.1676175445318222, acc: 0.9375, auc: 0.9903, precision: 0.9452, recall: 0.9452\n",
      "2018-12-26T18:24:14.468887, step: 640, loss: 0.2088947892189026, acc: 0.8984, auc: 0.9868, precision: 0.8406, recall: 0.9667\n",
      "2018-12-26T18:24:14.620732, step: 641, loss: 0.21403539180755615, acc: 0.9297, auc: 0.9807, precision: 0.9077, recall: 0.9516\n",
      "2018-12-26T18:24:14.772429, step: 642, loss: 0.16557800769805908, acc: 0.9609, auc: 0.9924, precision: 0.9275, recall: 1.0\n",
      "2018-12-26T18:24:14.931846, step: 643, loss: 0.16966718435287476, acc: 0.9531, auc: 0.9834, precision: 0.9853, recall: 0.9306\n",
      "2018-12-26T18:24:15.083297, step: 644, loss: 0.1778917908668518, acc: 0.9141, auc: 0.9852, precision: 0.9437, recall: 0.9054\n",
      "2018-12-26T18:24:15.232984, step: 645, loss: 0.17860057950019836, acc: 0.9297, auc: 0.9863, precision: 0.9848, recall: 0.8904\n",
      "2018-12-26T18:24:15.386989, step: 646, loss: 0.17975953221321106, acc: 0.8984, auc: 0.9936, precision: 0.9836, recall: 0.8333\n",
      "2018-12-26T18:24:15.541559, step: 647, loss: 0.18084201216697693, acc: 0.9062, auc: 0.989, precision: 0.9815, recall: 0.8281\n",
      "2018-12-26T18:24:15.697029, step: 648, loss: 0.15016470849514008, acc: 0.9531, auc: 0.9917, precision: 1.0, recall: 0.9104\n",
      "2018-12-26T18:24:15.846558, step: 649, loss: 0.14288078248500824, acc: 0.9609, auc: 0.9966, precision: 0.95, recall: 0.9661\n",
      "2018-12-26T18:24:15.999743, step: 650, loss: 0.12825295329093933, acc: 0.9531, auc: 0.9963, precision: 0.9385, recall: 0.9683\n",
      "2018-12-26T18:24:16.159341, step: 651, loss: 0.19326579570770264, acc: 0.9297, auc: 0.9792, precision: 0.9516, recall: 0.9077\n",
      "2018-12-26T18:24:16.321254, step: 652, loss: 0.20896631479263306, acc: 0.9141, auc: 0.9788, precision: 0.9077, recall: 0.9219\n",
      "2018-12-26T18:24:16.476093, step: 653, loss: 0.21418233215808868, acc: 0.9141, auc: 0.9748, precision: 0.9016, recall: 0.9167\n",
      "2018-12-26T18:24:16.629982, step: 654, loss: 0.14539295434951782, acc: 0.9922, auc: 0.9923, precision: 0.9828, recall: 1.0\n",
      "2018-12-26T18:24:16.769728, step: 655, loss: 0.12084398418664932, acc: 0.9609, auc: 0.9956, precision: 0.9839, recall: 0.9385\n",
      "2018-12-26T18:24:16.922356, step: 656, loss: 0.18280474841594696, acc: 0.9375, auc: 0.9832, precision: 0.9423, recall: 0.9074\n",
      "2018-12-26T18:24:17.081675, step: 657, loss: 0.11769919842481613, acc: 0.9609, auc: 0.998, precision: 1.0, recall: 0.9219\n",
      "2018-12-26T18:24:17.235496, step: 658, loss: 0.16730713844299316, acc: 0.9062, auc: 0.9936, precision: 1.0, recall: 0.8261\n",
      "2018-12-26T18:24:17.392016, step: 659, loss: 0.1785643845796585, acc: 0.9062, auc: 0.9875, precision: 0.9808, recall: 0.8226\n",
      "2018-12-26T18:24:17.545423, step: 660, loss: 0.17494559288024902, acc: 0.9062, auc: 0.9919, precision: 1.0, recall: 0.8154\n",
      "2018-12-26T18:24:17.677821, step: 661, loss: 0.15945982933044434, acc: 0.9453, auc: 0.986, precision: 0.9846, recall: 0.9143\n",
      "2018-12-26T18:24:17.831453, step: 662, loss: 0.1744156777858734, acc: 0.9453, auc: 0.9897, precision: 0.9344, recall: 0.95\n",
      "2018-12-26T18:24:17.984600, step: 663, loss: 0.1740274727344513, acc: 0.9531, auc: 0.9926, precision: 0.9643, recall: 0.931\n",
      "2018-12-26T18:24:18.137234, step: 664, loss: 0.18346159160137177, acc: 0.9609, auc: 0.9839, precision: 0.9718, recall: 0.9583\n",
      "2018-12-26T18:24:18.289860, step: 665, loss: 0.18240486085414886, acc: 0.9375, auc: 0.9888, precision: 0.8852, recall: 0.9818\n",
      "2018-12-26T18:24:18.447488, step: 666, loss: 0.18009285628795624, acc: 0.9219, auc: 0.985, precision: 0.9688, recall: 0.8857\n",
      "2018-12-26T18:24:18.604482, step: 667, loss: 0.12574639916419983, acc: 0.9531, auc: 0.9938, precision: 0.963, recall: 0.9286\n",
      "2018-12-26T18:24:18.754540, step: 668, loss: 0.1732066571712494, acc: 0.9141, auc: 0.9862, precision: 0.9677, recall: 0.8696\n",
      "2018-12-26T18:24:18.907488, step: 669, loss: 0.1446828544139862, acc: 0.9531, auc: 0.9907, precision: 0.9833, recall: 0.9219\n",
      "2018-12-26T18:24:19.059691, step: 670, loss: 0.15073898434638977, acc: 0.9375, auc: 0.9941, precision: 1.0, recall: 0.8621\n",
      "2018-12-26T18:24:19.221496, step: 671, loss: 0.17940610647201538, acc: 0.9219, auc: 0.9869, precision: 1.0, recall: 0.8667\n",
      "2018-12-26T18:24:19.375627, step: 672, loss: 0.19615504145622253, acc: 0.9219, auc: 0.9773, precision: 0.9661, recall: 0.8769\n",
      "2018-12-26T18:24:19.528567, step: 673, loss: 0.12861758470535278, acc: 0.9766, auc: 0.9917, precision: 0.9692, recall: 0.9844\n",
      "2018-12-26T18:24:19.684920, step: 674, loss: 0.17803722620010376, acc: 0.9375, auc: 0.9821, precision: 0.971, recall: 0.9178\n",
      "2018-12-26T18:24:19.836514, step: 675, loss: 0.1199585497379303, acc: 0.9688, auc: 0.9985, precision: 1.0, recall: 0.9429\n",
      "2018-12-26T18:24:19.995479, step: 676, loss: 0.13091027736663818, acc: 0.9531, auc: 0.996, precision: 0.9474, recall: 0.973\n",
      "2018-12-26T18:24:20.153208, step: 677, loss: 0.14456693828105927, acc: 0.9531, auc: 0.9914, precision: 0.9688, recall: 0.9394\n",
      "2018-12-26T18:24:20.309069, step: 678, loss: 0.1189824640750885, acc: 0.9609, auc: 0.9966, precision: 0.9692, recall: 0.9545\n",
      "2018-12-26T18:24:20.460716, step: 679, loss: 0.17568732798099518, acc: 0.9531, auc: 0.9873, precision: 0.9375, recall: 0.9677\n",
      "2018-12-26T18:24:20.624676, step: 680, loss: 0.13802024722099304, acc: 0.9609, auc: 0.9924, precision: 0.9833, recall: 0.9365\n",
      "2018-12-26T18:24:20.779706, step: 681, loss: 0.1922875940799713, acc: 0.9375, auc: 0.9765, precision: 0.9836, recall: 0.8955\n",
      "2018-12-26T18:24:20.932749, step: 682, loss: 0.1339261829853058, acc: 0.9688, auc: 0.9917, precision: 0.9833, recall: 0.9516\n",
      "2018-12-26T18:24:21.087609, step: 683, loss: 0.15439395606517792, acc: 0.9375, auc: 0.9911, precision: 0.9697, recall: 0.9143\n",
      "2018-12-26T18:24:21.243672, step: 684, loss: 0.14625269174575806, acc: 0.9375, auc: 0.9885, precision: 0.9483, recall: 0.9167\n",
      "2018-12-26T18:24:21.401861, step: 685, loss: 0.17045381665229797, acc: 0.9219, auc: 0.9834, precision: 0.9434, recall: 0.8772\n",
      "2018-12-26T18:24:21.561075, step: 686, loss: 0.21639862656593323, acc: 0.9297, auc: 0.9726, precision: 0.9643, recall: 0.8852\n",
      "2018-12-26T18:24:21.717057, step: 687, loss: 0.1534653902053833, acc: 0.9453, auc: 0.9897, precision: 0.9643, recall: 0.9153\n",
      "2018-12-26T18:24:21.875303, step: 688, loss: 0.1882140040397644, acc: 0.9375, auc: 0.9807, precision: 0.9219, recall: 0.9516\n",
      "2018-12-26T18:24:22.037793, step: 689, loss: 0.1436985284090042, acc: 0.9766, auc: 0.9941, precision: 0.9545, recall: 1.0\n",
      "2018-12-26T18:24:22.193569, step: 690, loss: 0.1505044549703598, acc: 0.9453, auc: 0.9877, precision: 0.942, recall: 0.9559\n",
      "2018-12-26T18:24:22.346623, step: 691, loss: 0.17228810489177704, acc: 0.9375, auc: 0.9854, precision: 0.9245, recall: 0.9245\n",
      "2018-12-26T18:24:22.499605, step: 692, loss: 0.15928694605827332, acc: 0.9219, auc: 0.9973, precision: 1.0, recall: 0.8529\n",
      "2018-12-26T18:24:22.654159, step: 693, loss: 0.1962110996246338, acc: 0.8984, auc: 0.9811, precision: 0.9423, recall: 0.8305\n",
      "2018-12-26T18:24:22.805416, step: 694, loss: 0.11307688057422638, acc: 0.9531, auc: 0.997, precision: 1.0, recall: 0.8947\n",
      "2018-12-26T18:24:22.960036, step: 695, loss: 0.16268852353096008, acc: 0.9297, auc: 0.989, precision: 1.0, recall: 0.85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-26T18:24:23.112185, step: 696, loss: 0.1095997542142868, acc: 0.9375, auc: 0.9973, precision: 0.9825, recall: 0.8889\n",
      "2018-12-26T18:24:23.262646, step: 697, loss: 0.13155946135520935, acc: 0.9609, auc: 0.9953, precision: 0.9811, recall: 0.9286\n",
      "2018-12-26T18:24:23.413748, step: 698, loss: 0.15019111335277557, acc: 0.9609, auc: 0.989, precision: 0.9697, recall: 0.9552\n",
      "2018-12-26T18:24:23.564561, step: 699, loss: 0.16074581444263458, acc: 0.9297, auc: 0.9885, precision: 0.931, recall: 0.9153\n",
      "2018-12-26T18:24:23.715958, step: 700, loss: 0.14645501971244812, acc: 0.9375, auc: 0.9899, precision: 0.9804, recall: 0.8772\n",
      "\n",
      "Evaluation:\n",
      "2018-12-26T18:24:29.616028, step: 700, loss: 0.32255097439414576, acc: 0.8581447368421053, auc: 0.937565789473684, precision: 0.8896368421052628, recall: 0.8199921052631577\n",
      "Saved model checkpoint to ../model/textCNN/model/my-model-700\n",
      "\n",
      "2018-12-26T18:24:30.048259, step: 701, loss: 0.2039598673582077, acc: 0.9141, auc: 0.9767, precision: 0.9143, recall: 0.9275\n",
      "2018-12-26T18:24:30.198982, step: 702, loss: 0.14395125210285187, acc: 0.9609, auc: 0.9897, precision: 0.9821, recall: 0.9322\n",
      "2018-12-26T18:24:30.352855, step: 703, loss: 0.1696264147758484, acc: 0.9375, auc: 0.9883, precision: 0.9833, recall: 0.8939\n",
      "2018-12-26T18:24:30.503733, step: 704, loss: 0.11011803895235062, acc: 0.9766, auc: 0.998, precision: 1.0, recall: 0.9552\n",
      "2018-12-26T18:24:30.664003, step: 705, loss: 0.15343880653381348, acc: 0.9297, auc: 0.9926, precision: 0.9811, recall: 0.8667\n",
      "2018-12-26T18:24:30.816201, step: 706, loss: 0.12165634334087372, acc: 0.9375, auc: 0.997, precision: 1.0, recall: 0.8904\n",
      "2018-12-26T18:24:30.966025, step: 707, loss: 0.211292564868927, acc: 0.9219, auc: 0.9746, precision: 0.9821, recall: 0.8594\n",
      "2018-12-26T18:24:31.118574, step: 708, loss: 0.13354846835136414, acc: 0.9688, auc: 0.9941, precision: 0.9828, recall: 0.95\n",
      "2018-12-26T18:24:31.267551, step: 709, loss: 0.13667137920856476, acc: 0.9609, auc: 0.9963, precision: 0.9344, recall: 0.9828\n",
      "2018-12-26T18:24:31.422923, step: 710, loss: 0.16725388169288635, acc: 0.9375, auc: 0.9849, precision: 1.0, recall: 0.873\n",
      "2018-12-26T18:24:31.582962, step: 711, loss: 0.13415856659412384, acc: 0.9375, auc: 0.9919, precision: 0.9841, recall: 0.8986\n",
      "2018-12-26T18:24:31.732864, step: 712, loss: 0.17479373514652252, acc: 0.9453, auc: 0.9824, precision: 0.9825, recall: 0.9032\n",
      "2018-12-26T18:24:31.885952, step: 713, loss: 0.16759955883026123, acc: 0.9375, auc: 0.9875, precision: 0.9412, recall: 0.9412\n",
      "2018-12-26T18:24:32.034182, step: 714, loss: 0.1294747292995453, acc: 0.9609, auc: 0.9927, precision: 0.9841, recall: 0.9394\n",
      "2018-12-26T18:24:32.189296, step: 715, loss: 0.18085114657878876, acc: 0.9297, auc: 0.9784, precision: 0.9464, recall: 0.8983\n",
      "2018-12-26T18:24:32.339813, step: 716, loss: 0.1254839301109314, acc: 0.9688, auc: 0.9966, precision: 0.9672, recall: 0.9672\n",
      "2018-12-26T18:24:32.500672, step: 717, loss: 0.09464879333972931, acc: 0.9609, auc: 0.9993, precision: 1.0, recall: 0.9296\n",
      "2018-12-26T18:24:32.670986, step: 718, loss: 0.14194081723690033, acc: 0.9453, auc: 0.9902, precision: 0.9825, recall: 0.9032\n",
      "2018-12-26T18:24:32.824109, step: 719, loss: 0.1568889021873474, acc: 0.9609, auc: 0.9877, precision: 0.98, recall: 0.9245\n",
      "2018-12-26T18:24:32.977251, step: 720, loss: 0.21662810444831848, acc: 0.9141, auc: 0.9703, precision: 0.9298, recall: 0.8833\n",
      "2018-12-26T18:24:33.128218, step: 721, loss: 0.23456597328186035, acc: 0.8828, auc: 0.9697, precision: 0.9643, recall: 0.806\n",
      "2018-12-26T18:24:33.277631, step: 722, loss: 0.14802256226539612, acc: 0.9531, auc: 0.9899, precision: 0.9796, recall: 0.9057\n",
      "2018-12-26T18:24:33.428923, step: 723, loss: 0.1653532236814499, acc: 0.9297, auc: 0.9853, precision: 0.9839, recall: 0.8841\n",
      "2018-12-26T18:24:33.580103, step: 724, loss: 0.1227993369102478, acc: 0.9688, auc: 0.9973, precision: 0.9697, recall: 0.9697\n",
      "2018-12-26T18:24:33.735959, step: 725, loss: 0.119812972843647, acc: 0.9531, auc: 0.9959, precision: 0.9733, recall: 0.9481\n",
      "2018-12-26T18:24:33.889884, step: 726, loss: 0.1790718287229538, acc: 0.9219, auc: 0.9905, precision: 0.8814, recall: 0.9455\n",
      "2018-12-26T18:24:34.039339, step: 727, loss: 0.15901905298233032, acc: 0.9375, auc: 0.9892, precision: 0.942, recall: 0.942\n",
      "2018-12-26T18:24:34.193610, step: 728, loss: 0.18081626296043396, acc: 0.9375, auc: 0.9866, precision: 0.9516, recall: 0.9219\n",
      "2018-12-26T18:24:34.347905, step: 729, loss: 0.15597161650657654, acc: 0.9375, auc: 0.9898, precision: 0.9571, recall: 0.9306\n",
      "2018-12-26T18:24:34.510634, step: 730, loss: 0.15911072492599487, acc: 0.9297, auc: 0.9902, precision: 0.9649, recall: 0.8871\n",
      "2018-12-26T18:24:34.673480, step: 731, loss: 0.13922268152236938, acc: 0.9453, auc: 0.9926, precision: 1.0, recall: 0.8793\n",
      "2018-12-26T18:24:34.827884, step: 732, loss: 0.2184000015258789, acc: 0.8906, auc: 0.9802, precision: 1.0, recall: 0.7846\n",
      "2018-12-26T18:24:34.980789, step: 733, loss: 0.18605703115463257, acc: 0.9062, auc: 0.9778, precision: 0.9643, recall: 0.8438\n",
      "2018-12-26T18:24:35.132081, step: 734, loss: 0.15505127608776093, acc: 0.9297, auc: 0.9921, precision: 0.9688, recall: 0.8986\n",
      "2018-12-26T18:24:35.284207, step: 735, loss: 0.11806406080722809, acc: 0.9531, auc: 0.9946, precision: 0.9452, recall: 0.9718\n",
      "2018-12-26T18:24:35.438784, step: 736, loss: 0.17612488567829132, acc: 0.9453, auc: 0.9892, precision: 0.95, recall: 0.9344\n",
      "2018-12-26T18:24:35.591327, step: 737, loss: 0.17178237438201904, acc: 0.9531, auc: 0.9885, precision: 0.9545, recall: 0.9545\n",
      "2018-12-26T18:24:35.747144, step: 738, loss: 0.2076387107372284, acc: 0.9453, auc: 0.9741, precision: 0.9571, recall: 0.9437\n",
      "2018-12-26T18:24:35.902692, step: 739, loss: 0.10659803450107574, acc: 0.9766, auc: 0.9973, precision: 0.9846, recall: 0.9697\n",
      "2018-12-26T18:24:36.059274, step: 740, loss: 0.16848579049110413, acc: 0.9297, auc: 0.9891, precision: 0.9138, recall: 0.9298\n",
      "2018-12-26T18:24:36.211341, step: 741, loss: 0.1154995858669281, acc: 0.9609, auc: 0.9934, precision: 1.0, recall: 0.9265\n",
      "2018-12-26T18:24:36.364478, step: 742, loss: 0.16306647658348083, acc: 0.9453, auc: 0.9882, precision: 0.9692, recall: 0.9265\n",
      "2018-12-26T18:24:36.513531, step: 743, loss: 0.14953483641147614, acc: 0.9375, auc: 0.989, precision: 0.9667, recall: 0.9062\n",
      "2018-12-26T18:24:36.677025, step: 744, loss: 0.2619171738624573, acc: 0.9062, auc: 0.9597, precision: 0.9773, recall: 0.7963\n",
      "2018-12-26T18:24:36.839370, step: 745, loss: 0.15312997996807098, acc: 0.9062, auc: 0.9934, precision: 0.9808, recall: 0.8226\n",
      "2018-12-26T18:24:36.994559, step: 746, loss: 0.20513197779655457, acc: 0.8906, auc: 0.9756, precision: 0.9474, recall: 0.8308\n",
      "2018-12-26T18:24:37.148335, step: 747, loss: 0.16699545085430145, acc: 0.9609, auc: 0.9824, precision: 0.9559, recall: 0.9701\n",
      "2018-12-26T18:24:37.301857, step: 748, loss: 0.1213323175907135, acc: 0.9609, auc: 0.9938, precision: 1.0, recall: 0.9306\n",
      "2018-12-26T18:24:37.456432, step: 749, loss: 0.19509181380271912, acc: 0.9531, auc: 0.9809, precision: 0.9091, recall: 1.0\n",
      "2018-12-26T18:24:37.612905, step: 750, loss: 0.14852607250213623, acc: 0.9453, auc: 0.9912, precision: 0.9688, recall: 0.9254\n",
      "2018-12-26T18:24:37.779105, step: 751, loss: 0.17132121324539185, acc: 0.9609, auc: 0.99, precision: 0.9403, recall: 0.9844\n",
      "2018-12-26T18:24:37.933740, step: 752, loss: 0.19200873374938965, acc: 0.9297, auc: 0.9752, precision: 0.9811, recall: 0.8667\n",
      "2018-12-26T18:24:38.091884, step: 753, loss: 0.14051592350006104, acc: 0.9531, auc: 0.9892, precision: 0.9688, recall: 0.9394\n",
      "2018-12-26T18:24:38.254469, step: 754, loss: 0.1484442949295044, acc: 0.9297, auc: 0.9907, precision: 0.9636, recall: 0.8833\n",
      "2018-12-26T18:24:38.410437, step: 755, loss: 0.15286967158317566, acc: 0.9297, auc: 0.9878, precision: 0.9365, recall: 0.9219\n",
      "2018-12-26T18:24:38.566693, step: 756, loss: 0.21011704206466675, acc: 0.9062, auc: 0.9781, precision: 1.0, recall: 0.7966\n",
      "2018-12-26T18:24:38.720568, step: 757, loss: 0.22568976879119873, acc: 0.8828, auc: 0.9674, precision: 0.9344, recall: 0.8382\n",
      "2018-12-26T18:24:38.874523, step: 758, loss: 0.1359734833240509, acc: 0.9219, auc: 0.9929, precision: 0.9808, recall: 0.85\n",
      "2018-12-26T18:24:39.030602, step: 759, loss: 0.13589972257614136, acc: 0.9688, auc: 0.9922, precision: 0.9848, recall: 0.9559\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-26T18:24:39.192812, step: 760, loss: 0.20203842222690582, acc: 0.9219, auc: 0.9804, precision: 0.9032, recall: 0.9333\n",
      "2018-12-26T18:24:39.347583, step: 761, loss: 0.18182706832885742, acc: 0.9297, auc: 0.9849, precision: 0.8983, recall: 0.9464\n",
      "2018-12-26T18:24:39.496212, step: 762, loss: 0.1684894561767578, acc: 0.9375, auc: 0.9897, precision: 0.9091, recall: 0.9677\n",
      "2018-12-26T18:24:39.647878, step: 763, loss: 0.18413296341896057, acc: 0.9375, auc: 0.9829, precision: 0.9143, recall: 0.9697\n",
      "2018-12-26T18:24:39.804101, step: 764, loss: 0.17371854186058044, acc: 0.9375, auc: 0.9884, precision: 0.9298, recall: 0.9298\n",
      "2018-12-26T18:24:39.956991, step: 765, loss: 0.15748950839042664, acc: 0.9297, auc: 0.9922, precision: 1.0, recall: 0.85\n",
      "2018-12-26T18:24:40.114446, step: 766, loss: 0.26522400975227356, acc: 0.9062, auc: 0.9719, precision: 1.0, recall: 0.8095\n",
      "2018-12-26T18:24:40.271013, step: 767, loss: 0.27305445075035095, acc: 0.875, auc: 0.965, precision: 0.9783, recall: 0.75\n",
      "2018-12-26T18:24:40.425394, step: 768, loss: 0.13860346376895905, acc: 0.9219, auc: 0.9909, precision: 0.9636, recall: 0.8689\n",
      "2018-12-26T18:24:40.590272, step: 769, loss: 0.1430162787437439, acc: 0.9453, auc: 0.9931, precision: 0.9846, recall: 0.9143\n",
      "2018-12-26T18:24:40.730879, step: 770, loss: 0.1961582452058792, acc: 0.9219, auc: 0.9873, precision: 0.9118, recall: 0.9394\n",
      "2018-12-26T18:24:40.882473, step: 771, loss: 0.12076709419488907, acc: 0.9609, auc: 0.9976, precision: 0.942, recall: 0.9848\n",
      "2018-12-26T18:24:41.034654, step: 772, loss: 0.14479970932006836, acc: 0.9531, auc: 0.9917, precision: 0.9524, recall: 0.9524\n",
      "2018-12-26T18:24:41.171490, step: 773, loss: 0.09072145819664001, acc: 0.9844, auc: 0.9973, precision: 1.0, recall: 0.9722\n",
      "2018-12-26T18:24:41.320917, step: 774, loss: 0.1235409677028656, acc: 0.9688, auc: 0.9887, precision: 0.9825, recall: 0.9492\n",
      "2018-12-26T18:24:41.459005, step: 775, loss: 0.1658743917942047, acc: 0.9297, auc: 0.9866, precision: 0.9661, recall: 0.8906\n",
      "start training model\n",
      "2018-12-26T18:24:41.642812, step: 776, loss: 0.11480112373828888, acc: 0.9531, auc: 0.9924, precision: 0.9844, recall: 0.9265\n",
      "2018-12-26T18:24:41.803557, step: 777, loss: 0.0897718146443367, acc: 0.9766, auc: 0.9988, precision: 1.0, recall: 0.9483\n",
      "2018-12-26T18:24:41.958534, step: 778, loss: 0.13363821804523468, acc: 0.9453, auc: 0.991, precision: 0.9667, recall: 0.9206\n",
      "2018-12-26T18:24:42.117398, step: 779, loss: 0.12217669188976288, acc: 0.9531, auc: 0.997, precision: 1.0, recall: 0.9178\n",
      "2018-12-26T18:24:42.269346, step: 780, loss: 0.148249551653862, acc: 0.9297, auc: 0.9894, precision: 0.9615, recall: 0.8772\n",
      "2018-12-26T18:24:42.424642, step: 781, loss: 0.15869182348251343, acc: 0.9531, auc: 0.9858, precision: 0.9844, recall: 0.9265\n",
      "2018-12-26T18:24:42.586225, step: 782, loss: 0.10418723523616791, acc: 0.9766, auc: 0.999, precision: 0.9667, recall: 0.9831\n",
      "2018-12-26T18:24:42.741898, step: 783, loss: 0.1839681714773178, acc: 0.9531, auc: 0.984, precision: 0.9565, recall: 0.9565\n",
      "2018-12-26T18:24:42.900592, step: 784, loss: 0.09964628517627716, acc: 0.9609, auc: 0.9968, precision: 0.9718, recall: 0.9583\n",
      "2018-12-26T18:24:43.052677, step: 785, loss: 0.14354120194911957, acc: 0.9297, auc: 0.9901, precision: 0.9559, recall: 0.9155\n",
      "2018-12-26T18:24:43.205127, step: 786, loss: 0.06318476796150208, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:24:43.356432, step: 787, loss: 0.09603850543498993, acc: 0.9844, auc: 0.9985, precision: 1.0, recall: 0.9677\n",
      "2018-12-26T18:24:43.514048, step: 788, loss: 0.13471108675003052, acc: 0.9531, auc: 0.9937, precision: 1.0, recall: 0.9077\n",
      "2018-12-26T18:24:43.666311, step: 789, loss: 0.16883701086044312, acc: 0.9219, auc: 0.9881, precision: 0.9714, recall: 0.8947\n",
      "2018-12-26T18:24:43.823302, step: 790, loss: 0.13771109282970428, acc: 0.9375, auc: 0.9912, precision: 0.9524, recall: 0.9231\n",
      "2018-12-26T18:24:43.973767, step: 791, loss: 0.11892776191234589, acc: 0.9766, auc: 0.9944, precision: 0.9672, recall: 0.9833\n",
      "2018-12-26T18:24:44.127285, step: 792, loss: 0.06491808593273163, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9697\n",
      "2018-12-26T18:24:44.271291, step: 793, loss: 0.1374458372592926, acc: 0.9609, auc: 0.9865, precision: 0.9804, recall: 0.9259\n",
      "2018-12-26T18:24:44.423568, step: 794, loss: 0.1264999359846115, acc: 0.9844, auc: 0.9968, precision: 0.9844, recall: 0.9844\n",
      "2018-12-26T18:24:44.574688, step: 795, loss: 0.21066050231456757, acc: 0.9141, auc: 0.9758, precision: 0.9643, recall: 0.8571\n",
      "2018-12-26T18:24:44.730554, step: 796, loss: 0.1054762527346611, acc: 0.9609, auc: 0.998, precision: 1.0, recall: 0.9206\n",
      "2018-12-26T18:24:44.884324, step: 797, loss: 0.12241177260875702, acc: 0.9688, auc: 0.998, precision: 1.0, recall: 0.9394\n",
      "2018-12-26T18:24:45.035519, step: 798, loss: 0.1068754643201828, acc: 0.9844, auc: 0.9963, precision: 0.9836, recall: 0.9836\n",
      "2018-12-26T18:24:45.193049, step: 799, loss: 0.12404653429985046, acc: 0.9531, auc: 0.9976, precision: 1.0, recall: 0.9048\n",
      "2018-12-26T18:24:45.346722, step: 800, loss: 0.14578349888324738, acc: 0.9219, auc: 0.9931, precision: 0.9833, recall: 0.8676\n",
      "\n",
      "Evaluation:\n",
      "2018-12-26T18:24:51.159039, step: 800, loss: 0.32640857955342845, acc: 0.8515631578947369, auc: 0.936744736842105, precision: 0.8962289473684208, recall: 0.7964631578947369\n",
      "Saved model checkpoint to ../model/textCNN/model/my-model-800\n",
      "\n",
      "2018-12-26T18:24:51.598219, step: 801, loss: 0.132444828748703, acc: 0.9609, auc: 0.9868, precision: 0.9846, recall: 0.9412\n",
      "2018-12-26T18:24:51.760811, step: 802, loss: 0.10136671364307404, acc: 0.9609, auc: 0.9976, precision: 0.9831, recall: 0.9355\n",
      "2018-12-26T18:24:51.924221, step: 803, loss: 0.12788639962673187, acc: 0.9609, auc: 0.9924, precision: 1.0, recall: 0.918\n",
      "2018-12-26T18:24:52.075327, step: 804, loss: 0.08717821538448334, acc: 0.9922, auc: 0.9976, precision: 1.0, recall: 0.9851\n",
      "2018-12-26T18:24:52.224329, step: 805, loss: 0.1113709881901741, acc: 0.9844, auc: 0.9944, precision: 0.9706, recall: 1.0\n",
      "2018-12-26T18:24:52.376395, step: 806, loss: 0.16403628885746002, acc: 0.9453, auc: 0.9853, precision: 0.9412, recall: 0.9552\n",
      "2018-12-26T18:24:52.540062, step: 807, loss: 0.09792719036340714, acc: 0.9766, auc: 0.9978, precision: 0.9701, recall: 0.9848\n",
      "2018-12-26T18:24:52.692007, step: 808, loss: 0.11881545186042786, acc: 0.9297, auc: 0.9963, precision: 0.9811, recall: 0.8667\n",
      "2018-12-26T18:24:52.843654, step: 809, loss: 0.1038200780749321, acc: 0.9688, auc: 0.9975, precision: 1.0, recall: 0.942\n",
      "2018-12-26T18:24:52.995091, step: 810, loss: 0.13773217797279358, acc: 0.9219, auc: 0.9946, precision: 0.9821, recall: 0.8594\n",
      "2018-12-26T18:24:53.149220, step: 811, loss: 0.13525065779685974, acc: 0.9375, auc: 0.9931, precision: 0.9636, recall: 0.8983\n",
      "2018-12-26T18:24:53.306386, step: 812, loss: 0.07936935871839523, acc: 0.9844, auc: 0.9998, precision: 1.0, recall: 0.971\n",
      "2018-12-26T18:24:53.460803, step: 813, loss: 0.17911767959594727, acc: 0.9219, auc: 0.9839, precision: 0.9118, recall: 0.9394\n",
      "2018-12-26T18:24:53.614953, step: 814, loss: 0.07736321538686752, acc: 0.9766, auc: 0.9998, precision: 1.0, recall: 0.9577\n",
      "2018-12-26T18:24:53.770548, step: 815, loss: 0.10189823061227798, acc: 0.9609, auc: 0.9963, precision: 0.9545, recall: 0.9692\n",
      "2018-12-26T18:24:53.923262, step: 816, loss: 0.10118739306926727, acc: 0.9766, auc: 0.9951, precision: 1.0, recall: 0.9474\n",
      "2018-12-26T18:24:54.076788, step: 817, loss: 0.11580336838960648, acc: 0.9609, auc: 0.9945, precision: 0.98, recall: 0.9245\n",
      "2018-12-26T18:24:54.231706, step: 818, loss: 0.116022028028965, acc: 0.9688, auc: 0.9924, precision: 0.9848, recall: 0.9559\n",
      "2018-12-26T18:24:54.386008, step: 819, loss: 0.07870616018772125, acc: 0.9766, auc: 0.9993, precision: 1.0, recall: 0.9508\n",
      "2018-12-26T18:24:54.538692, step: 820, loss: 0.11807918548583984, acc: 0.9609, auc: 0.9917, precision: 0.9692, recall: 0.9545\n",
      "2018-12-26T18:24:54.695367, step: 821, loss: 0.1124330386519432, acc: 0.9688, auc: 0.9958, precision: 0.9672, recall: 0.9672\n",
      "2018-12-26T18:24:54.853126, step: 822, loss: 0.10807771980762482, acc: 0.9844, auc: 0.9985, precision: 1.0, recall: 0.9636\n",
      "2018-12-26T18:24:55.007564, step: 823, loss: 0.13431403040885925, acc: 0.9453, auc: 0.9931, precision: 0.9545, recall: 0.9403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-26T18:24:55.162887, step: 824, loss: 0.1363455206155777, acc: 0.9453, auc: 0.9939, precision: 0.9677, recall: 0.9231\n",
      "2018-12-26T18:24:55.322736, step: 825, loss: 0.09664575755596161, acc: 0.9688, auc: 0.9975, precision: 1.0, recall: 0.9333\n",
      "2018-12-26T18:24:55.480841, step: 826, loss: 0.12753376364707947, acc: 0.9297, auc: 0.9985, precision: 1.0, recall: 0.8676\n",
      "2018-12-26T18:24:55.634207, step: 827, loss: 0.10766054689884186, acc: 0.9453, auc: 0.9988, precision: 1.0, recall: 0.875\n",
      "2018-12-26T18:24:55.792110, step: 828, loss: 0.1017734482884407, acc: 0.9766, auc: 0.9919, precision: 0.9697, recall: 0.9846\n",
      "2018-12-26T18:24:55.944846, step: 829, loss: 0.09143295884132385, acc: 0.9766, auc: 0.9987, precision: 1.0, recall: 0.961\n",
      "2018-12-26T18:24:56.107613, step: 830, loss: 0.09698882699012756, acc: 0.9922, auc: 0.999, precision: 1.0, recall: 0.9821\n",
      "2018-12-26T18:24:56.262069, step: 831, loss: 0.11730793118476868, acc: 0.9609, auc: 0.9936, precision: 0.9701, recall: 0.9559\n",
      "2018-12-26T18:24:56.418046, step: 832, loss: 0.1555704027414322, acc: 0.9375, auc: 0.9887, precision: 0.9206, recall: 0.9508\n",
      "2018-12-26T18:24:56.569090, step: 833, loss: 0.10151506960391998, acc: 0.9844, auc: 0.9992, precision: 0.9808, recall: 0.9808\n",
      "2018-12-26T18:24:56.721916, step: 834, loss: 0.11474582552909851, acc: 0.9609, auc: 0.9939, precision: 0.9733, recall: 0.9605\n",
      "2018-12-26T18:24:56.872915, step: 835, loss: 0.1405874341726303, acc: 0.9375, auc: 0.9927, precision: 1.0, recall: 0.8947\n",
      "2018-12-26T18:24:57.022512, step: 836, loss: 0.1156952753663063, acc: 0.9688, auc: 0.9935, precision: 1.0, recall: 0.9273\n",
      "2018-12-26T18:24:57.179742, step: 837, loss: 0.16797217726707458, acc: 0.9219, auc: 0.9844, precision: 0.9388, recall: 0.8679\n",
      "2018-12-26T18:24:57.332322, step: 838, loss: 0.07207624614238739, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9661\n",
      "2018-12-26T18:24:57.486919, step: 839, loss: 0.06189139187335968, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9692\n",
      "2018-12-26T18:24:57.641826, step: 840, loss: 0.13926929235458374, acc: 0.9766, auc: 0.9854, precision: 0.9808, recall: 0.9623\n",
      "2018-12-26T18:24:57.790653, step: 841, loss: 0.11754056811332703, acc: 0.9766, auc: 0.9961, precision: 1.0, recall: 0.9565\n",
      "2018-12-26T18:24:57.952338, step: 842, loss: 0.11401830613613129, acc: 0.9766, auc: 0.9983, precision: 0.9661, recall: 0.9828\n",
      "2018-12-26T18:24:58.105894, step: 843, loss: 0.14708229899406433, acc: 0.9375, auc: 0.9894, precision: 0.963, recall: 0.8966\n",
      "2018-12-26T18:24:58.259448, step: 844, loss: 0.08398385345935822, acc: 0.9688, auc: 0.9995, precision: 1.0, recall: 0.9403\n",
      "2018-12-26T18:24:58.412618, step: 845, loss: 0.182773619890213, acc: 0.9453, auc: 0.978, precision: 0.9375, recall: 0.9524\n",
      "2018-12-26T18:24:58.566154, step: 846, loss: 0.06977780163288116, acc: 0.9922, auc: 0.9995, precision: 1.0, recall: 0.9836\n",
      "2018-12-26T18:24:58.719917, step: 847, loss: 0.10607393831014633, acc: 0.9688, auc: 0.9961, precision: 0.9844, recall: 0.9545\n",
      "2018-12-26T18:24:58.880470, step: 848, loss: 0.16819573938846588, acc: 0.9453, auc: 0.9791, precision: 0.9811, recall: 0.8966\n",
      "2018-12-26T18:24:59.033335, step: 849, loss: 0.07253406941890717, acc: 0.9766, auc: 1.0, precision: 1.0, recall: 0.9524\n",
      "2018-12-26T18:24:59.191720, step: 850, loss: 0.1203082948923111, acc: 0.9609, auc: 0.9919, precision: 1.0, recall: 0.9153\n",
      "2018-12-26T18:24:59.344646, step: 851, loss: 0.12276671826839447, acc: 0.9531, auc: 0.9929, precision: 0.9833, recall: 0.9219\n",
      "2018-12-26T18:24:59.498962, step: 852, loss: 0.08852221071720123, acc: 0.9766, auc: 0.998, precision: 0.9853, recall: 0.971\n",
      "2018-12-26T18:24:59.651905, step: 853, loss: 0.17430806159973145, acc: 0.9453, auc: 0.9836, precision: 0.9403, recall: 0.9545\n",
      "2018-12-26T18:24:59.815152, step: 854, loss: 0.10674357414245605, acc: 0.9766, auc: 0.9973, precision: 0.9846, recall: 0.9697\n",
      "2018-12-26T18:24:59.972943, step: 855, loss: 0.12044471502304077, acc: 0.9609, auc: 0.9931, precision: 0.9444, recall: 0.9855\n",
      "2018-12-26T18:25:00.132034, step: 856, loss: 0.09571465849876404, acc: 0.9922, auc: 0.9983, precision: 1.0, recall: 0.9857\n",
      "2018-12-26T18:25:00.286318, step: 857, loss: 0.09754349291324615, acc: 0.9688, auc: 0.9965, precision: 1.0, recall: 0.9437\n",
      "2018-12-26T18:25:00.439673, step: 858, loss: 0.117283396422863, acc: 0.9609, auc: 0.9961, precision: 0.9818, recall: 0.931\n",
      "2018-12-26T18:25:00.593587, step: 859, loss: 0.12993557751178741, acc: 0.9375, auc: 0.9919, precision: 0.9483, recall: 0.9167\n",
      "2018-12-26T18:25:00.751147, step: 860, loss: 0.15627041459083557, acc: 0.9219, auc: 0.9888, precision: 0.9655, recall: 0.875\n",
      "2018-12-26T18:25:00.903045, step: 861, loss: 0.14652512967586517, acc: 0.9375, auc: 0.9892, precision: 0.9355, recall: 0.9355\n",
      "2018-12-26T18:25:01.061124, step: 862, loss: 0.1135345846414566, acc: 0.9688, auc: 0.9961, precision: 1.0, recall: 0.9322\n",
      "2018-12-26T18:25:01.221727, step: 863, loss: 0.10484255105257034, acc: 0.9609, auc: 0.9966, precision: 0.9697, recall: 0.9552\n",
      "2018-12-26T18:25:01.375250, step: 864, loss: 0.07847045361995697, acc: 0.9766, auc: 0.9993, precision: 0.9844, recall: 0.9692\n",
      "2018-12-26T18:25:01.528086, step: 865, loss: 0.11815108358860016, acc: 0.9688, auc: 0.9912, precision: 0.9836, recall: 0.9524\n",
      "2018-12-26T18:25:01.686609, step: 866, loss: 0.09185606241226196, acc: 0.9766, auc: 0.9983, precision: 1.0, recall: 0.9565\n",
      "2018-12-26T18:25:01.838718, step: 867, loss: 0.1395147740840912, acc: 0.9531, auc: 0.99, precision: 1.0, recall: 0.9091\n",
      "2018-12-26T18:25:01.997875, step: 868, loss: 0.13523176312446594, acc: 0.9531, auc: 0.9924, precision: 0.9412, recall: 0.9697\n",
      "2018-12-26T18:25:02.150561, step: 869, loss: 0.10623147338628769, acc: 0.9688, auc: 0.9973, precision: 0.9846, recall: 0.9552\n",
      "2018-12-26T18:25:02.307336, step: 870, loss: 0.071129210293293, acc: 0.9766, auc: 0.9988, precision: 0.9692, recall: 0.9844\n",
      "2018-12-26T18:25:02.459593, step: 871, loss: 0.12887710332870483, acc: 0.9531, auc: 0.9947, precision: 0.9286, recall: 0.963\n",
      "2018-12-26T18:25:02.613523, step: 872, loss: 0.13657045364379883, acc: 0.9531, auc: 0.9889, precision: 0.9701, recall: 0.942\n",
      "2018-12-26T18:25:02.766822, step: 873, loss: 0.13244621455669403, acc: 0.9453, auc: 0.9895, precision: 0.9492, recall: 0.9333\n",
      "2018-12-26T18:25:02.917925, step: 874, loss: 0.10587069392204285, acc: 0.9688, auc: 0.9968, precision: 1.0, recall: 0.9437\n",
      "2018-12-26T18:25:03.077929, step: 875, loss: 0.10135847330093384, acc: 0.9609, auc: 0.9973, precision: 1.0, recall: 0.9206\n",
      "2018-12-26T18:25:03.238315, step: 876, loss: 0.0899716392159462, acc: 0.9531, auc: 1.0, precision: 1.0, recall: 0.9118\n",
      "2018-12-26T18:25:03.401509, step: 877, loss: 0.08365193009376526, acc: 0.9766, auc: 0.999, precision: 1.0, recall: 0.9531\n",
      "2018-12-26T18:25:03.555084, step: 878, loss: 0.10274700820446014, acc: 0.9609, auc: 0.9968, precision: 0.9552, recall: 0.9697\n",
      "2018-12-26T18:25:03.708013, step: 879, loss: 0.09475962817668915, acc: 0.9766, auc: 0.9998, precision: 0.96, recall: 1.0\n",
      "2018-12-26T18:25:03.859274, step: 880, loss: 0.09923690557479858, acc: 0.9531, auc: 0.9951, precision: 0.9701, recall: 0.942\n",
      "2018-12-26T18:25:04.017965, step: 881, loss: 0.10251863300800323, acc: 0.9766, auc: 0.9973, precision: 0.9726, recall: 0.9861\n",
      "2018-12-26T18:25:04.171057, step: 882, loss: 0.09295471757650375, acc: 0.9766, auc: 0.999, precision: 0.9667, recall: 0.9831\n",
      "2018-12-26T18:25:04.323999, step: 883, loss: 0.16331297159194946, acc: 0.9609, auc: 0.9869, precision: 0.9344, recall: 0.9828\n",
      "2018-12-26T18:25:04.476587, step: 884, loss: 0.1366533637046814, acc: 0.9297, auc: 0.9906, precision: 0.9434, recall: 0.8929\n",
      "2018-12-26T18:25:04.629013, step: 885, loss: 0.11400140821933746, acc: 0.9531, auc: 0.9961, precision: 0.9828, recall: 0.9194\n",
      "2018-12-26T18:25:04.777547, step: 886, loss: 0.11160190403461456, acc: 0.9531, auc: 0.999, precision: 1.0, recall: 0.8824\n",
      "2018-12-26T18:25:04.949767, step: 887, loss: 0.12268573045730591, acc: 0.9375, auc: 0.9948, precision: 1.0, recall: 0.8621\n",
      "2018-12-26T18:25:05.148306, step: 888, loss: 0.22313083708286285, acc: 0.8906, auc: 0.9773, precision: 0.9683, recall: 0.8356\n",
      "2018-12-26T18:25:05.302209, step: 889, loss: 0.1339086890220642, acc: 0.9766, auc: 0.9872, precision: 1.0, recall: 0.9474\n",
      "2018-12-26T18:25:05.462634, step: 890, loss: 0.062331631779670715, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9706\n",
      "2018-12-26T18:25:05.621527, step: 891, loss: 0.11970123648643494, acc: 0.9766, auc: 0.9973, precision: 0.973, recall: 0.9863\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-26T18:25:05.780188, step: 892, loss: 0.11714136600494385, acc: 0.9688, auc: 0.9975, precision: 0.971, recall: 0.971\n",
      "2018-12-26T18:25:05.929311, step: 893, loss: 0.08582145720720291, acc: 0.9688, auc: 0.9988, precision: 0.9844, recall: 0.9545\n",
      "2018-12-26T18:25:06.087595, step: 894, loss: 0.12317420542240143, acc: 0.9531, auc: 0.9937, precision: 0.9677, recall: 0.9375\n",
      "2018-12-26T18:25:06.245185, step: 895, loss: 0.11257332563400269, acc: 0.9297, auc: 0.996, precision: 1.0, recall: 0.875\n",
      "2018-12-26T18:25:06.399708, step: 896, loss: 0.10323318839073181, acc: 0.9609, auc: 0.9973, precision: 0.95, recall: 0.9661\n",
      "2018-12-26T18:25:06.556623, step: 897, loss: 0.09234392642974854, acc: 0.9609, auc: 0.9973, precision: 0.9677, recall: 0.9524\n",
      "2018-12-26T18:25:06.690711, step: 898, loss: 0.09974022209644318, acc: 0.9531, auc: 0.9931, precision: 1.0, recall: 0.9016\n",
      "2018-12-26T18:25:06.859487, step: 899, loss: 0.1265273094177246, acc: 0.9609, auc: 0.9921, precision: 0.9818, recall: 0.931\n",
      "2018-12-26T18:25:07.010551, step: 900, loss: 0.08994649350643158, acc: 0.9766, auc: 0.9968, precision: 0.9825, recall: 0.9655\n",
      "\n",
      "Evaluation:\n",
      "2018-12-26T18:25:12.822196, step: 900, loss: 0.33420136021940333, acc: 0.8540368421052632, auc: 0.9373894736842104, precision: 0.9081473684210526, recall: 0.7915131578947371\n",
      "Saved model checkpoint to ../model/textCNN/model/my-model-900\n",
      "\n",
      "2018-12-26T18:25:13.249232, step: 901, loss: 0.08720821142196655, acc: 0.9844, auc: 0.999, precision: 1.0, recall: 0.9643\n",
      "2018-12-26T18:25:13.407510, step: 902, loss: 0.09960085898637772, acc: 0.9531, auc: 0.9968, precision: 0.9821, recall: 0.9167\n",
      "2018-12-26T18:25:13.566746, step: 903, loss: 0.0950305163860321, acc: 0.9844, auc: 0.997, precision: 1.0, recall: 0.9655\n",
      "2018-12-26T18:25:13.722900, step: 904, loss: 0.13377654552459717, acc: 0.9531, auc: 0.9897, precision: 1.0, recall: 0.9032\n",
      "2018-12-26T18:25:13.874164, step: 905, loss: 0.061888787895441055, acc: 0.9922, auc: 0.9998, precision: 1.0, recall: 0.9855\n",
      "2018-12-26T18:25:14.033822, step: 906, loss: 0.08767140656709671, acc: 0.9688, auc: 0.9993, precision: 1.0, recall: 0.9412\n",
      "2018-12-26T18:25:14.184867, step: 907, loss: 0.14100845158100128, acc: 0.9531, auc: 0.9895, precision: 0.9375, recall: 0.9677\n",
      "2018-12-26T18:25:14.338592, step: 908, loss: 0.0698455423116684, acc: 0.9922, auc: 1.0, precision: 0.9853, recall: 1.0\n",
      "2018-12-26T18:25:14.492884, step: 909, loss: 0.12180749326944351, acc: 0.9688, auc: 0.9961, precision: 0.9672, recall: 0.9672\n",
      "2018-12-26T18:25:14.651584, step: 910, loss: 0.13280119001865387, acc: 0.9453, auc: 0.9899, precision: 0.9483, recall: 0.9322\n",
      "2018-12-26T18:25:14.808336, step: 911, loss: 0.12864422798156738, acc: 0.9453, auc: 0.9943, precision: 0.9851, recall: 0.9167\n",
      "2018-12-26T18:25:14.965003, step: 912, loss: 0.14356282353401184, acc: 0.9609, auc: 0.9885, precision: 0.9706, recall: 0.9565\n",
      "2018-12-26T18:25:15.129709, step: 913, loss: 0.11632727086544037, acc: 0.9609, auc: 0.9929, precision: 0.9851, recall: 0.9429\n",
      "2018-12-26T18:25:15.280032, step: 914, loss: 0.08335727453231812, acc: 0.9844, auc: 0.9998, precision: 0.9839, recall: 0.9839\n",
      "2018-12-26T18:25:15.430816, step: 915, loss: 0.06913387030363083, acc: 0.9688, auc: 1.0, precision: 1.0, recall: 0.9437\n",
      "2018-12-26T18:25:15.582230, step: 916, loss: 0.10525263100862503, acc: 0.9609, auc: 0.9973, precision: 0.9815, recall: 0.9298\n",
      "2018-12-26T18:25:15.734903, step: 917, loss: 0.07712598145008087, acc: 0.9844, auc: 0.9995, precision: 0.9857, recall: 0.9857\n",
      "2018-12-26T18:25:15.889403, step: 918, loss: 0.1741504818201065, acc: 0.9453, auc: 0.9831, precision: 0.9825, recall: 0.9032\n",
      "2018-12-26T18:25:16.040067, step: 919, loss: 0.17271500825881958, acc: 0.9297, auc: 0.9834, precision: 0.9385, recall: 0.9242\n",
      "2018-12-26T18:25:16.189893, step: 920, loss: 0.1636442393064499, acc: 0.9609, auc: 0.9843, precision: 0.9825, recall: 0.9333\n",
      "2018-12-26T18:25:16.343607, step: 921, loss: 0.09334758669137955, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:25:16.497845, step: 922, loss: 0.1425706446170807, acc: 0.9531, auc: 0.9892, precision: 0.9483, recall: 0.9483\n",
      "2018-12-26T18:25:16.653041, step: 923, loss: 0.11677411198616028, acc: 0.9531, auc: 0.9963, precision: 0.9836, recall: 0.9231\n",
      "2018-12-26T18:25:16.826013, step: 924, loss: 0.13147243857383728, acc: 0.9531, auc: 0.9899, precision: 0.9333, recall: 0.9655\n",
      "2018-12-26T18:25:16.980092, step: 925, loss: 0.14652174711227417, acc: 0.9453, auc: 0.9841, precision: 0.9683, recall: 0.9242\n",
      "2018-12-26T18:25:17.132754, step: 926, loss: 0.0876222550868988, acc: 0.9609, auc: 0.9973, precision: 0.9649, recall: 0.9483\n",
      "2018-12-26T18:25:17.286007, step: 927, loss: 0.08726769685745239, acc: 0.9766, auc: 0.998, precision: 0.9818, recall: 0.9643\n",
      "2018-12-26T18:25:17.435260, step: 928, loss: 0.07843105494976044, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9821\n",
      "2018-12-26T18:25:17.594469, step: 929, loss: 0.09847898036241531, acc: 0.9688, auc: 0.9985, precision: 0.9592, recall: 0.9592\n",
      "2018-12-26T18:25:17.748762, step: 930, loss: 0.10600617527961731, acc: 0.9453, auc: 0.9983, precision: 1.0, recall: 0.9028\n",
      "start training model\n",
      "2018-12-26T18:25:17.934823, step: 931, loss: 0.08326779305934906, acc: 0.9531, auc: 0.9983, precision: 1.0, recall: 0.9143\n",
      "2018-12-26T18:25:18.093806, step: 932, loss: 0.07405082136392593, acc: 0.9688, auc: 1.0, precision: 1.0, recall: 0.9298\n",
      "2018-12-26T18:25:18.230890, step: 933, loss: 0.06015610694885254, acc: 0.9922, auc: 0.9998, precision: 1.0, recall: 0.9859\n",
      "2018-12-26T18:25:18.383023, step: 934, loss: 0.0914979949593544, acc: 0.9688, auc: 0.9985, precision: 0.9661, recall: 0.9661\n",
      "2018-12-26T18:25:18.537306, step: 935, loss: 0.16052410006523132, acc: 0.9609, auc: 0.9783, precision: 0.9524, recall: 0.9677\n",
      "2018-12-26T18:25:18.693846, step: 936, loss: 0.049765683710575104, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9841\n",
      "2018-12-26T18:25:18.844938, step: 937, loss: 0.07832346856594086, acc: 0.9922, auc: 0.9939, precision: 1.0, recall: 0.9846\n",
      "2018-12-26T18:25:18.997417, step: 938, loss: 0.058671288192272186, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9848\n",
      "2018-12-26T18:25:19.156788, step: 939, loss: 0.05409976840019226, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9831\n",
      "2018-12-26T18:25:19.313329, step: 940, loss: 0.06327489018440247, acc: 0.9922, auc: 0.9995, precision: 1.0, recall: 0.9859\n",
      "2018-12-26T18:25:19.463285, step: 941, loss: 0.08489373326301575, acc: 0.9609, auc: 0.998, precision: 0.9815, recall: 0.9298\n",
      "2018-12-26T18:25:19.618600, step: 942, loss: 0.0802743136882782, acc: 0.9766, auc: 0.9983, precision: 0.9833, recall: 0.9672\n",
      "2018-12-26T18:25:19.774487, step: 943, loss: 0.07429567724466324, acc: 0.9688, auc: 0.9993, precision: 0.9859, recall: 0.9589\n",
      "2018-12-26T18:25:19.926762, step: 944, loss: 0.053052209317684174, acc: 0.9922, auc: 1.0, precision: 0.9846, recall: 1.0\n",
      "2018-12-26T18:25:20.081322, step: 945, loss: 0.08895917236804962, acc: 0.9688, auc: 0.9983, precision: 0.9701, recall: 0.9701\n",
      "2018-12-26T18:25:20.239074, step: 946, loss: 0.05534624680876732, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9722\n",
      "2018-12-26T18:25:20.400411, step: 947, loss: 0.07370240986347198, acc: 0.9922, auc: 0.9993, precision: 0.9844, recall: 1.0\n",
      "2018-12-26T18:25:20.553422, step: 948, loss: 0.14002814888954163, acc: 0.9688, auc: 0.9792, precision: 1.0, recall: 0.9365\n",
      "2018-12-26T18:25:20.703848, step: 949, loss: 0.07747496664524078, acc: 0.9609, auc: 0.9982, precision: 1.0, recall: 0.9342\n",
      "2018-12-26T18:25:20.873042, step: 950, loss: 0.06430204957723618, acc: 0.9844, auc: 0.9998, precision: 0.9831, recall: 0.9831\n",
      "2018-12-26T18:25:21.028127, step: 951, loss: 0.09105197340250015, acc: 0.9609, auc: 0.9988, precision: 1.0, recall: 0.9242\n",
      "2018-12-26T18:25:21.184573, step: 952, loss: 0.07976188510656357, acc: 0.9766, auc: 0.999, precision: 1.0, recall: 0.9538\n",
      "2018-12-26T18:25:21.337119, step: 953, loss: 0.05743866786360741, acc: 0.9922, auc: 0.9998, precision: 1.0, recall: 0.9833\n",
      "2018-12-26T18:25:21.487022, step: 954, loss: 0.0752042680978775, acc: 0.9766, auc: 0.9978, precision: 0.9833, recall: 0.9672\n",
      "2018-12-26T18:25:21.639340, step: 955, loss: 0.07024800777435303, acc: 0.9922, auc: 0.9993, precision: 0.9848, recall: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-26T18:25:21.797718, step: 956, loss: 0.09875184297561646, acc: 0.9688, auc: 0.9976, precision: 1.0, recall: 0.9403\n",
      "2018-12-26T18:25:21.962444, step: 957, loss: 0.07550272345542908, acc: 0.9688, auc: 0.9992, precision: 0.9867, recall: 0.961\n",
      "2018-12-26T18:25:22.112900, step: 958, loss: 0.07963813841342926, acc: 0.9688, auc: 0.998, precision: 1.0, recall: 0.9365\n",
      "2018-12-26T18:25:22.262644, step: 959, loss: 0.07380829006433487, acc: 0.9844, auc: 0.9988, precision: 1.0, recall: 0.9692\n",
      "2018-12-26T18:25:22.415556, step: 960, loss: 0.0781538188457489, acc: 0.9844, auc: 0.9985, precision: 1.0, recall: 0.9701\n",
      "2018-12-26T18:25:22.573651, step: 961, loss: 0.05515670403838158, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9649\n",
      "2018-12-26T18:25:22.732849, step: 962, loss: 0.06645171344280243, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:25:22.886587, step: 963, loss: 0.06797751039266586, acc: 0.9766, auc: 0.998, precision: 1.0, recall: 0.9565\n",
      "2018-12-26T18:25:23.040126, step: 964, loss: 0.09816768765449524, acc: 0.9531, auc: 0.9961, precision: 0.9688, recall: 0.9394\n",
      "2018-12-26T18:25:23.191498, step: 965, loss: 0.1183396577835083, acc: 0.9766, auc: 0.9919, precision: 0.9825, recall: 0.9655\n",
      "2018-12-26T18:25:23.355353, step: 966, loss: 0.08828801661729813, acc: 0.9609, auc: 0.999, precision: 1.0, recall: 0.9231\n",
      "2018-12-26T18:25:23.510355, step: 967, loss: 0.06662073731422424, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:25:23.664204, step: 968, loss: 0.06321465969085693, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9677\n",
      "2018-12-26T18:25:23.817608, step: 969, loss: 0.12136872857809067, acc: 0.9766, auc: 0.9883, precision: 1.0, recall: 0.9412\n",
      "2018-12-26T18:25:23.970084, step: 970, loss: 0.07093250751495361, acc: 0.9844, auc: 0.9988, precision: 0.9851, recall: 0.9851\n",
      "2018-12-26T18:25:24.124612, step: 971, loss: 0.06065690889954567, acc: 0.9922, auc: 0.9993, precision: 1.0, recall: 0.9836\n",
      "2018-12-26T18:25:24.275824, step: 972, loss: 0.058972202241420746, acc: 0.9844, auc: 0.9998, precision: 1.0, recall: 0.9726\n",
      "2018-12-26T18:25:24.428948, step: 973, loss: 0.09418046474456787, acc: 0.9766, auc: 0.9988, precision: 1.0, recall: 0.9565\n",
      "2018-12-26T18:25:24.579585, step: 974, loss: 0.09167785942554474, acc: 0.9609, auc: 0.9978, precision: 0.9583, recall: 0.9718\n",
      "2018-12-26T18:25:24.730090, step: 975, loss: 0.08032950758934021, acc: 0.9844, auc: 0.9998, precision: 0.9831, recall: 0.9831\n",
      "2018-12-26T18:25:24.884026, step: 976, loss: 0.07530328631401062, acc: 0.9766, auc: 0.9993, precision: 1.0, recall: 0.9538\n",
      "2018-12-26T18:25:25.037602, step: 977, loss: 0.06264925003051758, acc: 0.9766, auc: 0.999, precision: 0.9836, recall: 0.9677\n",
      "2018-12-26T18:25:25.192813, step: 978, loss: 0.08421951532363892, acc: 0.9453, auc: 0.999, precision: 1.0, recall: 0.9054\n",
      "2018-12-26T18:25:25.346097, step: 979, loss: 0.06473194062709808, acc: 0.9922, auc: 0.9998, precision: 0.9841, recall: 1.0\n",
      "2018-12-26T18:25:25.505806, step: 980, loss: 0.04757137596607208, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:25:25.659721, step: 981, loss: 0.10132673382759094, acc: 0.9297, auc: 0.9978, precision: 0.9825, recall: 0.875\n",
      "2018-12-26T18:25:25.810537, step: 982, loss: 0.09843771159648895, acc: 0.9531, auc: 0.9948, precision: 0.971, recall: 0.9437\n",
      "2018-12-26T18:25:25.964076, step: 983, loss: 0.08823408931493759, acc: 0.9766, auc: 0.9946, precision: 0.9655, recall: 0.9825\n",
      "2018-12-26T18:25:26.113287, step: 984, loss: 0.0672217458486557, acc: 0.9766, auc: 0.9993, precision: 0.9821, recall: 0.9649\n",
      "2018-12-26T18:25:26.264045, step: 985, loss: 0.06234988942742348, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9844\n",
      "2018-12-26T18:25:26.417464, step: 986, loss: 0.09600920975208282, acc: 0.9609, auc: 0.9971, precision: 0.9846, recall: 0.9412\n",
      "2018-12-26T18:25:26.569015, step: 987, loss: 0.05346977710723877, acc: 0.9844, auc: 0.9995, precision: 0.9833, recall: 0.9833\n",
      "2018-12-26T18:25:26.720887, step: 988, loss: 0.06661707162857056, acc: 0.9766, auc: 1.0, precision: 1.0, recall: 0.96\n",
      "2018-12-26T18:25:26.874036, step: 989, loss: 0.05437244847416878, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:25:27.031696, step: 990, loss: 0.07212085276842117, acc: 0.9766, auc: 0.9985, precision: 0.973, recall: 0.9863\n",
      "2018-12-26T18:25:27.186125, step: 991, loss: 0.125230610370636, acc: 0.9766, auc: 0.9885, precision: 0.9853, recall: 0.971\n",
      "2018-12-26T18:25:27.339419, step: 992, loss: 0.06998427212238312, acc: 0.9922, auc: 0.9993, precision: 1.0, recall: 0.9851\n",
      "2018-12-26T18:25:27.490815, step: 993, loss: 0.07313033938407898, acc: 0.9766, auc: 0.9995, precision: 1.0, recall: 0.9444\n",
      "2018-12-26T18:25:27.645148, step: 994, loss: 0.0642833560705185, acc: 0.9922, auc: 0.9985, precision: 1.0, recall: 0.9844\n",
      "2018-12-26T18:25:27.831239, step: 995, loss: 0.05349638685584068, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9701\n",
      "2018-12-26T18:25:27.990726, step: 996, loss: 0.05185664817690849, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9841\n",
      "2018-12-26T18:25:28.140972, step: 997, loss: 0.06035815551877022, acc: 0.9766, auc: 0.9985, precision: 1.0, recall: 0.9516\n",
      "2018-12-26T18:25:28.296942, step: 998, loss: 0.055568210780620575, acc: 0.9844, auc: 0.9993, precision: 1.0, recall: 0.9697\n",
      "2018-12-26T18:25:28.451571, step: 999, loss: 0.063730388879776, acc: 0.9922, auc: 0.9995, precision: 1.0, recall: 0.9859\n",
      "2018-12-26T18:25:28.607151, step: 1000, loss: 0.09504489600658417, acc: 0.9766, auc: 0.9937, precision: 0.9839, recall: 0.9683\n",
      "\n",
      "Evaluation:\n",
      "2018-12-26T18:25:34.479901, step: 1000, loss: 0.34308731438297974, acc: 0.8647236842105263, auc: 0.9366578947368422, precision: 0.8679289473684212, recall: 0.864892105263158\n",
      "Saved model checkpoint to ../model/textCNN/model/my-model-1000\n",
      "\n",
      "2018-12-26T18:25:34.905524, step: 1001, loss: 0.09721919894218445, acc: 0.9766, auc: 0.9983, precision: 0.9833, recall: 0.9672\n",
      "2018-12-26T18:25:35.059332, step: 1002, loss: 0.058721475303173065, acc: 0.9922, auc: 0.9997, precision: 1.0, recall: 0.9808\n",
      "2018-12-26T18:25:35.212207, step: 1003, loss: 0.0797208696603775, acc: 0.9844, auc: 0.998, precision: 0.9839, recall: 0.9839\n",
      "2018-12-26T18:25:35.361653, step: 1004, loss: 0.108974888920784, acc: 0.9375, auc: 0.9944, precision: 0.9818, recall: 0.8852\n",
      "2018-12-26T18:25:35.515610, step: 1005, loss: 0.07840975373983383, acc: 0.9766, auc: 0.999, precision: 1.0, recall: 0.9571\n",
      "2018-12-26T18:25:35.671884, step: 1006, loss: 0.053229447454214096, acc: 0.9688, auc: 1.0, precision: 1.0, recall: 0.942\n",
      "2018-12-26T18:25:35.823372, step: 1007, loss: 0.08236350119113922, acc: 0.9688, auc: 0.9982, precision: 0.9792, recall: 0.94\n",
      "2018-12-26T18:25:35.979767, step: 1008, loss: 0.1148650124669075, acc: 0.9531, auc: 0.9958, precision: 1.0, recall: 0.9155\n",
      "2018-12-26T18:25:36.134890, step: 1009, loss: 0.06367164850234985, acc: 0.9766, auc: 0.9998, precision: 1.0, recall: 0.9552\n",
      "2018-12-26T18:25:36.287669, step: 1010, loss: 0.0636858269572258, acc: 0.9766, auc: 0.9993, precision: 1.0, recall: 0.9508\n",
      "2018-12-26T18:25:36.440740, step: 1011, loss: 0.058368273079395294, acc: 0.9922, auc: 1.0, precision: 0.9848, recall: 1.0\n",
      "2018-12-26T18:25:36.596253, step: 1012, loss: 0.0656796544790268, acc: 0.9766, auc: 0.9995, precision: 0.9722, recall: 0.9859\n",
      "2018-12-26T18:25:36.754793, step: 1013, loss: 0.09063383936882019, acc: 0.9844, auc: 0.998, precision: 0.9821, recall: 0.9821\n",
      "2018-12-26T18:25:36.906928, step: 1014, loss: 0.0872606411576271, acc: 0.9688, auc: 0.9988, precision: 0.9846, recall: 0.9552\n",
      "2018-12-26T18:25:37.093525, step: 1015, loss: 0.043418824672698975, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:25:37.248882, step: 1016, loss: 0.08613665401935577, acc: 0.9766, auc: 0.9985, precision: 0.9841, recall: 0.9688\n",
      "2018-12-26T18:25:37.405103, step: 1017, loss: 0.06396521627902985, acc: 0.9844, auc: 0.9993, precision: 0.9836, recall: 0.9836\n",
      "2018-12-26T18:25:37.557749, step: 1018, loss: 0.08109143376350403, acc: 0.9766, auc: 0.9995, precision: 1.0, recall: 0.9559\n",
      "2018-12-26T18:25:37.724355, step: 1019, loss: 0.07342872023582458, acc: 0.9766, auc: 0.9985, precision: 1.0, recall: 0.9545\n",
      "2018-12-26T18:25:37.878273, step: 1020, loss: 0.072950579226017, acc: 0.9766, auc: 0.9993, precision: 1.0, recall: 0.9492\n",
      "2018-12-26T18:25:38.037410, step: 1021, loss: 0.08042307198047638, acc: 0.9766, auc: 0.9968, precision: 0.9855, recall: 0.9714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-26T18:25:38.195610, step: 1022, loss: 0.08261704444885254, acc: 0.9531, auc: 0.9978, precision: 0.9833, recall: 0.9219\n",
      "2018-12-26T18:25:38.346379, step: 1023, loss: 0.1126318871974945, acc: 0.9688, auc: 0.9943, precision: 0.9821, recall: 0.9483\n",
      "2018-12-26T18:25:38.499130, step: 1024, loss: 0.07269015908241272, acc: 0.9844, auc: 0.9988, precision: 0.9828, recall: 0.9828\n",
      "2018-12-26T18:25:38.659227, step: 1025, loss: 0.10606968402862549, acc: 0.9766, auc: 0.9961, precision: 0.9516, recall: 1.0\n",
      "2018-12-26T18:25:38.813877, step: 1026, loss: 0.12150903791189194, acc: 0.9609, auc: 0.9885, precision: 0.9831, recall: 0.9355\n",
      "2018-12-26T18:25:38.967943, step: 1027, loss: 0.0672343522310257, acc: 0.9766, auc: 0.9985, precision: 1.0, recall: 0.9565\n",
      "2018-12-26T18:25:39.122335, step: 1028, loss: 0.06558346748352051, acc: 0.9844, auc: 0.9995, precision: 0.9846, recall: 0.9846\n",
      "2018-12-26T18:25:39.276645, step: 1029, loss: 0.0631273090839386, acc: 0.9922, auc: 0.9995, precision: 1.0, recall: 0.9828\n",
      "2018-12-26T18:25:39.430995, step: 1030, loss: 0.08396168053150177, acc: 0.9688, auc: 0.9988, precision: 1.0, recall: 0.9355\n",
      "2018-12-26T18:25:39.588328, step: 1031, loss: 0.08153295516967773, acc: 0.9531, auc: 0.9985, precision: 0.9828, recall: 0.9194\n",
      "2018-12-26T18:25:39.742374, step: 1032, loss: 0.05681704729795456, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9851\n",
      "2018-12-26T18:25:39.893289, step: 1033, loss: 0.11047337204217911, acc: 0.9531, auc: 0.996, precision: 0.9583, recall: 0.9583\n",
      "2018-12-26T18:25:40.040541, step: 1034, loss: 0.0814436823129654, acc: 0.9766, auc: 0.999, precision: 0.9667, recall: 0.9831\n",
      "2018-12-26T18:25:40.192350, step: 1035, loss: 0.1401902437210083, acc: 0.9531, auc: 0.9893, precision: 0.9254, recall: 0.9841\n",
      "2018-12-26T18:25:40.343327, step: 1036, loss: 0.05921521410346031, acc: 0.9922, auc: 0.9998, precision: 0.9848, recall: 1.0\n",
      "2018-12-26T18:25:40.503205, step: 1037, loss: 0.08300236612558365, acc: 0.9688, auc: 0.9983, precision: 0.9851, recall: 0.9565\n",
      "2018-12-26T18:25:40.657672, step: 1038, loss: 0.04530797153711319, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:25:40.819937, step: 1039, loss: 0.07635056227445602, acc: 0.9609, auc: 0.9993, precision: 1.0, recall: 0.9265\n",
      "2018-12-26T18:25:40.975164, step: 1040, loss: 0.1019618809223175, acc: 0.9531, auc: 0.9954, precision: 0.9683, recall: 0.9385\n",
      "2018-12-26T18:25:41.128820, step: 1041, loss: 0.07231717556715012, acc: 0.9609, auc: 0.9983, precision: 0.9692, recall: 0.9545\n",
      "2018-12-26T18:25:41.283695, step: 1042, loss: 0.07154382020235062, acc: 0.9766, auc: 0.998, precision: 0.9846, recall: 0.9697\n",
      "2018-12-26T18:25:41.451340, step: 1043, loss: 0.08941155672073364, acc: 0.9766, auc: 0.9955, precision: 0.9574, recall: 0.9783\n",
      "2018-12-26T18:25:41.605220, step: 1044, loss: 0.08460098505020142, acc: 0.9766, auc: 0.999, precision: 0.9649, recall: 0.9821\n",
      "2018-12-26T18:25:41.763620, step: 1045, loss: 0.09314194321632385, acc: 0.9766, auc: 0.9966, precision: 0.9831, recall: 0.9667\n",
      "2018-12-26T18:25:41.921433, step: 1046, loss: 0.061261408030986786, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9833\n",
      "2018-12-26T18:25:42.073869, step: 1047, loss: 0.09076467901468277, acc: 0.9609, auc: 0.9973, precision: 0.9839, recall: 0.9385\n",
      "2018-12-26T18:25:42.228478, step: 1048, loss: 0.07224538177251816, acc: 0.9688, auc: 0.9983, precision: 0.9839, recall: 0.9531\n",
      "2018-12-26T18:25:42.384837, step: 1049, loss: 0.09419611096382141, acc: 0.9688, auc: 0.9985, precision: 1.0, recall: 0.9355\n",
      "2018-12-26T18:25:42.536365, step: 1050, loss: 0.05645410344004631, acc: 0.9844, auc: 0.999, precision: 0.9821, recall: 0.9821\n",
      "2018-12-26T18:25:42.695449, step: 1051, loss: 0.09263750910758972, acc: 0.9688, auc: 0.9968, precision: 0.9677, recall: 0.9677\n",
      "2018-12-26T18:25:42.844462, step: 1052, loss: 0.06390576809644699, acc: 0.9766, auc: 0.9995, precision: 1.0, recall: 0.9545\n",
      "2018-12-26T18:25:42.998294, step: 1053, loss: 0.07818938791751862, acc: 0.9922, auc: 0.9993, precision: 0.9836, recall: 1.0\n",
      "2018-12-26T18:25:43.152711, step: 1054, loss: 0.09422215819358826, acc: 0.9531, auc: 0.9963, precision: 0.9706, recall: 0.9429\n",
      "2018-12-26T18:25:43.316559, step: 1055, loss: 0.10115077346563339, acc: 0.9531, auc: 0.9951, precision: 0.9545, recall: 0.9545\n",
      "2018-12-26T18:25:43.471160, step: 1056, loss: 0.09039121866226196, acc: 0.9766, auc: 1.0, precision: 0.9583, recall: 1.0\n",
      "2018-12-26T18:25:43.621334, step: 1057, loss: 0.05321333557367325, acc: 0.9922, auc: 0.9993, precision: 1.0, recall: 0.9848\n",
      "2018-12-26T18:25:43.772482, step: 1058, loss: 0.06284011155366898, acc: 0.9844, auc: 0.9985, precision: 1.0, recall: 0.9718\n",
      "2018-12-26T18:25:43.928464, step: 1059, loss: 0.07898520678281784, acc: 0.9688, auc: 0.9983, precision: 0.9844, recall: 0.9545\n",
      "2018-12-26T18:25:44.090397, step: 1060, loss: 0.08595488220453262, acc: 0.9609, auc: 0.9966, precision: 0.9821, recall: 0.9322\n",
      "2018-12-26T18:25:44.256499, step: 1061, loss: 0.07046741992235184, acc: 0.9844, auc: 0.9993, precision: 1.0, recall: 0.9722\n",
      "2018-12-26T18:25:44.417797, step: 1062, loss: 0.07177220284938812, acc: 0.9609, auc: 0.9975, precision: 0.9818, recall: 0.931\n",
      "2018-12-26T18:25:44.572911, step: 1063, loss: 0.10564223676919937, acc: 0.9688, auc: 0.9939, precision: 0.9828, recall: 0.95\n",
      "2018-12-26T18:25:44.724274, step: 1064, loss: 0.1063833087682724, acc: 0.9844, auc: 0.9872, precision: 1.0, recall: 0.9661\n",
      "2018-12-26T18:25:44.879321, step: 1065, loss: 0.09392702579498291, acc: 0.9766, auc: 0.9965, precision: 0.9655, recall: 0.9825\n",
      "2018-12-26T18:25:45.026990, step: 1066, loss: 0.05513222515583038, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:25:45.177364, step: 1067, loss: 0.07383792102336884, acc: 0.9922, auc: 0.9995, precision: 0.9828, recall: 1.0\n",
      "2018-12-26T18:25:45.329352, step: 1068, loss: 0.095780149102211, acc: 0.9844, auc: 0.9919, precision: 0.9855, recall: 0.9855\n",
      "2018-12-26T18:25:45.482547, step: 1069, loss: 0.08504500985145569, acc: 0.9531, auc: 0.9988, precision: 0.9844, recall: 0.9265\n",
      "2018-12-26T18:25:45.641621, step: 1070, loss: 0.07519002258777618, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.971\n",
      "2018-12-26T18:25:45.795522, step: 1071, loss: 0.07230408489704132, acc: 0.9688, auc: 0.999, precision: 1.0, recall: 0.9365\n",
      "2018-12-26T18:25:45.952072, step: 1072, loss: 0.11536557972431183, acc: 0.9531, auc: 0.9933, precision: 0.9455, recall: 0.9455\n",
      "2018-12-26T18:25:46.106352, step: 1073, loss: 0.049167558550834656, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:25:46.257320, step: 1074, loss: 0.10777171701192856, acc: 0.9688, auc: 0.9922, precision: 0.9538, recall: 0.9841\n",
      "2018-12-26T18:25:46.414573, step: 1075, loss: 0.05899600312113762, acc: 0.9844, auc: 0.999, precision: 1.0, recall: 0.9718\n",
      "2018-12-26T18:25:46.573063, step: 1076, loss: 0.09934499859809875, acc: 0.9609, auc: 0.9929, precision: 0.9848, recall: 0.942\n",
      "2018-12-26T18:25:46.726050, step: 1077, loss: 0.09829312562942505, acc: 0.9688, auc: 0.9936, precision: 1.0, recall: 0.9394\n",
      "2018-12-26T18:25:46.880441, step: 1078, loss: 0.06746269762516022, acc: 0.9766, auc: 0.9985, precision: 0.9833, recall: 0.9672\n",
      "2018-12-26T18:25:47.057937, step: 1079, loss: 0.07039539515972137, acc: 0.9922, auc: 0.9993, precision: 1.0, recall: 0.9839\n",
      "2018-12-26T18:25:47.219666, step: 1080, loss: 0.08352992683649063, acc: 0.9922, auc: 0.9934, precision: 1.0, recall: 0.9833\n",
      "2018-12-26T18:25:47.376809, step: 1081, loss: 0.10017256438732147, acc: 0.9844, auc: 0.9985, precision: 1.0, recall: 0.963\n",
      "2018-12-26T18:25:47.530094, step: 1082, loss: 0.07698234915733337, acc: 0.9844, auc: 0.9995, precision: 0.9825, recall: 0.9825\n",
      "2018-12-26T18:25:47.683905, step: 1083, loss: 0.05528506636619568, acc: 0.9766, auc: 1.0, precision: 1.0, recall: 0.9516\n",
      "2018-12-26T18:25:47.837663, step: 1084, loss: 0.07487338036298752, acc: 0.9688, auc: 0.9995, precision: 1.0, recall: 0.9394\n",
      "2018-12-26T18:25:47.995146, step: 1085, loss: 0.0980590283870697, acc: 0.9609, auc: 0.9968, precision: 1.0, recall: 0.9219\n",
      "start training model\n",
      "2018-12-26T18:25:48.176700, step: 1086, loss: 0.06506384909152985, acc: 0.9688, auc: 1.0, precision: 1.0, recall: 0.9375\n",
      "2018-12-26T18:25:48.336398, step: 1087, loss: 0.0788370817899704, acc: 0.9609, auc: 0.9998, precision: 1.0, recall: 0.9296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-26T18:25:48.491839, step: 1088, loss: 0.053668491542339325, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:25:48.646511, step: 1089, loss: 0.07297097146511078, acc: 0.9766, auc: 0.9995, precision: 0.9677, recall: 0.9836\n",
      "2018-12-26T18:25:48.798449, step: 1090, loss: 0.04759535938501358, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:25:48.955043, step: 1091, loss: 0.06271444261074066, acc: 0.9922, auc: 1.0, precision: 0.9844, recall: 1.0\n",
      "2018-12-26T18:25:49.105225, step: 1092, loss: 0.06352916359901428, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:25:49.257864, step: 1093, loss: 0.05754252150654793, acc: 0.9844, auc: 0.9988, precision: 1.0, recall: 0.9655\n",
      "2018-12-26T18:25:49.414837, step: 1094, loss: 0.04016788676381111, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9859\n",
      "2018-12-26T18:25:49.571050, step: 1095, loss: 0.06508957594633102, acc: 0.9766, auc: 0.9993, precision: 0.9815, recall: 0.9636\n",
      "2018-12-26T18:25:49.725193, step: 1096, loss: 0.05293109267950058, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9692\n",
      "2018-12-26T18:25:49.879872, step: 1097, loss: 0.03740406781435013, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:25:50.037219, step: 1098, loss: 0.0613093227148056, acc: 0.9688, auc: 0.9993, precision: 1.0, recall: 0.931\n",
      "2018-12-26T18:25:50.220525, step: 1099, loss: 0.06423931568861008, acc: 0.9688, auc: 0.9995, precision: 1.0, recall: 0.9231\n",
      "2018-12-26T18:25:50.391456, step: 1100, loss: 0.0629865974187851, acc: 0.9844, auc: 0.9998, precision: 1.0, recall: 0.9701\n",
      "\n",
      "Evaluation:\n",
      "2018-12-26T18:25:56.273435, step: 1100, loss: 0.35028428937259476, acc: 0.8589631578947369, auc: 0.9371552631578949, precision: 0.8966947368421052, recall: 0.8158789473684211\n",
      "Saved model checkpoint to ../model/textCNN/model/my-model-1100\n",
      "\n",
      "2018-12-26T18:25:56.710172, step: 1101, loss: 0.10162323713302612, acc: 0.9766, auc: 0.9895, precision: 1.0, recall: 0.9531\n",
      "2018-12-26T18:25:56.859739, step: 1102, loss: 0.05224642902612686, acc: 0.9844, auc: 0.9995, precision: 1.0, recall: 0.9688\n",
      "2018-12-26T18:25:57.008219, step: 1103, loss: 0.05390283465385437, acc: 0.9922, auc: 0.9998, precision: 1.0, recall: 0.9833\n",
      "2018-12-26T18:25:57.170412, step: 1104, loss: 0.04955912008881569, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:25:57.328795, step: 1105, loss: 0.06866410374641418, acc: 0.9688, auc: 0.9985, precision: 0.9846, recall: 0.9552\n",
      "2018-12-26T18:25:57.467062, step: 1106, loss: 0.04809265956282616, acc: 0.9844, auc: 1.0, precision: 0.9718, recall: 1.0\n",
      "2018-12-26T18:25:57.622299, step: 1107, loss: 0.0913655161857605, acc: 0.9766, auc: 0.9966, precision: 0.971, recall: 0.9853\n",
      "2018-12-26T18:25:57.782207, step: 1108, loss: 0.04814496636390686, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:25:57.945624, step: 1109, loss: 0.06248465180397034, acc: 0.9688, auc: 0.9993, precision: 0.9833, recall: 0.9516\n",
      "2018-12-26T18:25:58.100062, step: 1110, loss: 0.09667955338954926, acc: 0.9531, auc: 0.9939, precision: 0.95, recall: 0.95\n",
      "2018-12-26T18:25:58.258785, step: 1111, loss: 0.05781830847263336, acc: 0.9766, auc: 1.0, precision: 1.0, recall: 0.9552\n",
      "2018-12-26T18:25:58.421461, step: 1112, loss: 0.04546631500124931, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9821\n",
      "2018-12-26T18:25:58.577181, step: 1113, loss: 0.05449412390589714, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9839\n",
      "2018-12-26T18:25:58.733706, step: 1114, loss: 0.04769614711403847, acc: 0.9922, auc: 0.9998, precision: 1.0, recall: 0.9863\n",
      "2018-12-26T18:25:58.891195, step: 1115, loss: 0.06292985379695892, acc: 0.9922, auc: 0.9997, precision: 0.9818, recall: 1.0\n",
      "2018-12-26T18:25:59.044956, step: 1116, loss: 0.03450824320316315, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9733\n",
      "2018-12-26T18:25:59.194784, step: 1117, loss: 0.04063044488430023, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9718\n",
      "2018-12-26T18:25:59.370174, step: 1118, loss: 0.045443370938301086, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:25:59.523984, step: 1119, loss: 0.06657394021749496, acc: 0.9766, auc: 0.9988, precision: 0.9848, recall: 0.9701\n",
      "2018-12-26T18:25:59.678406, step: 1120, loss: 0.06517624109983444, acc: 0.9922, auc: 0.998, precision: 0.9841, recall: 1.0\n",
      "2018-12-26T18:25:59.837659, step: 1121, loss: 0.0830845832824707, acc: 0.9766, auc: 0.9951, precision: 0.9857, recall: 0.9718\n",
      "2018-12-26T18:25:59.992446, step: 1122, loss: 0.02926395833492279, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9848\n",
      "2018-12-26T18:26:00.147531, step: 1123, loss: 0.036059603095054626, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:26:00.301544, step: 1124, loss: 0.07536346465349197, acc: 0.9766, auc: 0.997, precision: 0.9808, recall: 0.9623\n",
      "2018-12-26T18:26:00.451557, step: 1125, loss: 0.04473692178726196, acc: 0.9766, auc: 1.0, precision: 1.0, recall: 0.9552\n",
      "2018-12-26T18:26:00.602728, step: 1126, loss: 0.047269005328416824, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9692\n",
      "2018-12-26T18:26:00.751415, step: 1127, loss: 0.046878185123205185, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9853\n",
      "2018-12-26T18:26:00.902441, step: 1128, loss: 0.05061277374625206, acc: 0.9766, auc: 0.9993, precision: 0.9853, recall: 0.971\n",
      "2018-12-26T18:26:01.055888, step: 1129, loss: 0.0445069894194603, acc: 0.9922, auc: 0.9995, precision: 1.0, recall: 0.9831\n",
      "2018-12-26T18:26:01.210852, step: 1130, loss: 0.05951549857854843, acc: 0.9922, auc: 1.0, precision: 0.9841, recall: 1.0\n",
      "2018-12-26T18:26:01.365026, step: 1131, loss: 0.029516948387026787, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:26:01.527275, step: 1132, loss: 0.03478860482573509, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9848\n",
      "2018-12-26T18:26:01.677845, step: 1133, loss: 0.04731200262904167, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9677\n",
      "2018-12-26T18:26:01.828671, step: 1134, loss: 0.0393654890358448, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9844\n",
      "2018-12-26T18:26:01.979558, step: 1135, loss: 0.04218614846467972, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:26:02.139146, step: 1136, loss: 0.04779662936925888, acc: 0.9922, auc: 0.9998, precision: 1.0, recall: 0.9828\n",
      "2018-12-26T18:26:02.302719, step: 1137, loss: 0.07153229415416718, acc: 0.9766, auc: 0.9983, precision: 0.9844, recall: 0.9692\n",
      "2018-12-26T18:26:02.461436, step: 1138, loss: 0.03882656991481781, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9844\n",
      "2018-12-26T18:26:02.607624, step: 1139, loss: 0.04112665727734566, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:26:02.763706, step: 1140, loss: 0.06856429576873779, acc: 0.9844, auc: 0.9995, precision: 1.0, recall: 0.9718\n",
      "2018-12-26T18:26:02.918552, step: 1141, loss: 0.06430941820144653, acc: 0.9766, auc: 0.999, precision: 1.0, recall: 0.9444\n",
      "2018-12-26T18:26:03.071599, step: 1142, loss: 0.02849438786506653, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:26:03.226892, step: 1143, loss: 0.05228061228990555, acc: 0.9844, auc: 0.9993, precision: 1.0, recall: 0.9672\n",
      "2018-12-26T18:26:03.381262, step: 1144, loss: 0.0524136945605278, acc: 0.9922, auc: 1.0, precision: 0.9855, recall: 1.0\n",
      "2018-12-26T18:26:03.535830, step: 1145, loss: 0.07031144946813583, acc: 0.9844, auc: 0.9988, precision: 0.9714, recall: 1.0\n",
      "2018-12-26T18:26:03.690203, step: 1146, loss: 0.06358226388692856, acc: 0.9844, auc: 0.9951, precision: 0.9833, recall: 0.9833\n",
      "2018-12-26T18:26:03.856070, step: 1147, loss: 0.038198258727788925, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9857\n",
      "2018-12-26T18:26:04.008335, step: 1148, loss: 0.052192322909832, acc: 0.9922, auc: 0.9995, precision: 1.0, recall: 0.9844\n",
      "2018-12-26T18:26:04.155796, step: 1149, loss: 0.05470002442598343, acc: 0.9844, auc: 0.9993, precision: 1.0, recall: 0.9697\n",
      "2018-12-26T18:26:04.306777, step: 1150, loss: 0.050316222012043, acc: 0.9922, auc: 0.9973, precision: 1.0, recall: 0.9828\n",
      "2018-12-26T18:26:04.461834, step: 1151, loss: 0.05673842877149582, acc: 0.9766, auc: 1.0, precision: 1.0, recall: 0.9524\n",
      "2018-12-26T18:26:04.619046, step: 1152, loss: 0.055941712111234665, acc: 0.9922, auc: 0.9998, precision: 1.0, recall: 0.9831\n",
      "2018-12-26T18:26:04.773610, step: 1153, loss: 0.05829286575317383, acc: 0.9922, auc: 1.0, precision: 0.9804, recall: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-26T18:26:04.937562, step: 1154, loss: 0.035522110760211945, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9844\n",
      "2018-12-26T18:26:05.092415, step: 1155, loss: 0.03976008668541908, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:26:05.247732, step: 1156, loss: 0.08312920480966568, acc: 0.9609, auc: 0.9976, precision: 0.9839, recall: 0.9385\n",
      "2018-12-26T18:26:05.416017, step: 1157, loss: 0.055238597095012665, acc: 0.9766, auc: 1.0, precision: 1.0, recall: 0.95\n",
      "2018-12-26T18:26:05.570646, step: 1158, loss: 0.05590253323316574, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9844\n",
      "2018-12-26T18:26:05.722007, step: 1159, loss: 0.034643735736608505, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:26:05.875117, step: 1160, loss: 0.07229678332805634, acc: 0.9766, auc: 0.9973, precision: 0.9839, recall: 0.9683\n",
      "2018-12-26T18:26:06.036328, step: 1161, loss: 0.06957182288169861, acc: 0.9844, auc: 0.9958, precision: 1.0, recall: 0.9688\n",
      "2018-12-26T18:26:06.189329, step: 1162, loss: 0.05242427438497543, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:26:06.342877, step: 1163, loss: 0.050178129225969315, acc: 0.9922, auc: 0.9998, precision: 1.0, recall: 0.9831\n",
      "2018-12-26T18:26:06.508016, step: 1164, loss: 0.08738192915916443, acc: 0.9766, auc: 0.9941, precision: 1.0, recall: 0.9545\n",
      "2018-12-26T18:26:06.660387, step: 1165, loss: 0.0661044716835022, acc: 0.9766, auc: 0.9992, precision: 1.0, recall: 0.96\n",
      "2018-12-26T18:26:06.822363, step: 1166, loss: 0.05527820438146591, acc: 0.9766, auc: 0.9995, precision: 0.9857, recall: 0.9718\n",
      "2018-12-26T18:26:06.981116, step: 1167, loss: 0.07318782806396484, acc: 0.9844, auc: 0.9983, precision: 1.0, recall: 0.9667\n",
      "2018-12-26T18:26:07.137707, step: 1168, loss: 0.05039222538471222, acc: 0.9844, auc: 0.9998, precision: 1.0, recall: 0.9706\n",
      "2018-12-26T18:26:07.291720, step: 1169, loss: 0.0757221132516861, acc: 0.9844, auc: 0.9973, precision: 0.9706, recall: 1.0\n",
      "2018-12-26T18:26:07.448993, step: 1170, loss: 0.04604770615696907, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9828\n",
      "2018-12-26T18:26:07.603009, step: 1171, loss: 0.055583011358976364, acc: 0.9922, auc: 0.999, precision: 1.0, recall: 0.9828\n",
      "2018-12-26T18:26:07.763236, step: 1172, loss: 0.0592188686132431, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9846\n",
      "2018-12-26T18:26:07.916362, step: 1173, loss: 0.05380968004465103, acc: 0.9766, auc: 0.999, precision: 0.9857, recall: 0.9718\n",
      "2018-12-26T18:26:08.069780, step: 1174, loss: 0.0654602125287056, acc: 0.9844, auc: 0.999, precision: 1.0, recall: 0.971\n",
      "2018-12-26T18:26:08.225291, step: 1175, loss: 0.04168134927749634, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:26:08.380588, step: 1176, loss: 0.06354578584432602, acc: 0.9688, auc: 0.9983, precision: 0.9841, recall: 0.9538\n",
      "2018-12-26T18:26:08.534185, step: 1177, loss: 0.048525772988796234, acc: 0.9922, auc: 0.9998, precision: 0.9853, recall: 1.0\n",
      "2018-12-26T18:26:08.685672, step: 1178, loss: 0.049366649240255356, acc: 0.9922, auc: 1.0, precision: 0.9851, recall: 1.0\n",
      "2018-12-26T18:26:08.838327, step: 1179, loss: 0.05701925605535507, acc: 0.9922, auc: 0.9955, precision: 1.0, recall: 0.9821\n",
      "2018-12-26T18:26:08.992133, step: 1180, loss: 0.05673118680715561, acc: 0.9844, auc: 0.9993, precision: 0.9818, recall: 0.9818\n",
      "2018-12-26T18:26:09.145114, step: 1181, loss: 0.07573187351226807, acc: 0.9688, auc: 0.9983, precision: 0.9661, recall: 0.9661\n",
      "2018-12-26T18:26:09.299928, step: 1182, loss: 0.036948010325431824, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:26:09.451468, step: 1183, loss: 0.04377374053001404, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.971\n",
      "2018-12-26T18:26:09.605030, step: 1184, loss: 0.05255546420812607, acc: 0.9766, auc: 1.0, precision: 1.0, recall: 0.9571\n",
      "2018-12-26T18:26:09.758413, step: 1185, loss: 0.08140915632247925, acc: 0.9766, auc: 0.9919, precision: 0.9697, recall: 0.9846\n",
      "2018-12-26T18:26:09.917796, step: 1186, loss: 0.04389728978276253, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9855\n",
      "2018-12-26T18:26:10.066069, step: 1187, loss: 0.04687550663948059, acc: 0.9844, auc: 0.9995, precision: 0.9831, recall: 0.9831\n",
      "2018-12-26T18:26:10.215713, step: 1188, loss: 0.05357332155108452, acc: 0.9922, auc: 0.9988, precision: 1.0, recall: 0.9846\n",
      "2018-12-26T18:26:10.363237, step: 1189, loss: 0.048428524285554886, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:26:10.513040, step: 1190, loss: 0.050263822078704834, acc: 0.9922, auc: 0.9995, precision: 1.0, recall: 0.9857\n",
      "2018-12-26T18:26:10.663831, step: 1191, loss: 0.06387356668710709, acc: 0.9844, auc: 0.999, precision: 0.9863, recall: 0.9863\n",
      "2018-12-26T18:26:10.813760, step: 1192, loss: 0.08095305413007736, acc: 0.9766, auc: 0.9939, precision: 1.0, recall: 0.9524\n",
      "2018-12-26T18:26:10.972704, step: 1193, loss: 0.06927068531513214, acc: 0.9766, auc: 0.9983, precision: 0.9839, recall: 0.9683\n",
      "2018-12-26T18:26:11.124523, step: 1194, loss: 0.043545596301555634, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9841\n",
      "2018-12-26T18:26:11.277028, step: 1195, loss: 0.046195775270462036, acc: 0.9844, auc: 0.9993, precision: 0.9853, recall: 0.9853\n",
      "2018-12-26T18:26:11.429421, step: 1196, loss: 0.05464818701148033, acc: 0.9922, auc: 0.9998, precision: 1.0, recall: 0.9848\n",
      "2018-12-26T18:26:11.583590, step: 1197, loss: 0.06214471906423569, acc: 0.9844, auc: 0.9993, precision: 1.0, recall: 0.9683\n",
      "2018-12-26T18:26:11.735117, step: 1198, loss: 0.062316104769706726, acc: 0.9922, auc: 0.9978, precision: 1.0, recall: 0.9857\n",
      "2018-12-26T18:26:11.886608, step: 1199, loss: 0.047072820365428925, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9833\n",
      "2018-12-26T18:26:12.028938, step: 1200, loss: 0.04947869852185249, acc: 0.9766, auc: 1.0, precision: 1.0, recall: 0.9524\n",
      "\n",
      "Evaluation:\n",
      "2018-12-26T18:26:17.909255, step: 1200, loss: 0.35719050349373566, acc: 0.8647236842105265, auc: 0.9358552631578944, precision: 0.8881210526315788, recall: 0.8391657894736844\n",
      "Saved model checkpoint to ../model/textCNN/model/my-model-1200\n",
      "\n",
      "2018-12-26T18:26:18.356891, step: 1201, loss: 0.048069074749946594, acc: 0.9922, auc: 0.9997, precision: 0.9811, recall: 1.0\n",
      "2018-12-26T18:26:18.510565, step: 1202, loss: 0.04884031414985657, acc: 0.9844, auc: 0.9998, precision: 0.9839, recall: 0.9839\n",
      "2018-12-26T18:26:18.665469, step: 1203, loss: 0.04903116077184677, acc: 0.9922, auc: 0.9998, precision: 1.0, recall: 0.9839\n",
      "2018-12-26T18:26:18.819058, step: 1204, loss: 0.043441042304039, acc: 0.9844, auc: 0.9998, precision: 1.0, recall: 0.9636\n",
      "2018-12-26T18:26:18.976317, step: 1205, loss: 0.06164730340242386, acc: 0.9766, auc: 1.0, precision: 1.0, recall: 0.9492\n",
      "2018-12-26T18:26:19.130634, step: 1206, loss: 0.06268851459026337, acc: 0.9844, auc: 0.9993, precision: 1.0, recall: 0.9726\n",
      "2018-12-26T18:26:19.289615, step: 1207, loss: 0.061853088438510895, acc: 0.9844, auc: 0.999, precision: 1.0, recall: 0.9667\n",
      "2018-12-26T18:26:19.444214, step: 1208, loss: 0.048142626881599426, acc: 0.9844, auc: 0.9995, precision: 1.0, recall: 0.9667\n",
      "2018-12-26T18:26:19.597784, step: 1209, loss: 0.04815749078989029, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9855\n",
      "2018-12-26T18:26:19.746714, step: 1210, loss: 0.05087485909461975, acc: 0.9922, auc: 0.9993, precision: 1.0, recall: 0.9853\n",
      "2018-12-26T18:26:19.903814, step: 1211, loss: 0.07925200462341309, acc: 0.9844, auc: 0.9944, precision: 0.9701, recall: 1.0\n",
      "2018-12-26T18:26:20.051530, step: 1212, loss: 0.030773263424634933, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:26:20.218045, step: 1213, loss: 0.042782001197338104, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:26:20.368827, step: 1214, loss: 0.0583629384636879, acc: 0.9844, auc: 0.9988, precision: 0.9848, recall: 0.9848\n",
      "2018-12-26T18:26:20.519840, step: 1215, loss: 0.05187585577368736, acc: 0.9766, auc: 0.999, precision: 0.9841, recall: 0.9688\n",
      "2018-12-26T18:26:20.673487, step: 1216, loss: 0.059728674590587616, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9841\n",
      "2018-12-26T18:26:20.827695, step: 1217, loss: 0.06396621465682983, acc: 0.9922, auc: 0.9978, precision: 1.0, recall: 0.9846\n",
      "2018-12-26T18:26:20.981667, step: 1218, loss: 0.08322560787200928, acc: 0.9688, auc: 0.9993, precision: 1.0, recall: 0.9394\n",
      "2018-12-26T18:26:21.137203, step: 1219, loss: 0.040258295834064484, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9655\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-26T18:26:21.291861, step: 1220, loss: 0.0455472357571125, acc: 0.9922, auc: 1.0, precision: 0.9841, recall: 1.0\n",
      "2018-12-26T18:26:21.446809, step: 1221, loss: 0.04718613252043724, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9844\n",
      "2018-12-26T18:26:21.605393, step: 1222, loss: 0.044151660054922104, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9833\n",
      "2018-12-26T18:26:21.757071, step: 1223, loss: 0.06043483316898346, acc: 0.9922, auc: 0.9983, precision: 1.0, recall: 0.9846\n",
      "2018-12-26T18:26:21.907178, step: 1224, loss: 0.10588078945875168, acc: 0.9688, auc: 0.9945, precision: 0.9726, recall: 0.9726\n",
      "2018-12-26T18:26:22.067666, step: 1225, loss: 0.04411960393190384, acc: 0.9844, auc: 0.9998, precision: 0.9821, recall: 0.9821\n",
      "2018-12-26T18:26:22.223569, step: 1226, loss: 0.06544626504182816, acc: 0.9844, auc: 0.9983, precision: 0.9861, recall: 0.9861\n",
      "2018-12-26T18:26:22.410143, step: 1227, loss: 0.05658412724733353, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9844\n",
      "2018-12-26T18:26:22.561835, step: 1228, loss: 0.06726457178592682, acc: 0.9844, auc: 0.999, precision: 1.0, recall: 0.9701\n",
      "2018-12-26T18:26:22.714868, step: 1229, loss: 0.05740605294704437, acc: 0.9766, auc: 0.999, precision: 0.9692, recall: 0.9844\n",
      "2018-12-26T18:26:22.867997, step: 1230, loss: 0.05529271811246872, acc: 0.9844, auc: 0.9995, precision: 1.0, recall: 0.9706\n",
      "2018-12-26T18:26:23.017836, step: 1231, loss: 0.05067332088947296, acc: 0.9844, auc: 0.9993, precision: 0.9839, recall: 0.9839\n",
      "2018-12-26T18:26:23.179030, step: 1232, loss: 0.05946706607937813, acc: 0.9844, auc: 0.999, precision: 0.9667, recall: 1.0\n",
      "2018-12-26T18:26:23.338538, step: 1233, loss: 0.02984653413295746, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:26:23.491639, step: 1234, loss: 0.05202370509505272, acc: 0.9766, auc: 0.999, precision: 0.9831, recall: 0.9667\n",
      "2018-12-26T18:26:23.647344, step: 1235, loss: 0.046275004744529724, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9868\n",
      "2018-12-26T18:26:23.812866, step: 1236, loss: 0.05201084166765213, acc: 0.9766, auc: 1.0, precision: 1.0, recall: 0.9595\n",
      "2018-12-26T18:26:23.965830, step: 1237, loss: 0.05148421972990036, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9683\n",
      "2018-12-26T18:26:24.117671, step: 1238, loss: 0.05096772685647011, acc: 0.9844, auc: 0.999, precision: 0.9848, recall: 0.9848\n",
      "2018-12-26T18:26:24.268867, step: 1239, loss: 0.0937691405415535, acc: 0.9688, auc: 0.9968, precision: 0.9559, recall: 0.9848\n",
      "2018-12-26T18:26:24.420217, step: 1240, loss: 0.05665692314505577, acc: 0.9766, auc: 0.9995, precision: 0.9815, recall: 0.9636\n",
      "start training model\n",
      "2018-12-26T18:26:24.597733, step: 1241, loss: 0.07557190209627151, acc: 0.9922, auc: 0.999, precision: 0.9848, recall: 1.0\n",
      "2018-12-26T18:26:24.754317, step: 1242, loss: 0.04793325066566467, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9672\n",
      "2018-12-26T18:26:24.904160, step: 1243, loss: 0.05778335779905319, acc: 0.9844, auc: 0.9985, precision: 1.0, recall: 0.9697\n",
      "2018-12-26T18:26:25.076356, step: 1244, loss: 0.04482239484786987, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9688\n",
      "2018-12-26T18:26:25.231296, step: 1245, loss: 0.03066866844892502, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:26:25.385554, step: 1246, loss: 0.03406424820423126, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9714\n",
      "2018-12-26T18:26:25.539151, step: 1247, loss: 0.02065347507596016, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:26:25.700118, step: 1248, loss: 0.0388747975230217, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9855\n",
      "2018-12-26T18:26:25.881632, step: 1249, loss: 0.04013795405626297, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:26:26.032361, step: 1250, loss: 0.024609515443444252, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:26:26.182073, step: 1251, loss: 0.033039651811122894, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9833\n",
      "2018-12-26T18:26:26.334518, step: 1252, loss: 0.03587736189365387, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:26:26.489427, step: 1253, loss: 0.029043320566415787, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:26:26.652970, step: 1254, loss: 0.041521430015563965, acc: 0.9922, auc: 0.9998, precision: 0.9831, recall: 1.0\n",
      "2018-12-26T18:26:26.840377, step: 1255, loss: 0.02007501944899559, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:26:26.991252, step: 1256, loss: 0.06187661364674568, acc: 0.9688, auc: 0.9998, precision: 1.0, recall: 0.9344\n",
      "2018-12-26T18:26:27.144654, step: 1257, loss: 0.027860725298523903, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:26:27.299472, step: 1258, loss: 0.06970182061195374, acc: 0.9688, auc: 0.9976, precision: 0.9831, recall: 0.9508\n",
      "2018-12-26T18:26:27.448508, step: 1259, loss: 0.05943559855222702, acc: 0.9766, auc: 0.9988, precision: 1.0, recall: 0.9524\n",
      "2018-12-26T18:26:27.605544, step: 1260, loss: 0.02245112508535385, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:26:27.779171, step: 1261, loss: 0.020194705575704575, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:26:27.928943, step: 1262, loss: 0.056269511580467224, acc: 0.9922, auc: 1.0, precision: 0.9848, recall: 1.0\n",
      "2018-12-26T18:26:28.089402, step: 1263, loss: 0.047355588525533676, acc: 0.9922, auc: 1.0, precision: 0.9833, recall: 1.0\n",
      "2018-12-26T18:26:28.244297, step: 1264, loss: 0.03898421302437782, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:26:28.403284, step: 1265, loss: 0.04349977523088455, acc: 0.9844, auc: 0.9998, precision: 0.9836, recall: 0.9836\n",
      "2018-12-26T18:26:28.558892, step: 1266, loss: 0.025059131905436516, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:26:28.712589, step: 1267, loss: 0.03753986954689026, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9683\n",
      "2018-12-26T18:26:28.871626, step: 1268, loss: 0.04117215797305107, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9839\n",
      "2018-12-26T18:26:29.030952, step: 1269, loss: 0.03888997808098793, acc: 0.9766, auc: 1.0, precision: 1.0, recall: 0.9492\n",
      "2018-12-26T18:26:29.184414, step: 1270, loss: 0.0364617258310318, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9844\n",
      "2018-12-26T18:26:29.336896, step: 1271, loss: 0.03885360062122345, acc: 0.9922, auc: 1.0, precision: 0.9831, recall: 1.0\n",
      "2018-12-26T18:26:29.489535, step: 1272, loss: 0.03947525471448898, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9863\n",
      "2018-12-26T18:26:29.643565, step: 1273, loss: 0.033789824694395065, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9844\n",
      "2018-12-26T18:26:29.797940, step: 1274, loss: 0.027957478538155556, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:26:29.947929, step: 1275, loss: 0.03375057503581047, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9846\n",
      "2018-12-26T18:26:30.100835, step: 1276, loss: 0.02437770552933216, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9853\n",
      "2018-12-26T18:26:30.265420, step: 1277, loss: 0.04186583310365677, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:26:30.421027, step: 1278, loss: 0.05323607847094536, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:26:30.576327, step: 1279, loss: 0.03893505409359932, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9841\n",
      "2018-12-26T18:26:30.728995, step: 1280, loss: 0.022282663732767105, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:26:30.884405, step: 1281, loss: 0.04121968150138855, acc: 0.9766, auc: 1.0, precision: 1.0, recall: 0.9524\n",
      "2018-12-26T18:26:31.037220, step: 1282, loss: 0.052287980914115906, acc: 0.9766, auc: 1.0, precision: 1.0, recall: 0.9583\n",
      "2018-12-26T18:26:31.188295, step: 1283, loss: 0.03112318180501461, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9697\n",
      "2018-12-26T18:26:31.339884, step: 1284, loss: 0.041585806757211685, acc: 0.9922, auc: 0.9998, precision: 0.9825, recall: 1.0\n",
      "2018-12-26T18:26:31.489624, step: 1285, loss: 0.030542077496647835, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:26:31.642616, step: 1286, loss: 0.02117949165403843, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:26:31.804861, step: 1287, loss: 0.026989329606294632, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-26T18:26:31.965060, step: 1288, loss: 0.038109198212623596, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:26:32.126678, step: 1289, loss: 0.04725267365574837, acc: 0.9922, auc: 1.0, precision: 0.9831, recall: 1.0\n",
      "2018-12-26T18:26:32.282290, step: 1290, loss: 0.04129287973046303, acc: 0.9844, auc: 0.9998, precision: 0.9836, recall: 0.9836\n",
      "2018-12-26T18:26:32.438870, step: 1291, loss: 0.033470869064331055, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:26:32.605175, step: 1292, loss: 0.0411866120994091, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9859\n",
      "2018-12-26T18:26:32.759254, step: 1293, loss: 0.025071196258068085, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9848\n",
      "2018-12-26T18:26:32.914655, step: 1294, loss: 0.03350221365690231, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9833\n",
      "2018-12-26T18:26:33.070988, step: 1295, loss: 0.04241412132978439, acc: 0.9766, auc: 1.0, precision: 1.0, recall: 0.9559\n",
      "2018-12-26T18:26:33.226409, step: 1296, loss: 0.043239109218120575, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9661\n",
      "2018-12-26T18:26:33.381160, step: 1297, loss: 0.026016851887106895, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:26:33.533470, step: 1298, loss: 0.03225315362215042, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:26:33.687054, step: 1299, loss: 0.028835954144597054, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9821\n",
      "2018-12-26T18:26:33.845630, step: 1300, loss: 0.04727054387331009, acc: 0.9922, auc: 1.0, precision: 0.9848, recall: 1.0\n",
      "\n",
      "Evaluation:\n",
      "2018-12-26T18:26:39.693328, step: 1300, loss: 0.3786565908475926, acc: 0.8622473684210524, auc: 0.9358868421052631, precision: 0.8701078947368422, recall: 0.8544342105263157\n",
      "Saved model checkpoint to ../model/textCNN/model/my-model-1300\n",
      "\n",
      "2018-12-26T18:26:40.125253, step: 1301, loss: 0.04302525520324707, acc: 0.9922, auc: 1.0, precision: 0.9848, recall: 1.0\n",
      "2018-12-26T18:26:40.290435, step: 1302, loss: 0.037093885242938995, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9833\n",
      "2018-12-26T18:26:40.455277, step: 1303, loss: 0.028934631496667862, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:26:40.616000, step: 1304, loss: 0.03956405818462372, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9718\n",
      "2018-12-26T18:26:40.771256, step: 1305, loss: 0.04715394601225853, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9677\n",
      "2018-12-26T18:26:40.924354, step: 1306, loss: 0.031173203140497208, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9697\n",
      "2018-12-26T18:26:41.085794, step: 1307, loss: 0.043870434165000916, acc: 0.9844, auc: 0.9998, precision: 1.0, recall: 0.9726\n",
      "2018-12-26T18:26:41.239894, step: 1308, loss: 0.03488142043352127, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9608\n",
      "2018-12-26T18:26:41.399391, step: 1309, loss: 0.029699556529521942, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9688\n",
      "2018-12-26T18:26:41.552607, step: 1310, loss: 0.03645111992955208, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9857\n",
      "2018-12-26T18:26:41.710122, step: 1311, loss: 0.029249630868434906, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:26:41.864874, step: 1312, loss: 0.03519616276025772, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:26:42.019376, step: 1313, loss: 0.04551338404417038, acc: 0.9922, auc: 0.9995, precision: 0.9846, recall: 1.0\n",
      "2018-12-26T18:26:42.172170, step: 1314, loss: 0.03589747101068497, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9643\n",
      "2018-12-26T18:26:42.329197, step: 1315, loss: 0.04338410496711731, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9701\n",
      "2018-12-26T18:26:42.483154, step: 1316, loss: 0.023029793053865433, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9851\n",
      "2018-12-26T18:26:42.636918, step: 1317, loss: 0.05378859490156174, acc: 0.9688, auc: 0.9995, precision: 1.0, recall: 0.9355\n",
      "2018-12-26T18:26:42.793644, step: 1318, loss: 0.020761722698807716, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:26:42.946540, step: 1319, loss: 0.029932811856269836, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.971\n",
      "2018-12-26T18:26:43.098761, step: 1320, loss: 0.02653445303440094, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:26:43.234999, step: 1321, loss: 0.021885260939598083, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:26:43.382417, step: 1322, loss: 0.02451055869460106, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:26:43.523980, step: 1323, loss: 0.04864417761564255, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9828\n",
      "2018-12-26T18:26:43.674073, step: 1324, loss: 0.03823091834783554, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9726\n",
      "2018-12-26T18:26:43.823875, step: 1325, loss: 0.03454851731657982, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9863\n",
      "2018-12-26T18:26:43.982089, step: 1326, loss: 0.0727866142988205, acc: 0.9844, auc: 0.9946, precision: 1.0, recall: 0.9672\n",
      "2018-12-26T18:26:44.146344, step: 1327, loss: 0.022267267107963562, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:26:44.300842, step: 1328, loss: 0.030935008078813553, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:26:44.454702, step: 1329, loss: 0.032146155834198, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:26:44.608774, step: 1330, loss: 0.0648006945848465, acc: 0.9922, auc: 0.9985, precision: 1.0, recall: 0.9848\n",
      "2018-12-26T18:26:44.761335, step: 1331, loss: 0.022905290126800537, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:26:44.930593, step: 1332, loss: 0.031790606677532196, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9821\n",
      "2018-12-26T18:26:45.083571, step: 1333, loss: 0.026986200362443924, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:26:45.238018, step: 1334, loss: 0.028976943343877792, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:26:45.390587, step: 1335, loss: 0.025702055543661118, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:26:45.541518, step: 1336, loss: 0.0297220628708601, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:26:45.695105, step: 1337, loss: 0.034132640808820724, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:26:45.846922, step: 1338, loss: 0.0360829159617424, acc: 0.9922, auc: 1.0, precision: 0.9836, recall: 1.0\n",
      "2018-12-26T18:26:45.998413, step: 1339, loss: 0.03547673299908638, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9855\n",
      "2018-12-26T18:26:46.157782, step: 1340, loss: 0.042933374643325806, acc: 0.9766, auc: 0.9995, precision: 0.9844, recall: 0.9692\n",
      "2018-12-26T18:26:46.316072, step: 1341, loss: 0.04283912107348442, acc: 0.9844, auc: 0.9998, precision: 1.0, recall: 0.9701\n",
      "2018-12-26T18:26:46.472997, step: 1342, loss: 0.01876995526254177, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:26:46.633579, step: 1343, loss: 0.03225332498550415, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9844\n",
      "2018-12-26T18:26:46.788016, step: 1344, loss: 0.04189493879675865, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9831\n",
      "2018-12-26T18:26:46.940822, step: 1345, loss: 0.03221508488059044, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:26:47.091281, step: 1346, loss: 0.03402698412537575, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:26:47.245301, step: 1347, loss: 0.03878968209028244, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9683\n",
      "2018-12-26T18:26:47.409911, step: 1348, loss: 0.06333423405885696, acc: 0.9766, auc: 0.9969, precision: 1.0, recall: 0.9388\n",
      "2018-12-26T18:26:47.563722, step: 1349, loss: 0.023045532405376434, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9851\n",
      "2018-12-26T18:26:47.717130, step: 1350, loss: 0.048269543796777725, acc: 0.9922, auc: 0.9963, precision: 1.0, recall: 0.9839\n",
      "2018-12-26T18:26:47.875167, step: 1351, loss: 0.02541087195277214, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9868\n",
      "2018-12-26T18:26:48.031568, step: 1352, loss: 0.04047334939241409, acc: 0.9922, auc: 1.0, precision: 0.9851, recall: 1.0\n",
      "2018-12-26T18:26:48.190260, step: 1353, loss: 0.02890358865261078, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9851\n",
      "2018-12-26T18:26:48.343562, step: 1354, loss: 0.026419637724757195, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9851\n",
      "2018-12-26T18:26:48.493325, step: 1355, loss: 0.026120275259017944, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-26T18:26:48.645649, step: 1356, loss: 0.02059594914317131, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:26:48.805176, step: 1357, loss: 0.043201744556427, acc: 0.9922, auc: 0.9988, precision: 1.0, recall: 0.9855\n",
      "2018-12-26T18:26:48.965942, step: 1358, loss: 0.032396040856838226, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9853\n",
      "2018-12-26T18:26:49.135467, step: 1359, loss: 0.09957434237003326, acc: 0.9688, auc: 0.9946, precision: 0.9538, recall: 0.9841\n",
      "2018-12-26T18:26:49.293683, step: 1360, loss: 0.019990889355540276, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:26:49.446589, step: 1361, loss: 0.03721063956618309, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:26:49.601517, step: 1362, loss: 0.027115436270833015, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9853\n",
      "2018-12-26T18:26:49.754529, step: 1363, loss: 0.01843993179500103, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9836\n",
      "2018-12-26T18:26:49.910805, step: 1364, loss: 0.02683734893798828, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:26:50.065001, step: 1365, loss: 0.034843746572732925, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9683\n",
      "2018-12-26T18:26:50.219301, step: 1366, loss: 0.03800562769174576, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9844\n",
      "2018-12-26T18:26:50.373161, step: 1367, loss: 0.03013862669467926, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9825\n",
      "2018-12-26T18:26:50.527149, step: 1368, loss: 0.029207048937678337, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:26:50.690704, step: 1369, loss: 0.09075397998094559, acc: 0.9766, auc: 0.9917, precision: 1.0, recall: 0.9524\n",
      "2018-12-26T18:26:50.849937, step: 1370, loss: 0.05002211779356003, acc: 0.9766, auc: 0.9993, precision: 0.9851, recall: 0.9706\n",
      "2018-12-26T18:26:51.112608, step: 1371, loss: 0.028443051502108574, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9859\n",
      "2018-12-26T18:26:51.266832, step: 1372, loss: 0.034507740288972855, acc: 0.9922, auc: 1.0, precision: 0.9855, recall: 1.0\n",
      "2018-12-26T18:26:51.417425, step: 1373, loss: 0.03850822150707245, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:26:51.568348, step: 1374, loss: 0.043483782559633255, acc: 0.9766, auc: 0.9995, precision: 0.9839, recall: 0.9683\n",
      "2018-12-26T18:26:51.722970, step: 1375, loss: 0.03529825061559677, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:26:51.883621, step: 1376, loss: 0.02445090562105179, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9821\n",
      "2018-12-26T18:26:52.036890, step: 1377, loss: 0.05561106652021408, acc: 0.9844, auc: 0.9983, precision: 0.9839, recall: 0.9839\n",
      "2018-12-26T18:26:52.191904, step: 1378, loss: 0.051166750490665436, acc: 0.9844, auc: 0.9998, precision: 1.0, recall: 0.9697\n",
      "2018-12-26T18:26:52.341865, step: 1379, loss: 0.05199361592531204, acc: 0.9766, auc: 0.9998, precision: 1.0, recall: 0.95\n",
      "2018-12-26T18:26:52.497131, step: 1380, loss: 0.0836007297039032, acc: 0.9531, auc: 0.9993, precision: 1.0, recall: 0.9155\n",
      "2018-12-26T18:26:52.657286, step: 1381, loss: 0.024883344769477844, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:26:52.805256, step: 1382, loss: 0.04375665634870529, acc: 0.9922, auc: 1.0, precision: 0.9844, recall: 1.0\n",
      "2018-12-26T18:26:52.965075, step: 1383, loss: 0.07165955752134323, acc: 0.9844, auc: 0.999, precision: 0.9726, recall: 1.0\n",
      "2018-12-26T18:26:53.123276, step: 1384, loss: 0.049944035708904266, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:26:53.276930, step: 1385, loss: 0.045176975429058075, acc: 0.9844, auc: 1.0, precision: 0.9677, recall: 1.0\n",
      "2018-12-26T18:26:53.428887, step: 1386, loss: 0.03461602330207825, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:26:53.581375, step: 1387, loss: 0.02373182214796543, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:26:53.733286, step: 1388, loss: 0.05037880688905716, acc: 0.9688, auc: 0.9995, precision: 1.0, recall: 0.9412\n",
      "2018-12-26T18:26:53.899745, step: 1389, loss: 0.07203961908817291, acc: 0.9609, auc: 1.0, precision: 1.0, recall: 0.9242\n",
      "2018-12-26T18:26:54.049492, step: 1390, loss: 0.07631385326385498, acc: 0.9609, auc: 1.0, precision: 1.0, recall: 0.9286\n",
      "2018-12-26T18:26:54.204426, step: 1391, loss: 0.06807640194892883, acc: 0.9844, auc: 0.998, precision: 1.0, recall: 0.9677\n",
      "2018-12-26T18:26:54.357969, step: 1392, loss: 0.02036316506564617, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9857\n",
      "2018-12-26T18:26:54.510558, step: 1393, loss: 0.05060185119509697, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:26:54.662526, step: 1394, loss: 0.030056916177272797, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:26:54.822281, step: 1395, loss: 0.04820527881383896, acc: 0.9922, auc: 1.0, precision: 0.9828, recall: 1.0\n",
      "start training model\n",
      "2018-12-26T18:26:55.011047, step: 1396, loss: 0.024081896990537643, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:26:55.164014, step: 1397, loss: 0.0365840420126915, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:26:55.317989, step: 1398, loss: 0.02271220088005066, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:26:55.471029, step: 1399, loss: 0.019444510340690613, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:26:55.628392, step: 1400, loss: 0.02784208580851555, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9839\n",
      "\n",
      "Evaluation:\n",
      "2018-12-26T18:27:01.478017, step: 1400, loss: 0.40780584122005265, acc: 0.8458052631578945, auc: 0.9331499999999999, precision: 0.9022815789473682, recall: 0.7795447368421055\n",
      "Saved model checkpoint to ../model/textCNN/model/my-model-1400\n",
      "\n",
      "2018-12-26T18:27:01.920845, step: 1401, loss: 0.037825580686330795, acc: 0.9922, auc: 0.9997, precision: 1.0, recall: 0.9815\n",
      "2018-12-26T18:27:02.075195, step: 1402, loss: 0.025313492864370346, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9836\n",
      "2018-12-26T18:27:02.228251, step: 1403, loss: 0.024368999525904655, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:27:02.382899, step: 1404, loss: 0.04611949995160103, acc: 0.9844, auc: 0.999, precision: 1.0, recall: 0.9667\n",
      "2018-12-26T18:27:02.536446, step: 1405, loss: 0.015450093895196915, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:27:02.687569, step: 1406, loss: 0.04415697976946831, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:27:02.847554, step: 1407, loss: 0.01265160646289587, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:27:03.000468, step: 1408, loss: 0.02392910048365593, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.973\n",
      "2018-12-26T18:27:03.156678, step: 1409, loss: 0.03774278610944748, acc: 0.9922, auc: 0.9993, precision: 1.0, recall: 0.9848\n",
      "2018-12-26T18:27:03.314742, step: 1410, loss: 0.02976316399872303, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:27:03.476333, step: 1411, loss: 0.027509121224284172, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:27:03.629127, step: 1412, loss: 0.017374815419316292, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:27:03.780930, step: 1413, loss: 0.017121365293860435, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:27:03.931431, step: 1414, loss: 0.024618204683065414, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:27:04.081830, step: 1415, loss: 0.024811724200844765, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:27:04.233739, step: 1416, loss: 0.031305283308029175, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:27:04.384256, step: 1417, loss: 0.026053782552480698, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9846\n",
      "2018-12-26T18:27:04.537958, step: 1418, loss: 0.01701781153678894, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:27:04.691283, step: 1419, loss: 0.026797866448760033, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:27:04.851653, step: 1420, loss: 0.027819780632853508, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:27:05.005076, step: 1421, loss: 0.017177265137434006, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:27:05.159202, step: 1422, loss: 0.02416316792368889, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9844\n",
      "2018-12-26T18:27:05.322851, step: 1423, loss: 0.02018246427178383, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-26T18:27:05.486299, step: 1424, loss: 0.02464735321700573, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:27:05.639290, step: 1425, loss: 0.02520758844912052, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:27:05.800130, step: 1426, loss: 0.021770808845758438, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:27:05.954624, step: 1427, loss: 0.02049967087805271, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:27:06.107507, step: 1428, loss: 0.045311495661735535, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:27:06.260751, step: 1429, loss: 0.028551999479532242, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9718\n",
      "2018-12-26T18:27:06.414026, step: 1430, loss: 0.02290562354028225, acc: 0.9922, auc: 1.0, precision: 0.9867, recall: 1.0\n",
      "2018-12-26T18:27:06.568088, step: 1431, loss: 0.021933987736701965, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:27:06.721246, step: 1432, loss: 0.018994836136698723, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:27:06.875173, step: 1433, loss: 0.02629590965807438, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9844\n",
      "2018-12-26T18:27:07.027199, step: 1434, loss: 0.028245093300938606, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9615\n",
      "2018-12-26T18:27:07.194517, step: 1435, loss: 0.02433595061302185, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9851\n",
      "2018-12-26T18:27:07.347871, step: 1436, loss: 0.010249251499772072, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:27:07.507623, step: 1437, loss: 0.01925315521657467, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:27:07.663103, step: 1438, loss: 0.025082044303417206, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9839\n",
      "2018-12-26T18:27:07.819862, step: 1439, loss: 0.02678602561354637, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9818\n",
      "2018-12-26T18:27:07.971156, step: 1440, loss: 0.02393317222595215, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:27:08.127713, step: 1441, loss: 0.04011646658182144, acc: 0.9844, auc: 0.9998, precision: 0.9839, recall: 0.9839\n",
      "2018-12-26T18:27:08.284306, step: 1442, loss: 0.022375838831067085, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:27:08.437088, step: 1443, loss: 0.018206549808382988, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9831\n",
      "2018-12-26T18:27:08.598312, step: 1444, loss: 0.030706174671649933, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9857\n",
      "2018-12-26T18:27:08.747357, step: 1445, loss: 0.02761877328157425, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9836\n",
      "2018-12-26T18:27:08.899411, step: 1446, loss: 0.020328430458903313, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:27:09.053870, step: 1447, loss: 0.03945428878068924, acc: 0.9922, auc: 0.9998, precision: 1.0, recall: 0.9841\n",
      "2018-12-26T18:27:09.207326, step: 1448, loss: 0.03458022326231003, acc: 0.9844, auc: 0.9998, precision: 1.0, recall: 0.9643\n",
      "2018-12-26T18:27:09.362142, step: 1449, loss: 0.03619169071316719, acc: 0.9922, auc: 1.0, precision: 0.9839, recall: 1.0\n",
      "2018-12-26T18:27:09.516251, step: 1450, loss: 0.027723105624318123, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:27:09.667253, step: 1451, loss: 0.01732534170150757, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:27:09.819436, step: 1452, loss: 0.019141346216201782, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:27:09.969896, step: 1453, loss: 0.03734668716788292, acc: 0.9922, auc: 0.9995, precision: 1.0, recall: 0.9841\n",
      "2018-12-26T18:27:10.123662, step: 1454, loss: 0.01517461333423853, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:27:10.286389, step: 1455, loss: 0.017331140115857124, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:27:10.444502, step: 1456, loss: 0.0408577024936676, acc: 0.9844, auc: 0.9993, precision: 1.0, recall: 0.9688\n",
      "2018-12-26T18:27:10.597400, step: 1457, loss: 0.021148942410945892, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9828\n",
      "2018-12-26T18:27:10.754390, step: 1458, loss: 0.016794273629784584, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:27:10.907188, step: 1459, loss: 0.020173843950033188, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9859\n",
      "2018-12-26T18:27:11.063172, step: 1460, loss: 0.025449037551879883, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9815\n",
      "2018-12-26T18:27:11.221069, step: 1461, loss: 0.02211287058889866, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:27:11.469639, step: 1462, loss: 0.03252633661031723, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:27:11.621278, step: 1463, loss: 0.016526207327842712, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:27:11.771022, step: 1464, loss: 0.047787174582481384, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9825\n",
      "2018-12-26T18:27:11.927711, step: 1465, loss: 0.04087015241384506, acc: 0.9766, auc: 1.0, precision: 1.0, recall: 0.9423\n",
      "2018-12-26T18:27:12.067422, step: 1466, loss: 0.01871853694319725, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:27:12.221262, step: 1467, loss: 0.016405316069722176, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:27:12.372575, step: 1468, loss: 0.04053613170981407, acc: 0.9844, auc: 0.9993, precision: 1.0, recall: 0.9701\n",
      "2018-12-26T18:27:12.523967, step: 1469, loss: 0.02957540564239025, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9683\n",
      "2018-12-26T18:27:12.675768, step: 1470, loss: 0.026280853897333145, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:27:12.829367, step: 1471, loss: 0.03455687314271927, acc: 0.9922, auc: 0.9998, precision: 1.0, recall: 0.9833\n",
      "2018-12-26T18:27:12.983378, step: 1472, loss: 0.04821702465415001, acc: 0.9922, auc: 0.9985, precision: 1.0, recall: 0.9868\n",
      "2018-12-26T18:27:13.136007, step: 1473, loss: 0.025923237204551697, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:27:13.293892, step: 1474, loss: 0.026512511074543, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:27:13.446637, step: 1475, loss: 0.019771508872509003, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:27:13.601460, step: 1476, loss: 0.034753598272800446, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:27:13.764762, step: 1477, loss: 0.013895167037844658, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:27:13.922993, step: 1478, loss: 0.022993534803390503, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:27:14.083678, step: 1479, loss: 0.04083952307701111, acc: 0.9766, auc: 0.9998, precision: 1.0, recall: 0.9508\n",
      "2018-12-26T18:27:14.235482, step: 1480, loss: 0.03868243843317032, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:27:14.388927, step: 1481, loss: 0.01873249188065529, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:27:14.552207, step: 1482, loss: 0.022540301084518433, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:27:14.737374, step: 1483, loss: 0.01606256142258644, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:27:14.889863, step: 1484, loss: 0.013857227750122547, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:27:15.046833, step: 1485, loss: 0.062122587114572525, acc: 0.9766, auc: 0.9988, precision: 1.0, recall: 0.9508\n",
      "2018-12-26T18:27:15.199555, step: 1486, loss: 0.028401726856827736, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9855\n",
      "2018-12-26T18:27:15.335011, step: 1487, loss: 0.03194091469049454, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9836\n",
      "2018-12-26T18:27:15.488324, step: 1488, loss: 0.019385095685720444, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:27:15.677622, step: 1489, loss: 0.01869499683380127, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:27:15.831013, step: 1490, loss: 0.04712861403822899, acc: 0.9922, auc: 0.998, precision: 0.9818, recall: 1.0\n",
      "2018-12-26T18:27:15.993949, step: 1491, loss: 0.024545490741729736, acc: 0.9922, auc: 1.0, precision: 0.9841, recall: 1.0\n",
      "2018-12-26T18:27:16.147901, step: 1492, loss: 0.015373099595308304, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:27:16.329705, step: 1493, loss: 0.032770585268735886, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9831\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-26T18:27:16.488419, step: 1494, loss: 0.02260580286383629, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:27:16.641383, step: 1495, loss: 0.04864628612995148, acc: 0.9688, auc: 1.0, precision: 1.0, recall: 0.9437\n",
      "2018-12-26T18:27:16.790607, step: 1496, loss: 0.024386420845985413, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9851\n",
      "2018-12-26T18:27:16.941393, step: 1497, loss: 0.02363758161664009, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:27:17.094155, step: 1498, loss: 0.019330209121108055, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:27:17.246800, step: 1499, loss: 0.03760554641485214, acc: 0.9922, auc: 1.0, precision: 0.9839, recall: 1.0\n",
      "2018-12-26T18:27:17.399463, step: 1500, loss: 0.04160323739051819, acc: 0.9922, auc: 0.9993, precision: 1.0, recall: 0.9844\n",
      "\n",
      "Evaluation:\n",
      "2018-12-26T18:27:23.287043, step: 1500, loss: 0.39950417648804815, acc: 0.861636842105263, auc: 0.9332026315789472, precision: 0.8726105263157896, recall: 0.8503184210526314\n",
      "Saved model checkpoint to ../model/textCNN/model/my-model-1500\n",
      "\n",
      "2018-12-26T18:27:23.718348, step: 1501, loss: 0.02736569568514824, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9859\n",
      "2018-12-26T18:27:23.868879, step: 1502, loss: 0.027235733345150948, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:27:24.021887, step: 1503, loss: 0.02514694258570671, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:27:24.179659, step: 1504, loss: 0.0191121194511652, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:27:24.335137, step: 1505, loss: 0.02648184821009636, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9853\n",
      "2018-12-26T18:27:24.486962, step: 1506, loss: 0.02841743640601635, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9841\n",
      "2018-12-26T18:27:24.647518, step: 1507, loss: 0.026648525148630142, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:27:24.796236, step: 1508, loss: 0.019044535234570503, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:27:24.977707, step: 1509, loss: 0.022150592878460884, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9836\n",
      "2018-12-26T18:27:25.129117, step: 1510, loss: 0.025263847783207893, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9833\n",
      "2018-12-26T18:27:25.281855, step: 1511, loss: 0.031152915209531784, acc: 0.9922, auc: 0.9998, precision: 1.0, recall: 0.9857\n",
      "2018-12-26T18:27:25.434635, step: 1512, loss: 0.03564002737402916, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:27:25.590008, step: 1513, loss: 0.02540702186524868, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:27:25.743622, step: 1514, loss: 0.04460069164633751, acc: 0.9922, auc: 1.0, precision: 0.9815, recall: 1.0\n",
      "2018-12-26T18:27:25.898707, step: 1515, loss: 0.027449969202280045, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9848\n",
      "2018-12-26T18:27:26.052668, step: 1516, loss: 0.05146390199661255, acc: 0.9844, auc: 0.9985, precision: 1.0, recall: 0.9701\n",
      "2018-12-26T18:27:26.207356, step: 1517, loss: 0.017168309539556503, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:27:26.364965, step: 1518, loss: 0.023105600848793983, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:27:26.518025, step: 1519, loss: 0.020131835713982582, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:27:26.672405, step: 1520, loss: 0.022977584972977638, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:27:26.831311, step: 1521, loss: 0.03575969487428665, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9714\n",
      "2018-12-26T18:27:26.985649, step: 1522, loss: 0.022683661431074142, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9857\n",
      "2018-12-26T18:27:27.143767, step: 1523, loss: 0.013846363872289658, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:27:27.296293, step: 1524, loss: 0.027472034096717834, acc: 0.9922, auc: 1.0, precision: 0.9859, recall: 1.0\n",
      "2018-12-26T18:27:27.448783, step: 1525, loss: 0.017582271248102188, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:27:27.602549, step: 1526, loss: 0.023816216737031937, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:27:27.759426, step: 1527, loss: 0.017070354893803596, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:27:27.913237, step: 1528, loss: 0.011384322308003902, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:27:28.071953, step: 1529, loss: 0.017011035233736038, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:27:28.224918, step: 1530, loss: 0.020703395828604698, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9844\n",
      "2018-12-26T18:27:28.377129, step: 1531, loss: 0.02509261853992939, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9848\n",
      "2018-12-26T18:27:28.530940, step: 1532, loss: 0.027706041932106018, acc: 0.9844, auc: 0.9998, precision: 1.0, recall: 0.9701\n",
      "2018-12-26T18:27:28.691323, step: 1533, loss: 0.02203543111681938, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:27:28.848911, step: 1534, loss: 0.014226671308279037, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:27:28.985999, step: 1535, loss: 0.04697468504309654, acc: 0.9844, auc: 0.9998, precision: 0.9697, recall: 1.0\n",
      "2018-12-26T18:27:29.137332, step: 1536, loss: 0.04333993420004845, acc: 0.9922, auc: 1.0, precision: 0.9853, recall: 1.0\n",
      "2018-12-26T18:27:29.289356, step: 1537, loss: 0.01663850247859955, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:27:29.443480, step: 1538, loss: 0.017164111137390137, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:27:29.602187, step: 1539, loss: 0.028864752501249313, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9848\n",
      "2018-12-26T18:27:29.755649, step: 1540, loss: 0.031809136271476746, acc: 0.9766, auc: 1.0, precision: 1.0, recall: 0.9559\n",
      "2018-12-26T18:27:29.910930, step: 1541, loss: 0.021855566650629044, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:27:30.065603, step: 1542, loss: 0.04244719445705414, acc: 0.9922, auc: 1.0, precision: 0.9811, recall: 1.0\n",
      "2018-12-26T18:27:30.216564, step: 1543, loss: 0.027110416442155838, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9831\n",
      "2018-12-26T18:27:30.374812, step: 1544, loss: 0.01429980993270874, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:27:30.534127, step: 1545, loss: 0.02964535355567932, acc: 0.9922, auc: 1.0, precision: 0.9831, recall: 1.0\n",
      "2018-12-26T18:27:30.689354, step: 1546, loss: 0.01984066516160965, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9844\n",
      "2018-12-26T18:27:30.848039, step: 1547, loss: 0.021306848153471947, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:27:31.004122, step: 1548, loss: 0.054936692118644714, acc: 0.9531, auc: 1.0, precision: 1.0, recall: 0.9231\n",
      "2018-12-26T18:27:31.159124, step: 1549, loss: 0.022190988063812256, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-26T18:27:31.312358, step: 1550, loss: 0.016358066350221634, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "INFO:tensorflow:No assets to save.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: b'../model/textCNN/savedModel/saved_model.pb'\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "\n",
    "# 生成训练集和验证集\n",
    "trainReviews = data.trainReviews\n",
    "trainLabels = data.trainLabels\n",
    "evalReviews = data.evalReviews\n",
    "evalLabels = data.evalLabels\n",
    "\n",
    "wordEmbedding = data.wordEmbedding\n",
    "\n",
    "# 定义计算图\n",
    "with tf.Graph().as_default():\n",
    "\n",
    "    session_conf = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
    "    session_conf.gpu_options.allow_growth=True\n",
    "    session_conf.gpu_options.per_process_gpu_memory_fraction = 0.9  # 配置gpu占用率  \n",
    "\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    \n",
    "    # 定义会话\n",
    "    with sess.as_default():\n",
    "        cnn = TextCNN(config, wordEmbedding)\n",
    "        \n",
    "        globalStep = tf.Variable(0, name=\"globalStep\", trainable=False)\n",
    "        # 定义优化函数，传入学习速率参数\n",
    "        optimizer = tf.train.AdamOptimizer(config.training.learningRate)\n",
    "        # 计算梯度,得到梯度和变量\n",
    "        gradsAndVars = optimizer.compute_gradients(cnn.loss)\n",
    "        # 将梯度应用到变量下，生成训练器\n",
    "        trainOp = optimizer.apply_gradients(gradsAndVars, global_step=globalStep)\n",
    "        \n",
    "        # 用summary绘制tensorBoard\n",
    "        gradSummaries = []\n",
    "        for g, v in gradsAndVars:\n",
    "            if g is not None:\n",
    "                tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "        \n",
    "        outDir = os.path.abspath(os.path.join(os.path.curdir, \"summarys\"))\n",
    "        print(\"Writing to {}\\n\".format(outDir))\n",
    "        \n",
    "        lossSummary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "        summaryOp = tf.summary.merge_all()\n",
    "        \n",
    "        trainSummaryDir = os.path.join(outDir, \"train\")\n",
    "        trainSummaryWriter = tf.summary.FileWriter(trainSummaryDir, sess.graph)\n",
    "        \n",
    "        evalSummaryDir = os.path.join(outDir, \"eval\")\n",
    "        evalSummaryWriter = tf.summary.FileWriter(evalSummaryDir, sess.graph)\n",
    "        \n",
    "        \n",
    "        # 初始化所有变量\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=5)\n",
    "        \n",
    "        # 保存模型的一种方式，保存为pb文件\n",
    "        builder = tf.saved_model.builder.SavedModelBuilder(\"../model/textCNN/savedModel\")\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        def trainStep(batchX, batchY):\n",
    "            \"\"\"\n",
    "            训练函数\n",
    "            \"\"\"   \n",
    "            feed_dict = {\n",
    "              cnn.inputX: batchX,\n",
    "              cnn.inputY: batchY,\n",
    "              cnn.dropoutKeepProb: config.model.dropoutKeepProb\n",
    "            }\n",
    "            _, summary, step, loss, predictions, binaryPreds = sess.run(\n",
    "                [trainOp, summaryOp, globalStep, cnn.loss, cnn.predictions, cnn.binaryPreds],\n",
    "                feed_dict)\n",
    "            timeStr = datetime.datetime.now().isoformat()\n",
    "            acc, auc, precision, recall = genMetrics(batchY, predictions, binaryPreds)\n",
    "            print(\"{}, step: {}, loss: {}, acc: {}, auc: {}, precision: {}, recall: {}\".format(timeStr, step, loss, acc, auc, precision, recall))\n",
    "            trainSummaryWriter.add_summary(summary, step)\n",
    "\n",
    "        def devStep(batchX, batchY):\n",
    "            \"\"\"\n",
    "            验证函数\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.inputX: batchX,\n",
    "              cnn.inputY: batchY,\n",
    "              cnn.dropoutKeepProb: 1.0\n",
    "            }\n",
    "            summary, step, loss, predictions, binaryPreds = sess.run(\n",
    "                [summaryOp, globalStep, cnn.loss, cnn.predictions, cnn.binaryPreds],\n",
    "                feed_dict)\n",
    "            \n",
    "            acc, auc, precision, recall = genMetrics(batchY, predictions, binaryPreds)\n",
    "            \n",
    "            evalSummaryWriter.add_summary(summary, step)\n",
    "            \n",
    "            return loss, acc, auc, precision, recall\n",
    "        \n",
    "        for i in range(config.training.epoches):\n",
    "            # 训练模型\n",
    "            print(\"start training model\")\n",
    "            for batchTrain in nextBatch(trainReviews, trainLabels, config.batchSize):\n",
    "                trainStep(batchTrain[0], batchTrain[1])\n",
    "\n",
    "                currentStep = tf.train.global_step(sess, globalStep) \n",
    "                if currentStep % config.training.evaluateEvery == 0:\n",
    "                    print(\"\\nEvaluation:\")\n",
    "                    \n",
    "                    losses = []\n",
    "                    accs = []\n",
    "                    aucs = []\n",
    "                    precisions = []\n",
    "                    recalls = []\n",
    "                    \n",
    "                    for batchEval in nextBatch(evalReviews, evalLabels, config.batchSize):\n",
    "                        loss, acc, auc, precision, recall = devStep(batchEval[0], batchEval[1])\n",
    "                        losses.append(loss)\n",
    "                        accs.append(acc)\n",
    "                        aucs.append(auc)\n",
    "                        precisions.append(precision)\n",
    "                        recalls.append(recall)\n",
    "                        \n",
    "                    time_str = datetime.datetime.now().isoformat()\n",
    "                    print(\"{}, step: {}, loss: {}, acc: {}, auc: {}, precision: {}, recall: {}\".format(time_str, currentStep, mean(losses), \n",
    "                                                                                                       mean(accs), mean(aucs), mean(precisions),\n",
    "                                                                                                       mean(recalls)))\n",
    "                    \n",
    "                if currentStep % config.training.checkpointEvery == 0:\n",
    "                    # 保存模型的另一种方法，保存checkpoint文件\n",
    "                    path = saver.save(sess, \"../model/textCNN/model/my-model\", global_step=currentStep)\n",
    "                    print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "                    \n",
    "        inputs = {\"inputX\": tf.saved_model.utils.build_tensor_info(cnn.inputX),\n",
    "                  \"keepProb\": tf.saved_model.utils.build_tensor_info(cnn.dropoutKeepProb)}\n",
    "\n",
    "        outputs = {\"binaryPreds\": tf.saved_model.utils.build_tensor_info(cnn.binaryPreds)}\n",
    "\n",
    "        prediction_signature = tf.saved_model.signature_def_utils.build_signature_def(inputs=inputs, outputs=outputs,\n",
    "                                                                                      method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME)\n",
    "        legacy_init_op = tf.group(tf.tables_initializer(), name=\"legacy_init_op\")\n",
    "        builder.add_meta_graph_and_variables(sess, [tf.saved_model.tag_constants.SERVING],\n",
    "                                            signature_def_map={\"predict\": prediction_signature}, legacy_init_op=legacy_init_op)\n",
    "\n",
    "        builder.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
