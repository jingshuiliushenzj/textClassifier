{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "from collections import Counter\n",
    "from math import sqrt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "from bilm import TokenBatcher, BidirectionalLanguageModel, weight_layers, dump_token_embeddings, Batcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置参数\n",
    "\n",
    "class TrainingConfig(object):\n",
    "    epoches = 10\n",
    "    evaluateEvery = 100\n",
    "    checkpointEvery = 100\n",
    "    learningRate = 0.001\n",
    "    \n",
    "class ModelConfig(object):\n",
    "    embeddingSize = 256  # 这个值和ELMo的词向量大小一致\n",
    "    \n",
    "    hiddenSizes = [128]  # LSTM结构的神经元个数\n",
    "    \n",
    "    dropoutKeepProb = 0.5\n",
    "    l2RegLambda = 0.0\n",
    "    \n",
    "class Config(object):\n",
    "    sequenceLength = 200  # 取了所有序列长度的均值\n",
    "    batchSize = 128\n",
    "    \n",
    "    dataSource = \"../data/preProcess/labeledTrain.csv\"\n",
    "    \n",
    "    stopWordSource = \"../data/english\"\n",
    "    \n",
    "    optionFile = \"modelParams/elmo_options.json\"\n",
    "    weightFile = \"modelParams/elmo_weights.hdf5\"\n",
    "    vocabFile = \"modelParams/vocab.txt\"\n",
    "    tokenEmbeddingFile = 'modelParams/elmo_token_embeddings.hdf5'\n",
    "    \n",
    "    numClasses = 2\n",
    "    \n",
    "    rate = 0.8  # 训练集的比例\n",
    "    \n",
    "    training = TrainingConfig()\n",
    "    \n",
    "    model = ModelConfig()\n",
    "\n",
    "    \n",
    "# 实例化配置参数对象\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据预处理的类，生成训练集和测试集\n",
    "\n",
    "class Dataset(object):\n",
    "    def __init__(self, config):\n",
    "        self._dataSource = config.dataSource\n",
    "        self._stopWordSource = config.stopWordSource  \n",
    "        self._optionFile = config.optionFile\n",
    "        self._weightFile = config.weightFile\n",
    "        self._vocabFile = config.vocabFile\n",
    "        self._tokenEmbeddingFile = config.tokenEmbeddingFile\n",
    "        \n",
    "        self._sequenceLength = config.sequenceLength  # 每条输入的序列处理为定长\n",
    "        self._embeddingSize = config.model.embeddingSize\n",
    "        self._batchSize = config.batchSize\n",
    "        self._rate = config.rate\n",
    "        \n",
    "        self.trainReviews = []\n",
    "        self.trainLabels = []\n",
    "        \n",
    "        self.evalReviews = []\n",
    "        self.evalLabels = []\n",
    "        \n",
    "    def _readData(self, filePath):\n",
    "        \"\"\"\n",
    "        从csv文件中读取数据集\n",
    "        \"\"\"\n",
    "        \n",
    "        df = pd.read_csv(filePath)\n",
    "        labels = df[\"sentiment\"].tolist()\n",
    "        review = df[\"review\"].tolist()\n",
    "        reviews = [line.strip().split() for line in review]\n",
    "\n",
    "        return reviews, labels\n",
    "    \n",
    "    def _genVocabFile(self, reviews):\n",
    "        \"\"\"\n",
    "        用我们的训练数据生成一个词汇文件，并加入三个特殊字符\n",
    "        \"\"\"\n",
    "        allWords = [word for review in reviews for word in review]\n",
    "        wordCount = Counter(allWords)  # 统计词频\n",
    "        sortWordCount = sorted(wordCount.items(), key=lambda x: x[1], reverse=True)\n",
    "        words = [item[0] for item in sortWordCount.items()]\n",
    "        allTokens = ['<S>', '</S>', '<UNK>'] + words\n",
    "        with open(self._vocabFile, 'w') as fout:\n",
    "            fout.write('\\n'.join(allTokens))\n",
    "    \n",
    "    def _fixedSeq(self, reviews):\n",
    "        \"\"\"\n",
    "        将长度超过200的截断为200的长度\n",
    "        \"\"\"\n",
    "        return [review[:self._sequenceLength] for review in reviews]\n",
    "    \n",
    "    def _genElmoEmbedding(self):\n",
    "        \"\"\"\n",
    "        调用ELMO源码中的dump_token_embeddings方法，基于字符的表示生成词的向量表示。并保存成hdf5文件，文件中的\"embedding\"键对应的value就是\n",
    "        词汇表文件中各词汇的向量表示，这些词汇的向量表示之后会作为BiLM的初始化输入。\n",
    "        \"\"\"\n",
    "        dump_token_embeddings(\n",
    "            self._vocabFile, self._optionFile, self._weightFile, self._tokenEmbeddingFile)\n",
    "\n",
    "    def _genTrainEvalData(self, x, y, rate):\n",
    "        \"\"\"\n",
    "        生成训练集和验证集\n",
    "        \"\"\"\n",
    "        y = [[item] for item in y]\n",
    "        trainIndex = int(len(x) * rate)\n",
    "        \n",
    "        trainReviews = x[:trainIndex]\n",
    "        trainLabels = y[:trainIndex]\n",
    "        \n",
    "        evalReviews = x[trainIndex:]\n",
    "        evalLabels = y[trainIndex:]\n",
    "\n",
    "        return trainReviews, trainLabels, evalReviews, evalLabels\n",
    "        \n",
    "            \n",
    "    def dataGen(self):\n",
    "        \"\"\"\n",
    "        初始化训练集和验证集\n",
    "        \"\"\"\n",
    "        \n",
    "        # 初始化数据集\n",
    "        reviews, labels = self._readData(self._dataSource)\n",
    "        \n",
    "        \n",
    "#         self._genVocabFile(reviews) # 生成vocabFile\n",
    "#         self._genElmoEmbedding()  # 生成elmo_token_embedding\n",
    "        \n",
    "        reviews = self._fixedSeq(reviews)\n",
    "        \n",
    "        # 初始化训练集和测试集\n",
    "        trainReviews, trainLabels, evalReviews, evalLabels = self._genTrainEvalData(reviews, labels, self._rate)\n",
    "        self.trainReviews = trainReviews\n",
    "        self.trainLabels = trainLabels\n",
    "        \n",
    "        self.evalReviews = evalReviews\n",
    "        self.evalLabels = evalLabels\n",
    "                \n",
    "data = Dataset(config)\n",
    "data.dataGen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape: 20000\n",
      "train label shape: 20000\n",
      "eval data shape: 5000\n"
     ]
    }
   ],
   "source": [
    "print(\"train data shape: {}\".format(len(data.trainReviews)))\n",
    "print(\"train label shape: {}\".format(len(data.trainLabels)))\n",
    "print(\"eval data shape: {}\".format(len(data.evalReviews)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出batch数据集\n",
    "def nextBatch(x, y, batchSize):\n",
    "        \"\"\"\n",
    "        生成batch数据集，用生成器的方式输出\n",
    "        \"\"\"\n",
    "        # 每一个epoch时，都要打乱数据集\n",
    "        midVal = list(zip(x, y))\n",
    "        random.shuffle(midVal)\n",
    "        x, y = zip(*midVal)\n",
    "        x = list(x)\n",
    "        y = list(y)\n",
    "\n",
    "        numBatches = len(x) // batchSize\n",
    "\n",
    "        for i in range(numBatches):\n",
    "            start = i * batchSize\n",
    "            end = start + batchSize\n",
    "            batchX =x[start: end]\n",
    "            batchY = y[start: end]\n",
    "\n",
    "            yield batchX, batchY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建模型\n",
    "class BiLSTMAttention(object):\n",
    "    \"\"\"\n",
    "    Text CNN 用于文本分类\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "\n",
    "        # 定义模型的输入\n",
    "        self.inputX = tf.placeholder(tf.float32, [None, config.sequenceLength, config.model.embeddingSize], name=\"inputX\")\n",
    "        self.inputY = tf.placeholder(tf.float32, [None, 1], name=\"inputY\")\n",
    "        \n",
    "        self.dropoutKeepProb = tf.placeholder(tf.float32, name=\"dropoutKeepProb\")\n",
    "        \n",
    "        # 定义l2损失\n",
    "        l2Loss = tf.constant(0.0)\n",
    "        \n",
    "        with tf.name_scope(\"embedding\"):\n",
    "            embeddingW = tf.get_variable(\n",
    "                \"embeddingW\",\n",
    "                shape=[config.model.embeddingSize, config.model.embeddingSize],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "            reshapeInputX = tf.reshape(self.inputX, shape=[-1, config.model.embeddingSize])\n",
    "            \n",
    "            self.embeddedWords = tf.reshape(tf.matmul(reshapeInputX, embeddingW), shape=[-1, config.sequenceLength, config.model.embeddingSize])\n",
    "            self.embeddedWords = tf.nn.dropout(self.embeddedWords, self.dropoutKeepProb)\n",
    "            \n",
    "        # 定义两层双向LSTM的模型结构\n",
    "        with tf.name_scope(\"Bi-LSTM\"):\n",
    "            for idx, hiddenSize in enumerate(config.model.hiddenSizes):\n",
    "                with tf.name_scope(\"Bi-LSTM\" + str(idx)):\n",
    "                    # 定义前向LSTM结构\n",
    "                    lstmFwCell = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(num_units=hiddenSize, state_is_tuple=True),\n",
    "                                                                 output_keep_prob=self.dropoutKeepProb)\n",
    "                    # 定义反向LSTM结构\n",
    "                    lstmBwCell = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(num_units=hiddenSize, state_is_tuple=True),\n",
    "                                                                 output_keep_prob=self.dropoutKeepProb)\n",
    "\n",
    "\n",
    "                    # 采用动态rnn，可以动态的输入序列的长度，若没有输入，则取序列的全长\n",
    "                    # outputs是一个元祖(output_fw, output_bw)，其中两个元素的维度都是[batch_size, max_time, hidden_size],fw和bw的hidden_size一样\n",
    "                    # self.current_state 是最终的状态，二元组(state_fw, state_bw)，state_fw=[batch_size, s]，s是一个元祖(h, c)\n",
    "                    outputs_, self.current_state = tf.nn.bidirectional_dynamic_rnn(lstmFwCell, lstmBwCell, \n",
    "                                                                                  self.embeddedWords, dtype=tf.float32,\n",
    "                                                                                  scope=\"bi-lstm\" + str(idx))\n",
    "        \n",
    "                    # 对outputs中的fw和bw的结果拼接 [batch_size, time_step, hidden_size * 2]\n",
    "                    self.embeddedWords = tf.concat(outputs_, 2)\n",
    "        \n",
    "        outputs = tf.split(self.embeddedWords, 2, -1)\n",
    "        \n",
    "        # 在Bi-LSTM+Attention的论文中，将前向和后向的输出相加\n",
    "        with tf.name_scope(\"Attention\"):\n",
    "            H = outputs[0] + outputs[1]\n",
    "\n",
    "            # 得到Attention的输出\n",
    "            output = self._attention(H)\n",
    "            outputSize = config.model.hiddenSizes[-1]\n",
    "        \n",
    "        # 全连接层的输出\n",
    "        with tf.name_scope(\"output\"):\n",
    "            outputW = tf.get_variable(\n",
    "                \"outputW\",\n",
    "                shape=[outputSize, 1],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "            outputB= tf.Variable(tf.constant(0.1, shape=[1]), name=\"outputB\")\n",
    "            l2Loss += tf.nn.l2_loss(outputW)\n",
    "            l2Loss += tf.nn.l2_loss(outputB)\n",
    "            self.predictions = tf.nn.xw_plus_b(output, outputW, outputB, name=\"predictions\")\n",
    "            self.binaryPreds = tf.cast(tf.greater_equal(self.predictions, 0.5), tf.float32, name=\"binaryPreds\")\n",
    "        \n",
    "        # 计算二元交叉熵损失\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            \n",
    "            losses = tf.nn.sigmoid_cross_entropy_with_logits(logits=self.predictions, labels=self.inputY)\n",
    "            self.loss = tf.reduce_mean(losses) + config.model.l2RegLambda * l2Loss\n",
    "    \n",
    "    def _attention(self, H):\n",
    "        \"\"\"\n",
    "        利用Attention机制得到句子的向量表示\n",
    "        \"\"\"\n",
    "        # 获得最后一层LSTM的神经元数量\n",
    "        hiddenSize = config.model.hiddenSizes[-1]\n",
    "        \n",
    "        # 初始化一个权重向量，是可训练的参数\n",
    "        W = tf.Variable(tf.random_normal([hiddenSize], stddev=0.1))\n",
    "        \n",
    "        # 对Bi-LSTM的输出用激活函数做非线性转换\n",
    "        M = tf.tanh(H)\n",
    "        \n",
    "        # 对W和M做矩阵运算，W=[batch_size, time_step, hidden_size]，计算前做维度转换成[batch_size * time_step, hidden_size]\n",
    "        # newM = [batch_size, time_step, 1]，每一个时间步的输出由向量转换成一个数字\n",
    "        newM = tf.matmul(tf.reshape(M, [-1, hiddenSize]), tf.reshape(W, [-1, 1]))\n",
    "        \n",
    "        # 对newM做维度转换成[batch_size, time_step]\n",
    "        restoreM = tf.reshape(newM, [-1, config.sequenceLength])\n",
    "        \n",
    "        # 用softmax做归一化处理[batch_size, time_step]\n",
    "        self.alpha = tf.nn.softmax(restoreM)\n",
    "        \n",
    "        # 利用求得的alpha的值对H进行加权求和，用矩阵运算直接操作\n",
    "        r = tf.matmul(tf.transpose(H, [0, 2, 1]), tf.reshape(self.alpha, [-1, config.sequenceLength, 1]))\n",
    "        \n",
    "        # 将三维压缩成二维sequeezeR=[batch_size, hidden_size]\n",
    "        sequeezeR = tf.squeeze(r)\n",
    "        \n",
    "        sentenceRepren = tf.tanh(sequeezeR)\n",
    "        \n",
    "        # 对Attention的输出可以做dropout处理\n",
    "        output = tf.nn.dropout(sentenceRepren, self.dropoutKeepProb)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义性能指标函数\n",
    "\n",
    "def mean(item):\n",
    "    return sum(item) / len(item)\n",
    "\n",
    "\n",
    "def genMetrics(trueY, predY, binaryPredY):\n",
    "    \"\"\"\n",
    "    生成acc和auc值\n",
    "    \"\"\"\n",
    "    auc = roc_auc_score(trueY, predY)\n",
    "    accuracy = accuracy_score(trueY, binaryPredY)\n",
    "    precision = precision_score(trueY, binaryPredY)\n",
    "    recall = recall_score(trueY, binaryPredY)\n",
    "    \n",
    "    return round(accuracy, 4), round(auc, 4), round(precision, 4), round(recall, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING SKIP CONNECTIONS\n",
      "INFO:tensorflow:Summary name embeddingW:0/grad/hist is illegal; using embeddingW_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embeddingW:0/grad/sparsity is illegal; using embeddingW_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name bidirectional_rnn/fw/multi_rnn_cell/cell_0/lstm_cell/kernel:0/grad/hist is illegal; using bidirectional_rnn/fw/multi_rnn_cell/cell_0/lstm_cell/kernel_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name bidirectional_rnn/fw/multi_rnn_cell/cell_0/lstm_cell/kernel:0/grad/sparsity is illegal; using bidirectional_rnn/fw/multi_rnn_cell/cell_0/lstm_cell/kernel_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name bidirectional_rnn/fw/multi_rnn_cell/cell_0/lstm_cell/bias:0/grad/hist is illegal; using bidirectional_rnn/fw/multi_rnn_cell/cell_0/lstm_cell/bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name bidirectional_rnn/fw/multi_rnn_cell/cell_0/lstm_cell/bias:0/grad/sparsity is illegal; using bidirectional_rnn/fw/multi_rnn_cell/cell_0/lstm_cell/bias_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name bidirectional_rnn/bw/multi_rnn_cell/cell_0/lstm_cell/kernel:0/grad/hist is illegal; using bidirectional_rnn/bw/multi_rnn_cell/cell_0/lstm_cell/kernel_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name bidirectional_rnn/bw/multi_rnn_cell/cell_0/lstm_cell/kernel:0/grad/sparsity is illegal; using bidirectional_rnn/bw/multi_rnn_cell/cell_0/lstm_cell/kernel_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name bidirectional_rnn/bw/multi_rnn_cell/cell_0/lstm_cell/bias:0/grad/hist is illegal; using bidirectional_rnn/bw/multi_rnn_cell/cell_0/lstm_cell/bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name bidirectional_rnn/bw/multi_rnn_cell/cell_0/lstm_cell/bias:0/grad/sparsity is illegal; using bidirectional_rnn/bw/multi_rnn_cell/cell_0/lstm_cell/bias_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name Attention/Variable:0/grad/hist is illegal; using Attention/Variable_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name Attention/Variable:0/grad/sparsity is illegal; using Attention/Variable_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name outputW:0/grad/hist is illegal; using outputW_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name outputW:0/grad/sparsity is illegal; using outputW_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/outputB:0/grad/hist is illegal; using output/outputB_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/outputB:0/grad/sparsity is illegal; using output/outputB_0/grad/sparsity instead.\n",
      "Writing to /data4T/share/jiangxinyang848/textClassifier/ELMo/summarys\n",
      "\n",
      "start training model\n",
      "2019-01-07T19:30:44.103310, step: 1, loss: 0.7134121060371399, acc: 0.4141, auc: 0.4008, precision: 0.25, recall: 0.0282\n",
      "2019-01-07T19:30:45.840655, step: 2, loss: 0.6727493405342102, acc: 0.4609, auc: 0.6672, precision: 0.6667, recall: 0.0286\n",
      "2019-01-07T19:30:48.112047, step: 3, loss: 0.6814926862716675, acc: 0.5625, auc: 0.6169, precision: 0.6429, recall: 0.2812\n",
      "2019-01-07T19:30:50.343944, step: 4, loss: 0.6925935745239258, acc: 0.5156, auc: 0.5422, precision: 0.8, recall: 0.1176\n",
      "2019-01-07T19:30:52.377307, step: 5, loss: 0.6441318988800049, acc: 0.6172, auc: 0.7096, precision: 0.7188, recall: 0.3651\n",
      "2019-01-07T19:30:53.944997, step: 6, loss: 0.6385297775268555, acc: 0.5234, auc: 0.6887, precision: 0.6522, recall: 0.2206\n",
      "2019-01-07T19:30:55.613486, step: 7, loss: 0.6117496490478516, acc: 0.6328, auc: 0.7463, precision: 0.8095, recall: 0.2833\n",
      "2019-01-07T19:30:57.356193, step: 8, loss: 0.6429930925369263, acc: 0.5547, auc: 0.6879, precision: 0.75, recall: 0.1846\n",
      "2019-01-07T19:30:59.446539, step: 9, loss: 0.6276957988739014, acc: 0.6562, auc: 0.6868, precision: 0.7143, recall: 0.283\n",
      "2019-01-07T19:31:01.120099, step: 10, loss: 0.5971968770027161, acc: 0.6094, auc: 0.7698, precision: 0.913, recall: 0.3043\n",
      "2019-01-07T19:31:02.862705, step: 11, loss: 0.6013011336326599, acc: 0.5781, auc: 0.7386, precision: 0.7143, recall: 0.4167\n",
      "2019-01-07T19:31:04.558692, step: 12, loss: 0.6640322804450989, acc: 0.6719, auc: 0.7458, precision: 0.6232, recall: 0.7288\n",
      "2019-01-07T19:31:06.687856, step: 13, loss: 0.5645319223403931, acc: 0.6797, auc: 0.7851, precision: 0.8158, recall: 0.4769\n",
      "2019-01-07T19:31:08.483886, step: 14, loss: 0.5591650009155273, acc: 0.625, auc: 0.8223, precision: 0.9167, recall: 0.3235\n",
      "2019-01-07T19:31:10.152911, step: 15, loss: 0.6143022775650024, acc: 0.6328, auc: 0.7279, precision: 0.7941, recall: 0.403\n",
      "2019-01-07T19:31:11.833254, step: 16, loss: 0.5345460176467896, acc: 0.7109, auc: 0.8215, precision: 0.78, recall: 0.6\n",
      "2019-01-07T19:31:13.475267, step: 17, loss: 0.6445561051368713, acc: 0.6875, auc: 0.7254, precision: 0.6957, recall: 0.5517\n",
      "2019-01-07T19:31:15.053048, step: 18, loss: 0.5732543468475342, acc: 0.6562, auc: 0.7654, precision: 0.7105, recall: 0.45\n",
      "2019-01-07T19:31:16.731227, step: 19, loss: 0.539820671081543, acc: 0.7031, auc: 0.8132, precision: 0.9412, recall: 0.4706\n",
      "2019-01-07T19:31:18.357023, step: 20, loss: 0.536496639251709, acc: 0.75, auc: 0.801, precision: 0.8333, recall: 0.5833\n",
      "2019-01-07T19:31:20.532388, step: 21, loss: 0.5567790865898132, acc: 0.6719, auc: 0.7852, precision: 0.7778, recall: 0.5224\n",
      "2019-01-07T19:31:22.266744, step: 22, loss: 0.5088480710983276, acc: 0.7422, auc: 0.8336, precision: 0.8113, recall: 0.6515\n",
      "2019-01-07T19:31:23.877795, step: 23, loss: 0.5307449102401733, acc: 0.7266, auc: 0.8248, precision: 0.7826, recall: 0.5902\n",
      "2019-01-07T19:31:26.035213, step: 24, loss: 0.494819700717926, acc: 0.7109, auc: 0.8534, precision: 0.7949, recall: 0.5167\n",
      "2019-01-07T19:31:28.186081, step: 25, loss: 0.44985899329185486, acc: 0.7812, auc: 0.8801, precision: 0.9111, recall: 0.6308\n",
      "2019-01-07T19:31:29.737763, step: 26, loss: 0.5554978251457214, acc: 0.7188, auc: 0.7941, precision: 0.7647, recall: 0.7222\n",
      "2019-01-07T19:31:31.671431, step: 27, loss: 0.6268795132637024, acc: 0.7188, auc: 0.8434, precision: 0.6437, recall: 0.918\n",
      "2019-01-07T19:31:33.690016, step: 28, loss: 0.5583466291427612, acc: 0.6953, auc: 0.7856, precision: 0.7551, recall: 0.5781\n",
      "2019-01-07T19:31:35.335192, step: 29, loss: 0.46041861176490784, acc: 0.7734, auc: 0.8846, precision: 0.9375, recall: 0.5263\n",
      "2019-01-07T19:31:37.013196, step: 30, loss: 0.6724156737327576, acc: 0.6406, auc: 0.8339, precision: 0.9655, recall: 0.3836\n",
      "2019-01-07T19:31:38.649638, step: 31, loss: 0.5592218637466431, acc: 0.6875, auc: 0.7932, precision: 0.8293, recall: 0.5075\n",
      "2019-01-07T19:31:40.505886, step: 32, loss: 0.5452408790588379, acc: 0.7656, auc: 0.847, precision: 0.7353, recall: 0.8065\n",
      "2019-01-07T19:31:42.130714, step: 33, loss: 0.6909370422363281, acc: 0.6641, auc: 0.7363, precision: 0.6197, recall: 0.7333\n",
      "2019-01-07T19:31:43.848803, step: 34, loss: 0.5199342966079712, acc: 0.7812, auc: 0.8254, precision: 0.8056, recall: 0.8056\n",
      "2019-01-07T19:31:45.523746, step: 35, loss: 0.5562877655029297, acc: 0.7578, auc: 0.8144, precision: 0.76, recall: 0.6667\n",
      "2019-01-07T19:31:47.183012, step: 36, loss: 0.5353860855102539, acc: 0.6875, auc: 0.8204, precision: 0.875, recall: 0.5\n",
      "2019-01-07T19:31:48.956846, step: 37, loss: 0.53525710105896, acc: 0.6641, auc: 0.8131, precision: 0.7619, recall: 0.2963\n",
      "2019-01-07T19:31:50.802639, step: 38, loss: 0.49173006415367126, acc: 0.7031, auc: 0.8977, precision: 1.0, recall: 0.4154\n",
      "2019-01-07T19:31:52.564361, step: 39, loss: 0.5446023941040039, acc: 0.6562, auc: 0.8252, precision: 0.92, recall: 0.3538\n",
      "2019-01-07T19:31:54.204938, step: 40, loss: 0.5855516791343689, acc: 0.5781, auc: 0.7692, precision: 0.775, recall: 0.4079\n",
      "2019-01-07T19:31:56.453235, step: 41, loss: 0.5418710708618164, acc: 0.7578, auc: 0.8445, precision: 0.7119, recall: 0.75\n",
      "2019-01-07T19:31:58.111658, step: 42, loss: 0.5575596690177917, acc: 0.6953, auc: 0.8085, precision: 0.6857, recall: 0.7385\n",
      "2019-01-07T19:31:59.775445, step: 43, loss: 0.5077488422393799, acc: 0.7812, auc: 0.8467, precision: 0.8333, recall: 0.7031\n",
      "2019-01-07T19:32:01.539407, step: 44, loss: 0.5585774779319763, acc: 0.7578, auc: 0.8193, precision: 0.7742, recall: 0.7385\n",
      "2019-01-07T19:32:03.164168, step: 45, loss: 0.5663617849349976, acc: 0.6953, auc: 0.7758, precision: 0.825, recall: 0.5077\n",
      "2019-01-07T19:32:04.858744, step: 46, loss: 0.4662686884403229, acc: 0.75, auc: 0.8826, precision: 0.8636, recall: 0.5938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-07T19:32:07.026175, step: 47, loss: 0.5496591329574585, acc: 0.6484, auc: 0.8222, precision: 0.8529, recall: 0.4203\n",
      "2019-01-07T19:32:08.576589, step: 48, loss: 0.5208593606948853, acc: 0.6875, auc: 0.8337, precision: 0.8571, recall: 0.4615\n",
      "2019-01-07T19:32:10.289107, step: 49, loss: 0.48199906945228577, acc: 0.7578, auc: 0.8461, precision: 0.8421, recall: 0.5614\n",
      "2019-01-07T19:32:11.984827, step: 50, loss: 0.5084757804870605, acc: 0.7109, auc: 0.8352, precision: 0.85, recall: 0.5231\n",
      "2019-01-07T19:32:13.665589, step: 51, loss: 0.48681581020355225, acc: 0.7969, auc: 0.862, precision: 0.8372, recall: 0.6545\n",
      "2019-01-07T19:32:15.303584, step: 52, loss: 0.5239123106002808, acc: 0.6875, auc: 0.8213, precision: 0.6667, recall: 0.5965\n",
      "2019-01-07T19:32:16.920389, step: 53, loss: 0.5375458598136902, acc: 0.7031, auc: 0.8189, precision: 0.7679, recall: 0.6324\n",
      "2019-01-07T19:32:18.660985, step: 54, loss: 0.46002197265625, acc: 0.7734, auc: 0.8698, precision: 0.7857, recall: 0.7213\n",
      "2019-01-07T19:32:20.832936, step: 55, loss: 0.5069010257720947, acc: 0.7266, auc: 0.8453, precision: 0.8649, recall: 0.5161\n",
      "2019-01-07T19:32:22.582887, step: 56, loss: 0.4802490472793579, acc: 0.7344, auc: 0.8759, precision: 0.9429, recall: 0.5077\n",
      "2019-01-07T19:32:24.249954, step: 57, loss: 0.5224007368087769, acc: 0.7109, auc: 0.8172, precision: 0.8367, recall: 0.5857\n",
      "2019-01-07T19:32:25.909066, step: 58, loss: 0.5681714415550232, acc: 0.7422, auc: 0.8033, precision: 0.8148, recall: 0.6567\n",
      "2019-01-07T19:32:27.557850, step: 59, loss: 0.516281247138977, acc: 0.7656, auc: 0.8365, precision: 0.8077, recall: 0.6774\n",
      "2019-01-07T19:32:29.280844, step: 60, loss: 0.43034234642982483, acc: 0.7656, auc: 0.8868, precision: 0.8361, recall: 0.7183\n",
      "2019-01-07T19:32:30.982675, step: 61, loss: 0.543235182762146, acc: 0.7188, auc: 0.8202, precision: 0.7115, recall: 0.6379\n",
      "2019-01-07T19:32:32.661821, step: 62, loss: 0.4352260231971741, acc: 0.7734, auc: 0.8837, precision: 0.9074, recall: 0.6712\n",
      "2019-01-07T19:32:34.446050, step: 63, loss: 0.583091676235199, acc: 0.6719, auc: 0.7749, precision: 0.766, recall: 0.5373\n",
      "2019-01-07T19:32:36.145673, step: 64, loss: 0.3853971064090729, acc: 0.8125, auc: 0.9234, precision: 0.9273, recall: 0.7183\n",
      "2019-01-07T19:32:37.908909, step: 65, loss: 0.430711030960083, acc: 0.8047, auc: 0.8879, precision: 0.8679, recall: 0.7188\n",
      "2019-01-07T19:32:39.684155, step: 66, loss: 0.4782486855983734, acc: 0.7812, auc: 0.8608, precision: 0.74, recall: 0.7115\n",
      "2019-01-07T19:32:41.332324, step: 67, loss: 0.5565792322158813, acc: 0.7109, auc: 0.792, precision: 0.6957, recall: 0.5818\n",
      "2019-01-07T19:32:43.007755, step: 68, loss: 0.49217069149017334, acc: 0.7344, auc: 0.8508, precision: 0.8485, recall: 0.4912\n",
      "2019-01-07T19:32:44.653910, step: 69, loss: 0.5520052909851074, acc: 0.7188, auc: 0.7983, precision: 0.8235, recall: 0.4828\n",
      "2019-01-07T19:32:46.268341, step: 70, loss: 0.5514218807220459, acc: 0.7266, auc: 0.8045, precision: 0.7755, recall: 0.6129\n",
      "2019-01-07T19:32:47.948682, step: 71, loss: 0.4125455617904663, acc: 0.8047, auc: 0.9006, precision: 0.88, recall: 0.6984\n",
      "2019-01-07T19:32:49.642762, step: 72, loss: 0.37447160482406616, acc: 0.8594, auc: 0.9276, precision: 0.8958, recall: 0.7679\n",
      "2019-01-07T19:32:51.399609, step: 73, loss: 0.5238070487976074, acc: 0.7578, auc: 0.8149, precision: 0.8148, recall: 0.6769\n",
      "2019-01-07T19:32:53.029402, step: 74, loss: 0.4359983205795288, acc: 0.8125, auc: 0.8873, precision: 0.7966, recall: 0.7966\n",
      "2019-01-07T19:32:54.836949, step: 75, loss: 0.5121653079986572, acc: 0.7891, auc: 0.8444, precision: 0.7969, recall: 0.7846\n",
      "2019-01-07T19:32:56.467837, step: 76, loss: 0.46949195861816406, acc: 0.7266, auc: 0.8713, precision: 0.8571, recall: 0.5538\n",
      "2019-01-07T19:32:58.078320, step: 77, loss: 0.5245318412780762, acc: 0.7266, auc: 0.8606, precision: 0.9512, recall: 0.5417\n",
      "2019-01-07T19:32:59.740780, step: 78, loss: 0.48832032084465027, acc: 0.7188, auc: 0.8515, precision: 0.8889, recall: 0.6154\n",
      "2019-01-07T19:33:01.360214, step: 79, loss: 0.4875852167606354, acc: 0.75, auc: 0.8689, precision: 0.7581, recall: 0.7344\n",
      "2019-01-07T19:33:03.224016, step: 80, loss: 0.5483386516571045, acc: 0.7734, auc: 0.8011, precision: 0.8082, recall: 0.7973\n",
      "2019-01-07T19:33:05.075898, step: 81, loss: 0.5073555111885071, acc: 0.7734, auc: 0.8672, precision: 0.75, recall: 0.8308\n",
      "2019-01-07T19:33:06.689390, step: 82, loss: 0.4575634002685547, acc: 0.8203, auc: 0.875, precision: 0.85, recall: 0.7846\n",
      "2019-01-07T19:33:08.415627, step: 83, loss: 0.5159865021705627, acc: 0.7422, auc: 0.817, precision: 0.8537, recall: 0.5645\n",
      "2019-01-07T19:33:10.050117, step: 84, loss: 0.5472917556762695, acc: 0.6641, auc: 0.817, precision: 0.825, recall: 0.4783\n",
      "2019-01-07T19:33:12.057663, step: 85, loss: 0.4867143929004669, acc: 0.6953, auc: 0.8514, precision: 0.8125, recall: 0.4407\n",
      "2019-01-07T19:33:14.136505, step: 86, loss: 0.4402931034564972, acc: 0.7734, auc: 0.8864, precision: 0.8913, recall: 0.6308\n",
      "2019-01-07T19:33:16.337441, step: 87, loss: 0.4593181610107422, acc: 0.7969, auc: 0.8787, precision: 0.8519, recall: 0.7188\n",
      "2019-01-07T19:33:17.954078, step: 88, loss: 0.5013104677200317, acc: 0.7578, auc: 0.833, precision: 0.7833, recall: 0.7231\n",
      "2019-01-07T19:33:19.604065, step: 89, loss: 0.4535333216190338, acc: 0.8281, auc: 0.8851, precision: 0.8448, recall: 0.7903\n",
      "2019-01-07T19:33:21.161926, step: 90, loss: 0.4692155718803406, acc: 0.7344, auc: 0.86, precision: 0.898, recall: 0.6027\n",
      "2019-01-07T19:33:22.913557, step: 91, loss: 0.34608563780784607, acc: 0.875, auc: 0.948, precision: 0.9016, recall: 0.8462\n",
      "2019-01-07T19:33:24.976051, step: 92, loss: 0.4542086124420166, acc: 0.7656, auc: 0.8728, precision: 0.8182, recall: 0.6923\n",
      "2019-01-07T19:33:26.707106, step: 93, loss: 0.47758370637893677, acc: 0.7578, auc: 0.8598, precision: 0.9091, recall: 0.597\n",
      "2019-01-07T19:33:28.316905, step: 94, loss: 0.404996395111084, acc: 0.8281, auc: 0.9041, precision: 0.9333, recall: 0.6885\n",
      "2019-01-07T19:33:29.923958, step: 95, loss: 0.43245211243629456, acc: 0.8203, auc: 0.8879, precision: 0.8571, recall: 0.7619\n",
      "2019-01-07T19:33:31.631898, step: 96, loss: 0.4859805405139923, acc: 0.7578, auc: 0.8468, precision: 0.8039, recall: 0.6613\n",
      "2019-01-07T19:33:33.325320, step: 97, loss: 0.3991283178329468, acc: 0.8281, auc: 0.9084, precision: 0.8462, recall: 0.7586\n",
      "2019-01-07T19:33:34.937003, step: 98, loss: 0.43064647912979126, acc: 0.75, auc: 0.8789, precision: 0.881, recall: 0.5781\n",
      "2019-01-07T19:33:37.061234, step: 99, loss: 0.498721182346344, acc: 0.7344, auc: 0.8782, precision: 0.9744, recall: 0.5352\n",
      "2019-01-07T19:33:38.707937, step: 100, loss: 0.4111703038215637, acc: 0.7969, auc: 0.8955, precision: 0.8246, recall: 0.746\n",
      "\n",
      "Evaluation:\n",
      "2019-01-07T19:34:48.373451, step: 100, loss: 0.46380538818163747, acc: 0.7864641025641026, auc: 0.8684282051282052, precision: 0.8127410256410256, recall: 0.7498820512820512\n",
      "2019-01-07T19:34:50.084455, step: 101, loss: 0.5485406517982483, acc: 0.7734, auc: 0.8104, precision: 0.8235, recall: 0.6774\n",
      "2019-01-07T19:34:51.742936, step: 102, loss: 0.5002821087837219, acc: 0.7812, auc: 0.8498, precision: 0.7547, recall: 0.7273\n",
      "2019-01-07T19:34:53.401953, step: 103, loss: 0.48142021894454956, acc: 0.7656, auc: 0.8511, precision: 0.7778, recall: 0.6364\n",
      "2019-01-07T19:34:55.120805, step: 104, loss: 0.5169845223426819, acc: 0.7344, auc: 0.8426, precision: 0.8182, recall: 0.5806\n",
      "2019-01-07T19:34:56.796086, step: 105, loss: 0.41538357734680176, acc: 0.7734, auc: 0.9009, precision: 0.907, recall: 0.6094\n",
      "2019-01-07T19:34:59.031363, step: 106, loss: 0.5034124851226807, acc: 0.75, auc: 0.8373, precision: 0.7869, recall: 0.7164\n",
      "2019-01-07T19:35:00.685107, step: 107, loss: 0.47684168815612793, acc: 0.8047, auc: 0.8809, precision: 0.7636, recall: 0.7778\n",
      "2019-01-07T19:35:02.317371, step: 108, loss: 0.3656690716743469, acc: 0.8516, auc: 0.933, precision: 0.8545, recall: 0.8103\n",
      "2019-01-07T19:35:03.994520, step: 109, loss: 0.4403175711631775, acc: 0.8047, auc: 0.885, precision: 0.94, recall: 0.6812\n",
      "2019-01-07T19:35:05.600834, step: 110, loss: 0.5035355687141418, acc: 0.7266, auc: 0.8327, precision: 0.7857, recall: 0.5593\n",
      "2019-01-07T19:35:07.308702, step: 111, loss: 0.42317476868629456, acc: 0.7734, auc: 0.8926, precision: 0.9048, recall: 0.6032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-07T19:35:08.971647, step: 112, loss: 0.41812634468078613, acc: 0.8203, auc: 0.8955, precision: 0.9348, recall: 0.6825\n",
      "2019-01-07T19:35:10.640816, step: 113, loss: 0.458964079618454, acc: 0.7969, auc: 0.8784, precision: 0.8269, recall: 0.7167\n",
      "2019-01-07T19:35:12.372822, step: 114, loss: 0.46779465675354004, acc: 0.7656, auc: 0.8616, precision: 0.7692, recall: 0.6897\n",
      "2019-01-07T19:35:14.035467, step: 115, loss: 0.4172801077365875, acc: 0.7812, auc: 0.8906, precision: 0.8462, recall: 0.6875\n",
      "2019-01-07T19:35:15.751887, step: 116, loss: 0.42990991473197937, acc: 0.7656, auc: 0.8821, precision: 0.7885, recall: 0.6833\n",
      "2019-01-07T19:35:17.449226, step: 117, loss: 0.4330533444881439, acc: 0.7891, auc: 0.8799, precision: 0.8333, recall: 0.678\n",
      "2019-01-07T19:35:19.154455, step: 118, loss: 0.5181000232696533, acc: 0.7344, auc: 0.8367, precision: 0.8627, recall: 0.6197\n",
      "2019-01-07T19:35:21.483591, step: 119, loss: 0.4686945378780365, acc: 0.7969, auc: 0.8554, precision: 0.871, recall: 0.75\n",
      "2019-01-07T19:35:23.199128, step: 120, loss: 0.5023648738861084, acc: 0.7656, auc: 0.8352, precision: 0.8596, recall: 0.6901\n",
      "2019-01-07T19:35:24.867970, step: 121, loss: 0.42691218852996826, acc: 0.8438, auc: 0.904, precision: 0.8267, recall: 0.8986\n",
      "2019-01-07T19:35:26.595310, step: 122, loss: 0.41164690256118774, acc: 0.8203, auc: 0.8912, precision: 0.8955, recall: 0.7895\n",
      "2019-01-07T19:35:28.304807, step: 123, loss: 0.4054749011993408, acc: 0.8203, auc: 0.9101, precision: 0.8286, recall: 0.8406\n",
      "2019-01-07T19:35:30.052218, step: 124, loss: 0.5166648626327515, acc: 0.7656, auc: 0.8355, precision: 0.82, recall: 0.6613\n",
      "2019-01-07T19:35:32.308042, step: 125, loss: 0.4809924066066742, acc: 0.7578, auc: 0.8557, precision: 0.7451, recall: 0.6786\n",
      "2019-01-07T19:35:34.056321, step: 126, loss: 0.43004098534584045, acc: 0.7578, auc: 0.8858, precision: 0.8611, recall: 0.5439\n",
      "2019-01-07T19:35:36.349199, step: 127, loss: 0.44279980659484863, acc: 0.7188, auc: 0.9109, precision: 0.9355, recall: 0.4603\n",
      "2019-01-07T19:35:38.090461, step: 128, loss: 0.49798476696014404, acc: 0.7266, auc: 0.8331, precision: 0.8065, recall: 0.463\n",
      "2019-01-07T19:35:39.650368, step: 129, loss: 0.4622749090194702, acc: 0.7656, auc: 0.8631, precision: 0.8636, recall: 0.6129\n",
      "2019-01-07T19:35:41.438032, step: 130, loss: 0.3976336419582367, acc: 0.8438, auc: 0.9071, precision: 0.8621, recall: 0.8065\n",
      "2019-01-07T19:35:43.138044, step: 131, loss: 0.45756107568740845, acc: 0.8047, auc: 0.8849, precision: 0.8033, recall: 0.7903\n",
      "2019-01-07T19:35:45.143880, step: 132, loss: 0.49617359042167664, acc: 0.7656, auc: 0.8481, precision: 0.7818, recall: 0.7049\n",
      "2019-01-07T19:35:46.857555, step: 133, loss: 0.47054523229599, acc: 0.7188, auc: 0.8622, precision: 0.8571, recall: 0.5455\n",
      "2019-01-07T19:35:48.971190, step: 134, loss: 0.4393419325351715, acc: 0.7422, auc: 0.8848, precision: 0.8571, recall: 0.6176\n",
      "2019-01-07T19:35:50.649189, step: 135, loss: 0.4992716610431671, acc: 0.7578, auc: 0.8457, precision: 0.8958, recall: 0.6232\n",
      "2019-01-07T19:35:52.502350, step: 136, loss: 0.46040111780166626, acc: 0.7812, auc: 0.8622, precision: 0.88, recall: 0.6667\n",
      "2019-01-07T19:35:54.658096, step: 137, loss: 0.4596490263938904, acc: 0.8359, auc: 0.9088, precision: 0.7812, recall: 0.8772\n",
      "2019-01-07T19:35:56.388552, step: 138, loss: 0.40195387601852417, acc: 0.8203, auc: 0.9116, precision: 0.8226, recall: 0.8095\n",
      "2019-01-07T19:35:58.177502, step: 139, loss: 0.4270061254501343, acc: 0.7734, auc: 0.8817, precision: 0.8772, recall: 0.6944\n",
      "2019-01-07T19:35:59.877274, step: 140, loss: 0.5040615200996399, acc: 0.7656, auc: 0.8315, precision: 0.814, recall: 0.614\n",
      "2019-01-07T19:36:01.560180, step: 141, loss: 0.4512535631656647, acc: 0.7578, auc: 0.8897, precision: 0.9111, recall: 0.6029\n",
      "2019-01-07T19:36:03.864816, step: 142, loss: 0.37658238410949707, acc: 0.8359, auc: 0.9083, precision: 0.8095, recall: 0.7234\n",
      "2019-01-07T19:36:05.941050, step: 143, loss: 0.48047029972076416, acc: 0.7969, auc: 0.8629, precision: 0.9245, recall: 0.6901\n",
      "2019-01-07T19:36:07.675527, step: 144, loss: 0.4155304431915283, acc: 0.8203, auc: 0.895, precision: 0.9524, recall: 0.6557\n",
      "2019-01-07T19:36:09.328786, step: 145, loss: 0.5244355797767639, acc: 0.75, auc: 0.8281, precision: 0.9302, recall: 0.5797\n",
      "2019-01-07T19:36:11.120402, step: 146, loss: 0.48687657713890076, acc: 0.7812, auc: 0.8563, precision: 0.7887, recall: 0.8116\n",
      "2019-01-07T19:36:12.895265, step: 147, loss: 0.592477560043335, acc: 0.7812, auc: 0.8165, precision: 0.7463, recall: 0.8197\n",
      "2019-01-07T19:36:14.567677, step: 148, loss: 0.4295284152030945, acc: 0.8125, auc: 0.8987, precision: 0.7941, recall: 0.8438\n",
      "2019-01-07T19:36:16.210642, step: 149, loss: 0.4330236315727234, acc: 0.7891, auc: 0.8956, precision: 0.7619, recall: 0.8\n",
      "2019-01-07T19:36:18.542847, step: 150, loss: 0.5032787919044495, acc: 0.6953, auc: 0.8398, precision: 0.88, recall: 0.5714\n",
      "2019-01-07T19:36:20.468105, step: 151, loss: 0.43802309036254883, acc: 0.7422, auc: 0.8936, precision: 0.8889, recall: 0.5246\n",
      "2019-01-07T19:36:22.199339, step: 152, loss: 0.3986932933330536, acc: 0.7969, auc: 0.9029, precision: 0.9048, recall: 0.6333\n",
      "2019-01-07T19:36:24.429617, step: 153, loss: 0.4504939317703247, acc: 0.7422, auc: 0.8833, precision: 0.8936, recall: 0.6\n",
      "2019-01-07T19:36:26.481970, step: 154, loss: 0.40867307782173157, acc: 0.8203, auc: 0.8918, precision: 0.825, recall: 0.6735\n",
      "2019-01-07T19:36:28.237116, step: 155, loss: 0.42254874110221863, acc: 0.8047, auc: 0.8914, precision: 0.8269, recall: 0.7288\n",
      "2019-01-07T19:36:30.100726, step: 156, loss: 0.3905501365661621, acc: 0.8359, auc: 0.9091, precision: 0.8852, recall: 0.7941\n",
      "start training model\n",
      "2019-01-07T19:36:31.794168, step: 157, loss: 0.46433794498443604, acc: 0.7656, auc: 0.8666, precision: 0.7857, recall: 0.7097\n",
      "2019-01-07T19:36:33.489695, step: 158, loss: 0.42703554034233093, acc: 0.8281, auc: 0.9018, precision: 0.84, recall: 0.75\n",
      "2019-01-07T19:36:35.151155, step: 159, loss: 0.4375489354133606, acc: 0.7656, auc: 0.8907, precision: 0.9535, recall: 0.5942\n",
      "2019-01-07T19:36:36.830160, step: 160, loss: 0.3627168536186218, acc: 0.8203, auc: 0.9326, precision: 0.9762, recall: 0.6508\n",
      "2019-01-07T19:36:39.087094, step: 161, loss: 0.4594196081161499, acc: 0.7344, auc: 0.8639, precision: 0.85, recall: 0.5484\n",
      "2019-01-07T19:36:41.162336, step: 162, loss: 0.4173692762851715, acc: 0.8047, auc: 0.8937, precision: 0.8936, recall: 0.6774\n",
      "2019-01-07T19:36:43.017065, step: 163, loss: 0.4287112355232239, acc: 0.8281, auc: 0.8921, precision: 0.8333, recall: 0.7407\n",
      "2019-01-07T19:36:44.812444, step: 164, loss: 0.47343209385871887, acc: 0.7891, auc: 0.863, precision: 0.8167, recall: 0.7538\n",
      "2019-01-07T19:36:46.477980, step: 165, loss: 0.36023327708244324, acc: 0.875, auc: 0.9334, precision: 0.8947, recall: 0.8361\n",
      "2019-01-07T19:36:48.135838, step: 166, loss: 0.44493478536605835, acc: 0.7891, auc: 0.8767, precision: 0.8627, recall: 0.6875\n",
      "2019-01-07T19:36:49.834335, step: 167, loss: 0.3316422402858734, acc: 0.8203, auc: 0.9399, precision: 0.907, recall: 0.6724\n",
      "2019-01-07T19:36:51.530092, step: 168, loss: 0.4224399924278259, acc: 0.7812, auc: 0.9017, precision: 0.9697, recall: 0.5424\n",
      "2019-01-07T19:36:53.257858, step: 169, loss: 0.4318024218082428, acc: 0.7656, auc: 0.8926, precision: 0.8571, recall: 0.6462\n",
      "2019-01-07T19:36:55.155624, step: 170, loss: 0.30550143122673035, acc: 0.8594, auc: 0.9482, precision: 0.9787, recall: 0.7302\n",
      "2019-01-07T19:36:57.449387, step: 171, loss: 0.40910691022872925, acc: 0.8281, auc: 0.9066, precision: 0.8125, recall: 0.8387\n",
      "2019-01-07T19:36:59.172883, step: 172, loss: 0.41968491673469543, acc: 0.7656, auc: 0.8863, precision: 0.8261, recall: 0.76\n",
      "2019-01-07T19:37:00.972165, step: 173, loss: 0.3974021077156067, acc: 0.8438, auc: 0.9104, precision: 0.8438, recall: 0.8438\n",
      "2019-01-07T19:37:02.664939, step: 174, loss: 0.41575491428375244, acc: 0.7891, auc: 0.8923, precision: 0.918, recall: 0.7179\n",
      "2019-01-07T19:37:04.478619, step: 175, loss: 0.41802090406417847, acc: 0.7891, auc: 0.8886, precision: 0.8261, recall: 0.6667\n",
      "2019-01-07T19:37:06.143843, step: 176, loss: 0.4494130313396454, acc: 0.7812, auc: 0.8811, precision: 0.8679, recall: 0.6866\n",
      "2019-01-07T19:37:07.850806, step: 177, loss: 0.4546501338481903, acc: 0.7656, auc: 0.8728, precision: 0.8298, recall: 0.6393\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-07T19:37:09.617823, step: 178, loss: 0.4835854172706604, acc: 0.7344, auc: 0.8638, precision: 0.8627, recall: 0.6197\n",
      "2019-01-07T19:37:11.900036, step: 179, loss: 0.484719842672348, acc: 0.7422, auc: 0.8475, precision: 0.7895, recall: 0.6818\n",
      "2019-01-07T19:37:14.172660, step: 180, loss: 0.43855419754981995, acc: 0.8438, auc: 0.8976, precision: 0.8333, recall: 0.9028\n",
      "2019-01-07T19:37:16.182450, step: 181, loss: 0.47969862818717957, acc: 0.7969, auc: 0.8747, precision: 0.7705, recall: 0.7966\n",
      "2019-01-07T19:37:18.407787, step: 182, loss: 0.4235095977783203, acc: 0.7969, auc: 0.8867, precision: 0.8596, recall: 0.7313\n",
      "2019-01-07T19:37:20.112804, step: 183, loss: 0.3972729444503784, acc: 0.7969, auc: 0.9076, precision: 0.9348, recall: 0.6515\n",
      "2019-01-07T19:37:22.152642, step: 184, loss: 0.42737120389938354, acc: 0.7656, auc: 0.8929, precision: 0.8333, recall: 0.6034\n",
      "2019-01-07T19:37:23.920184, step: 185, loss: 0.36068832874298096, acc: 0.8281, auc: 0.9263, precision: 0.9767, recall: 0.6667\n",
      "2019-01-07T19:37:25.652933, step: 186, loss: 0.44491416215896606, acc: 0.7422, auc: 0.8894, precision: 0.9189, recall: 0.5312\n",
      "2019-01-07T19:37:27.418175, step: 187, loss: 0.4750785827636719, acc: 0.8125, auc: 0.8537, precision: 0.9231, recall: 0.7059\n",
      "2019-01-07T19:37:29.202983, step: 188, loss: 0.47092247009277344, acc: 0.7656, auc: 0.8531, precision: 0.8305, recall: 0.7101\n",
      "2019-01-07T19:37:30.942588, step: 189, loss: 0.5294285416603088, acc: 0.7969, auc: 0.8402, precision: 0.7887, recall: 0.8358\n",
      "2019-01-07T19:37:32.784932, step: 190, loss: 0.41292256116867065, acc: 0.8281, auc: 0.9107, precision: 0.8, recall: 0.8955\n",
      "2019-01-07T19:37:34.452736, step: 191, loss: 0.47619903087615967, acc: 0.7812, auc: 0.8668, precision: 0.7639, recall: 0.8333\n",
      "2019-01-07T19:37:36.264040, step: 192, loss: 0.4818323254585266, acc: 0.75, auc: 0.8497, precision: 0.875, recall: 0.5645\n",
      "2019-01-07T19:37:37.903179, step: 193, loss: 0.3275164067745209, acc: 0.7969, auc: 0.9582, precision: 0.9545, recall: 0.6364\n",
      "2019-01-07T19:37:39.728731, step: 194, loss: 0.4492480158805847, acc: 0.6875, auc: 0.8901, precision: 0.8649, recall: 0.4776\n",
      "2019-01-07T19:37:41.529645, step: 195, loss: 0.37292417883872986, acc: 0.8828, auc: 0.9187, precision: 0.9487, recall: 0.74\n",
      "2019-01-07T19:37:43.280037, step: 196, loss: 0.3653481602668762, acc: 0.8125, auc: 0.9342, precision: 0.9412, recall: 0.6957\n",
      "2019-01-07T19:37:45.193959, step: 197, loss: 0.45370590686798096, acc: 0.7969, auc: 0.8686, precision: 0.84, recall: 0.7\n",
      "2019-01-07T19:37:46.901469, step: 198, loss: 0.38149210810661316, acc: 0.8672, auc: 0.9229, precision: 0.8621, recall: 0.8475\n",
      "2019-01-07T19:37:48.647945, step: 199, loss: 0.3523762822151184, acc: 0.7969, auc: 0.9208, precision: 0.8657, recall: 0.7733\n",
      "2019-01-07T19:37:50.434954, step: 200, loss: 0.48011356592178345, acc: 0.7734, auc: 0.8554, precision: 0.7547, recall: 0.7143\n",
      "\n",
      "Evaluation:\n",
      "2019-01-07T19:39:01.502878, step: 200, loss: 0.4057882947799487, acc: 0.8018769230769228, auc: 0.8970051282051283, precision: 0.8609384615384615, recall: 0.7238358974358974\n",
      "2019-01-07T19:39:03.177750, step: 201, loss: 0.4396847188472748, acc: 0.7891, auc: 0.8768, precision: 0.8421, recall: 0.7273\n",
      "2019-01-07T19:39:04.862293, step: 202, loss: 0.4435844421386719, acc: 0.7891, auc: 0.8723, precision: 0.8367, recall: 0.6833\n",
      "2019-01-07T19:39:06.692338, step: 203, loss: 0.42601478099823, acc: 0.7812, auc: 0.907, precision: 0.95, recall: 0.5938\n",
      "2019-01-07T19:39:08.884848, step: 204, loss: 0.4709170162677765, acc: 0.7344, auc: 0.8502, precision: 0.8542, recall: 0.6029\n",
      "2019-01-07T19:39:10.592360, step: 205, loss: 0.3696770966053009, acc: 0.8438, auc: 0.9186, precision: 0.907, recall: 0.7091\n",
      "2019-01-07T19:39:12.893761, step: 206, loss: 0.40738654136657715, acc: 0.8125, auc: 0.9015, precision: 0.875, recall: 0.7424\n",
      "2019-01-07T19:39:14.676497, step: 207, loss: 0.4718349575996399, acc: 0.8047, auc: 0.8805, precision: 0.7705, recall: 0.8103\n",
      "2019-01-07T19:39:16.377207, step: 208, loss: 0.3643414080142975, acc: 0.8516, auc: 0.9184, precision: 0.8704, recall: 0.7966\n",
      "2019-01-07T19:39:18.709146, step: 209, loss: 0.49400556087493896, acc: 0.7734, auc: 0.8522, precision: 0.85, recall: 0.5965\n",
      "2019-01-07T19:39:20.983139, step: 210, loss: 0.5010071992874146, acc: 0.7578, auc: 0.8431, precision: 0.8889, recall: 0.6575\n",
      "2019-01-07T19:39:22.820869, step: 211, loss: 0.35252639651298523, acc: 0.8516, auc: 0.9243, precision: 0.9091, recall: 0.7812\n",
      "2019-01-07T19:39:24.556834, step: 212, loss: 0.41791555285453796, acc: 0.7734, auc: 0.8927, precision: 0.8136, recall: 0.7273\n",
      "2019-01-07T19:39:26.712273, step: 213, loss: 0.47650083899497986, acc: 0.7812, auc: 0.8643, precision: 0.6731, recall: 0.7609\n",
      "2019-01-07T19:39:28.377531, step: 214, loss: 0.44616585969924927, acc: 0.7734, auc: 0.8757, precision: 0.8913, recall: 0.6308\n",
      "2019-01-07T19:39:30.139734, step: 215, loss: 0.44539687037467957, acc: 0.7266, auc: 0.8848, precision: 0.8333, recall: 0.597\n",
      "2019-01-07T19:39:31.838025, step: 216, loss: 0.41305798292160034, acc: 0.7734, auc: 0.9017, precision: 0.8824, recall: 0.6618\n",
      "2019-01-07T19:39:33.564942, step: 217, loss: 0.32036393880844116, acc: 0.8281, auc: 0.9455, precision: 0.9074, recall: 0.7424\n",
      "2019-01-07T19:39:35.262147, step: 218, loss: 0.4222038686275482, acc: 0.8281, auc: 0.8806, precision: 0.8986, recall: 0.8052\n",
      "2019-01-07T19:39:36.921189, step: 219, loss: 0.48615652322769165, acc: 0.7656, auc: 0.8591, precision: 0.7941, recall: 0.7714\n",
      "2019-01-07T19:39:38.576933, step: 220, loss: 0.42206352949142456, acc: 0.8125, auc: 0.9133, precision: 0.7671, recall: 0.8889\n",
      "2019-01-07T19:39:40.295429, step: 221, loss: 0.38929975032806396, acc: 0.8594, auc: 0.9357, precision: 0.8036, recall: 0.8654\n",
      "2019-01-07T19:39:42.370041, step: 222, loss: 0.45164746046066284, acc: 0.7891, auc: 0.8691, precision: 0.8615, recall: 0.7568\n",
      "2019-01-07T19:39:44.139806, step: 223, loss: 0.4249156713485718, acc: 0.7891, auc: 0.8952, precision: 0.8913, recall: 0.6508\n",
      "2019-01-07T19:39:46.417700, step: 224, loss: 0.4219416677951813, acc: 0.8203, auc: 0.8954, precision: 0.9697, recall: 0.5926\n",
      "2019-01-07T19:39:48.642885, step: 225, loss: 0.4805769622325897, acc: 0.7578, auc: 0.8686, precision: 0.9048, recall: 0.5846\n",
      "2019-01-07T19:39:50.520603, step: 226, loss: 0.3814679980278015, acc: 0.7969, auc: 0.9212, precision: 0.9412, recall: 0.6761\n",
      "2019-01-07T19:39:52.952533, step: 227, loss: 0.4265560507774353, acc: 0.7578, auc: 0.8792, precision: 0.8475, recall: 0.6944\n",
      "2019-01-07T19:39:55.078346, step: 228, loss: 0.41700875759124756, acc: 0.8281, auc: 0.8975, precision: 0.807, recall: 0.807\n",
      "2019-01-07T19:39:56.847473, step: 229, loss: 0.38440608978271484, acc: 0.8359, auc: 0.9265, precision: 0.8182, recall: 0.8571\n",
      "2019-01-07T19:39:59.053186, step: 230, loss: 0.4073440432548523, acc: 0.8594, auc: 0.9288, precision: 0.8197, recall: 0.8772\n",
      "2019-01-07T19:40:00.778711, step: 231, loss: 0.2671414315700531, acc: 0.8906, auc: 0.9711, precision: 0.9273, recall: 0.8361\n",
      "2019-01-07T19:40:02.573420, step: 232, loss: 0.3792070150375366, acc: 0.7891, auc: 0.9258, precision: 0.8958, recall: 0.6615\n",
      "2019-01-07T19:40:04.828549, step: 233, loss: 0.4432642459869385, acc: 0.7656, auc: 0.8963, precision: 0.9111, recall: 0.6119\n",
      "2019-01-07T19:40:07.168560, step: 234, loss: 0.3784513771533966, acc: 0.7891, auc: 0.9134, precision: 0.8462, recall: 0.6111\n",
      "2019-01-07T19:40:08.873005, step: 235, loss: 0.45172247290611267, acc: 0.7344, auc: 0.8734, precision: 0.7917, recall: 0.6129\n",
      "2019-01-07T19:40:10.617299, step: 236, loss: 0.3813296854496002, acc: 0.8125, auc: 0.9095, precision: 0.8909, recall: 0.7313\n",
      "2019-01-07T19:40:12.400226, step: 237, loss: 0.4289673864841461, acc: 0.8203, auc: 0.8838, precision: 0.8286, recall: 0.8406\n",
      "2019-01-07T19:40:14.175952, step: 238, loss: 0.39520612359046936, acc: 0.8359, auc: 0.8986, precision: 0.863, recall: 0.8514\n",
      "2019-01-07T19:40:15.933356, step: 239, loss: 0.4599341154098511, acc: 0.7891, auc: 0.8861, precision: 0.7344, recall: 0.8246\n",
      "2019-01-07T19:40:17.651604, step: 240, loss: 0.4499609172344208, acc: 0.7578, auc: 0.87, precision: 0.8182, recall: 0.6818\n",
      "2019-01-07T19:40:19.339686, step: 241, loss: 0.3423219919204712, acc: 0.8047, auc: 0.9332, precision: 0.9231, recall: 0.6957\n",
      "2019-01-07T19:40:21.707514, step: 242, loss: 0.4185553789138794, acc: 0.8125, auc: 0.8896, precision: 0.8077, recall: 0.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-07T19:40:23.487033, step: 243, loss: 0.4164600670337677, acc: 0.7969, auc: 0.8924, precision: 0.8511, recall: 0.678\n",
      "2019-01-07T19:40:25.256645, step: 244, loss: 0.41904938220977783, acc: 0.7812, auc: 0.8994, precision: 0.9333, recall: 0.6269\n",
      "2019-01-07T19:40:27.016296, step: 245, loss: 0.46247798204421997, acc: 0.7812, auc: 0.8671, precision: 0.8679, recall: 0.6866\n",
      "2019-01-07T19:40:28.744180, step: 246, loss: 0.48344600200653076, acc: 0.7969, auc: 0.8543, precision: 0.8, recall: 0.7742\n",
      "2019-01-07T19:40:30.457187, step: 247, loss: 0.447444349527359, acc: 0.7656, auc: 0.8644, precision: 0.82, recall: 0.6613\n",
      "2019-01-07T19:40:32.180101, step: 248, loss: 0.38423416018486023, acc: 0.8438, auc: 0.9145, precision: 0.8644, recall: 0.8095\n",
      "2019-01-07T19:40:33.942124, step: 249, loss: 0.4129587709903717, acc: 0.8203, auc: 0.8967, precision: 0.8889, recall: 0.7385\n",
      "2019-01-07T19:40:36.362653, step: 250, loss: 0.38055840134620667, acc: 0.8125, auc: 0.9067, precision: 0.8596, recall: 0.7538\n",
      "2019-01-07T19:40:38.445018, step: 251, loss: 0.5809346437454224, acc: 0.7344, auc: 0.7954, precision: 0.7959, recall: 0.619\n",
      "2019-01-07T19:40:40.172245, step: 252, loss: 0.3505604565143585, acc: 0.7969, auc: 0.9386, precision: 0.9362, recall: 0.6567\n",
      "2019-01-07T19:40:42.516709, step: 253, loss: 0.43780970573425293, acc: 0.8047, auc: 0.8884, precision: 0.9565, recall: 0.6567\n",
      "2019-01-07T19:40:44.324504, step: 254, loss: 0.39378899335861206, acc: 0.8359, auc: 0.9094, precision: 0.8644, recall: 0.7969\n",
      "2019-01-07T19:40:46.481617, step: 255, loss: 0.3600061535835266, acc: 0.8047, auc: 0.9189, precision: 0.8393, recall: 0.746\n",
      "2019-01-07T19:40:48.217177, step: 256, loss: 0.3973623216152191, acc: 0.8047, auc: 0.902, precision: 0.8667, recall: 0.7536\n",
      "2019-01-07T19:40:49.949593, step: 257, loss: 0.36078155040740967, acc: 0.8516, auc: 0.9274, precision: 0.8939, recall: 0.831\n",
      "2019-01-07T19:40:51.768584, step: 258, loss: 0.3938866853713989, acc: 0.8125, auc: 0.9026, precision: 0.8136, recall: 0.7869\n",
      "2019-01-07T19:40:53.532205, step: 259, loss: 0.39060887694358826, acc: 0.8281, auc: 0.9103, precision: 0.8627, recall: 0.7458\n",
      "2019-01-07T19:40:55.848410, step: 260, loss: 0.43047285079956055, acc: 0.7734, auc: 0.8906, precision: 0.9388, recall: 0.6389\n",
      "2019-01-07T19:40:58.039699, step: 261, loss: 0.32917535305023193, acc: 0.875, auc: 0.9359, precision: 0.9057, recall: 0.8136\n",
      "2019-01-07T19:40:59.730982, step: 262, loss: 0.3812718987464905, acc: 0.8281, auc: 0.9231, precision: 0.92, recall: 0.7188\n",
      "2019-01-07T19:41:01.377644, step: 263, loss: 0.35470879077911377, acc: 0.8438, auc: 0.9239, precision: 0.975, recall: 0.6724\n",
      "2019-01-07T19:41:03.728980, step: 264, loss: 0.40413862466812134, acc: 0.8359, auc: 0.8902, precision: 0.88, recall: 0.7458\n",
      "2019-01-07T19:41:05.897782, step: 265, loss: 0.4125940203666687, acc: 0.7969, auc: 0.8954, precision: 0.8571, recall: 0.7273\n",
      "2019-01-07T19:41:07.628380, step: 266, loss: 0.3393559455871582, acc: 0.8516, auc: 0.9321, precision: 0.9216, recall: 0.7581\n",
      "2019-01-07T19:41:09.352237, step: 267, loss: 0.415910929441452, acc: 0.7969, auc: 0.9023, precision: 0.7586, recall: 0.7857\n",
      "2019-01-07T19:41:11.157263, step: 268, loss: 0.41670161485671997, acc: 0.8047, auc: 0.9017, precision: 0.9153, recall: 0.7297\n",
      "2019-01-07T19:41:12.849026, step: 269, loss: 0.37806713581085205, acc: 0.8047, auc: 0.9288, precision: 0.9375, recall: 0.6716\n",
      "2019-01-07T19:41:14.573522, step: 270, loss: 0.38261306285858154, acc: 0.8516, auc: 0.9101, precision: 0.871, recall: 0.8308\n",
      "2019-01-07T19:41:16.649126, step: 271, loss: 0.44779306650161743, acc: 0.7734, auc: 0.8726, precision: 0.7966, recall: 0.7344\n",
      "2019-01-07T19:41:18.465275, step: 272, loss: 0.34251439571380615, acc: 0.8516, auc: 0.9251, precision: 0.9333, recall: 0.7887\n",
      "2019-01-07T19:41:20.801747, step: 273, loss: 0.33483484387397766, acc: 0.8984, auc: 0.9468, precision: 0.8788, recall: 0.9206\n",
      "2019-01-07T19:41:22.531429, step: 274, loss: 0.39406830072402954, acc: 0.8359, auc: 0.9001, precision: 0.9231, recall: 0.7385\n",
      "2019-01-07T19:41:24.840091, step: 275, loss: 0.3598448634147644, acc: 0.8047, auc: 0.9223, precision: 0.8571, recall: 0.7385\n",
      "2019-01-07T19:41:26.753406, step: 276, loss: 0.358629435300827, acc: 0.8203, auc: 0.9221, precision: 0.8627, recall: 0.7333\n",
      "2019-01-07T19:41:28.474078, step: 277, loss: 0.3959491550922394, acc: 0.7891, auc: 0.8987, precision: 0.8364, recall: 0.7188\n",
      "2019-01-07T19:41:30.216514, step: 278, loss: 0.5117986798286438, acc: 0.75, auc: 0.8565, precision: 0.8364, recall: 0.6667\n",
      "2019-01-07T19:41:31.907797, step: 279, loss: 0.32829853892326355, acc: 0.8203, auc: 0.9409, precision: 0.9038, recall: 0.7231\n",
      "2019-01-07T19:41:33.712494, step: 280, loss: 0.42816412448883057, acc: 0.8047, auc: 0.875, precision: 0.7925, recall: 0.75\n",
      "2019-01-07T19:41:35.582343, step: 281, loss: 0.33103540539741516, acc: 0.8438, auc: 0.9344, precision: 0.8769, recall: 0.8261\n",
      "2019-01-07T19:41:37.369658, step: 282, loss: 0.4051320552825928, acc: 0.8594, auc: 0.9138, precision: 0.8421, recall: 0.8421\n",
      "2019-01-07T19:41:39.601842, step: 283, loss: 0.42194461822509766, acc: 0.8047, auc: 0.8955, precision: 0.88, recall: 0.6984\n",
      "2019-01-07T19:41:41.468279, step: 284, loss: 0.3286913335323334, acc: 0.8438, auc: 0.9379, precision: 0.9556, recall: 0.7049\n",
      "2019-01-07T19:41:43.416343, step: 285, loss: 0.45275506377220154, acc: 0.75, auc: 0.8752, precision: 0.8364, recall: 0.6667\n",
      "2019-01-07T19:41:45.292431, step: 286, loss: 0.3330102860927582, acc: 0.7891, auc: 0.9337, precision: 0.8542, recall: 0.6721\n",
      "2019-01-07T19:41:47.019059, step: 287, loss: 0.4136701822280884, acc: 0.7734, auc: 0.8994, precision: 0.8958, recall: 0.6418\n",
      "2019-01-07T19:41:48.767575, step: 288, loss: 0.33417224884033203, acc: 0.8438, auc: 0.9372, precision: 0.8947, recall: 0.7846\n",
      "2019-01-07T19:41:50.476040, step: 289, loss: 0.39527541399002075, acc: 0.8672, auc: 0.9179, precision: 0.8485, recall: 0.8889\n",
      "2019-01-07T19:41:52.166498, step: 290, loss: 0.39585328102111816, acc: 0.8672, auc: 0.9251, precision: 0.8182, recall: 0.9153\n",
      "2019-01-07T19:41:54.022429, step: 291, loss: 0.41440698504447937, acc: 0.8359, auc: 0.8959, precision: 0.8462, recall: 0.8333\n",
      "2019-01-07T19:41:55.835714, step: 292, loss: 0.3448912501335144, acc: 0.8203, auc: 0.9312, precision: 0.9107, recall: 0.7391\n",
      "2019-01-07T19:41:57.580867, step: 293, loss: 0.46164923906326294, acc: 0.7266, auc: 0.8794, precision: 0.8378, recall: 0.5167\n",
      "2019-01-07T19:41:59.917413, step: 294, loss: 0.32243916392326355, acc: 0.7734, auc: 0.9391, precision: 0.8958, recall: 0.6418\n",
      "2019-01-07T19:42:01.649791, step: 295, loss: 0.3734387159347534, acc: 0.8281, auc: 0.9207, precision: 0.9245, recall: 0.7313\n",
      "2019-01-07T19:42:03.463035, step: 296, loss: 0.42554548382759094, acc: 0.7969, auc: 0.8772, precision: 0.8293, recall: 0.6415\n",
      "2019-01-07T19:42:05.206131, step: 297, loss: 0.37375879287719727, acc: 0.8281, auc: 0.9219, precision: 0.8136, recall: 0.8136\n",
      "2019-01-07T19:42:07.523739, step: 298, loss: 0.3645860552787781, acc: 0.8359, auc: 0.9177, precision: 0.85, recall: 0.8095\n",
      "2019-01-07T19:42:09.246341, step: 299, loss: 0.4184737801551819, acc: 0.7812, auc: 0.892, precision: 0.8519, recall: 0.697\n",
      "2019-01-07T19:42:11.536315, step: 300, loss: 0.38407400250434875, acc: 0.7891, auc: 0.9103, precision: 0.8511, recall: 0.6667\n",
      "\n",
      "Evaluation:\n",
      "2019-01-07T19:43:22.880973, step: 300, loss: 0.4025821945606134, acc: 0.8036871794871795, auc: 0.9130743589743588, precision: 0.8985358974358973, recall: 0.6876974358974358\n",
      "2019-01-07T19:43:24.544265, step: 301, loss: 0.3338248133659363, acc: 0.8281, auc: 0.9338, precision: 0.8654, recall: 0.75\n",
      "2019-01-07T19:43:26.149400, step: 302, loss: 0.3456728458404541, acc: 0.8281, auc: 0.9292, precision: 0.8909, recall: 0.7538\n",
      "2019-01-07T19:43:27.801811, step: 303, loss: 0.42080289125442505, acc: 0.7891, auc: 0.886, precision: 0.8235, recall: 0.7\n",
      "2019-01-07T19:43:29.436561, step: 304, loss: 0.37001919746398926, acc: 0.8359, auc: 0.9137, precision: 0.9273, recall: 0.75\n",
      "2019-01-07T19:43:31.080450, step: 305, loss: 0.347877562046051, acc: 0.8516, auc: 0.9288, precision: 0.9375, recall: 0.7377\n",
      "2019-01-07T19:43:32.689080, step: 306, loss: 0.33680257201194763, acc: 0.8281, auc: 0.9316, precision: 0.8667, recall: 0.7879\n",
      "2019-01-07T19:43:34.375566, step: 307, loss: 0.3096635341644287, acc: 0.8672, auc: 0.9392, precision: 0.9375, recall: 0.8219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-07T19:43:36.040405, step: 308, loss: 0.37028688192367554, acc: 0.875, auc: 0.9357, precision: 0.8289, recall: 0.9545\n",
      "2019-01-07T19:43:37.579987, step: 309, loss: 0.405372679233551, acc: 0.8594, auc: 0.9042, precision: 0.9, recall: 0.8182\n",
      "2019-01-07T19:43:39.313300, step: 310, loss: 0.34649449586868286, acc: 0.8594, auc: 0.9295, precision: 0.8298, recall: 0.7959\n",
      "2019-01-07T19:43:40.982032, step: 311, loss: 0.4277195632457733, acc: 0.7812, auc: 0.9188, precision: 0.9615, recall: 0.6579\n",
      "2019-01-07T19:43:42.616049, step: 312, loss: 0.3329507112503052, acc: 0.8359, auc: 0.9355, precision: 0.9535, recall: 0.6833\n",
      "start training model\n",
      "2019-01-07T19:43:44.176280, step: 313, loss: 0.44136855006217957, acc: 0.8125, auc: 0.8792, precision: 0.9016, recall: 0.7534\n",
      "2019-01-07T19:43:45.801536, step: 314, loss: 0.3267339766025543, acc: 0.8359, auc: 0.9504, precision: 1.0, recall: 0.6818\n",
      "2019-01-07T19:43:47.468912, step: 315, loss: 0.4468207061290741, acc: 0.8125, auc: 0.8774, precision: 0.8361, recall: 0.7846\n",
      "2019-01-07T19:43:49.123546, step: 316, loss: 0.49179506301879883, acc: 0.7812, auc: 0.8766, precision: 0.75, recall: 0.7759\n",
      "2019-01-07T19:43:50.788756, step: 317, loss: 0.34679675102233887, acc: 0.8438, auc: 0.934, precision: 0.8214, recall: 0.8214\n",
      "2019-01-07T19:43:52.451947, step: 318, loss: 0.4090671241283417, acc: 0.8359, auc: 0.8975, precision: 0.8525, recall: 0.8125\n",
      "2019-01-07T19:43:54.062246, step: 319, loss: 0.35389086604118347, acc: 0.8281, auc: 0.9241, precision: 0.9057, recall: 0.7385\n",
      "2019-01-07T19:43:55.832428, step: 320, loss: 0.43181082606315613, acc: 0.7656, auc: 0.8892, precision: 0.9048, recall: 0.5938\n",
      "2019-01-07T19:43:57.464658, step: 321, loss: 0.4087778925895691, acc: 0.7891, auc: 0.8987, precision: 0.8704, recall: 0.7015\n",
      "2019-01-07T19:43:59.129291, step: 322, loss: 0.3813849687576294, acc: 0.8203, auc: 0.9113, precision: 0.9302, recall: 0.6667\n",
      "2019-01-07T19:44:00.740130, step: 323, loss: 0.3682985305786133, acc: 0.8438, auc: 0.9197, precision: 0.8525, recall: 0.8254\n",
      "2019-01-07T19:44:02.356374, step: 324, loss: 0.4087885916233063, acc: 0.7969, auc: 0.8931, precision: 0.8511, recall: 0.678\n",
      "2019-01-07T19:44:03.981799, step: 325, loss: 0.34303542971611023, acc: 0.8516, auc: 0.9324, precision: 0.8868, recall: 0.7833\n",
      "2019-01-07T19:44:05.600989, step: 326, loss: 0.33285313844680786, acc: 0.8359, auc: 0.9348, precision: 0.8852, recall: 0.7941\n",
      "2019-01-07T19:44:07.251911, step: 327, loss: 0.3014678955078125, acc: 0.8438, auc: 0.9479, precision: 0.902, recall: 0.7541\n",
      "2019-01-07T19:44:08.956250, step: 328, loss: 0.31312018632888794, acc: 0.875, auc: 0.9412, precision: 0.9365, recall: 0.831\n",
      "2019-01-07T19:44:10.576520, step: 329, loss: 0.29129505157470703, acc: 0.8672, auc: 0.9526, precision: 0.9333, recall: 0.8116\n",
      "2019-01-07T19:44:12.229133, step: 330, loss: 0.309955894947052, acc: 0.8516, auc: 0.9437, precision: 0.9394, recall: 0.8052\n",
      "2019-01-07T19:44:13.889694, step: 331, loss: 0.34294092655181885, acc: 0.8828, auc: 0.935, precision: 0.8909, recall: 0.8448\n",
      "2019-01-07T19:44:15.573107, step: 332, loss: 0.41580963134765625, acc: 0.8359, auc: 0.9042, precision: 0.8545, recall: 0.7833\n",
      "2019-01-07T19:44:17.196117, step: 333, loss: 0.3856693506240845, acc: 0.8281, auc: 0.9057, precision: 0.8772, recall: 0.7692\n",
      "2019-01-07T19:44:18.894955, step: 334, loss: 0.3203674852848053, acc: 0.875, auc: 0.9346, precision: 0.8627, recall: 0.8302\n",
      "2019-01-07T19:44:20.516034, step: 335, loss: 0.3138394057750702, acc: 0.8359, auc: 0.9362, precision: 0.8718, recall: 0.68\n",
      "2019-01-07T19:44:22.155299, step: 336, loss: 0.2785733938217163, acc: 0.8594, auc: 0.957, precision: 0.9259, recall: 0.7812\n",
      "2019-01-07T19:44:23.881097, step: 337, loss: 0.429709255695343, acc: 0.7578, auc: 0.8982, precision: 0.925, recall: 0.5692\n",
      "2019-01-07T19:44:25.549074, step: 338, loss: 0.43633800745010376, acc: 0.8047, auc: 0.8986, precision: 0.9123, recall: 0.7222\n",
      "2019-01-07T19:44:27.188869, step: 339, loss: 0.36668670177459717, acc: 0.8359, auc: 0.9174, precision: 0.8421, recall: 0.8\n",
      "2019-01-07T19:44:28.833953, step: 340, loss: 0.3815808892250061, acc: 0.8438, auc: 0.9221, precision: 0.8413, recall: 0.8413\n",
      "2019-01-07T19:44:30.495796, step: 341, loss: 0.3206086754798889, acc: 0.875, auc: 0.9401, precision: 0.918, recall: 0.8358\n",
      "2019-01-07T19:44:32.142846, step: 342, loss: 0.3455144464969635, acc: 0.8594, auc: 0.933, precision: 0.8704, recall: 0.8103\n",
      "2019-01-07T19:44:33.801069, step: 343, loss: 0.3224028944969177, acc: 0.8828, auc: 0.9358, precision: 0.931, recall: 0.8308\n",
      "2019-01-07T19:44:35.425068, step: 344, loss: 0.3687865138053894, acc: 0.8359, auc: 0.9225, precision: 0.902, recall: 0.7419\n",
      "2019-01-07T19:44:37.040647, step: 345, loss: 0.35293224453926086, acc: 0.8438, auc: 0.9321, precision: 0.9574, recall: 0.7143\n",
      "2019-01-07T19:44:38.760756, step: 346, loss: 0.3921673893928528, acc: 0.7812, auc: 0.9117, precision: 0.8824, recall: 0.5556\n",
      "2019-01-07T19:44:40.385175, step: 347, loss: 0.3792761266231537, acc: 0.8359, auc: 0.9236, precision: 0.9216, recall: 0.7344\n",
      "2019-01-07T19:44:42.074320, step: 348, loss: 0.3172384798526764, acc: 0.8359, auc: 0.9435, precision: 0.9545, recall: 0.6885\n",
      "2019-01-07T19:44:43.775530, step: 349, loss: 0.446933776140213, acc: 0.8125, auc: 0.8815, precision: 0.7742, recall: 0.8276\n",
      "2019-01-07T19:44:45.400957, step: 350, loss: 0.4101061224937439, acc: 0.8516, auc: 0.9054, precision: 0.8451, recall: 0.8824\n",
      "2019-01-07T19:44:47.023133, step: 351, loss: 0.37640395760536194, acc: 0.8516, auc: 0.9084, precision: 0.8772, recall: 0.8065\n",
      "2019-01-07T19:44:48.681093, step: 352, loss: 0.38320475816726685, acc: 0.8047, auc: 0.9101, precision: 0.8776, recall: 0.6935\n",
      "2019-01-07T19:44:50.429083, step: 353, loss: 0.37953153252601624, acc: 0.8125, auc: 0.9133, precision: 0.8679, recall: 0.7302\n",
      "2019-01-07T19:44:52.100908, step: 354, loss: 0.345755934715271, acc: 0.8281, auc: 0.9272, precision: 0.9057, recall: 0.7385\n",
      "2019-01-07T19:44:53.733230, step: 355, loss: 0.42393016815185547, acc: 0.7422, auc: 0.8838, precision: 0.8333, recall: 0.6154\n",
      "2019-01-07T19:44:55.425612, step: 356, loss: 0.38325560092926025, acc: 0.8281, auc: 0.9113, precision: 0.8667, recall: 0.7091\n",
      "2019-01-07T19:44:57.065478, step: 357, loss: 0.3398738503456116, acc: 0.875, auc: 0.9372, precision: 0.9038, recall: 0.8103\n",
      "2019-01-07T19:44:58.696215, step: 358, loss: 0.3596039116382599, acc: 0.8047, auc: 0.9228, precision: 0.8571, recall: 0.7385\n",
      "2019-01-07T19:45:00.314150, step: 359, loss: 0.3887841999530792, acc: 0.8281, auc: 0.9057, precision: 0.875, recall: 0.7241\n",
      "2019-01-07T19:45:01.900771, step: 360, loss: 0.3282817602157593, acc: 0.8438, auc: 0.9442, precision: 0.9636, recall: 0.7465\n",
      "2019-01-07T19:45:03.504278, step: 361, loss: 0.34123796224594116, acc: 0.875, auc: 0.9281, precision: 0.918, recall: 0.8358\n",
      "2019-01-07T19:45:05.160850, step: 362, loss: 0.3309714198112488, acc: 0.8359, auc: 0.9341, precision: 0.8621, recall: 0.7937\n",
      "2019-01-07T19:45:06.900702, step: 363, loss: 0.3754728436470032, acc: 0.8594, auc: 0.9144, precision: 0.9153, recall: 0.806\n",
      "2019-01-07T19:45:08.512489, step: 364, loss: 0.43636152148246765, acc: 0.8203, auc: 0.8843, precision: 0.8438, recall: 0.806\n",
      "2019-01-07T19:45:10.121117, step: 365, loss: 0.2885476350784302, acc: 0.9062, auc: 0.9477, precision: 0.94, recall: 0.8393\n",
      "2019-01-07T19:45:11.717427, step: 366, loss: 0.3392481207847595, acc: 0.875, auc: 0.9349, precision: 1.0, recall: 0.7377\n",
      "2019-01-07T19:45:13.289744, step: 367, loss: 0.3752310276031494, acc: 0.8516, auc: 0.912, precision: 0.8868, recall: 0.7833\n",
      "2019-01-07T19:45:15.005904, step: 368, loss: 0.45482146739959717, acc: 0.7969, auc: 0.8757, precision: 0.8393, recall: 0.7344\n",
      "2019-01-07T19:45:16.665282, step: 369, loss: 0.4236493408679962, acc: 0.8594, auc: 0.8953, precision: 0.8448, recall: 0.8448\n",
      "2019-01-07T19:45:18.369930, step: 370, loss: 0.40371382236480713, acc: 0.8281, auc: 0.8979, precision: 0.8868, recall: 0.746\n",
      "2019-01-07T19:45:20.097871, step: 371, loss: 0.32841625809669495, acc: 0.8594, auc: 0.936, precision: 0.9038, recall: 0.7833\n",
      "2019-01-07T19:45:21.785878, step: 372, loss: 0.3380391001701355, acc: 0.8359, auc: 0.9275, precision: 0.873, recall: 0.8088\n",
      "2019-01-07T19:45:23.472836, step: 373, loss: 0.25256770849227905, acc: 0.8984, auc: 0.9685, precision: 0.9412, recall: 0.8276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-07T19:45:25.143935, step: 374, loss: 0.39432111382484436, acc: 0.7812, auc: 0.9, precision: 0.8654, recall: 0.6818\n",
      "2019-01-07T19:45:26.749424, step: 375, loss: 0.34056761860847473, acc: 0.8594, auc: 0.9294, precision: 0.8833, recall: 0.8281\n",
      "2019-01-07T19:45:28.421373, step: 376, loss: 0.26857537031173706, acc: 0.8594, auc: 0.9568, precision: 0.8727, recall: 0.8136\n",
      "2019-01-07T19:45:30.164105, step: 377, loss: 0.2359531670808792, acc: 0.9141, auc: 0.9687, precision: 0.9508, recall: 0.8788\n",
      "2019-01-07T19:45:31.836471, step: 378, loss: 0.39489689469337463, acc: 0.8125, auc: 0.9035, precision: 0.8333, recall: 0.7143\n",
      "2019-01-07T19:45:33.473992, step: 379, loss: 0.3219752013683319, acc: 0.8594, auc: 0.9416, precision: 0.9608, recall: 0.7538\n",
      "2019-01-07T19:45:35.117465, step: 380, loss: 0.4253154695034027, acc: 0.7969, auc: 0.8897, precision: 0.8036, recall: 0.75\n",
      "2019-01-07T19:45:36.748644, step: 381, loss: 0.4030427038669586, acc: 0.8359, auc: 0.9113, precision: 0.8214, recall: 0.807\n",
      "2019-01-07T19:45:38.397085, step: 382, loss: 0.35862892866134644, acc: 0.8438, auc: 0.9216, precision: 0.92, recall: 0.7419\n",
      "2019-01-07T19:45:39.977834, step: 383, loss: 0.6145204305648804, acc: 0.7109, auc: 0.8296, precision: 0.875, recall: 0.5753\n",
      "2019-01-07T19:45:41.648316, step: 384, loss: 0.3516811728477478, acc: 0.8594, auc: 0.9233, precision: 0.8302, recall: 0.8302\n",
      "2019-01-07T19:45:43.320961, step: 385, loss: 0.3350146412849426, acc: 0.8594, auc: 0.9377, precision: 0.8358, recall: 0.8889\n",
      "2019-01-07T19:45:44.995964, step: 386, loss: 0.34727346897125244, acc: 0.8516, auc: 0.9392, precision: 0.8714, recall: 0.8592\n",
      "2019-01-07T19:45:46.677811, step: 387, loss: 0.32077309489250183, acc: 0.875, auc: 0.9436, precision: 0.8939, recall: 0.8676\n",
      "2019-01-07T19:45:48.364469, step: 388, loss: 0.31311899423599243, acc: 0.8516, auc: 0.9462, precision: 0.9444, recall: 0.7612\n",
      "2019-01-07T19:45:50.020733, step: 389, loss: 0.3247382640838623, acc: 0.8359, auc: 0.9377, precision: 0.9153, recall: 0.7714\n",
      "2019-01-07T19:45:51.637590, step: 390, loss: 0.30133169889450073, acc: 0.8359, auc: 0.9512, precision: 0.8889, recall: 0.8\n",
      "2019-01-07T19:45:53.333623, step: 391, loss: 0.3132954239845276, acc: 0.8516, auc: 0.9409, precision: 0.9434, recall: 0.7576\n",
      "2019-01-07T19:45:54.937825, step: 392, loss: 0.31095027923583984, acc: 0.8438, auc: 0.9462, precision: 0.8475, recall: 0.8197\n",
      "2019-01-07T19:45:56.607516, step: 393, loss: 0.3880566954612732, acc: 0.8047, auc: 0.9154, precision: 0.7719, recall: 0.7857\n",
      "2019-01-07T19:45:58.250313, step: 394, loss: 0.28915348649024963, acc: 0.8906, auc: 0.9552, precision: 0.9123, recall: 0.8525\n",
      "2019-01-07T19:45:59.907904, step: 395, loss: 0.24337542057037354, acc: 0.8984, auc: 0.9699, precision: 0.9583, recall: 0.807\n",
      "2019-01-07T19:46:01.623980, step: 396, loss: 0.36387985944747925, acc: 0.8047, auc: 0.9305, precision: 0.9565, recall: 0.6567\n",
      "2019-01-07T19:46:03.292584, step: 397, loss: 0.27893391251564026, acc: 0.8516, auc: 0.9557, precision: 0.95, recall: 0.6909\n",
      "2019-01-07T19:46:04.930014, step: 398, loss: 0.3924786150455475, acc: 0.8281, auc: 0.9084, precision: 0.9077, recall: 0.7867\n",
      "2019-01-07T19:46:06.600546, step: 399, loss: 0.3555086851119995, acc: 0.8438, auc: 0.9298, precision: 0.8254, recall: 0.8525\n",
      "2019-01-07T19:46:08.312045, step: 400, loss: 0.36901605129241943, acc: 0.875, auc: 0.932, precision: 0.8462, recall: 0.9016\n",
      "\n",
      "Evaluation:\n",
      "2019-01-07T19:47:12.049183, step: 400, loss: 0.37740934659273195, acc: 0.8463564102564103, auc: 0.9253282051282051, precision: 0.842697435897436, recall: 0.8566846153846157\n",
      "2019-01-07T19:47:13.743879, step: 401, loss: 0.4076545238494873, acc: 0.8438, auc: 0.9092, precision: 0.8333, recall: 0.8036\n",
      "2019-01-07T19:47:15.376046, step: 402, loss: 0.28252917528152466, acc: 0.875, auc: 0.9482, precision: 0.8906, recall: 0.8636\n",
      "2019-01-07T19:47:17.063053, step: 403, loss: 0.43171781301498413, acc: 0.7969, auc: 0.8919, precision: 0.8723, recall: 0.6721\n",
      "2019-01-07T19:47:18.780046, step: 404, loss: 0.3746374249458313, acc: 0.8281, auc: 0.9188, precision: 0.9535, recall: 0.6721\n",
      "2019-01-07T19:47:20.465545, step: 405, loss: 0.3456706702709198, acc: 0.8125, auc: 0.9375, precision: 0.8846, recall: 0.7188\n",
      "2019-01-07T19:47:22.176132, step: 406, loss: 0.3362753689289093, acc: 0.8125, auc: 0.9354, precision: 0.9111, recall: 0.6721\n",
      "2019-01-07T19:47:23.805936, step: 407, loss: 0.33817431330680847, acc: 0.8438, auc: 0.9335, precision: 0.9375, recall: 0.7895\n",
      "2019-01-07T19:47:25.468737, step: 408, loss: 0.40619105100631714, acc: 0.8516, auc: 0.9014, precision: 0.84, recall: 0.7925\n",
      "2019-01-07T19:47:27.134936, step: 409, loss: 0.36952632665634155, acc: 0.8438, auc: 0.9226, precision: 0.8413, recall: 0.8413\n",
      "2019-01-07T19:47:28.836963, step: 410, loss: 0.32505056262016296, acc: 0.8828, auc: 0.9376, precision: 0.871, recall: 0.8852\n",
      "2019-01-07T19:47:30.572245, step: 411, loss: 0.3281926214694977, acc: 0.8359, auc: 0.9345, precision: 0.8769, recall: 0.8143\n",
      "2019-01-07T19:47:32.339171, step: 412, loss: 0.37681329250335693, acc: 0.8594, auc: 0.9187, precision: 0.8475, recall: 0.8475\n",
      "2019-01-07T19:47:34.044964, step: 413, loss: 0.38868629932403564, acc: 0.8125, auc: 0.9048, precision: 0.8545, recall: 0.746\n",
      "2019-01-07T19:47:35.739971, step: 414, loss: 0.3746250867843628, acc: 0.8203, auc: 0.9199, precision: 0.9512, recall: 0.65\n",
      "2019-01-07T19:47:37.354051, step: 415, loss: 0.4224628210067749, acc: 0.7969, auc: 0.9106, precision: 0.8909, recall: 0.7101\n",
      "2019-01-07T19:47:39.070402, step: 416, loss: 0.27131086587905884, acc: 0.875, auc: 0.958, precision: 0.9111, recall: 0.7736\n",
      "2019-01-07T19:47:40.712850, step: 417, loss: 0.3904463052749634, acc: 0.7734, auc: 0.9104, precision: 0.9091, recall: 0.6757\n",
      "2019-01-07T19:47:42.380293, step: 418, loss: 0.29597607254981995, acc: 0.8828, auc: 0.9499, precision: 0.9184, recall: 0.8036\n",
      "2019-01-07T19:47:44.068271, step: 419, loss: 0.368537575006485, acc: 0.8203, auc: 0.913, precision: 0.8571, recall: 0.7941\n",
      "2019-01-07T19:47:45.768903, step: 420, loss: 0.35847923159599304, acc: 0.8516, auc: 0.93, precision: 0.8197, recall: 0.8621\n",
      "2019-01-07T19:47:47.492697, step: 421, loss: 0.31396785378456116, acc: 0.8516, auc: 0.9485, precision: 0.8814, recall: 0.8125\n",
      "2019-01-07T19:47:49.199750, step: 422, loss: 0.3907514810562134, acc: 0.8359, auc: 0.9101, precision: 0.8462, recall: 0.8333\n",
      "2019-01-07T19:47:50.900061, step: 423, loss: 0.39285045862197876, acc: 0.8125, auc: 0.9054, precision: 0.85, recall: 0.7727\n",
      "2019-01-07T19:47:52.544139, step: 424, loss: 0.25109487771987915, acc: 0.8828, auc: 0.9681, precision: 0.9412, recall: 0.8\n",
      "2019-01-07T19:47:54.176747, step: 425, loss: 0.3223766088485718, acc: 0.8281, auc: 0.936, precision: 0.9259, recall: 0.7353\n",
      "2019-01-07T19:47:55.872664, step: 426, loss: 0.36750441789627075, acc: 0.8125, auc: 0.9252, precision: 0.9375, recall: 0.6818\n",
      "2019-01-07T19:47:57.547490, step: 427, loss: 0.3841862380504608, acc: 0.8281, auc: 0.9096, precision: 0.902, recall: 0.7302\n",
      "2019-01-07T19:47:59.247731, step: 428, loss: 0.3636116087436676, acc: 0.8438, auc: 0.9323, precision: 0.8167, recall: 0.8448\n",
      "2019-01-07T19:48:00.952777, step: 429, loss: 0.4499121308326721, acc: 0.8125, auc: 0.8749, precision: 0.85, recall: 0.7727\n",
      "2019-01-07T19:48:02.680686, step: 430, loss: 0.329000324010849, acc: 0.8828, auc: 0.9366, precision: 0.8929, recall: 0.8475\n",
      "2019-01-07T19:48:04.332682, step: 431, loss: 0.3525979816913605, acc: 0.8359, auc: 0.9272, precision: 0.8596, recall: 0.7903\n",
      "2019-01-07T19:48:06.057064, step: 432, loss: 0.3672674298286438, acc: 0.8047, auc: 0.9204, precision: 0.9394, recall: 0.5741\n",
      "2019-01-07T19:48:07.783651, step: 433, loss: 0.30271756649017334, acc: 0.8672, auc: 0.9469, precision: 0.9583, recall: 0.7541\n",
      "2019-01-07T19:48:09.509175, step: 434, loss: 0.33129850029945374, acc: 0.8359, auc: 0.9395, precision: 0.9388, recall: 0.7188\n",
      "2019-01-07T19:48:11.224619, step: 435, loss: 0.39668869972229004, acc: 0.8047, auc: 0.9056, precision: 0.9038, recall: 0.7015\n",
      "2019-01-07T19:48:12.989577, step: 436, loss: 0.36419790983200073, acc: 0.8516, auc: 0.919, precision: 0.8923, recall: 0.8286\n",
      "2019-01-07T19:48:14.642528, step: 437, loss: 0.3206661343574524, acc: 0.8516, auc: 0.961, precision: 0.8154, recall: 0.8833\n",
      "2019-01-07T19:48:16.335768, step: 438, loss: 0.3492847681045532, acc: 0.8672, auc: 0.9347, precision: 0.8714, recall: 0.8841\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-07T19:48:18.071754, step: 439, loss: 0.29645586013793945, acc: 0.8906, auc: 0.9477, precision: 0.9455, recall: 0.8254\n",
      "2019-01-07T19:48:19.839654, step: 440, loss: 0.4285438358783722, acc: 0.8203, auc: 0.898, precision: 0.9492, recall: 0.7368\n",
      "2019-01-07T19:48:21.522652, step: 441, loss: 0.30801743268966675, acc: 0.8594, auc: 0.9451, precision: 0.8679, recall: 0.807\n",
      "2019-01-07T19:48:23.232931, step: 442, loss: 0.4089735746383667, acc: 0.8203, auc: 0.9007, precision: 0.9412, recall: 0.7059\n",
      "2019-01-07T19:48:24.906011, step: 443, loss: 0.36729681491851807, acc: 0.8125, auc: 0.9118, precision: 0.8594, recall: 0.7857\n",
      "2019-01-07T19:48:26.681548, step: 444, loss: 0.30609002709388733, acc: 0.8516, auc: 0.9469, precision: 0.9138, recall: 0.791\n",
      "2019-01-07T19:48:28.396731, step: 445, loss: 0.43497157096862793, acc: 0.8281, auc: 0.8974, precision: 0.8308, recall: 0.8308\n",
      "2019-01-07T19:48:30.135827, step: 446, loss: 0.3167794346809387, acc: 0.8438, auc: 0.9196, precision: 0.9146, recall: 0.8523\n",
      "2019-01-07T19:48:31.823443, step: 447, loss: 0.3620489537715912, acc: 0.8594, auc: 0.9213, precision: 0.8571, recall: 0.9041\n",
      "2019-01-07T19:48:33.472234, step: 448, loss: 0.359111487865448, acc: 0.8125, auc: 0.9232, precision: 0.831, recall: 0.831\n",
      "2019-01-07T19:48:35.201407, step: 449, loss: 0.3850139379501343, acc: 0.7812, auc: 0.9228, precision: 0.9623, recall: 0.6623\n",
      "2019-01-07T19:48:36.853089, step: 450, loss: 0.4160958528518677, acc: 0.8125, auc: 0.8882, precision: 0.8462, recall: 0.7971\n",
      "2019-01-07T19:48:38.573383, step: 451, loss: 0.3898623287677765, acc: 0.8125, auc: 0.9071, precision: 0.8929, recall: 0.7353\n",
      "2019-01-07T19:48:40.240337, step: 452, loss: 0.2850513160228729, acc: 0.8594, auc: 0.9543, precision: 0.9107, recall: 0.7969\n",
      "2019-01-07T19:48:41.969216, step: 453, loss: 0.3211160898208618, acc: 0.8594, auc: 0.9361, precision: 0.9298, recall: 0.791\n",
      "2019-01-07T19:48:43.651650, step: 454, loss: 0.4641296863555908, acc: 0.7812, auc: 0.8796, precision: 0.7606, recall: 0.8308\n",
      "2019-01-07T19:48:45.324859, step: 455, loss: 0.363273561000824, acc: 0.8516, auc: 0.9336, precision: 0.8261, recall: 0.8906\n",
      "2019-01-07T19:48:47.004746, step: 456, loss: 0.39933016896247864, acc: 0.7344, auc: 0.9064, precision: 0.8846, recall: 0.6216\n",
      "2019-01-07T19:48:48.705290, step: 457, loss: 0.3257319927215576, acc: 0.8516, auc: 0.9474, precision: 0.9831, recall: 0.7632\n",
      "2019-01-07T19:48:50.364450, step: 458, loss: 0.38061198592185974, acc: 0.7969, auc: 0.9246, precision: 0.9107, recall: 0.7083\n",
      "2019-01-07T19:48:52.016774, step: 459, loss: 0.3935296833515167, acc: 0.8359, auc: 0.9143, precision: 0.8036, recall: 0.8182\n",
      "2019-01-07T19:48:53.611953, step: 460, loss: 0.3286835849285126, acc: 0.8672, auc: 0.9392, precision: 0.8772, recall: 0.8333\n",
      "2019-01-07T19:48:55.308532, step: 461, loss: 0.32779666781425476, acc: 0.8359, auc: 0.9334, precision: 0.8594, recall: 0.8209\n",
      "2019-01-07T19:48:57.013035, step: 462, loss: 0.4056693911552429, acc: 0.8047, auc: 0.8962, precision: 0.8333, recall: 0.7692\n",
      "2019-01-07T19:48:58.687839, step: 463, loss: 0.3867681324481964, acc: 0.8203, auc: 0.9027, precision: 0.8689, recall: 0.7794\n",
      "2019-01-07T19:49:00.392334, step: 464, loss: 0.3817804157733917, acc: 0.8203, auc: 0.9106, precision: 0.8571, recall: 0.7241\n",
      "2019-01-07T19:49:02.017849, step: 465, loss: 0.4321235716342926, acc: 0.8047, auc: 0.892, precision: 0.8627, recall: 0.7097\n",
      "2019-01-07T19:49:03.688085, step: 466, loss: 0.283302366733551, acc: 0.8594, auc: 0.9645, precision: 0.963, recall: 0.7647\n",
      "2019-01-07T19:49:05.425010, step: 467, loss: 0.39217299222946167, acc: 0.8281, auc: 0.8996, precision: 0.9362, recall: 0.6984\n",
      "2019-01-07T19:49:07.112551, step: 468, loss: 0.30971863865852356, acc: 0.9062, auc: 0.9675, precision: 0.8696, recall: 0.9524\n",
      "start training model\n",
      "2019-01-07T19:49:09.036365, step: 469, loss: 0.39261001348495483, acc: 0.8359, auc: 0.931, precision: 0.7656, recall: 0.8909\n",
      "2019-01-07T19:49:10.724996, step: 470, loss: 0.24550309777259827, acc: 0.8828, auc: 0.9723, precision: 0.9492, recall: 0.8235\n",
      "2019-01-07T19:49:12.472515, step: 471, loss: 0.3198148310184479, acc: 0.8594, auc: 0.9457, precision: 0.9655, recall: 0.7778\n",
      "2019-01-07T19:49:14.141611, step: 472, loss: 0.3668970763683319, acc: 0.8125, auc: 0.9251, precision: 0.925, recall: 0.6379\n",
      "2019-01-07T19:49:15.779869, step: 473, loss: 0.29266783595085144, acc: 0.8672, auc: 0.9592, precision: 0.9792, recall: 0.746\n",
      "2019-01-07T19:49:17.441277, step: 474, loss: 0.3492003083229065, acc: 0.8438, auc: 0.9273, precision: 0.9796, recall: 0.7164\n",
      "2019-01-07T19:49:19.085572, step: 475, loss: 0.36214619874954224, acc: 0.8359, auc: 0.9191, precision: 0.8545, recall: 0.7833\n",
      "2019-01-07T19:49:20.717057, step: 476, loss: 0.40689972043037415, acc: 0.8438, auc: 0.902, precision: 0.8594, recall: 0.8333\n",
      "2019-01-07T19:49:22.336590, step: 477, loss: 0.40347349643707275, acc: 0.8594, auc: 0.9233, precision: 0.8, recall: 0.9333\n",
      "2019-01-07T19:49:23.976602, step: 478, loss: 0.35577064752578735, acc: 0.8438, auc: 0.9213, precision: 0.9231, recall: 0.8\n",
      "2019-01-07T19:49:25.561630, step: 479, loss: 0.3394004702568054, acc: 0.8359, auc: 0.9301, precision: 0.907, recall: 0.6964\n",
      "2019-01-07T19:49:27.256682, step: 480, loss: 0.3471237123012543, acc: 0.8359, auc: 0.9239, precision: 0.8367, recall: 0.7593\n",
      "2019-01-07T19:49:28.911894, step: 481, loss: 0.32715269923210144, acc: 0.875, auc: 0.9339, precision: 0.9636, recall: 0.791\n",
      "2019-01-07T19:49:30.592590, step: 482, loss: 0.2906917929649353, acc: 0.8594, auc: 0.9526, precision: 0.9259, recall: 0.7812\n",
      "2019-01-07T19:49:32.295711, step: 483, loss: 0.3358725607395172, acc: 0.8438, auc: 0.9328, precision: 0.9286, recall: 0.7647\n",
      "2019-01-07T19:49:33.963872, step: 484, loss: 0.3285594582557678, acc: 0.8594, auc: 0.9358, precision: 0.9434, recall: 0.7692\n",
      "2019-01-07T19:49:35.710097, step: 485, loss: 0.34236156940460205, acc: 0.875, auc: 0.9264, precision: 0.9091, recall: 0.8197\n",
      "2019-01-07T19:49:37.444419, step: 486, loss: 0.31014513969421387, acc: 0.8906, auc: 0.9401, precision: 0.9, recall: 0.871\n",
      "2019-01-07T19:49:39.225240, step: 487, loss: 0.39957690238952637, acc: 0.8281, auc: 0.9073, precision: 0.8, recall: 0.8525\n",
      "2019-01-07T19:49:40.996050, step: 488, loss: 0.3350358009338379, acc: 0.8594, auc: 0.9308, precision: 0.9355, recall: 0.8056\n",
      "2019-01-07T19:49:42.641172, step: 489, loss: 0.24046413600444794, acc: 0.9297, auc: 0.9666, precision: 0.9365, recall: 0.9219\n",
      "2019-01-07T19:49:44.325981, step: 490, loss: 0.30362406373023987, acc: 0.8672, auc: 0.9453, precision: 0.9423, recall: 0.7778\n",
      "2019-01-07T19:49:45.961410, step: 491, loss: 0.3419634997844696, acc: 0.8359, auc: 0.9286, precision: 0.8246, recall: 0.8103\n",
      "2019-01-07T19:49:47.617784, step: 492, loss: 0.37328284978866577, acc: 0.8672, auc: 0.9154, precision: 0.9216, recall: 0.7833\n",
      "2019-01-07T19:49:49.352453, step: 493, loss: 0.37002861499786377, acc: 0.8281, auc: 0.9249, precision: 0.9535, recall: 0.6721\n",
      "2019-01-07T19:49:50.988377, step: 494, loss: 0.337710976600647, acc: 0.8281, auc: 0.935, precision: 0.9167, recall: 0.7639\n",
      "2019-01-07T19:49:52.666161, step: 495, loss: 0.26634353399276733, acc: 0.8984, auc: 0.9541, precision: 0.9388, recall: 0.8214\n",
      "2019-01-07T19:49:54.312009, step: 496, loss: 0.40293729305267334, acc: 0.8281, auc: 0.905, precision: 0.8116, recall: 0.8615\n",
      "2019-01-07T19:49:55.992071, step: 497, loss: 0.37941646575927734, acc: 0.8594, auc: 0.9182, precision: 0.8333, recall: 0.8621\n",
      "2019-01-07T19:49:57.648065, step: 498, loss: 0.3291657865047455, acc: 0.8594, auc: 0.9373, precision: 0.9355, recall: 0.8056\n",
      "2019-01-07T19:49:59.305156, step: 499, loss: 0.30903178453445435, acc: 0.8516, auc: 0.9478, precision: 0.9623, recall: 0.75\n",
      "2019-01-07T19:50:00.983936, step: 500, loss: 0.359418123960495, acc: 0.8125, auc: 0.9169, precision: 0.8478, recall: 0.6964\n",
      "\n",
      "Evaluation:\n",
      "2019-01-07T19:51:07.421588, step: 500, loss: 0.3578800115829859, acc: 0.8369410256410259, auc: 0.9268846153846153, precision: 0.8952076923076923, recall: 0.7695897435897435\n",
      "2019-01-07T19:51:09.052420, step: 501, loss: 0.3588583767414093, acc: 0.8516, auc: 0.9215, precision: 0.9, recall: 0.806\n",
      "2019-01-07T19:51:10.721773, step: 502, loss: 0.25994014739990234, acc: 0.875, auc: 0.9672, precision: 0.9322, recall: 0.8209\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-07T19:51:12.420021, step: 503, loss: 0.2705842852592468, acc: 0.8906, auc: 0.9613, precision: 0.8929, recall: 0.8621\n",
      "2019-01-07T19:51:14.113092, step: 504, loss: 0.28808966279029846, acc: 0.8906, auc: 0.9566, precision: 0.8621, recall: 0.8929\n",
      "2019-01-07T19:51:15.833891, step: 505, loss: 0.277313232421875, acc: 0.8828, auc: 0.9565, precision: 0.8983, recall: 0.8548\n",
      "2019-01-07T19:51:17.576001, step: 506, loss: 0.2986338138580322, acc: 0.8281, auc: 0.9457, precision: 0.8947, recall: 0.7612\n",
      "2019-01-07T19:51:19.241314, step: 507, loss: 0.3907879590988159, acc: 0.7969, auc: 0.915, precision: 0.8824, recall: 0.6923\n",
      "2019-01-07T19:51:20.949855, step: 508, loss: 0.3794049024581909, acc: 0.8203, auc: 0.9287, precision: 0.9787, recall: 0.6765\n",
      "2019-01-07T19:51:22.616229, step: 509, loss: 0.26025670766830444, acc: 0.8594, auc: 0.9663, precision: 0.9615, recall: 0.7576\n",
      "2019-01-07T19:51:24.284769, step: 510, loss: 0.33729588985443115, acc: 0.8672, auc: 0.9355, precision: 0.8689, recall: 0.8548\n",
      "2019-01-07T19:51:26.021505, step: 511, loss: 0.3546940088272095, acc: 0.8438, auc: 0.9402, precision: 0.8243, recall: 0.8971\n",
      "2019-01-07T19:51:27.645477, step: 512, loss: 0.3653768301010132, acc: 0.8438, auc: 0.9338, precision: 0.8108, recall: 0.9091\n",
      "2019-01-07T19:51:29.335844, step: 513, loss: 0.4013706147670746, acc: 0.8203, auc: 0.9021, precision: 0.7736, recall: 0.7885\n",
      "2019-01-07T19:51:31.055806, step: 514, loss: 0.3318352699279785, acc: 0.8203, auc: 0.9419, precision: 0.9429, recall: 0.6111\n",
      "2019-01-07T19:51:32.732225, step: 515, loss: 0.36423933506011963, acc: 0.8281, auc: 0.9273, precision: 0.9189, recall: 0.6415\n",
      "2019-01-07T19:51:34.453470, step: 516, loss: 0.4680178165435791, acc: 0.7891, auc: 0.9038, precision: 0.9512, recall: 0.6094\n",
      "2019-01-07T19:51:36.148696, step: 517, loss: 0.34302330017089844, acc: 0.8125, auc: 0.936, precision: 0.9298, recall: 0.726\n",
      "2019-01-07T19:51:37.845771, step: 518, loss: 0.26579397916793823, acc: 0.8906, auc: 0.9641, precision: 0.9811, recall: 0.8\n",
      "2019-01-07T19:51:39.540478, step: 519, loss: 0.4265640676021576, acc: 0.8359, auc: 0.9084, precision: 0.8182, recall: 0.9\n",
      "2019-01-07T19:51:41.287034, step: 520, loss: 0.43390196561813354, acc: 0.8281, auc: 0.9525, precision: 0.7286, recall: 0.9444\n",
      "2019-01-07T19:51:42.997690, step: 521, loss: 0.2590444087982178, acc: 0.9297, auc: 0.9705, precision: 0.8947, recall: 0.9855\n",
      "2019-01-07T19:51:44.692544, step: 522, loss: 0.39832115173339844, acc: 0.8047, auc: 0.908, precision: 0.8281, recall: 0.791\n",
      "2019-01-07T19:51:46.379996, step: 523, loss: 0.3387947082519531, acc: 0.8047, auc: 0.9331, precision: 0.9057, recall: 0.7059\n",
      "2019-01-07T19:51:48.148283, step: 524, loss: 0.35469627380371094, acc: 0.8125, auc: 0.9425, precision: 0.9455, recall: 0.7123\n",
      "2019-01-07T19:51:49.891549, step: 525, loss: 0.40538519620895386, acc: 0.8203, auc: 0.9071, precision: 0.9677, recall: 0.5769\n",
      "2019-01-07T19:51:51.602214, step: 526, loss: 0.4423673450946808, acc: 0.7656, auc: 0.9086, precision: 0.975, recall: 0.5735\n",
      "2019-01-07T19:51:53.312870, step: 527, loss: 0.2855103611946106, acc: 0.8672, auc: 0.9526, precision: 0.9194, recall: 0.8261\n",
      "2019-01-07T19:51:55.052658, step: 528, loss: 0.3245755732059479, acc: 0.875, auc: 0.9379, precision: 0.9265, recall: 0.8514\n",
      "2019-01-07T19:51:56.885245, step: 529, loss: 0.45242810249328613, acc: 0.8281, auc: 0.9385, precision: 0.7324, recall: 0.9455\n",
      "2019-01-07T19:51:58.613533, step: 530, loss: 0.36668962240219116, acc: 0.875, auc: 0.9441, precision: 0.8333, recall: 0.9375\n",
      "2019-01-07T19:52:00.306129, step: 531, loss: 0.2981897294521332, acc: 0.9219, auc: 0.9658, precision: 0.8966, recall: 0.9286\n",
      "2019-01-07T19:52:01.965036, step: 532, loss: 0.36305809020996094, acc: 0.8281, auc: 0.9194, precision: 0.8793, recall: 0.7727\n",
      "2019-01-07T19:52:03.647134, step: 533, loss: 0.3042207360267639, acc: 0.8516, auc: 0.9599, precision: 0.9796, recall: 0.7273\n",
      "2019-01-07T19:52:05.270242, step: 534, loss: 0.3549129068851471, acc: 0.8047, auc: 0.93, precision: 0.925, recall: 0.6271\n",
      "2019-01-07T19:52:06.974351, step: 535, loss: 0.3472403287887573, acc: 0.7734, auc: 0.9312, precision: 0.8889, recall: 0.625\n",
      "2019-01-07T19:52:08.680215, step: 536, loss: 0.3899819552898407, acc: 0.8359, auc: 0.9229, precision: 0.96, recall: 0.7164\n",
      "2019-01-07T19:52:10.343545, step: 537, loss: 0.4540943205356598, acc: 0.7266, auc: 0.8962, precision: 0.8913, recall: 0.5775\n",
      "2019-01-07T19:52:12.059423, step: 538, loss: 0.38066351413726807, acc: 0.8438, auc: 0.9107, precision: 0.873, recall: 0.8209\n",
      "2019-01-07T19:52:13.705503, step: 539, loss: 0.2864032983779907, acc: 0.8828, auc: 0.9531, precision: 0.8906, recall: 0.8769\n",
      "2019-01-07T19:52:15.495818, step: 540, loss: 0.29431307315826416, acc: 0.8672, auc: 0.9596, precision: 0.8676, recall: 0.8806\n",
      "2019-01-07T19:52:17.152333, step: 541, loss: 0.38222774863243103, acc: 0.8516, auc: 0.9172, precision: 0.8358, recall: 0.875\n",
      "2019-01-07T19:52:18.862477, step: 542, loss: 0.3956981599330902, acc: 0.8203, auc: 0.9027, precision: 0.8406, recall: 0.8286\n",
      "2019-01-07T19:52:20.598255, step: 543, loss: 0.3398301899433136, acc: 0.8672, auc: 0.9385, precision: 0.8493, recall: 0.9118\n",
      "2019-01-07T19:52:22.311603, step: 544, loss: 0.39062970876693726, acc: 0.8203, auc: 0.9055, precision: 0.8448, recall: 0.7778\n",
      "2019-01-07T19:52:23.965893, step: 545, loss: 0.27334368228912354, acc: 0.8906, auc: 0.9585, precision: 0.9062, recall: 0.8788\n",
      "2019-01-07T19:52:25.690293, step: 546, loss: 0.42325231432914734, acc: 0.8125, auc: 0.9002, precision: 0.8913, recall: 0.6833\n",
      "2019-01-07T19:52:27.392233, step: 547, loss: 0.4154380261898041, acc: 0.7969, auc: 0.9222, precision: 0.9767, recall: 0.6269\n",
      "2019-01-07T19:52:29.108496, step: 548, loss: 0.39524638652801514, acc: 0.8047, auc: 0.9248, precision: 0.9231, recall: 0.6957\n",
      "2019-01-07T19:52:30.867318, step: 549, loss: 0.3192843198776245, acc: 0.8203, auc: 0.9499, precision: 0.9375, recall: 0.6923\n",
      "2019-01-07T19:52:32.644173, step: 550, loss: 0.3011651635169983, acc: 0.8828, auc: 0.9504, precision: 0.9298, recall: 0.8281\n",
      "2019-01-07T19:52:34.308391, step: 551, loss: 0.28958427906036377, acc: 0.9141, auc: 0.9621, precision: 0.8986, recall: 0.9394\n",
      "2019-01-07T19:52:35.976318, step: 552, loss: 0.31669849157333374, acc: 0.8594, auc: 0.9392, precision: 0.8824, recall: 0.8571\n",
      "2019-01-07T19:52:37.687576, step: 553, loss: 0.4246385991573334, acc: 0.8672, auc: 0.9225, precision: 0.8214, recall: 0.8679\n",
      "2019-01-07T19:52:39.322243, step: 554, loss: 0.28940266370773315, acc: 0.8828, auc: 0.9597, precision: 0.875, recall: 0.8889\n",
      "2019-01-07T19:52:41.089005, step: 555, loss: 0.29686158895492554, acc: 0.8828, auc: 0.9463, precision: 0.931, recall: 0.8308\n",
      "2019-01-07T19:52:42.812669, step: 556, loss: 0.32342860102653503, acc: 0.8203, auc: 0.935, precision: 0.8776, recall: 0.7167\n",
      "2019-01-07T19:52:44.500930, step: 557, loss: 0.3372945189476013, acc: 0.8672, auc: 0.9294, precision: 0.9153, recall: 0.8182\n",
      "2019-01-07T19:52:46.320746, step: 558, loss: 0.3805653750896454, acc: 0.8438, auc: 0.9135, precision: 0.9167, recall: 0.7333\n",
      "2019-01-07T19:52:48.052250, step: 559, loss: 0.2782646715641022, acc: 0.8672, auc: 0.968, precision: 0.9792, recall: 0.746\n",
      "2019-01-07T19:52:49.789325, step: 560, loss: 0.3251922130584717, acc: 0.8438, auc: 0.9436, precision: 0.9231, recall: 0.75\n",
      "2019-01-07T19:52:51.508188, step: 561, loss: 0.35758331418037415, acc: 0.8281, auc: 0.9239, precision: 0.9245, recall: 0.7313\n",
      "2019-01-07T19:52:53.228467, step: 562, loss: 0.33347147703170776, acc: 0.8438, auc: 0.9319, precision: 0.8871, recall: 0.8088\n",
      "2019-01-07T19:52:54.960190, step: 563, loss: 0.36884164810180664, acc: 0.8438, auc: 0.9144, precision: 0.8983, recall: 0.791\n",
      "2019-01-07T19:52:56.648175, step: 564, loss: 0.34205105900764465, acc: 0.8594, auc: 0.9414, precision: 0.8462, recall: 0.8148\n",
      "2019-01-07T19:52:58.380456, step: 565, loss: 0.3919554352760315, acc: 0.8359, auc: 0.9091, precision: 0.8529, recall: 0.8406\n",
      "2019-01-07T19:53:00.128611, step: 566, loss: 0.28032034635543823, acc: 0.9062, auc: 0.9563, precision: 0.9074, recall: 0.875\n",
      "2019-01-07T19:53:01.844802, step: 567, loss: 0.28248104453086853, acc: 0.8672, auc: 0.9509, precision: 0.9623, recall: 0.7727\n",
      "2019-01-07T19:53:03.610188, step: 568, loss: 0.43934762477874756, acc: 0.8047, auc: 0.89, precision: 0.9216, recall: 0.6912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-07T19:53:05.347753, step: 569, loss: 0.24337232112884521, acc: 0.8672, auc: 0.969, precision: 0.9245, recall: 0.7903\n",
      "2019-01-07T19:53:07.060599, step: 570, loss: 0.2934703230857849, acc: 0.8516, auc: 0.9462, precision: 0.9091, recall: 0.7273\n",
      "2019-01-07T19:53:08.739823, step: 571, loss: 0.341219037771225, acc: 0.8203, auc: 0.9333, precision: 0.9298, recall: 0.7361\n",
      "2019-01-07T19:53:10.427678, step: 572, loss: 0.26218053698539734, acc: 0.8906, auc: 0.9633, precision: 0.9444, recall: 0.8226\n",
      "2019-01-07T19:53:12.065751, step: 573, loss: 0.27557572722435, acc: 0.8594, auc: 0.9584, precision: 0.9153, recall: 0.806\n",
      "2019-01-07T19:53:13.767471, step: 574, loss: 0.3926341235637665, acc: 0.8125, auc: 0.9066, precision: 0.7895, recall: 0.7895\n",
      "2019-01-07T19:53:15.380772, step: 575, loss: 0.3282316029071808, acc: 0.8828, auc: 0.9345, precision: 0.9, recall: 0.8873\n",
      "2019-01-07T19:53:17.068663, step: 576, loss: 0.2974424362182617, acc: 0.8516, auc: 0.9483, precision: 0.8596, recall: 0.8167\n",
      "2019-01-07T19:53:18.708160, step: 577, loss: 0.39368367195129395, acc: 0.7969, auc: 0.9067, precision: 0.8871, recall: 0.7432\n",
      "2019-01-07T19:53:20.393545, step: 578, loss: 0.3153870701789856, acc: 0.8516, auc: 0.947, precision: 0.8947, recall: 0.7969\n",
      "2019-01-07T19:53:22.172823, step: 579, loss: 0.23449935019016266, acc: 0.9141, auc: 0.9705, precision: 0.9206, recall: 0.9062\n",
      "2019-01-07T19:53:23.877005, step: 580, loss: 0.38091611862182617, acc: 0.8047, auc: 0.9179, precision: 0.9057, recall: 0.7059\n",
      "2019-01-07T19:53:25.567952, step: 581, loss: 0.27707067131996155, acc: 0.8438, auc: 0.9577, precision: 0.9434, recall: 0.7463\n",
      "2019-01-07T19:53:27.335144, step: 582, loss: 0.5355676412582397, acc: 0.8047, auc: 0.8439, precision: 0.8302, recall: 0.7333\n",
      "2019-01-07T19:53:29.134388, step: 583, loss: 0.31853801012039185, acc: 0.8359, auc: 0.9375, precision: 0.8571, recall: 0.75\n",
      "2019-01-07T19:53:30.901246, step: 584, loss: 0.3067227602005005, acc: 0.8359, auc: 0.938, precision: 0.8444, recall: 0.7308\n",
      "2019-01-07T19:53:32.690280, step: 585, loss: 0.39777418971061707, acc: 0.7656, auc: 0.9045, precision: 0.7797, recall: 0.7302\n",
      "2019-01-07T19:53:34.399871, step: 586, loss: 0.3411848545074463, acc: 0.8359, auc: 0.9306, precision: 0.8475, recall: 0.8065\n",
      "2019-01-07T19:53:36.120457, step: 587, loss: 0.4176890254020691, acc: 0.7891, auc: 0.8904, precision: 0.8776, recall: 0.6719\n",
      "2019-01-07T19:53:37.849095, step: 588, loss: 0.4701874256134033, acc: 0.8047, auc: 0.8698, precision: 0.86, recall: 0.7049\n",
      "2019-01-07T19:53:39.504578, step: 589, loss: 0.3668487071990967, acc: 0.8203, auc: 0.9194, precision: 0.918, recall: 0.7568\n",
      "2019-01-07T19:53:41.175529, step: 590, loss: 0.30005788803100586, acc: 0.8828, auc: 0.9447, precision: 0.9143, recall: 0.8767\n",
      "2019-01-07T19:53:43.040087, step: 591, loss: 0.3494100570678711, acc: 0.8438, auc: 0.9316, precision: 0.8621, recall: 0.8065\n",
      "2019-01-07T19:53:44.729826, step: 592, loss: 0.3723517954349518, acc: 0.8203, auc: 0.9137, precision: 0.9091, recall: 0.7353\n",
      "2019-01-07T19:53:46.436477, step: 593, loss: 0.35939231514930725, acc: 0.8281, auc: 0.9199, precision: 0.8596, recall: 0.7778\n",
      "2019-01-07T19:53:48.199485, step: 594, loss: 0.2590143084526062, acc: 0.8906, auc: 0.9658, precision: 0.9636, recall: 0.8154\n",
      "2019-01-07T19:53:49.968014, step: 595, loss: 0.41599246859550476, acc: 0.7812, auc: 0.8858, precision: 0.7719, recall: 0.7458\n",
      "2019-01-07T19:53:51.667161, step: 596, loss: 0.3921722173690796, acc: 0.7656, auc: 0.9154, precision: 0.94, recall: 0.6351\n",
      "2019-01-07T19:53:53.330260, step: 597, loss: 0.3009909391403198, acc: 0.8672, auc: 0.9502, precision: 0.8868, recall: 0.8103\n",
      "2019-01-07T19:53:54.981468, step: 598, loss: 0.34777653217315674, acc: 0.875, auc: 0.9243, precision: 0.86, recall: 0.8269\n",
      "2019-01-07T19:53:56.712037, step: 599, loss: 0.45938220620155334, acc: 0.8047, auc: 0.8742, precision: 0.7885, recall: 0.7455\n",
      "2019-01-07T19:53:58.379787, step: 600, loss: 0.3325427174568176, acc: 0.8516, auc: 0.9358, precision: 0.96, recall: 0.7385\n",
      "\n",
      "Evaluation:\n",
      "2019-01-07T19:55:05.716355, step: 600, loss: 0.3767148783573738, acc: 0.8213076923076923, auc: 0.9242641025641024, precision: 0.9088641025641027, recall: 0.7183512820512821\n",
      "2019-01-07T19:55:07.403721, step: 601, loss: 0.26483726501464844, acc: 0.8359, auc: 0.9681, precision: 0.8958, recall: 0.7288\n",
      "2019-01-07T19:55:09.068695, step: 602, loss: 0.30620619654655457, acc: 0.8594, auc: 0.9464, precision: 0.9455, recall: 0.7761\n",
      "2019-01-07T19:55:10.735713, step: 603, loss: 0.25279831886291504, acc: 0.9062, auc: 0.9675, precision: 0.9231, recall: 0.8571\n",
      "2019-01-07T19:55:12.481638, step: 604, loss: 0.3272603750228882, acc: 0.8438, auc: 0.9423, precision: 0.9434, recall: 0.7463\n",
      "2019-01-07T19:55:14.303755, step: 605, loss: 0.291115939617157, acc: 0.8594, auc: 0.9487, precision: 0.9074, recall: 0.7903\n",
      "2019-01-07T19:55:16.055620, step: 606, loss: 0.35851895809173584, acc: 0.8359, auc: 0.9163, precision: 0.8966, recall: 0.7761\n",
      "2019-01-07T19:55:17.780832, step: 607, loss: 0.30214083194732666, acc: 0.8906, auc: 0.9454, precision: 0.9054, recall: 0.9054\n",
      "2019-01-07T19:55:19.457071, step: 608, loss: 0.36088383197784424, acc: 0.8047, auc: 0.925, precision: 0.8033, recall: 0.7903\n",
      "2019-01-07T19:55:21.268271, step: 609, loss: 0.28063008189201355, acc: 0.9062, auc: 0.9578, precision: 0.875, recall: 0.875\n",
      "2019-01-07T19:55:22.995512, step: 610, loss: 0.3730471134185791, acc: 0.8359, auc: 0.9136, precision: 0.8772, recall: 0.7812\n",
      "2019-01-07T19:55:24.723732, step: 611, loss: 0.3644868731498718, acc: 0.8828, auc: 0.9204, precision: 0.9444, recall: 0.8095\n",
      "2019-01-07T19:55:26.508799, step: 612, loss: 0.3126165270805359, acc: 0.8516, auc: 0.9467, precision: 0.9796, recall: 0.7273\n",
      "2019-01-07T19:55:28.224345, step: 613, loss: 0.33235737681388855, acc: 0.8906, auc: 0.9299, precision: 0.9298, recall: 0.8413\n",
      "2019-01-07T19:55:29.972189, step: 614, loss: 0.33719295263290405, acc: 0.8672, auc: 0.9365, precision: 0.9242, recall: 0.8356\n",
      "2019-01-07T19:55:31.769805, step: 615, loss: 0.3187130391597748, acc: 0.8828, auc: 0.9457, precision: 0.8519, recall: 0.8679\n",
      "2019-01-07T19:55:33.505618, step: 616, loss: 0.41529664397239685, acc: 0.8359, auc: 0.8928, precision: 0.8793, recall: 0.7846\n",
      "2019-01-07T19:55:35.242320, step: 617, loss: 0.28396469354629517, acc: 0.8828, auc: 0.9493, precision: 0.9231, recall: 0.8571\n",
      "2019-01-07T19:55:36.962399, step: 618, loss: 0.2967971861362457, acc: 0.8594, auc: 0.9501, precision: 0.8971, recall: 0.8472\n",
      "2019-01-07T19:55:38.727258, step: 619, loss: 0.29772233963012695, acc: 0.8672, auc: 0.9473, precision: 0.9123, recall: 0.8125\n",
      "2019-01-07T19:55:40.519859, step: 620, loss: 0.30093538761138916, acc: 0.8672, auc: 0.944, precision: 0.9038, recall: 0.7966\n",
      "2019-01-07T19:55:42.277559, step: 621, loss: 0.31223630905151367, acc: 0.8359, auc: 0.944, precision: 0.9091, recall: 0.7576\n",
      "2019-01-07T19:55:43.991872, step: 622, loss: 0.3205621540546417, acc: 0.8438, auc: 0.9425, precision: 0.9434, recall: 0.7463\n",
      "2019-01-07T19:55:45.753189, step: 623, loss: 0.29458698630332947, acc: 0.8828, auc: 0.9502, precision: 0.98, recall: 0.7778\n",
      "2019-01-07T19:55:47.643463, step: 624, loss: 0.2969655394554138, acc: 0.8672, auc: 0.9514, precision: 0.9, recall: 0.8308\n",
      "start training model\n",
      "2019-01-07T19:55:49.361310, step: 625, loss: 0.260386198759079, acc: 0.875, auc: 0.9631, precision: 0.95, recall: 0.8143\n",
      "2019-01-07T19:55:51.112650, step: 626, loss: 0.3151504099369049, acc: 0.8594, auc: 0.9436, precision: 0.8704, recall: 0.8103\n",
      "2019-01-07T19:55:52.883974, step: 627, loss: 0.2305426299571991, acc: 0.875, auc: 0.9731, precision: 0.98, recall: 0.7656\n",
      "2019-01-07T19:55:54.641155, step: 628, loss: 0.30330806970596313, acc: 0.8594, auc: 0.9443, precision: 0.9245, recall: 0.7778\n",
      "2019-01-07T19:55:56.347132, step: 629, loss: 0.3425484001636505, acc: 0.8516, auc: 0.9292, precision: 0.9245, recall: 0.7656\n",
      "2019-01-07T19:55:58.101456, step: 630, loss: 0.3408067226409912, acc: 0.8516, auc: 0.9344, precision: 0.8281, recall: 0.8689\n",
      "2019-01-07T19:55:59.793084, step: 631, loss: 0.4057779312133789, acc: 0.8203, auc: 0.9052, precision: 0.8421, recall: 0.7742\n",
      "2019-01-07T19:56:01.491834, step: 632, loss: 0.29922008514404297, acc: 0.8672, auc: 0.9462, precision: 0.8824, recall: 0.8036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-07T19:56:03.256078, step: 633, loss: 0.2729530334472656, acc: 0.8828, auc: 0.9543, precision: 0.9355, recall: 0.8406\n",
      "2019-01-07T19:56:04.977324, step: 634, loss: 0.3732309937477112, acc: 0.8359, auc: 0.916, precision: 0.913, recall: 0.7119\n",
      "2019-01-07T19:56:06.647111, step: 635, loss: 0.35369789600372314, acc: 0.7969, auc: 0.9244, precision: 0.8947, recall: 0.6071\n",
      "2019-01-07T19:56:08.440318, step: 636, loss: 0.3302629292011261, acc: 0.8984, auc: 0.9389, precision: 0.9623, recall: 0.8226\n",
      "2019-01-07T19:56:10.207547, step: 637, loss: 0.2338360846042633, acc: 0.9219, auc: 0.9669, precision: 0.9592, recall: 0.8545\n",
      "2019-01-07T19:56:11.881485, step: 638, loss: 0.3343663215637207, acc: 0.8203, auc: 0.9294, precision: 0.85, recall: 0.7846\n",
      "2019-01-07T19:56:13.563527, step: 639, loss: 0.29882681369781494, acc: 0.8828, auc: 0.9435, precision: 0.918, recall: 0.8485\n",
      "2019-01-07T19:56:15.297409, step: 640, loss: 0.24661129713058472, acc: 0.9453, auc: 0.9618, precision: 0.9574, recall: 0.9\n",
      "2019-01-07T19:56:17.059851, step: 641, loss: 0.2652813792228699, acc: 0.8828, auc: 0.9591, precision: 0.8833, recall: 0.8689\n",
      "2019-01-07T19:56:18.832360, step: 642, loss: 0.26635104417800903, acc: 0.875, auc: 0.9624, precision: 0.9434, recall: 0.7937\n",
      "2019-01-07T19:56:20.611213, step: 643, loss: 0.2994479537010193, acc: 0.8984, auc: 0.9504, precision: 0.918, recall: 0.875\n",
      "2019-01-07T19:56:22.395813, step: 644, loss: 0.2874489426612854, acc: 0.8594, auc: 0.9547, precision: 0.9455, recall: 0.7761\n",
      "2019-01-07T19:56:24.083991, step: 645, loss: 0.2853732705116272, acc: 0.875, auc: 0.9517, precision: 0.9, recall: 0.8438\n",
      "2019-01-07T19:56:25.780673, step: 646, loss: 0.2508767247200012, acc: 0.8984, auc: 0.9666, precision: 0.918, recall: 0.875\n",
      "2019-01-07T19:56:27.469132, step: 647, loss: 0.29770153760910034, acc: 0.8672, auc: 0.9448, precision: 0.9016, recall: 0.8333\n",
      "2019-01-07T19:56:29.187985, step: 648, loss: 0.3371964395046234, acc: 0.8359, auc: 0.9319, precision: 0.8525, recall: 0.8125\n",
      "2019-01-07T19:56:30.977193, step: 649, loss: 0.3363332450389862, acc: 0.8359, auc: 0.9326, precision: 0.7917, recall: 0.7755\n",
      "2019-01-07T19:56:32.699673, step: 650, loss: 0.3048234283924103, acc: 0.8203, auc: 0.947, precision: 0.8727, recall: 0.75\n",
      "2019-01-07T19:56:34.461303, step: 651, loss: 0.30395588278770447, acc: 0.8281, auc: 0.9478, precision: 0.9318, recall: 0.6833\n",
      "2019-01-07T19:56:36.200805, step: 652, loss: 0.26095131039619446, acc: 0.8594, auc: 0.9713, precision: 0.9811, recall: 0.7536\n",
      "2019-01-07T19:56:37.977969, step: 653, loss: 0.32662272453308105, acc: 0.8281, auc: 0.9438, precision: 0.9286, recall: 0.7429\n",
      "2019-01-07T19:56:39.825560, step: 654, loss: 0.24939776957035065, acc: 0.8828, auc: 0.9643, precision: 0.8788, recall: 0.8923\n",
      "2019-01-07T19:56:41.629152, step: 655, loss: 0.29842400550842285, acc: 0.8594, auc: 0.9447, precision: 0.8696, recall: 0.8696\n",
      "2019-01-07T19:56:43.422348, step: 656, loss: 0.2579842805862427, acc: 0.9219, auc: 0.9672, precision: 0.9385, recall: 0.9104\n",
      "2019-01-07T19:56:45.248993, step: 657, loss: 0.406436562538147, acc: 0.8438, auc: 0.9292, precision: 0.8143, recall: 0.8906\n",
      "2019-01-07T19:56:47.056517, step: 658, loss: 0.3149903416633606, acc: 0.8438, auc: 0.9399, precision: 0.8594, recall: 0.8333\n",
      "2019-01-07T19:56:48.836505, step: 659, loss: 0.23764148354530334, acc: 0.8906, auc: 0.9704, precision: 0.9483, recall: 0.8333\n",
      "2019-01-07T19:56:50.637581, step: 660, loss: 0.41934067010879517, acc: 0.7734, auc: 0.9321, precision: 0.9231, recall: 0.5806\n",
      "2019-01-07T19:56:52.415415, step: 661, loss: 0.3228238523006439, acc: 0.8438, auc: 0.945, precision: 0.9298, recall: 0.7681\n",
      "2019-01-07T19:56:54.219302, step: 662, loss: 0.4044334888458252, acc: 0.7891, auc: 0.9173, precision: 0.9259, recall: 0.6849\n",
      "2019-01-07T19:56:56.000505, step: 663, loss: 0.3635369539260864, acc: 0.8594, auc: 0.9166, precision: 0.8772, recall: 0.8197\n",
      "2019-01-07T19:56:57.796259, step: 664, loss: 0.2917710840702057, acc: 0.8984, auc: 0.9582, precision: 0.8769, recall: 0.9194\n",
      "2019-01-07T19:56:59.597500, step: 665, loss: 0.2809768617153168, acc: 0.9062, auc: 0.967, precision: 0.8806, recall: 0.9365\n",
      "2019-01-07T19:57:01.391645, step: 666, loss: 0.3878544270992279, acc: 0.8516, auc: 0.9106, precision: 0.871, recall: 0.8308\n",
      "2019-01-07T19:57:03.189555, step: 667, loss: 0.40425676107406616, acc: 0.8438, auc: 0.9314, precision: 0.8056, recall: 0.9062\n",
      "2019-01-07T19:57:04.918098, step: 668, loss: 0.2414139360189438, acc: 0.875, auc: 0.968, precision: 0.9167, recall: 0.8333\n",
      "2019-01-07T19:57:06.651699, step: 669, loss: 0.34822237491607666, acc: 0.8047, auc: 0.9421, precision: 0.9318, recall: 0.6508\n",
      "2019-01-07T19:57:08.496883, step: 670, loss: 0.3251875638961792, acc: 0.8516, auc: 0.9503, precision: 0.975, recall: 0.6842\n",
      "2019-01-07T19:57:10.320364, step: 671, loss: 0.3615913391113281, acc: 0.7891, auc: 0.9413, precision: 0.9211, recall: 0.5932\n",
      "2019-01-07T19:57:12.092636, step: 672, loss: 0.3759516775608063, acc: 0.8281, auc: 0.9108, precision: 0.8571, recall: 0.7368\n",
      "2019-01-07T19:57:13.887428, step: 673, loss: 0.28619781136512756, acc: 0.8906, auc: 0.9563, precision: 0.9385, recall: 0.8592\n",
      "2019-01-07T19:57:15.706075, step: 674, loss: 0.2960817515850067, acc: 0.8672, auc: 0.9472, precision: 0.9062, recall: 0.8406\n",
      "2019-01-07T19:57:17.491882, step: 675, loss: 0.3482169806957245, acc: 0.8516, auc: 0.9468, precision: 0.8095, recall: 0.8793\n",
      "2019-01-07T19:57:19.202009, step: 676, loss: 0.43679875135421753, acc: 0.8203, auc: 0.8936, precision: 0.8025, recall: 0.9028\n",
      "2019-01-07T19:57:20.989997, step: 677, loss: 0.2883456349372864, acc: 0.8984, auc: 0.9582, precision: 0.8676, recall: 0.9365\n",
      "2019-01-07T19:57:22.752615, step: 678, loss: 0.3280247449874878, acc: 0.8438, auc: 0.9318, precision: 0.8723, recall: 0.7455\n",
      "2019-01-07T19:57:24.476594, step: 679, loss: 0.2678494155406952, acc: 0.8906, auc: 0.9626, precision: 0.9434, recall: 0.8197\n",
      "2019-01-07T19:57:26.227554, step: 680, loss: 0.3306035101413727, acc: 0.8125, auc: 0.9602, precision: 0.9787, recall: 0.6667\n",
      "2019-01-07T19:57:28.009698, step: 681, loss: 0.37006184458732605, acc: 0.8516, auc: 0.9211, precision: 0.9767, recall: 0.7\n",
      "2019-01-07T19:57:29.833157, step: 682, loss: 0.3170013427734375, acc: 0.8359, auc: 0.9385, precision: 0.8824, recall: 0.75\n",
      "2019-01-07T19:57:31.609634, step: 683, loss: 0.31118857860565186, acc: 0.8359, auc: 0.9396, precision: 0.8689, recall: 0.803\n",
      "2019-01-07T19:57:33.456926, step: 684, loss: 0.28246527910232544, acc: 0.9062, auc: 0.96, precision: 0.8814, recall: 0.9123\n",
      "2019-01-07T19:57:35.161267, step: 685, loss: 0.3244268298149109, acc: 0.8438, auc: 0.936, precision: 0.8525, recall: 0.8254\n",
      "2019-01-07T19:57:36.968829, step: 686, loss: 0.36148130893707275, acc: 0.8594, auc: 0.9289, precision: 0.8548, recall: 0.8548\n",
      "2019-01-07T19:57:38.687920, step: 687, loss: 0.3070254623889923, acc: 0.875, auc: 0.9423, precision: 0.9344, recall: 0.8261\n",
      "2019-01-07T19:57:40.392778, step: 688, loss: 0.2977093458175659, acc: 0.875, auc: 0.9431, precision: 0.9306, recall: 0.859\n",
      "2019-01-07T19:57:42.112974, step: 689, loss: 0.32484978437423706, acc: 0.8359, auc: 0.9323, precision: 0.9091, recall: 0.7576\n",
      "2019-01-07T19:57:43.922223, step: 690, loss: 0.32742756605148315, acc: 0.7969, auc: 0.9355, precision: 0.8793, recall: 0.7286\n",
      "2019-01-07T19:57:45.695722, step: 691, loss: 0.37184303998947144, acc: 0.8594, auc: 0.9167, precision: 0.8548, recall: 0.8548\n",
      "2019-01-07T19:57:47.389448, step: 692, loss: 0.3594566881656647, acc: 0.8047, auc: 0.9189, precision: 0.8723, recall: 0.6833\n",
      "2019-01-07T19:57:49.201319, step: 693, loss: 0.2897832691669464, acc: 0.8672, auc: 0.9499, precision: 0.92, recall: 0.7797\n",
      "2019-01-07T19:57:50.945323, step: 694, loss: 0.30199289321899414, acc: 0.8828, auc: 0.9457, precision: 0.9636, recall: 0.803\n",
      "2019-01-07T19:57:52.662775, step: 695, loss: 0.34029707312583923, acc: 0.8359, auc: 0.9291, precision: 0.8596, recall: 0.7903\n",
      "2019-01-07T19:57:54.432284, step: 696, loss: 0.35596489906311035, acc: 0.7891, auc: 0.9234, precision: 0.9057, recall: 0.6857\n",
      "2019-01-07T19:57:56.200558, step: 697, loss: 0.409397155046463, acc: 0.8359, auc: 0.8966, precision: 0.8364, recall: 0.7931\n",
      "2019-01-07T19:57:57.993002, step: 698, loss: 0.3786914646625519, acc: 0.8594, auc: 0.9219, precision: 0.8393, recall: 0.8393\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-07T19:57:59.780965, step: 699, loss: 0.3479347229003906, acc: 0.8516, auc: 0.9353, precision: 0.8361, recall: 0.85\n",
      "2019-01-07T19:58:01.540506, step: 700, loss: 0.2810538709163666, acc: 0.8828, auc: 0.9591, precision: 0.8727, recall: 0.8571\n",
      "\n",
      "Evaluation:\n",
      "2019-01-07T19:59:10.738283, step: 700, loss: 0.34041016835432786, acc: 0.8459589743589745, auc: 0.9336410256410256, precision: 0.9001743589743588, recall: 0.7810128205128205\n",
      "2019-01-07T19:59:12.463846, step: 701, loss: 0.33732765913009644, acc: 0.8281, auc: 0.9303, precision: 0.8545, recall: 0.7705\n",
      "2019-01-07T19:59:14.253041, step: 702, loss: 0.2996540069580078, acc: 0.9219, auc: 0.9427, precision: 0.96, recall: 0.8571\n",
      "2019-01-07T19:59:16.114288, step: 703, loss: 0.4371894299983978, acc: 0.7734, auc: 0.8893, precision: 0.9024, recall: 0.5968\n",
      "2019-01-07T19:59:17.882704, step: 704, loss: 0.3464062213897705, acc: 0.7656, auc: 0.9451, precision: 0.9167, recall: 0.6286\n",
      "2019-01-07T19:59:19.575442, step: 705, loss: 0.3424730896949768, acc: 0.8125, auc: 0.9288, precision: 0.8689, recall: 0.7681\n",
      "2019-01-07T19:59:21.300506, step: 706, loss: 0.26584887504577637, acc: 0.8984, auc: 0.9656, precision: 0.9464, recall: 0.8413\n",
      "2019-01-07T19:59:23.088218, step: 707, loss: 0.36297982931137085, acc: 0.8672, auc: 0.9386, precision: 0.85, recall: 0.8644\n",
      "2019-01-07T19:59:24.887547, step: 708, loss: 0.37888777256011963, acc: 0.8672, auc: 0.9426, precision: 0.8243, recall: 0.9385\n",
      "2019-01-07T19:59:26.620194, step: 709, loss: 0.21289795637130737, acc: 0.9141, auc: 0.9806, precision: 0.9672, recall: 0.8676\n",
      "2019-01-07T19:59:28.315122, step: 710, loss: 0.2489028126001358, acc: 0.8672, auc: 0.9685, precision: 0.9508, recall: 0.8056\n",
      "2019-01-07T19:59:30.073341, step: 711, loss: 0.3102799355983734, acc: 0.8594, auc: 0.9475, precision: 0.96, recall: 0.75\n",
      "2019-01-07T19:59:31.889883, step: 712, loss: 0.31204649806022644, acc: 0.8359, auc: 0.9487, precision: 0.9355, recall: 0.7733\n",
      "2019-01-07T19:59:33.603810, step: 713, loss: 0.19490331411361694, acc: 0.9297, auc: 0.9839, precision: 0.9825, recall: 0.875\n",
      "2019-01-07T19:59:35.378888, step: 714, loss: 0.37330299615859985, acc: 0.8125, auc: 0.9216, precision: 0.791, recall: 0.8413\n",
      "2019-01-07T19:59:37.164311, step: 715, loss: 0.35382863879203796, acc: 0.8438, auc: 0.9213, precision: 0.875, recall: 0.8235\n",
      "2019-01-07T19:59:38.987501, step: 716, loss: 0.3079550266265869, acc: 0.8672, auc: 0.9426, precision: 0.9048, recall: 0.8382\n",
      "2019-01-07T19:59:40.759638, step: 717, loss: 0.3576829433441162, acc: 0.8516, auc: 0.9257, precision: 0.8615, recall: 0.8485\n",
      "2019-01-07T19:59:42.553875, step: 718, loss: 0.2993660569190979, acc: 0.8672, auc: 0.9489, precision: 0.875, recall: 0.8305\n",
      "2019-01-07T19:59:44.396309, step: 719, loss: 0.24919939041137695, acc: 0.8906, auc: 0.9672, precision: 0.8833, recall: 0.8833\n",
      "2019-01-07T19:59:46.198979, step: 720, loss: 0.2865646183490753, acc: 0.8594, auc: 0.9526, precision: 0.9344, recall: 0.8028\n",
      "2019-01-07T19:59:47.927239, step: 721, loss: 0.23073196411132812, acc: 0.9141, auc: 0.9696, precision: 0.9672, recall: 0.8676\n",
      "2019-01-07T19:59:49.722292, step: 722, loss: 0.2674451470375061, acc: 0.875, auc: 0.9598, precision: 0.9643, recall: 0.7941\n",
      "2019-01-07T19:59:51.473271, step: 723, loss: 0.29252246022224426, acc: 0.8672, auc: 0.948, precision: 0.8852, recall: 0.8438\n",
      "2019-01-07T19:59:53.213378, step: 724, loss: 0.47085410356521606, acc: 0.8047, auc: 0.8739, precision: 0.8571, recall: 0.7714\n",
      "2019-01-07T19:59:54.981924, step: 725, loss: 0.3563910722732544, acc: 0.8359, auc: 0.9219, precision: 0.8769, recall: 0.8143\n",
      "2019-01-07T19:59:56.762124, step: 726, loss: 0.2953937351703644, acc: 0.8906, auc: 0.9483, precision: 0.92, recall: 0.8961\n",
      "2019-01-07T19:59:58.520374, step: 727, loss: 0.25831833481788635, acc: 0.8828, auc: 0.9684, precision: 0.8841, recall: 0.8971\n",
      "2019-01-07T20:00:00.237789, step: 728, loss: 0.27317118644714355, acc: 0.9062, auc: 0.9644, precision: 0.9062, recall: 0.9062\n",
      "2019-01-07T20:00:01.968637, step: 729, loss: 0.23792892694473267, acc: 0.9062, auc: 0.9694, precision: 0.9231, recall: 0.8955\n",
      "2019-01-07T20:00:03.792959, step: 730, loss: 0.31934723258018494, acc: 0.8438, auc: 0.9341, precision: 0.8548, recall: 0.8281\n",
      "2019-01-07T20:00:05.575196, step: 731, loss: 0.41939860582351685, acc: 0.8047, auc: 0.9097, precision: 0.931, recall: 0.72\n",
      "2019-01-07T20:00:07.308124, step: 732, loss: 0.3079357147216797, acc: 0.8984, auc: 0.9407, precision: 0.9362, recall: 0.8148\n",
      "2019-01-07T20:00:09.040832, step: 733, loss: 0.3635651767253876, acc: 0.8516, auc: 0.9219, precision: 0.8704, recall: 0.7966\n",
      "2019-01-07T20:00:10.811901, step: 734, loss: 0.31562548875808716, acc: 0.8672, auc: 0.939, precision: 0.918, recall: 0.8235\n",
      "2019-01-07T20:00:12.516483, step: 735, loss: 0.3272448182106018, acc: 0.8594, auc: 0.9333, precision: 0.9273, recall: 0.7846\n",
      "2019-01-07T20:00:14.277299, step: 736, loss: 0.3181647062301636, acc: 0.8672, auc: 0.9449, precision: 0.9167, recall: 0.8209\n",
      "2019-01-07T20:00:15.976823, step: 737, loss: 0.3305193781852722, acc: 0.8125, auc: 0.9302, precision: 0.8421, recall: 0.7619\n",
      "2019-01-07T20:00:17.775516, step: 738, loss: 0.373948872089386, acc: 0.8359, auc: 0.9155, precision: 0.8936, recall: 0.7241\n",
      "2019-01-07T20:00:19.551736, step: 739, loss: 0.3031482994556427, acc: 0.8672, auc: 0.9426, precision: 0.875, recall: 0.7925\n",
      "2019-01-07T20:00:21.365523, step: 740, loss: 0.35965654253959656, acc: 0.8438, auc: 0.9265, precision: 0.9231, recall: 0.75\n",
      "2019-01-07T20:00:23.183795, step: 741, loss: 0.3615317940711975, acc: 0.8203, auc: 0.9271, precision: 0.9455, recall: 0.7222\n",
      "2019-01-07T20:00:24.983879, step: 742, loss: 0.2758210599422455, acc: 0.8438, auc: 0.9532, precision: 0.8846, recall: 0.7667\n",
      "2019-01-07T20:00:26.763952, step: 743, loss: 0.29528385400772095, acc: 0.8828, auc: 0.9567, precision: 0.873, recall: 0.8871\n",
      "2019-01-07T20:00:28.476612, step: 744, loss: 0.2506040930747986, acc: 0.8984, auc: 0.9644, precision: 0.8667, recall: 0.9123\n",
      "2019-01-07T20:00:30.172783, step: 745, loss: 0.4042443037033081, acc: 0.8125, auc: 0.8969, precision: 0.7925, recall: 0.7636\n",
      "2019-01-07T20:00:31.920327, step: 746, loss: 0.3600251376628876, acc: 0.7969, auc: 0.9237, precision: 0.902, recall: 0.6866\n",
      "2019-01-07T20:00:33.696945, step: 747, loss: 0.30390119552612305, acc: 0.8594, auc: 0.942, precision: 0.88, recall: 0.7857\n",
      "2019-01-07T20:00:35.584856, step: 748, loss: 0.3085215985774994, acc: 0.8359, auc: 0.947, precision: 0.92, recall: 0.7302\n",
      "2019-01-07T20:00:37.433140, step: 749, loss: 0.25256437063217163, acc: 0.8594, auc: 0.9724, precision: 0.9286, recall: 0.7879\n",
      "2019-01-07T20:00:39.263921, step: 750, loss: 0.31920307874679565, acc: 0.8828, auc: 0.9414, precision: 0.8814, recall: 0.8667\n",
      "2019-01-07T20:00:41.105557, step: 751, loss: 0.25443369150161743, acc: 0.9141, auc: 0.9704, precision: 0.9048, recall: 0.9194\n",
      "2019-01-07T20:00:42.988336, step: 752, loss: 0.3804881274700165, acc: 0.8516, auc: 0.9271, precision: 0.8197, recall: 0.8621\n",
      "2019-01-07T20:00:44.801426, step: 753, loss: 0.3972818851470947, acc: 0.8359, auc: 0.9226, precision: 0.9444, recall: 0.7391\n",
      "2019-01-07T20:00:46.605148, step: 754, loss: 0.2811169922351837, acc: 0.875, auc: 0.9507, precision: 0.9273, recall: 0.8095\n",
      "2019-01-07T20:00:48.345510, step: 755, loss: 0.3312176465988159, acc: 0.8438, auc: 0.9348, precision: 0.875, recall: 0.75\n",
      "2019-01-07T20:00:50.164806, step: 756, loss: 0.34444206953048706, acc: 0.8125, auc: 0.9277, precision: 0.8654, recall: 0.7258\n",
      "2019-01-07T20:00:51.964989, step: 757, loss: 0.30489543080329895, acc: 0.8828, auc: 0.9422, precision: 0.8846, recall: 0.8364\n",
      "2019-01-07T20:00:53.750895, step: 758, loss: 0.3261054456233978, acc: 0.8281, auc: 0.9339, precision: 0.8983, recall: 0.7681\n",
      "2019-01-07T20:00:55.501419, step: 759, loss: 0.3762837052345276, acc: 0.7891, auc: 0.9073, precision: 0.8833, recall: 0.726\n",
      "2019-01-07T20:00:57.349369, step: 760, loss: 0.34332168102264404, acc: 0.8516, auc: 0.9279, precision: 0.8852, recall: 0.8182\n",
      "2019-01-07T20:00:59.210228, step: 761, loss: 0.3047668933868408, acc: 0.8906, auc: 0.9404, precision: 0.9242, recall: 0.8714\n",
      "2019-01-07T20:01:01.069218, step: 762, loss: 0.3402526378631592, acc: 0.8672, auc: 0.9492, precision: 0.8065, recall: 0.9091\n",
      "2019-01-07T20:01:02.848388, step: 763, loss: 0.34397467970848083, acc: 0.8359, auc: 0.9257, precision: 0.8852, recall: 0.7941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-07T20:01:04.656148, step: 764, loss: 0.25104930996894836, acc: 0.8984, auc: 0.9607, precision: 0.9815, recall: 0.8154\n",
      "2019-01-07T20:01:06.454144, step: 765, loss: 0.4526858627796173, acc: 0.7969, auc: 0.8848, precision: 0.8958, recall: 0.6719\n",
      "2019-01-07T20:01:08.289542, step: 766, loss: 0.2533494234085083, acc: 0.9062, auc: 0.9619, precision: 0.931, recall: 0.871\n",
      "2019-01-07T20:01:10.059985, step: 767, loss: 0.293709397315979, acc: 0.8672, auc: 0.9464, precision: 0.9074, recall: 0.8033\n",
      "2019-01-07T20:01:11.860685, step: 768, loss: 0.33804434537887573, acc: 0.8203, auc: 0.936, precision: 0.9038, recall: 0.7231\n",
      "2019-01-07T20:01:13.636679, step: 769, loss: 0.2573462724685669, acc: 0.8984, auc: 0.9624, precision: 0.9298, recall: 0.8548\n",
      "2019-01-07T20:01:15.469838, step: 770, loss: 0.23690593242645264, acc: 0.9297, auc: 0.9673, precision: 0.9565, recall: 0.9167\n",
      "2019-01-07T20:01:17.300234, step: 771, loss: 0.31719231605529785, acc: 0.8984, auc: 0.9533, precision: 0.8594, recall: 0.9322\n",
      "2019-01-07T20:01:19.136887, step: 772, loss: 0.36109092831611633, acc: 0.8516, auc: 0.9209, precision: 0.9091, recall: 0.7812\n",
      "2019-01-07T20:01:20.993542, step: 773, loss: 0.3071313500404358, acc: 0.8594, auc: 0.9409, precision: 0.9245, recall: 0.7778\n",
      "2019-01-07T20:01:22.746455, step: 774, loss: 0.3230912983417511, acc: 0.8594, auc: 0.9355, precision: 0.9423, recall: 0.7656\n",
      "2019-01-07T20:01:24.584561, step: 775, loss: 0.4171607494354248, acc: 0.7969, auc: 0.9014, precision: 0.8852, recall: 0.7397\n",
      "2019-01-07T20:01:26.378570, step: 776, loss: 0.2660555839538574, acc: 0.8438, auc: 0.9479, precision: 0.8806, recall: 0.831\n",
      "2019-01-07T20:01:28.169202, step: 777, loss: 0.30341649055480957, acc: 0.8594, auc: 0.9465, precision: 0.8983, recall: 0.8154\n",
      "2019-01-07T20:01:30.034532, step: 778, loss: 0.4186580777168274, acc: 0.8359, auc: 0.9175, precision: 0.7937, recall: 0.8621\n",
      "2019-01-07T20:01:31.816727, step: 779, loss: 0.34049883484840393, acc: 0.8203, auc: 0.9265, precision: 0.8814, recall: 0.7647\n",
      "2019-01-07T20:01:33.545173, step: 780, loss: 0.28610360622406006, acc: 0.8984, auc: 0.9541, precision: 0.9077, recall: 0.8939\n",
      "start training model\n",
      "2019-01-07T20:01:35.685011, step: 781, loss: 0.34632444381713867, acc: 0.8203, auc: 0.9293, precision: 0.907, recall: 0.6724\n",
      "2019-01-07T20:01:37.523050, step: 782, loss: 0.2940831184387207, acc: 0.8359, auc: 0.948, precision: 0.9535, recall: 0.6833\n",
      "2019-01-07T20:01:39.297235, step: 783, loss: 0.29049649834632874, acc: 0.8672, auc: 0.9619, precision: 0.9828, recall: 0.7808\n",
      "2019-01-07T20:01:41.180861, step: 784, loss: 0.28246113657951355, acc: 0.8516, auc: 0.9506, precision: 0.8909, recall: 0.7903\n",
      "2019-01-07T20:01:43.069557, step: 785, loss: 0.31576913595199585, acc: 0.8516, auc: 0.931, precision: 0.8696, recall: 0.8571\n",
      "2019-01-07T20:01:44.872921, step: 786, loss: 0.2838350534439087, acc: 0.9062, auc: 0.9593, precision: 0.8929, recall: 0.8929\n",
      "2019-01-07T20:01:46.764696, step: 787, loss: 0.3584130108356476, acc: 0.8281, auc: 0.9234, precision: 0.8103, recall: 0.8103\n",
      "2019-01-07T20:01:48.524288, step: 788, loss: 0.2564801573753357, acc: 0.875, auc: 0.9584, precision: 0.9149, recall: 0.7818\n",
      "2019-01-07T20:01:50.301082, step: 789, loss: 0.24283693730831146, acc: 0.9062, auc: 0.9622, precision: 0.9254, recall: 0.8986\n",
      "2019-01-07T20:01:52.121536, step: 790, loss: 0.4728010892868042, acc: 0.8047, auc: 0.8785, precision: 0.878, recall: 0.6429\n",
      "2019-01-07T20:01:53.900671, step: 791, loss: 0.22342324256896973, acc: 0.9219, auc: 0.9726, precision: 0.9388, recall: 0.8679\n",
      "2019-01-07T20:01:55.687994, step: 792, loss: 0.287632554769516, acc: 0.9141, auc: 0.9424, precision: 0.9643, recall: 0.8571\n",
      "2019-01-07T20:01:57.441998, step: 793, loss: 0.35135892033576965, acc: 0.8594, auc: 0.9265, precision: 0.8507, recall: 0.8769\n",
      "2019-01-07T20:01:59.268664, step: 794, loss: 0.2556915879249573, acc: 0.8828, auc: 0.9631, precision: 0.9455, recall: 0.8125\n",
      "2019-01-07T20:02:01.040094, step: 795, loss: 0.22204971313476562, acc: 0.9219, auc: 0.975, precision: 0.9048, recall: 0.9344\n",
      "2019-01-07T20:02:02.751650, step: 796, loss: 0.3021549880504608, acc: 0.8594, auc: 0.9488, precision: 0.9483, recall: 0.7857\n",
      "2019-01-07T20:02:04.512484, step: 797, loss: 0.1969534307718277, acc: 0.9219, auc: 0.9801, precision: 0.9683, recall: 0.8841\n",
      "2019-01-07T20:02:06.261094, step: 798, loss: 0.28217825293540955, acc: 0.875, auc: 0.9544, precision: 0.875, recall: 0.9\n",
      "2019-01-07T20:02:08.104773, step: 799, loss: 0.32644590735435486, acc: 0.8828, auc: 0.9382, precision: 0.9273, recall: 0.8226\n",
      "2019-01-07T20:02:09.893581, step: 800, loss: 0.38692814111709595, acc: 0.8047, auc: 0.9106, precision: 0.8519, recall: 0.7302\n",
      "\n",
      "Evaluation:\n",
      "2019-01-07T20:03:21.706559, step: 800, loss: 0.3667191729331628, acc: 0.8335435897435898, auc: 0.9312512820512823, precision: 0.9082923076923078, recall: 0.7443923076923077\n",
      "2019-01-07T20:03:23.524523, step: 801, loss: 0.329134076833725, acc: 0.8672, auc: 0.9416, precision: 0.9423, recall: 0.7778\n",
      "2019-01-07T20:03:25.340386, step: 802, loss: 0.269525945186615, acc: 0.8828, auc: 0.9603, precision: 0.9556, recall: 0.7679\n",
      "2019-01-07T20:03:27.133563, step: 803, loss: 0.2518887221813202, acc: 0.9062, auc: 0.9601, precision: 0.9259, recall: 0.8621\n",
      "2019-01-07T20:03:28.951743, step: 804, loss: 0.39210548996925354, acc: 0.7969, auc: 0.9031, precision: 0.8475, recall: 0.7463\n",
      "2019-01-07T20:03:30.768177, step: 805, loss: 0.29502224922180176, acc: 0.8516, auc: 0.9475, precision: 0.8889, recall: 0.8235\n",
      "2019-01-07T20:03:32.683909, step: 806, loss: 0.3010607361793518, acc: 0.8516, auc: 0.9433, precision: 0.8788, recall: 0.8406\n",
      "2019-01-07T20:03:34.451508, step: 807, loss: 0.3115675747394562, acc: 0.8672, auc: 0.9442, precision: 0.8788, recall: 0.8657\n",
      "2019-01-07T20:03:36.216697, step: 808, loss: 0.24603159725666046, acc: 0.8984, auc: 0.9675, precision: 0.9455, recall: 0.8387\n",
      "2019-01-07T20:03:38.076955, step: 809, loss: 0.21925947070121765, acc: 0.8984, auc: 0.9678, precision: 0.9464, recall: 0.8413\n",
      "2019-01-07T20:03:39.920681, step: 810, loss: 0.25260570645332336, acc: 0.9141, auc: 0.9631, precision: 1.0, recall: 0.8429\n",
      "2019-01-07T20:03:41.821951, step: 811, loss: 0.2774224281311035, acc: 0.8516, auc: 0.9541, precision: 0.8772, recall: 0.8065\n",
      "2019-01-07T20:03:43.648902, step: 812, loss: 0.21680068969726562, acc: 0.9062, auc: 0.9735, precision: 0.9524, recall: 0.8696\n",
      "2019-01-07T20:03:45.456978, step: 813, loss: 0.2510661780834198, acc: 0.8984, auc: 0.9631, precision: 0.9455, recall: 0.8387\n",
      "2019-01-07T20:03:47.340232, step: 814, loss: 0.3228929042816162, acc: 0.8359, auc: 0.9355, precision: 0.8615, recall: 0.8235\n",
      "2019-01-07T20:03:49.892912, step: 815, loss: 0.2571796476840973, acc: 0.8828, auc: 0.9612, precision: 0.9455, recall: 0.8125\n",
      "2019-01-07T20:03:52.361734, step: 816, loss: 0.26736682653427124, acc: 0.8594, auc: 0.961, precision: 0.9344, recall: 0.8028\n",
      "2019-01-07T20:03:54.279427, step: 817, loss: 0.36178046464920044, acc: 0.7969, auc: 0.9264, precision: 0.8727, recall: 0.7164\n",
      "2019-01-07T20:03:56.228594, step: 818, loss: 0.2418498396873474, acc: 0.9219, auc: 0.9679, precision: 0.9394, recall: 0.9118\n",
      "2019-01-07T20:03:58.106912, step: 819, loss: 0.387681782245636, acc: 0.8438, auc: 0.9258, precision: 0.7797, recall: 0.8679\n",
      "2019-01-07T20:03:59.932072, step: 820, loss: 0.29189491271972656, acc: 0.8516, auc: 0.9504, precision: 0.9206, recall: 0.8056\n",
      "2019-01-07T20:04:01.867781, step: 821, loss: 0.19373029470443726, acc: 0.9219, auc: 0.9807, precision: 0.9474, recall: 0.8852\n",
      "2019-01-07T20:04:03.696746, step: 822, loss: 0.4487631320953369, acc: 0.8125, auc: 0.8964, precision: 0.8889, recall: 0.7273\n",
      "2019-01-07T20:04:05.578396, step: 823, loss: 0.20654134452342987, acc: 0.9062, auc: 0.9768, precision: 0.918, recall: 0.8889\n",
      "2019-01-07T20:04:07.495237, step: 824, loss: 0.2700728178024292, acc: 0.8906, auc: 0.9601, precision: 0.9492, recall: 0.8358\n",
      "2019-01-07T20:04:09.349312, step: 825, loss: 0.28243038058280945, acc: 0.8594, auc: 0.9537, precision: 0.9118, recall: 0.8378\n",
      "2019-01-07T20:04:11.116690, step: 826, loss: 0.31181126832962036, acc: 0.8516, auc: 0.9397, precision: 0.871, recall: 0.8308\n",
      "2019-01-07T20:04:12.980636, step: 827, loss: 0.272209495306015, acc: 0.8828, auc: 0.9549, precision: 0.9216, recall: 0.8103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-07T20:04:14.866533, step: 828, loss: 0.2554095685482025, acc: 0.8984, auc: 0.9618, precision: 0.9388, recall: 0.8214\n",
      "2019-01-07T20:04:16.715288, step: 829, loss: 0.4225353002548218, acc: 0.8281, auc: 0.9094, precision: 0.7925, recall: 0.7925\n",
      "2019-01-07T20:04:18.520263, step: 830, loss: 0.24015021324157715, acc: 0.9141, auc: 0.9652, precision: 0.9455, recall: 0.8667\n",
      "2019-01-07T20:04:20.428878, step: 831, loss: 0.31889042258262634, acc: 0.8203, auc: 0.9427, precision: 0.8974, recall: 0.6481\n",
      "2019-01-07T20:04:22.300191, step: 832, loss: 0.29529890418052673, acc: 0.8516, auc: 0.9519, precision: 0.9123, recall: 0.7879\n",
      "2019-01-07T20:04:24.189859, step: 833, loss: 0.33030831813812256, acc: 0.8516, auc: 0.9404, precision: 0.9388, recall: 0.7419\n",
      "2019-01-07T20:04:25.992406, step: 834, loss: 0.3482765853404999, acc: 0.8516, auc: 0.929, precision: 0.931, recall: 0.7826\n",
      "2019-01-07T20:04:27.825663, step: 835, loss: 0.31254878640174866, acc: 0.8906, auc: 0.9576, precision: 0.8281, recall: 0.9464\n",
      "2019-01-07T20:04:29.637309, step: 836, loss: 0.2141105979681015, acc: 0.9141, auc: 0.9758, precision: 0.9355, recall: 0.8923\n",
      "2019-01-07T20:04:31.592864, step: 837, loss: 0.4554786682128906, acc: 0.7812, auc: 0.885, precision: 0.7887, recall: 0.8116\n",
      "2019-01-07T20:04:33.812977, step: 838, loss: 0.3021221160888672, acc: 0.8516, auc: 0.9496, precision: 0.9483, recall: 0.7746\n",
      "2019-01-07T20:04:35.743020, step: 839, loss: 0.2939472198486328, acc: 0.8672, auc: 0.9472, precision: 0.9016, recall: 0.8333\n",
      "2019-01-07T20:04:37.632215, step: 840, loss: 0.2540023624897003, acc: 0.8672, auc: 0.9592, precision: 0.913, recall: 0.8514\n",
      "2019-01-07T20:04:39.489991, step: 841, loss: 0.3786405622959137, acc: 0.8516, auc: 0.9167, precision: 0.9365, recall: 0.7973\n",
      "2019-01-07T20:04:41.320840, step: 842, loss: 0.39042627811431885, acc: 0.8594, auc: 0.9116, precision: 0.8923, recall: 0.8406\n",
      "2019-01-07T20:04:43.225170, step: 843, loss: 0.31907185912132263, acc: 0.8516, auc: 0.9407, precision: 0.8333, recall: 0.873\n",
      "2019-01-07T20:04:45.096338, step: 844, loss: 0.27093732357025146, acc: 0.8984, auc: 0.9643, precision: 0.9014, recall: 0.9143\n",
      "2019-01-07T20:04:47.083587, step: 845, loss: 0.25387346744537354, acc: 0.8828, auc: 0.9678, precision: 0.931, recall: 0.8308\n",
      "2019-01-07T20:04:48.957684, step: 846, loss: 0.2647315561771393, acc: 0.8828, auc: 0.9611, precision: 0.8983, recall: 0.8548\n",
      "2019-01-07T20:04:50.884327, step: 847, loss: 0.3406175971031189, acc: 0.7969, auc: 0.945, precision: 0.9348, recall: 0.6515\n",
      "2019-01-07T20:04:52.795003, step: 848, loss: 0.33618587255477905, acc: 0.8438, auc: 0.9272, precision: 0.9167, recall: 0.7333\n",
      "2019-01-07T20:04:54.721514, step: 849, loss: 0.2869136333465576, acc: 0.8516, auc: 0.9496, precision: 0.9038, recall: 0.7705\n",
      "2019-01-07T20:04:56.600744, step: 850, loss: 0.2774319648742676, acc: 0.875, auc: 0.954, precision: 0.8947, recall: 0.8361\n",
      "2019-01-07T20:04:58.497537, step: 851, loss: 0.31223264336586, acc: 0.8828, auc: 0.9414, precision: 0.9016, recall: 0.8594\n",
      "2019-01-07T20:05:00.373520, step: 852, loss: 0.26724886894226074, acc: 0.8672, auc: 0.9574, precision: 0.8788, recall: 0.8657\n",
      "2019-01-07T20:05:02.272379, step: 853, loss: 0.2599504292011261, acc: 0.8828, auc: 0.961, precision: 0.9206, recall: 0.8529\n",
      "2019-01-07T20:05:04.212143, step: 854, loss: 0.29050230979919434, acc: 0.8594, auc: 0.9468, precision: 0.8704, recall: 0.8103\n",
      "2019-01-07T20:05:06.065293, step: 855, loss: 0.23187430202960968, acc: 0.9141, auc: 0.9665, precision: 0.9464, recall: 0.8689\n",
      "2019-01-07T20:05:07.940014, step: 856, loss: 0.30939462780952454, acc: 0.8281, auc: 0.9449, precision: 0.9167, recall: 0.7639\n",
      "2019-01-07T20:05:09.834143, step: 857, loss: 0.3227316439151764, acc: 0.8516, auc: 0.9373, precision: 0.902, recall: 0.7667\n",
      "2019-01-07T20:05:11.734229, step: 858, loss: 0.2599208354949951, acc: 0.8984, auc: 0.9629, precision: 0.9048, recall: 0.8906\n",
      "2019-01-07T20:05:13.644479, step: 859, loss: 0.2768037021160126, acc: 0.8516, auc: 0.9545, precision: 0.9062, recall: 0.8169\n",
      "2019-01-07T20:05:15.428670, step: 860, loss: 0.36605194211006165, acc: 0.8203, auc: 0.925, precision: 0.8136, recall: 0.8\n",
      "2019-01-07T20:05:17.315346, step: 861, loss: 0.270843505859375, acc: 0.8906, auc: 0.961, precision: 0.8833, recall: 0.8833\n",
      "2019-01-07T20:05:19.156119, step: 862, loss: 0.25909924507141113, acc: 0.8906, auc: 0.9607, precision: 0.9245, recall: 0.8305\n",
      "2019-01-07T20:05:21.016733, step: 863, loss: 0.33618730306625366, acc: 0.8359, auc: 0.9307, precision: 0.913, recall: 0.7119\n",
      "2019-01-07T20:05:22.832180, step: 864, loss: 0.3083689212799072, acc: 0.8359, auc: 0.9473, precision: 0.9111, recall: 0.7069\n",
      "2019-01-07T20:05:24.630991, step: 865, loss: 0.2632691562175751, acc: 0.9219, auc: 0.9554, precision: 0.9412, recall: 0.8727\n",
      "2019-01-07T20:05:26.458188, step: 866, loss: 0.2155459225177765, acc: 0.9062, auc: 0.9735, precision: 0.9074, recall: 0.875\n",
      "2019-01-07T20:05:28.237989, step: 867, loss: 0.18510030210018158, acc: 0.9609, auc: 0.9841, precision: 0.9677, recall: 0.9524\n",
      "2019-01-07T20:05:30.100806, step: 868, loss: 0.34399086236953735, acc: 0.8281, auc: 0.9326, precision: 0.8167, recall: 0.8167\n",
      "2019-01-07T20:05:31.989051, step: 869, loss: 0.40960633754730225, acc: 0.7812, auc: 0.9013, precision: 0.8043, recall: 0.6607\n",
      "2019-01-07T20:05:33.832542, step: 870, loss: 0.27051204442977905, acc: 0.875, auc: 0.9554, precision: 0.8889, recall: 0.7843\n",
      "2019-01-07T20:05:35.676157, step: 871, loss: 0.37010329961776733, acc: 0.8281, auc: 0.919, precision: 0.9, recall: 0.7714\n",
      "2019-01-07T20:05:37.551629, step: 872, loss: 0.42159777879714966, acc: 0.8125, auc: 0.9036, precision: 0.9302, recall: 0.6557\n",
      "2019-01-07T20:05:39.424135, step: 873, loss: 0.29462945461273193, acc: 0.8359, auc: 0.947, precision: 0.8889, recall: 0.7619\n",
      "2019-01-07T20:05:41.361068, step: 874, loss: 0.4563165307044983, acc: 0.8203, auc: 0.8836, precision: 0.8485, recall: 0.8116\n",
      "2019-01-07T20:05:43.317440, step: 875, loss: 0.3163607716560364, acc: 0.875, auc: 0.9377, precision: 0.8816, recall: 0.9054\n",
      "2019-01-07T20:05:45.164582, step: 876, loss: 0.33773601055145264, acc: 0.875, auc: 0.9487, precision: 0.8657, recall: 0.8923\n",
      "2019-01-07T20:05:46.973204, step: 877, loss: 0.2731163501739502, acc: 0.8984, auc: 0.9648, precision: 0.875, recall: 0.9403\n",
      "2019-01-07T20:05:48.789284, step: 878, loss: 0.24876591563224792, acc: 0.875, auc: 0.9623, precision: 0.9242, recall: 0.8472\n",
      "2019-01-07T20:05:50.689569, step: 879, loss: 0.3412919044494629, acc: 0.8281, auc: 0.9238, precision: 0.8727, recall: 0.7619\n",
      "2019-01-07T20:05:52.641105, step: 880, loss: 0.3380671739578247, acc: 0.8516, auc: 0.9395, precision: 0.9206, recall: 0.8056\n",
      "2019-01-07T20:05:54.545874, step: 881, loss: 0.35170018672943115, acc: 0.8359, auc: 0.9443, precision: 0.9773, recall: 0.6825\n",
      "2019-01-07T20:05:56.421646, step: 882, loss: 0.3058260679244995, acc: 0.8672, auc: 0.9433, precision: 0.9565, recall: 0.7458\n",
      "2019-01-07T20:05:58.344621, step: 883, loss: 0.2613380551338196, acc: 0.8906, auc: 0.9612, precision: 0.918, recall: 0.8615\n",
      "2019-01-07T20:06:00.271622, step: 884, loss: 0.3240180015563965, acc: 0.8516, auc: 0.9344, precision: 0.8871, recall: 0.8209\n",
      "2019-01-07T20:06:02.164721, step: 885, loss: 0.32453179359436035, acc: 0.8438, auc: 0.9441, precision: 0.8413, recall: 0.8413\n",
      "2019-01-07T20:06:04.033321, step: 886, loss: 0.2879886031150818, acc: 0.8906, auc: 0.958, precision: 0.8806, recall: 0.9077\n",
      "2019-01-07T20:06:05.968941, step: 887, loss: 0.2935791015625, acc: 0.8828, auc: 0.9507, precision: 0.8889, recall: 0.875\n",
      "2019-01-07T20:06:07.844372, step: 888, loss: 0.32098716497421265, acc: 0.8203, auc: 0.9335, precision: 0.8302, recall: 0.7586\n",
      "2019-01-07T20:06:09.741223, step: 889, loss: 0.3282471299171448, acc: 0.7891, auc: 0.9558, precision: 0.9302, recall: 0.625\n",
      "2019-01-07T20:06:11.602217, step: 890, loss: 0.2526812255382538, acc: 0.8594, auc: 0.9701, precision: 0.9778, recall: 0.7213\n",
      "2019-01-07T20:06:13.496745, step: 891, loss: 0.2445724904537201, acc: 0.8438, auc: 0.9765, precision: 0.98, recall: 0.7206\n",
      "2019-01-07T20:06:15.428784, step: 892, loss: 0.2204253077507019, acc: 0.8828, auc: 0.9751, precision: 0.9412, recall: 0.8533\n",
      "2019-01-07T20:06:17.355676, step: 893, loss: 0.29821205139160156, acc: 0.8594, auc: 0.9494, precision: 0.8525, recall: 0.8525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-07T20:06:19.252789, step: 894, loss: 0.34591928124427795, acc: 0.8906, auc: 0.9379, precision: 0.8395, recall: 0.9855\n",
      "2019-01-07T20:06:21.162809, step: 895, loss: 0.279893696308136, acc: 0.8828, auc: 0.9631, precision: 0.8667, recall: 0.9286\n",
      "2019-01-07T20:06:23.010579, step: 896, loss: 0.2198055535554886, acc: 0.9062, auc: 0.9749, precision: 0.9394, recall: 0.8857\n",
      "2019-01-07T20:06:24.872689, step: 897, loss: 0.28938210010528564, acc: 0.8672, auc: 0.9492, precision: 0.9016, recall: 0.8333\n",
      "2019-01-07T20:06:26.693091, step: 898, loss: 0.30613917112350464, acc: 0.8516, auc: 0.9461, precision: 0.8824, recall: 0.7759\n",
      "2019-01-07T20:06:28.564584, step: 899, loss: 0.33581751585006714, acc: 0.8672, auc: 0.9309, precision: 0.918, recall: 0.8235\n",
      "2019-01-07T20:06:30.431841, step: 900, loss: 0.2455681562423706, acc: 0.8906, auc: 0.9645, precision: 0.96, recall: 0.8\n",
      "\n",
      "Evaluation:\n",
      "2019-01-07T20:07:44.416160, step: 900, loss: 0.4159230880248241, acc: 0.8227153846153845, auc: 0.9323358974358974, precision: 0.9196923076923077, recall: 0.7131102564102562\n",
      "2019-01-07T20:07:46.339287, step: 901, loss: 0.2850514352321625, acc: 0.8516, auc: 0.9524, precision: 0.9423, recall: 0.7538\n",
      "2019-01-07T20:07:48.214930, step: 902, loss: 0.21976763010025024, acc: 0.8906, auc: 0.9739, precision: 0.9474, recall: 0.8308\n",
      "2019-01-07T20:07:50.125022, step: 903, loss: 0.3664034307003021, acc: 0.8281, auc: 0.9253, precision: 0.8864, recall: 0.6964\n",
      "2019-01-07T20:07:51.994511, step: 904, loss: 0.3590182662010193, acc: 0.8359, auc: 0.9227, precision: 0.8491, recall: 0.7759\n",
      "2019-01-07T20:07:54.013795, step: 905, loss: 0.2766806483268738, acc: 0.8906, auc: 0.9566, precision: 0.875, recall: 0.875\n",
      "2019-01-07T20:07:55.849567, step: 906, loss: 0.32810473442077637, acc: 0.875, auc: 0.9419, precision: 0.8451, recall: 0.9231\n",
      "2019-01-07T20:07:57.797222, step: 907, loss: 0.2122678905725479, acc: 0.9297, auc: 0.9757, precision: 0.9, recall: 0.9184\n",
      "2019-01-07T20:07:59.715695, step: 908, loss: 0.2814028263092041, acc: 0.875, auc: 0.9499, precision: 0.9016, recall: 0.8462\n",
      "2019-01-07T20:08:01.617025, step: 909, loss: 0.33940720558166504, acc: 0.8672, auc: 0.9328, precision: 0.9231, recall: 0.8333\n",
      "2019-01-07T20:08:03.489192, step: 910, loss: 0.29704833030700684, acc: 0.8828, auc: 0.9426, precision: 0.8958, recall: 0.8113\n",
      "2019-01-07T20:08:05.377238, step: 911, loss: 0.2826268970966339, acc: 0.8438, auc: 0.9599, precision: 0.96, recall: 0.7273\n",
      "2019-01-07T20:08:07.204843, step: 912, loss: 0.3527096211910248, acc: 0.8281, auc: 0.9361, precision: 0.9434, recall: 0.7246\n",
      "2019-01-07T20:08:09.037941, step: 913, loss: 0.4192829430103302, acc: 0.8281, auc: 0.9098, precision: 0.9423, recall: 0.7206\n",
      "2019-01-07T20:08:10.944381, step: 914, loss: 0.2565672993659973, acc: 0.8906, auc: 0.9628, precision: 0.9206, recall: 0.8657\n",
      "2019-01-07T20:08:12.814133, step: 915, loss: 0.2734422981739044, acc: 0.8984, auc: 0.9739, precision: 0.8732, recall: 0.9394\n",
      "2019-01-07T20:08:14.697338, step: 916, loss: 0.3895485997200012, acc: 0.8281, auc: 0.9239, precision: 0.8313, recall: 0.8961\n",
      "2019-01-07T20:08:16.599870, step: 917, loss: 0.37175124883651733, acc: 0.8672, auc: 0.9399, precision: 0.8261, recall: 0.9194\n",
      "2019-01-07T20:08:18.440900, step: 918, loss: 0.2791972756385803, acc: 0.8516, auc: 0.9509, precision: 0.9206, recall: 0.8056\n",
      "2019-01-07T20:08:20.312532, step: 919, loss: 0.2907443344593048, acc: 0.8672, auc: 0.9479, precision: 0.9318, recall: 0.7455\n",
      "2019-01-07T20:08:22.205751, step: 920, loss: 0.3976020812988281, acc: 0.7891, auc: 0.926, precision: 0.913, recall: 0.6462\n",
      "2019-01-07T20:08:24.039910, step: 921, loss: 0.44748833775520325, acc: 0.7734, auc: 0.8973, precision: 0.9143, recall: 0.5517\n",
      "2019-01-07T20:08:25.945287, step: 922, loss: 0.28108537197113037, acc: 0.8203, auc: 0.9587, precision: 0.9333, recall: 0.6774\n",
      "2019-01-07T20:08:27.871834, step: 923, loss: 0.31680023670196533, acc: 0.8203, auc: 0.9401, precision: 0.8929, recall: 0.7463\n",
      "2019-01-07T20:08:29.756482, step: 924, loss: 0.3505117893218994, acc: 0.8438, auc: 0.9358, precision: 0.8448, recall: 0.8167\n",
      "2019-01-07T20:08:31.601006, step: 925, loss: 0.29228976368904114, acc: 0.8906, auc: 0.9621, precision: 0.8615, recall: 0.918\n",
      "2019-01-07T20:08:33.435868, step: 926, loss: 0.3433430790901184, acc: 0.8828, auc: 0.9499, precision: 0.8429, recall: 0.9365\n",
      "2019-01-07T20:08:35.308942, step: 927, loss: 0.3050346076488495, acc: 0.8828, auc: 0.9501, precision: 0.8806, recall: 0.8939\n",
      "2019-01-07T20:08:37.236404, step: 928, loss: 0.3578742742538452, acc: 0.8047, auc: 0.9186, precision: 0.8727, recall: 0.7273\n",
      "2019-01-07T20:08:39.233136, step: 929, loss: 0.29396235942840576, acc: 0.8438, auc: 0.9569, precision: 0.9464, recall: 0.7571\n",
      "2019-01-07T20:08:41.120060, step: 930, loss: 0.3369014263153076, acc: 0.8125, auc: 0.9502, precision: 0.9423, recall: 0.7\n",
      "2019-01-07T20:08:43.100731, step: 931, loss: 0.31311386823654175, acc: 0.8203, auc: 0.9471, precision: 0.9474, recall: 0.6316\n",
      "2019-01-07T20:08:44.973891, step: 932, loss: 0.2252720594406128, acc: 0.9219, auc: 0.9765, precision: 0.9254, recall: 0.9254\n",
      "2019-01-07T20:08:46.857215, step: 933, loss: 0.28564006090164185, acc: 0.875, auc: 0.9552, precision: 0.9048, recall: 0.8507\n",
      "2019-01-07T20:08:48.837237, step: 934, loss: 0.32016679644584656, acc: 0.8672, auc: 0.9598, precision: 0.7931, recall: 0.902\n",
      "2019-01-07T20:08:50.785114, step: 935, loss: 0.281044065952301, acc: 0.8516, auc: 0.9544, precision: 0.9344, recall: 0.7917\n",
      "2019-01-07T20:08:52.773814, step: 936, loss: 0.260397732257843, acc: 0.9141, auc: 0.9674, precision: 0.9423, recall: 0.8596\n",
      "start training model\n",
      "2019-01-07T20:08:54.739790, step: 937, loss: 0.24325202405452728, acc: 0.8828, auc: 0.97, precision: 0.931, recall: 0.8308\n",
      "2019-01-07T20:08:56.608511, step: 938, loss: 0.28510573506355286, acc: 0.8359, auc: 0.9458, precision: 0.8889, recall: 0.7619\n",
      "2019-01-07T20:08:58.447604, step: 939, loss: 0.3310757875442505, acc: 0.8594, auc: 0.933, precision: 0.9194, recall: 0.8143\n",
      "2019-01-07T20:09:00.335959, step: 940, loss: 0.34328895807266235, acc: 0.8203, auc: 0.9374, precision: 0.8929, recall: 0.7463\n",
      "2019-01-07T20:09:02.161300, step: 941, loss: 0.2826026678085327, acc: 0.875, auc: 0.9571, precision: 0.9, recall: 0.8036\n",
      "2019-01-07T20:09:04.041286, step: 942, loss: 0.2730494737625122, acc: 0.875, auc: 0.9514, precision: 0.9259, recall: 0.8065\n",
      "2019-01-07T20:09:05.893475, step: 943, loss: 0.26777249574661255, acc: 0.8906, auc: 0.9607, precision: 0.9298, recall: 0.8413\n",
      "2019-01-07T20:09:07.838426, step: 944, loss: 0.2622959613800049, acc: 0.8984, auc: 0.9635, precision: 0.9273, recall: 0.85\n",
      "2019-01-07T20:09:09.729990, step: 945, loss: 0.27812400460243225, acc: 0.8828, auc: 0.9565, precision: 0.9615, recall: 0.7937\n",
      "2019-01-07T20:09:11.676939, step: 946, loss: 0.2592781186103821, acc: 0.875, auc: 0.9604, precision: 0.963, recall: 0.7879\n",
      "2019-01-07T20:09:13.603686, step: 947, loss: 0.2682068347930908, acc: 0.8828, auc: 0.9572, precision: 0.9123, recall: 0.8387\n",
      "2019-01-07T20:09:15.501226, step: 948, loss: 0.23938962817192078, acc: 0.8828, auc: 0.9648, precision: 0.9057, recall: 0.8276\n",
      "2019-01-07T20:09:17.461152, step: 949, loss: 0.2999606132507324, acc: 0.875, auc: 0.9465, precision: 0.871, recall: 0.871\n",
      "2019-01-07T20:09:19.413567, step: 950, loss: 0.25978609919548035, acc: 0.8828, auc: 0.9582, precision: 0.9783, recall: 0.7627\n",
      "2019-01-07T20:09:21.337342, step: 951, loss: 0.32660675048828125, acc: 0.8438, auc: 0.9363, precision: 0.875, recall: 0.8235\n",
      "2019-01-07T20:09:23.261584, step: 952, loss: 0.25062382221221924, acc: 0.8984, auc: 0.9634, precision: 0.9677, recall: 0.8451\n",
      "2019-01-07T20:09:25.193072, step: 953, loss: 0.26094290614128113, acc: 0.9141, auc: 0.9572, precision: 0.8868, recall: 0.9038\n",
      "2019-01-07T20:09:27.108246, step: 954, loss: 0.22648346424102783, acc: 0.8984, auc: 0.9756, precision: 0.8889, recall: 0.9032\n",
      "2019-01-07T20:09:28.980833, step: 955, loss: 0.2634051442146301, acc: 0.8672, auc: 0.9582, precision: 0.9074, recall: 0.8033\n",
      "2019-01-07T20:09:30.820736, step: 956, loss: 0.3311987519264221, acc: 0.8516, auc: 0.936, precision: 0.9184, recall: 0.75\n",
      "2019-01-07T20:09:32.744734, step: 957, loss: 0.305022656917572, acc: 0.8594, auc: 0.9449, precision: 0.9153, recall: 0.806\n",
      "2019-01-07T20:09:34.703386, step: 958, loss: 0.2183215320110321, acc: 0.8828, auc: 0.9755, precision: 0.9333, recall: 0.8358\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-07T20:09:36.668534, step: 959, loss: 0.29198628664016724, acc: 0.8828, auc: 0.9442, precision: 0.9194, recall: 0.8507\n",
      "2019-01-07T20:09:38.576328, step: 960, loss: 0.3308532238006592, acc: 0.875, auc: 0.9374, precision: 0.8676, recall: 0.8939\n",
      "2019-01-07T20:09:40.461667, step: 961, loss: 0.3285728693008423, acc: 0.8516, auc: 0.934, precision: 0.9062, recall: 0.8169\n",
      "2019-01-07T20:09:42.372627, step: 962, loss: 0.22361217439174652, acc: 0.8828, auc: 0.9706, precision: 0.9333, recall: 0.8358\n",
      "2019-01-07T20:09:44.260062, step: 963, loss: 0.309948205947876, acc: 0.8984, auc: 0.9443, precision: 0.8889, recall: 0.9032\n",
      "2019-01-07T20:09:46.200639, step: 964, loss: 0.32875654101371765, acc: 0.8672, auc: 0.9362, precision: 0.8814, recall: 0.8387\n",
      "2019-01-07T20:09:48.148770, step: 965, loss: 0.30561962723731995, acc: 0.8906, auc: 0.9433, precision: 0.9322, recall: 0.8462\n",
      "2019-01-07T20:09:50.138989, step: 966, loss: 0.3292686939239502, acc: 0.8438, auc: 0.935, precision: 0.9412, recall: 0.7385\n",
      "2019-01-07T20:09:52.093236, step: 967, loss: 0.2860278785228729, acc: 0.8828, auc: 0.9479, precision: 0.9333, recall: 0.7778\n",
      "2019-01-07T20:09:54.029192, step: 968, loss: 0.30968809127807617, acc: 0.8359, auc: 0.9419, precision: 0.8841, recall: 0.8243\n",
      "2019-01-07T20:09:56.000975, step: 969, loss: 0.23602890968322754, acc: 0.9219, auc: 0.9768, precision: 0.9062, recall: 0.9355\n",
      "2019-01-07T20:09:57.952599, step: 970, loss: 0.28168725967407227, acc: 0.8438, auc: 0.9497, precision: 0.8571, recall: 0.7636\n",
      "2019-01-07T20:09:59.932525, step: 971, loss: 0.2562156319618225, acc: 0.9062, auc: 0.9601, precision: 0.9153, recall: 0.8852\n",
      "2019-01-07T20:10:01.909645, step: 972, loss: 0.2797505557537079, acc: 0.8672, auc: 0.96, precision: 0.9615, recall: 0.7692\n",
      "2019-01-07T20:10:03.844987, step: 973, loss: 0.25998654961586, acc: 0.8438, auc: 0.9638, precision: 0.9375, recall: 0.7258\n",
      "2019-01-07T20:10:05.708908, step: 974, loss: 0.256183922290802, acc: 0.8906, auc: 0.9627, precision: 0.9655, recall: 0.8235\n",
      "2019-01-07T20:10:07.588002, step: 975, loss: 0.2391422688961029, acc: 0.8906, auc: 0.9673, precision: 0.9231, recall: 0.8696\n",
      "2019-01-07T20:10:09.527760, step: 976, loss: 0.25966066122055054, acc: 0.8984, auc: 0.9592, precision: 0.9242, recall: 0.8841\n",
      "2019-01-07T20:10:11.533670, step: 977, loss: 0.2511146366596222, acc: 0.9141, auc: 0.9891, precision: 0.8358, recall: 1.0\n",
      "2019-01-07T20:10:13.456884, step: 978, loss: 0.27350181341171265, acc: 0.875, auc: 0.9536, precision: 0.9206, recall: 0.8406\n",
      "2019-01-07T20:10:15.400113, step: 979, loss: 0.2087269425392151, acc: 0.9062, auc: 0.975, precision: 0.9508, recall: 0.8657\n",
      "2019-01-07T20:10:17.396023, step: 980, loss: 0.17915616929531097, acc: 0.9297, auc: 0.9836, precision: 0.9403, recall: 0.9265\n",
      "2019-01-07T20:10:19.317309, step: 981, loss: 0.2712511420249939, acc: 0.875, auc: 0.9558, precision: 0.9032, recall: 0.8485\n",
      "2019-01-07T20:10:21.259642, step: 982, loss: 0.23151305317878723, acc: 0.8828, auc: 0.9723, precision: 0.9667, recall: 0.8169\n",
      "2019-01-07T20:10:23.240293, step: 983, loss: 0.22133754193782806, acc: 0.9219, auc: 0.9765, precision: 0.913, recall: 0.9403\n",
      "2019-01-07T20:10:25.213287, step: 984, loss: 0.37052470445632935, acc: 0.8203, auc: 0.9215, precision: 0.7857, recall: 0.8\n",
      "2019-01-07T20:10:27.168185, step: 985, loss: 0.22667403519153595, acc: 0.9219, auc: 0.9692, precision: 0.9375, recall: 0.9091\n",
      "2019-01-07T20:10:29.154791, step: 986, loss: 0.2319214642047882, acc: 0.8906, auc: 0.9722, precision: 0.8889, recall: 0.8889\n",
      "2019-01-07T20:10:31.112762, step: 987, loss: 0.23100917041301727, acc: 0.9062, auc: 0.9706, precision: 0.9661, recall: 0.8507\n",
      "2019-01-07T20:10:33.051948, step: 988, loss: 0.3410797119140625, acc: 0.8516, auc: 0.9367, precision: 0.8525, recall: 0.8387\n",
      "2019-01-07T20:10:34.976810, step: 989, loss: 0.27189379930496216, acc: 0.8594, auc: 0.9535, precision: 0.9153, recall: 0.806\n",
      "2019-01-07T20:10:36.993963, step: 990, loss: 0.210688978433609, acc: 0.8906, auc: 0.9784, precision: 0.9677, recall: 0.8333\n",
      "2019-01-07T20:10:38.959080, step: 991, loss: 0.27921706438064575, acc: 0.8828, auc: 0.952, precision: 0.8269, recall: 0.8776\n",
      "2019-01-07T20:10:40.844626, step: 992, loss: 0.30964159965515137, acc: 0.875, auc: 0.9437, precision: 0.8841, recall: 0.8841\n",
      "2019-01-07T20:10:42.800537, step: 993, loss: 0.26097819209098816, acc: 0.875, auc: 0.9581, precision: 0.9062, recall: 0.8529\n",
      "2019-01-07T20:10:44.767064, step: 994, loss: 0.3193213939666748, acc: 0.8594, auc: 0.9418, precision: 0.8929, recall: 0.8065\n",
      "2019-01-07T20:10:46.763858, step: 995, loss: 0.36797478795051575, acc: 0.8672, auc: 0.9313, precision: 0.9355, recall: 0.8169\n",
      "2019-01-07T20:10:48.700208, step: 996, loss: 0.345162034034729, acc: 0.8516, auc: 0.9274, precision: 0.8837, recall: 0.7308\n",
      "2019-01-07T20:10:50.640891, step: 997, loss: 0.2266678810119629, acc: 0.9297, auc: 0.9746, precision: 0.8833, recall: 0.9636\n",
      "2019-01-07T20:10:52.509183, step: 998, loss: 0.306683748960495, acc: 0.8672, auc: 0.9424, precision: 0.8615, recall: 0.875\n",
      "2019-01-07T20:10:54.464484, step: 999, loss: 0.2829016447067261, acc: 0.8438, auc: 0.9488, precision: 0.8654, recall: 0.7759\n",
      "2019-01-07T20:10:56.413071, step: 1000, loss: 0.31855741143226624, acc: 0.8359, auc: 0.9466, precision: 0.9111, recall: 0.7069\n",
      "\n",
      "Evaluation:\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "\n",
    "# 生成训练集和验证集\n",
    "trainReviews = data.trainReviews\n",
    "trainLabels = data.trainLabels\n",
    "evalReviews = data.evalReviews\n",
    "evalLabels = data.evalLabels\n",
    "\n",
    "# 定义计算图\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "\n",
    "    session_conf = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
    "    session_conf.gpu_options.allow_growth=True\n",
    "    session_conf.gpu_options.per_process_gpu_memory_fraction = 0.9  # 配置gpu占用率  \n",
    "\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    \n",
    "    # 定义会话\n",
    "    with sess.as_default():\n",
    "        cnn = BiLSTMAttention(config)\n",
    "        \n",
    "        # 实例化BiLM对象，这个必须放置在全局下，不能在elmo函数中定义，否则会出现重复生成tensorflow节点。\n",
    "        with tf.variable_scope(\"bilm\", reuse=True):\n",
    "            bilm = BidirectionalLanguageModel(\n",
    "                    config.optionFile,\n",
    "                    config.weightFile,\n",
    "                    use_character_inputs=False,\n",
    "                    embedding_weight_file=config.tokenEmbeddingFile\n",
    "                    )\n",
    "        inputData = tf.placeholder('int32', shape=(None, None))\n",
    "        \n",
    "        # 调用bilm中的__call__方法生成op对象\n",
    "        inputEmbeddingsOp = bilm(inputData) \n",
    "        \n",
    "        # 计算ELMo向量表示\n",
    "        elmoInput = weight_layers('input', inputEmbeddingsOp, l2_coef=0.0)\n",
    "        \n",
    "        globalStep = tf.Variable(0, name=\"globalStep\", trainable=False)\n",
    "        # 定义优化函数，传入学习速率参数\n",
    "        optimizer = tf.train.AdamOptimizer(config.training.learningRate)\n",
    "        # 计算梯度,得到梯度和变量\n",
    "        gradsAndVars = optimizer.compute_gradients(cnn.loss)\n",
    "        # 将梯度应用到变量下，生成训练器\n",
    "        trainOp = optimizer.apply_gradients(gradsAndVars, global_step=globalStep)\n",
    "        \n",
    "        # 用summary绘制tensorBoard\n",
    "        gradSummaries = []\n",
    "        for g, v in gradsAndVars:\n",
    "            if g is not None:\n",
    "                tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "        \n",
    "        outDir = os.path.abspath(os.path.join(os.path.curdir, \"summarys\"))\n",
    "        print(\"Writing to {}\\n\".format(outDir))\n",
    "        \n",
    "        lossSummary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "        summaryOp = tf.summary.merge_all()\n",
    "        \n",
    "        trainSummaryDir = os.path.join(outDir, \"train\")\n",
    "        trainSummaryWriter = tf.summary.FileWriter(trainSummaryDir, sess.graph)\n",
    "        \n",
    "        evalSummaryDir = os.path.join(outDir, \"eval\")\n",
    "        evalSummaryWriter = tf.summary.FileWriter(evalSummaryDir, sess.graph)\n",
    "        \n",
    "        \n",
    "        # 初始化所有变量\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=5)\n",
    "        \n",
    "        # 保存模型的一种方式，保存为pb文件\n",
    "#         builder = tf.saved_model.builder.SavedModelBuilder(\"../model/textCNN/savedModel\")\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "        def elmo(reviews):\n",
    "            \"\"\"\n",
    "            对每一个输入的batch都动态的生成词向量表示\n",
    "            \"\"\"\n",
    "\n",
    "#           tf.reset_default_graph()\n",
    "            # TokenBatcher是生成词表示的batch类\n",
    "            batcher = TokenBatcher(config.vocabFile)\n",
    "            with tf.Session() as sess:\n",
    "                sess.run(tf.global_variables_initializer())\n",
    "                \n",
    "                # 生成batch数据\n",
    "                inputDataIndex = batcher.batch_sentences(reviews)\n",
    "\n",
    "                # 计算ELMo的向量表示\n",
    "                elmoInputVec = sess.run(\n",
    "                    [elmoInput['weighted_op']],\n",
    "                    feed_dict={inputData: inputDataIndex}\n",
    "                )\n",
    "\n",
    "            return elmoInputVec\n",
    "\n",
    "        def trainStep(batchX, batchY):\n",
    "            \"\"\"\n",
    "            训练函数\n",
    "            \"\"\"   \n",
    "            \n",
    "            feed_dict = {\n",
    "              cnn.inputX: elmo(batchX)[0],  # inputX直接用动态生成的ELMo向量表示代入\n",
    "              cnn.inputY: np.array(batchY, dtype=\"float32\"),\n",
    "              cnn.dropoutKeepProb: config.model.dropoutKeepProb\n",
    "            }\n",
    "            _, summary, step, loss, predictions, binaryPreds = sess.run(\n",
    "                [trainOp, summaryOp, globalStep, cnn.loss, cnn.predictions, cnn.binaryPreds],\n",
    "                feed_dict)\n",
    "            timeStr = datetime.datetime.now().isoformat()\n",
    "            acc, auc, precision, recall = genMetrics(batchY, predictions, binaryPreds)\n",
    "            print(\"{}, step: {}, loss: {}, acc: {}, auc: {}, precision: {}, recall: {}\".format(timeStr, step, loss, acc, auc, precision, recall))\n",
    "            trainSummaryWriter.add_summary(summary, step)\n",
    "\n",
    "        def devStep(batchX, batchY):\n",
    "            \"\"\"\n",
    "            验证函数\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.inputX: elmo(batchX)[0],\n",
    "              cnn.inputY: np.array(batchY, dtype=\"float32\"),\n",
    "              cnn.dropoutKeepProb: 1.0\n",
    "            }\n",
    "            summary, step, loss, predictions, binaryPreds = sess.run(\n",
    "                [summaryOp, globalStep, cnn.loss, cnn.predictions, cnn.binaryPreds],\n",
    "                feed_dict)\n",
    "            \n",
    "            acc, auc, precision, recall = genMetrics(batchY, predictions, binaryPreds)\n",
    "            \n",
    "            evalSummaryWriter.add_summary(summary, step)\n",
    "            \n",
    "            return loss, acc, auc, precision, recall\n",
    "        \n",
    "        for i in range(config.training.epoches):\n",
    "            # 训练模型\n",
    "            print(\"start training model\")\n",
    "            for batchTrain in nextBatch(trainReviews, trainLabels, config.batchSize):\n",
    "                trainStep(batchTrain[0], batchTrain[1])\n",
    "\n",
    "                currentStep = tf.train.global_step(sess, globalStep) \n",
    "                if currentStep % config.training.evaluateEvery == 0:\n",
    "                    print(\"\\nEvaluation:\")\n",
    "                    \n",
    "                    losses = []\n",
    "                    accs = []\n",
    "                    aucs = []\n",
    "                    precisions = []\n",
    "                    recalls = []\n",
    "                    \n",
    "                    for batchEval in nextBatch(evalReviews, evalLabels, config.batchSize):\n",
    "                        loss, acc, auc, precision, recall = devStep(batchEval[0], batchEval[1])\n",
    "                        losses.append(loss)\n",
    "                        accs.append(acc)\n",
    "                        aucs.append(auc)\n",
    "                        precisions.append(precision)\n",
    "                        recalls.append(recall)\n",
    "                        \n",
    "                    time_str = datetime.datetime.now().isoformat()\n",
    "                    print(\"{}, step: {}, loss: {}, acc: {}, auc: {}, precision: {}, recall: {}\".format(time_str, currentStep, mean(losses), \n",
    "                                                                                                       mean(accs), mean(aucs), mean(precisions),\n",
    "                                                                                                       mean(recalls)))\n",
    "                    \n",
    "#                 if currentStep % config.training.checkpointEvery == 0:\n",
    "#                     # 保存模型的另一种方法，保存checkpoint文件\n",
    "#                     path = saver.save(sess, \"../model/textCNN/model/my-model\", global_step=currentStep)\n",
    "#                     print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "                    \n",
    "#         inputs = {\"inputX\": tf.saved_model.utils.build_tensor_info(cnn.inputX),\n",
    "#                   \"keepProb\": tf.saved_model.utils.build_tensor_info(cnn.dropoutKeepProb)}\n",
    "\n",
    "#         outputs = {\"binaryPreds\": tf.saved_model.utils.build_tensor_info(cnn.binaryPreds)}\n",
    "\n",
    "#         prediction_signature = tf.saved_model.signature_def_utils.build_signature_def(inputs=inputs, outputs=outputs,\n",
    "#                                                                                       method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME)\n",
    "#         legacy_init_op = tf.group(tf.tables_initializer(), name=\"legacy_init_op\")\n",
    "#         builder.add_meta_graph_and_variables(sess, [tf.saved_model.tag_constants.SERVING],\n",
    "#                                             signature_def_map={\"predict\": prediction_signature}, legacy_init_op=legacy_init_op)\n",
    "\n",
    "#         builder.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
